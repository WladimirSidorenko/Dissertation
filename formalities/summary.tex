\documentclass[11pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Packages
\usepackage{amsmath}
\usepackage{breakcites}
\usepackage[authoryear]{natbib}
\usepackage{paralist}
\usepackage{titling}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Commands
\newcommand{\ienocomma}{\textit{i.e.}}
\newcommand{\ie}{\ienocomma,}
\newcommand{\eg}{\textit{e.g.},}
\newcommand{\F}[0]{$F_1$}
\newcommand{\markable}[1]{\texttt{#1}}
\newcommand{\attribute}[1]{\emph{\texttt{#1}}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\renewcommand{\cite}{\citep}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Lengths
\setlength{\droptitle}{-7em}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Variables
\author{Uladzimir Sidarenka}
\title{  {\large Summary of the Dissertation}\\[0.5em]
  {\Large ``Sentiment Analysis of German Twitter''}}
\date{\vspace{-3ex}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Document
\begin{document}
\maketitle

Online communication services have not only revolutionized our
interaction with other people (allowing us to stay in touch with
friends, exchange ideas with colleagues, and get latest news from
celebrities and politicians without leaving the desktop) but also
opened the doors to numerous new dangers, such as cyber-attacks,
online bullying, and deliberate manipulation of public
opinion---dangers that can hardly be averted manually due to the
immense amount of data exchanged on online platforms.  Therefore, if
we want to protect our society from these looming threats, we urgently
need more robust, higher-quality natural language processing (NLP)
applications, which can recognize such menaces automatically, by
analyzing uncensored texts.  Unfortunately, most NLP programs today
have been created for standard language, as we know it from
newspapers, or, in the best case, adapted to the specifics of English
social media.

This thesis reduces the existing deficit by entering the new frontier
of German online communication and addressing one of its most prolific
forms---users' conversations on Twitter.  In particular, it explores
the ways and means by how people express their opinions on this
service, examines current approaches to automatic mining of these
feelings, and proposes new sentiment analysis methods, which
outperform state-of-the-art techniques.

I perform these tasks on three different linguistic levels:
\begin{itemize}
  \item\emph{subsentential}, in which I try to predict the polarity of
    single words, and automatically determine the main components of
    an opinion (its target, holder, and the actual evaluative
    expression) within a sentence;

  \item\emph{sentential}, where I consider the whole microblog as a
    single statement and classify its overall semantic orientation;

  \item and, finally, \emph{suprasentential}, in which I try to
    improve on message-level sentiment classification by inferring the
    overall polarity of a tweet from the semantic orientation of its
    single sentences, taking into account their semantic and pragmatic
    links (discourse relations).
\end{itemize}
At each of these levels, I evaluate the most prominent classes of
existing systems, seek to outperform their scores with my own
solutions, and estimate the effect of different hyper-paramaters and
input properties (such as word embedding types, feature groups, and
text normalization) on the classification results.

\section{Research Questions}

By doing so, I hope to find answers to the following questions:
\begin{itemize}
\item Can we apply opinion mining methods devised for standard English
  to German Twitter?
\item Which groups of approaches are best suited for which sentiment
  analysis tasks?
\item How much do word- and discourse-level analyses affect
  message-level sentiment classification?
\item Does text normalization help analyze sentiments?
\item Can we do better than existing systems?
\end{itemize}

\section{Outline of this Work}

To get these answers, I proceed in the following way:

\subsection{Chapter~1: Introduction to Sentiment Analysis}

In Chapter~1, I give a brief introduction to the main goals and
definitions of sentiment analysis, and make a digression into the
history of this field, tracing its roots back to the ancient
philosophy, and following the course if its development through the
main milestones in psychology, narratology, and linguistics of the
20th century.  In the final sections, I elaborate on the recent
advances in automatic opinion mining of social media, and pay special
attention to the current state of the art in sentiment analysis of
Twitter.

\subsection{Chapter~2: Sentiment Corpus}

Afterwards, in Chapter~2, I introduce the Potsdam Twitter Sentiment
(PotTS) corpus---a collection of $7,992$ German microblogs pertaining
to the German federal elections, papal conclave, general political
discussions, and casual everyday conversations.

I obtained messages for the first three groups by tracking the public
Twitter API between March and September 2013, using extensive lists of
hand-picked keywords, which, in my opinion, best described these
topics.  As casual everyday posts, I considered microblogs from the
German Twitter Snapshot~\cite{Scheffler:14}, which comprises
$\approx97\%$ all German tweets (17M messages) posted in April 2013.

To ensure a representative number of subjective opinions in the
resulting corpus, I grouped all downloaded microblogs pertaining to
the same topic into three formal classes:
\begin{inparaenum}[(i)]
  \item tweets that contained a polar term from the sentiment lexicon
    SentiWS~\cite{Remus:10},
  \item messages that had an emoticon, and, finally,
  \item all remaining microblogs.
\end{inparaenum}
Afterwards, I sampled an equal number of tweets (666) from each of the
four topics, from each of these three formal categories.

In the next step, I defined an annotation scheme---a list of elements
that had to be annotated by human experts.  These were:
\begin{itemize}
  \item\emph{sentiment}s, which were specified as polar subjective
    evaluative opinions about people, entities, or events;

  \item\emph{target}s, which represented entities or events being
    evaluated;

  \item\emph{source}s, which denoted the immediate author(s) or
    holder(s) of an opinion;

  \item\emph{polar term}s, which were words or idioms that had a
    distinguishable evaluative lexical meaning;

  \item\emph{intensifier}s, which represented elements that increased
    the expressivity and subjective sense of polar terms;

  \item\emph{diminishers}s, which vice versa reduced the strength of a
    polar term;

  \item and, finally, \emph{negation}s, which completely reversed the
    semantic orientation of a polar item (\eg{} ``\emph{nicht} gut''
    [\emph{not} good] or ``\emph{kein} schlechtes Beispiel''
    [\emph{not} bad advice]).
\end{itemize}

The annotation process was performed in three steps: first, two
linguists labeled one half of the data after only minimal training;
then, I automatically highlighted their divergent analyses and asked
them to resolve these differences; finally, the annotators continued
with the analysis of the remaining files.

To estimate the inter-rater reliability, I introduce two modified
versions of the popular $\kappa$-metric~\cite{Cohen:60}---binary and
proportional kappa---which differ in the way how they treat multiple
annotations of the same tokens (\eg{} if one phrase was labeled as
\emph{sentiment} several times by one annotator) and how they deal
with partial matches of different annotations (\eg{} if two
\emph{sentiment}s labeled by different experts agree on some, but not
all of their tokens).  Using these measures, I estimate the
inter-annotator agreement (IAA) of the experts at different stages of
their work.  This study shows that, initially, both assistants could
hardly agree on the mere notion of targeted opinions, but their
disagreements could be resolved with the help of the adjudication
procedure, which was applied in step two.  Despite a small drop of the
IAA scores in the final stage, all $\kappa$-values still remained at
the level of at least moderate reliability.

Finally, I demonstrate that the initial selection criteria (topics and
formal classes) had a strong impact on the number and agreement of
annotated sentiments and polar terms, with tweets about federal
elections and messages without prefiltered topics being the most
prolific sources of these elements.

\subsection{Chapter~3: Sentiment Lexicons}

In Chapter~3, I turn to the first subsentential sentiment analysis
task, in which I try to predict the polarity (positive, negative, or
neutral) of single words in the text.

For this purpose, I first evaluate the existing manually-curated
German sentiment lexicons---SentiWS~\cite{Remus:10}, German Polarity
Clues~\cite{Waltinger:10}, and the Zurich Polarity
List~\cite{Clematide:10}---by directly comparing their entries with
annotated polar terms in the PotTS corpus.  This evaluation shows that
the last resource (Zurich Polarity List) outperforms the other two
competitors in terms of both macro- and micro-averaged \F{}-scores,
reaching 0.589 and 0.955 \F{} respectively.

Afterwards, I compare three major groups of automatic
lexicon-generation methods: dictionary-, corpus-, and
word-embedding--based ones.  The first of these groups induces
polarity lists from a manually annotated lexical taxonomy
\cite[\textsc{GermaNet}; ][]{Hamp:97}---a graph whose nodes represent
sets of synonyms (synsets), which are connected to each other by their
semantic relations (\eg{} meronymy-holonymy, hypernymy-hyponymy, or
antonymy).  Corpus-based algorithms, on the contrary, infer sentiment
lexicons from collocation statistics computed on raw, unlabeled data,
by taking a set of terms with known semantic orientation (seed terms)
and looking for other words that frequently co-occur with these seeds
in text.  In the same vein, word-embedding--based methods generate
lists of polar terms by looking for words whose vector representations
(which are automatically learned by a classifier that predicts the
occurrence of other words in the nearby context) are most similar to
the embeddings of seed terms.

For the last group, I propose four new algrorithms, which induce
sentiment lexicons by clustering words vectors with the methods of
$k$-nearest neighbors, nearest centroids, principal component
analysis, and my own linear projection algorithm.  In the last of
these solutions, I first determine a line that maximizes the distance
between the projections of seed terms with opposite semantic
orientations, and then project the vectors of all remaining terms on
this line, considering the distance of these projections to the origin
as the respective polarity score.  My algorithm surpasses all existing
alternatives in terms of micro-averaged \F{}-score, reaching 0.963
micro-\F{}, and yields a very competitive macro-\F{} (0.462),
outperforming all corpus- and word-embedding--based alternatives
except for the method of $k$-nearest neighbors.

In general, as shown by evaluation, the linear projection and $k$-NN
algorithms are the only viable lexicon-generation solutions that
dispense with any manually annotated linguistic resources and whose
results are at least close to the scores of dictionary-based and
manually-curated sentiment lexicons.  Additional experiments with
different input settings also show that the quality of almost all
automatically generated polarity lists can be further improved by
using larger and less ambiguous sets of initial seed terms.

\subsection{Chapter~4: Aspect-Based Sentiment Analysis}

Another subsentential sentiment analysis task---that of automatic
prediction of targets, holders, and evaluative text spans---is
addressed in Chapter~4.  Following the current state of the art, I
consider this objective as a sequence-labeling (SL) problem and
approach it with two most popular SL techniques: conditional random
fields~\cite[CRFs; ][]{Lafferty:01} and recurrent neural networks
\cite[RNNs; ][]{Hochreiter:97,Cho:14a}.  For the former method, I
devise an extensive set of feature attributes, which reflect different
lexical, mophologic, and syntactic traits of the input; using these
attributes, CRFs achieve 0.287 macro-\F{} for the three classes
(sentiments, sources, and targets) on the PotTS test set.  An
additional feature ablation of test demonstrates that complex
features, which simutaneously reflect lexical and syntactic
properties, are most useful for predicting sentiment spans; pure
lexical attributes are most helpful when classifying opinion sources;
and, finally, syntactic features work best when predicting targets.

Unfortunately, recurrent neural networks yield lower results than
CRFs, with the best scores (0.27 macro-\F{}) achieved by the
long-short term memory network~\cite{Hochreiter:97}.  Their prediction
quality, however, can be improved by initializing the word embedding
matrix with word2vec vectors~\cite{Mikolov:13} and optimizing these
values along with other parameters during the training.  Besides this,
I propose to use a special fallback strategy for approximating
task-specific vector representations of words ($V$) that have word2vec
embeddings ($W$), but are absent in the training set.  In particular,
I suggest computing a matrix M using the method of the least squares,
which minimizes the absolute difference between the trained
task-specific vectors $V$ and their original embeddings W times that
matrix ($M^{\top}W$): $M := \argmin_{W}\lVert V_{TS} - M^T\cdot
V^*_{W2V}\rVert_F,$.

In the concluding sections, I investigate whether using alternative
structure of computational graphs for CRFs and RNNs and providing
unnormalized text to these methods as input can improve their results.
As it turns out, conditional random fields benefit from increasing the
order of their dependencies and performing predictions on spans rather
individual tokens; RNNs, on the contrary, achieve their best scores
with the standard 1st-order linear-chain structure.  Both approaches,
however, strongly benefit from text normalization, suffering a quality
drop by almost 3\% when this component is switched off.  Finally, I
compare two different approaches to the definition of \emph{sentiment}
spans: In the first approach I consider a \emph{sentiment} as minimal
complete syntactic unit (typically a noun or verb phrase) that
simultaneously comprise both the target of an evaluation and the
evaluative phrase; in the second approach, I only assign the
\emph{sentiment} tag the polar term that evaluates the target.  A
comparison of both schemes shows that the former approach leads to
better classification scores for \emph{source}s and \emph{target}s of
opinions, whereas the second take is more amenable to the prediction
of \emph{sentiment}s.

\subsection{Chapter~5: Message-Level Sentiment Analysis}

Afterwards, in Chapter~\ref{chap:cgsa}, we addressed one of the most
popular objective in contemporary sentiment analysis---message-level
sentiment analysis (MLSA).  To get a better overview of the numerous
existing systems, we compared three larger families of MLSA
methods---dictionary-, machine-learning--, and deep-learning--based
ones, finding that the last two groups performed significantly better
than the lexicon-based approaches (the best macro-\F{}--scores of
machine- and deep-learning methods run up to 0.677 and 0.69
respectively, whereas the best lexicon-based solution
[\citeauthor{Hu:04}, \citeyear{Hu:04}] only reached 0.641 macro-\F{}).
Apart from this, we improved the results of many reimplemented
approaches by changing their default configuration (\eg{} abandoning
polarity changing rules of lexicon-based systems, using alternative
classifiers for ML-based classifiers, or taking the least-squares
embeddings for DL-based methods).  In addition to the numerous
reimplementations of popular existing algorithms, we also proposed our
own solution---lexicon-based attention (LBA), in which we tried to
unite the lexicon and deep-learning paradigms by taking a
bidirectional LSTM network and explicitly pointing its attention to
the polar terms that appeared in the analyzed messages.  With this
solution, we not only outperformed all alternative DL systems, but
also improved on the scores of ML-based classifiers, attaining 0.69
macro-\F{} and 0.73 micro-\F{} on the PotTS corpus.  Similarly to our
findings of the previous chapter, we observed a strong positive effect
of text normalization and task-specific embeddings with the
least-squares approximation;

Now the we have reached the end of the chapter, we would like to
remind the reader that in this part of the thesis we have made the
following findings and contributions:
\begin{itemize}
  \item we have compared three major families of message-level
    sentiment analysis methods---lexicon-, machine-learning-- and
    deep-learning--based ones, finding that the last two groups
    significantly outperform lexicon-driven systems;
  \item surprisingly, among all compared lexicon methods, the most
    simple one (the classifier of~\citeauthor{Hu:04}
    [\citeyear{Hu:04}]) produced the best macro- and micro-averaged
    \F{}-results on the PotTS corpus (0.615 and 0.685 respectively)
    and also yielded the highest macro \F{}-measure on the SB10k
    dataset (0.421).  Other systems, however, could have improved
    their scores if they better handled the negation of polar terms
    (after switching off the negation component in the method
    of~\citeauthor{Musto:14}, its macro-\F{} on the PotTS corpus
    increased to 0.641, surpassing the benchmark
    of~\citeauthor{Hu:04});
  \item as expected, the ML-based system of~\citet{Mohammad:13}---the
    winner of the inaugural run of SemEval task in sentiment analysis
    of Twitter~\cite{Nakov:13}---also surpassed other ML competitors,
    achieving highly competitive results: 0.674 macro- and 0.727
    micro-\F{} on the PotTS data, and 0.564 macro- and 0.752
    micro-averaged \F{}-measure on the SB10k test set;
  \item as in the previous case, however, these results could have
    been improved if the classifier dispensed with character-level and
    part-of-speech features and used logistic regression instead of
    SVM;
  \item a much more varied situation was observed with
    deep-learning--based systems, which frequently simply fell into
    always predicting the majority class for all tweets, but sometimes
    yielded extraordinarily good results as it was the case with our
    proposed lexicon-based attention system, which attained 0.69
    macro-\F{} on the PotTS corpus and 0.55 macro \F{}-score on the
    SB10k dataset (0.73 and 0.75 micro-\F{} respectively), setting a
    new state of the art for the former data;
  \item speaking of word embeddings, we should note that almost all
    DL-based approaches showed fairly low scores when they used
    randomly initialized task-specific embeddings, but notably
    improved their results after switching to pre-trained word2vec
    vectors, and benefited even more from the least-squares fallback;
  \item against our expectations, we could not overcome the majority
    class pitfall of DL-based systems after adding more distantly
    supervised training data, which, in general, only lowered the
    scores of both ML- and DL-based methods.  Since this result
    contradicts the findings of other authors, we hypothesize that
    this degradation is primarily due to the differences in the class
    distributions between automatically and manually labeled tweets;
  \item on the other hand, we could see that using more qualitative
    sentiment lexicons (especially manually curated and
    dictionary-based ones) resulted in further improvements for the
    systems that relied on this lexical resource;
  \item last but not least, we proved the utility of the text
    normalization step, which brought about significant improvements
    for all tested methods, as confirmed by our last ablation test.
\end{itemize}

\subsection{Chapter~6: Discourse-Aware Sentiment Analysis}

Finally, in the last part, we tried to improve the results of the
proposed LBA method by making it aware of the discourse structure.
For this purpose, we segmented all microblogs from the PotTS and SB10k
corpora into elementary discourse units, individually analyzing each
of these segments with our MLSA classifier, and then estimated the
overall polarity of a tweet by joining the polarity scores of its EDUs
over the RST tree.  We proposed three different ways of doing this
joining (latent CRFs, latent-marginalized CRFs, and Recursive
Dirichlet Process), obtaining better results than existing
discourse-aware sentiment methods and also outperforming the original
discourse-unaware baseline.  In the concluding experiments, we further
improved these scores by using manually annotated RST trees and richer
subsets of discourse relations.

At this point, our chapter has come to an end and, concluding it, we
would like to recap that in this part of the thesis:
\begin{itemize}
  \item we have presented an overview of the most popular approaches
    to automatic discourse analysis (RST, PDTB, and SDRT) and
    explained why we think that one of these frameworks (Rhetorical
    Structure Theory) would be more amenable to the purposes of
    discourse-aware sentiment analysis than the others;
  \item to substantiate our claims and to see whether the
    lexicon-based attention system introduced in the previous chapter
    would indeed benefit from awareness of discourse structure, we
    segmented all microblogs from the PotTS and SB10k corpora into
    elementary discourse units using the SVM-based segmenter
    of~\citet{Sidarenka:15} and parsed these messages with the RST
    parser of~\citet{Ji:14}, which had been previously retrained on
    the Potsdam Commentary Corpus~\cite{Stede:14};
  \item afterwards, we estimated the results of existing
    discourse-aware sentiment methods (the systems
    of~\citeauthor{Wang:15}~[\citeyear{Wang:15}] and
    \citeauthor{Bhatia:15}~[\citeyear{Bhatia:15}]) and also evaluated
    two simpler baselines (in which we predicted semantic orientation
    of a tweet by taking the polarity of its last and root EDUs),
    getting the best results with the R2N2 solution
    of~\citet{Bhatia:15} (0.657 and 0.559 macro-\F{} on PotTS and
    SB10k respectively);
  \item we could, however, improve on these scores and also outperform
    the plain LBA system (although by a not very large margin) with
    our three proposed discourse-aware sentiment solutions (latent and
    latent-marginalized conditional random fields and Recursive
    Dirichlet Process), pushing the macro-averaged \F{}-score on PotTS
    up to 0.678 and increasing the result on SB10k to 0.56 macro-\F{};
  \item a subsequent evaluation of these approaches with different
    settings showed that the results of all discourse-aware methods
    largely correlated with the scores of the base sentiment
    classifier and also revealed an important drawback of the
    latent-marginalized CRFs, which failed to predict any positive or
    negative instance on the test set of the SB10k corpus when trained
    in combination with the lexicon-based approach of~\citet{Hu:04};
  \item nevertheless, almost all DASA solutions could improve their
    scores when tested on manually annotated RST trees or used with a
    richer set of discourse relations.
\end{itemize}

\section{Conclusions}
%% \addcontentsline{toc}{section}{Conclusions}

Now that we have gone past all these landmarks, it is time to unbag
the questions which we had asked ourselves at the beginning of this
endeavor, and try to answer them again, equipped with all knowledge
that we have acquired during our run.  Here we go:

\begin{itemize}
  \item\textbf{Can we apply opinion mining methods devised for
    standard English to German Twitter?}

    Yes, we can, but the success of these approaches might
    significantly vary depending on the task, the size and the
    reliability of the training data, as well as the evaluation metric
    that we use. For example, dictionary-based lexicon methods
    achieved fairly good results on their objective, but this success
    was mostly due to the high quality of the \textsc{GermaNet}
    annotation.  On the other hand, our manually labeled PotTS corpus
    was evidently too small for aspect-based sentiment systems, which
    failed to generalize to unseen tweets despite their very high
    scores on the training set.  Message-level sentiment approaches,
    vice versa, seemed to be quite happy with the size of the training
    dataset, attaining good results on both corpora (PotTS and SB10k).
    Nevertheless, we again experienced a lack of data while working on
    discourse-aware enhancements, many of which hit the same ceiling
    of the macro-averaged \F{}-scores.

    Apart from these difficulties arising from insufficient data, we
    also noticed a significant degradation of the scores for systems
    whose original tasks and evaluation metrics were different from
    ours.  For example, the lexicon generation method of
    \citet{Esuli:05} was originally designed to assign polarity scores
    to all \emph{synsets} found in the \textsc{WordNet} and not to
    produce a list of polar \emph{words}.  Similarly, the RNTN
    approach of \citet{Socher:13} was trained and evaluated on all
    syntactic subtrees of a document and not only at the top text
    level.  Likewise, the system of~\citet{Yessenalina:11} was devised
    for doing ordinal logistic regression and not polarity
    classification, as in our case.  As a result, all these approaches
    showed lower scores than their competitors in our evaluation, even
    though they are undoubtedly well suited for their original data
    and tasks.

    Due to the high diversity of methods, metrics, and tasks, it is
    difficult to provide a general recipe for transferring existing
    English sentiment systems to German Twitter, but we still would
    like to formulate at least a few rules of thumb, which came up
    during our experiments:
    \begin{itemize}
      \item\textbf{Prefer methods which are closest to your training
        objective} and which were trained under similar conditions
        w.r.t. the amount of data, their class distribution and
        domain;
      \item\textbf{Put every single setting of these methods into
        question}---bear in mind that things which work well in the
        original cases are not guaranteed to work in your
        situation.\footnote{In this respect, it is important to
          realize that every classification task is merely an attempt
          to solve a system of equations, so that methods which are
          good at solving one system might completely fail to solve
          other equations.}  The more options you try, the better will
        be your results;
      \item\textbf{Try using manually labeled resources for your
        target domain}, if they are available, but pay attention at
        the quality of their annotation---it often matters more than
        the corpus size;
      \item If there are manually annotated data, \textbf{prefer
        machine-learning methods to hard-coded rules}--- they will
        penalize their bad components automatically by themselves;
      \item\textbf{Do not use randomly initialized word embeddings for
        deep-learning systems}---initialize them with language-model
        vectors (which are cheap to obtain).  Otherwise, your model
        might get stuck in a very bad local optimum.
    \end{itemize}

  \item\textbf{Which groups of approaches are best suited for which
    sentiment tasks?}

    Based on our evaluation, we answer to this question as follows:
    \begin{itemize}
      \item\emph{Sentiment lexicon generation} is more amenable to
        dictionary-based solutions, provided that there exists a
        sufficiently big, reliably annotated lexical taxonomy for
        these systems.  If there is no such resource, one should
        better resort to word-embedding--based algorithms;

      \item With a limited amount of training data, \emph{aspect-based
        sentiment analysis} can be better addressed with probabilistic
        graphical models such as conditional random fields with
        hand-crafted features;

      \item On the other hand, plain \emph{message-level sentiment
        analysis} can be efficiently tackled with both machine- and
        deep-learning algorithms (such as SVM, logistic regression, or
        RNN);

      \item But probabilistic graphical models strike back at
        \emph{discourse-aware sentiment methods}, where they might
        even outperform pure neural-network solutions, although the
        margin of these improvements is not that large.
    \end{itemize}

    Thus, probabilistic model can still hold their ground when it
    comes to structured prediction, but the difference of these
    algorithms from and their improvements upon neural networks are
    gradually vanishing.

  \item\textbf{How much do word- and discourse-level analyses affect
    message-level sentiment classification?}

    Our evaluation in Section~\ref{cgsa:subsec:eval:lexicons} showed
    that the macro-averaged \F{}-scores of our proposed lexicon-based
    attention system varied by up to 14\% (from 0.64 to 0.69
    macro-\F{} on the PotTS corpus, and from 0.44 to 0.58 on SB10k)
    depending on the lexicon used by this approach.  At the same,
    discourse enhancements could only improve the results of LBA by at
    most 1.5\% percent (from 0.677 to 0.678 on PotTS, and from 0.557
    to 0.572 on SB10k).  Although it appears as if the lexicon
    component were more important to a sentiment system, we would like
    to preclude such incorrect conclusion, because
    \begin{inparaenum}[(a)]
      \item a full-fledged sentiment solution should take into account
        both linguistic levels (words and discourse) and
      \item these relative results might look different if we expand
        the analyzed domain to longer documents or apply
        discourse-aware methods to complete discussion threads.
    \end{inparaenum}

  \item\textbf{Does text normalization help analyze sentiments?}

    Yes, it definitely does.  As we could see in
    Chapters~\ref{chap:fgsa} and \ref{chap:cgsa}, normalization
    significantly improves the quality of aspect-based and
    message-level sentiment analyses, boosting the results on the
    former task by up to 4\% (see
    Table~\ref{snt-fgsa:tbl:normalization}) and improving the
    macro-averaged \F{}-measure of message-level sentiment methods by
    up to 25\% (see Table~\ref{snt-cgsa:tbl:res-no-normalization});

    The only question that remained unanswered in this context is
    which normalization steps exactly improve the scores of sentiment
    systems.  To make up for this omission, we separately deactivated
    each individual step of our text normalization pipeline
    (unification of Twitter phenomena, spelling correction, and
    normalization of slang terms) and rerun our message-level
    classification experiments using the lexicon-based attention
    system.  As we can see from the results in
    Table~\ref{afterword:tbl:lba-normalization-steps}, the
    micro-averaged \F{}-scores on both datasets benefit most from the
    unification of Twitter-specific phenomena, sinking by almost 19\%
    when this component is deactivated.  This step is also most useful
    for the macro-\F{} on the SB10k corpus, whereas the macro-average
    on PotTS mostly capitalizes on the normalization of slang terms.

  \item\textbf{Can we do better than existing approaches?}

    Yes, we can:
    \begin{itemize}
    \item we improved the macro-averaged results of exisitng
      lexicon-generation methods with our proposed linear projection
      algorithms;
    \item we increased the scores of aspect-based analysis by
      redefining the topologies of CRFs;
    \item our lexicon-based attention network outperformed many of
      its competitors on message-level classification;
    \item and, finally,we surpassed the discourse-unware baseline and
      other existing discourse-aware sentiment solutions with the
      proposed latent-marginalized CRFs and Recursive Dirichlet
      Process.
    \end{itemize}
\end{itemize}

% Bibliography
\bibliographystyle{apalike}
\bibliography{../bibliography}

\end{document}
