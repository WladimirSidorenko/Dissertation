\section{Sentence-Level Sentiment Analysis}\label{sec:snt:slsa}



\subsection{Sentence-Level Sentiment Analysis with Deep Neural Networks}

\subsection{Sentence-Level Sentiment Analysis with Deep Neural Networks}

\subsection{Evaluation}

\subsection{Related Work}

\citet{Hu:04}

\citet{Kim:04}

One of the first attempts to analyze sentiments on Twitter was made by
\citet{Go:09}.  For their experiments, the authors collected a set of
1,600,000 tweets containing smileys.  Based on these emoticons, they
automatically derived polarity classes for these messages (positive or
negative) and used them to train a Na\"{\i}ve Bayes, a MaxEnt, and an
SVM classifier.  The best $F$-score for this two-class classification
problem could be achieved by the last system and run up to 82.2\%.

Bermingham and Smeaton, 2010

Similar work was done later by \citet{Pak:10}, who used the Na\"{\i}ve
Bayes approach to differentiate between neutral, positive, and
negative microblogs; and \citet{Barbosa:10}, who gathered a collection
of 200,000 tweets, subsequently analyzing them with three publicly
available sentiment web-services; the results of these services were
then used as noisy labels for training an SVM classifier.

\citet{Agarwal:11} compared a simple unigram-based SVM approach with
two other full-fledged systems, one which relied on a rich set of
manually defined features, and another used partial tree
kernels~\cite{Moschitti:06}.  The authors evaluated these methods on a
commercially acquired corpus of 8,753 foreign-language tweets, which
were automatically translated into English, finding that a combination
of these methods worked best for both two- and three-way
classification tasks.

\citet{Mohammad:13}

\citet{Kobayashi:07}
Wiebe, Bruce, \& O'Hara 1999
Hatzivassiloglou \& Wiebe 2000
Wiebe 2000; Wiebe et al. 2002
Yu \& Hatzivassiloglou 2003


\citet{Yessenalina:11}

A real breakthrough in the use of deep neural networks for the
sentence-level sentiment analysis happened with the pioneering work
of~\citet{Socher:11}, who first introduced a recursive autoencoder
(RAE).  In this system, the authors obtained a fixed-width vector
representation for complex phrases $\vec{v}$ by recursively merging
the vectors of adjacent tokens (say $\vec{w}_1$ and $\vec{w}_2$),
first multiplying these vectors with a compositional matrix $W$ and
then applying a non-linear function ($softmax$) to the resulting
product:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}\right)
\end{align*}
Using a max-margin classifier on top of the resulting phrase
representation, \citet{Socher:11} could improve the state-of-the-art
results on predicting the sentence-level polarity of user's blog
posts~\cite{Potts:10} and also outperformed the system
of~\citet{Nasukawa:03} on the MPQA data set~\cite{Wiebe:05}.

Later on, \citet{Socher:12} further improved these scores with the
help of a recursive matrix-vectors space model (RMVSM), in which each
word was associated with a 2-tuple of a vector and matrix---e.g.,
$(\vec{w}_1, W_1)$ and $(\vec{w}_2, W_2)$---and the compositionality
function was redefined as follows:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  W_2\cdot\vec{w}_1\\
  W_1\cdot\vec{w}_2
  \end{bmatrix}\right)
\end{align*}

\citet{Wang:15}

\subsection{Summary and Conclusions}\label{slsa:subsec:conclusions}
