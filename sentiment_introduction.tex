% FILE: sentiment.tex  Version 0.0.1
% AUTHOR: Uladzimir Sidarenka

% This is a modified version of the file main.tex developed by the
% University Duisburg-Essen, Duisburg, AG Prof. Dr. Günter Törner
% Verena Gondek, Andy Braune, Henning Kerstan Fachbereich Mathematik
% Lotharstr. 65., 47057 Duisburg entstanden im Rahmen des
% DFG-Projektes DissOnlineTutor in Zusammenarbeit mit der
% Humboldt-Universitaet zu Berlin AG Elektronisches Publizieren Joanna
% Rycko und der DNB - Deutsche Nationalbibliothek

\part{Sentiment Analysis}

Interpersonal communication is not only a way to share objective
information with other people but also a vibrant channel to convey
one's subjective thoughts, attitudes, and feelings.  It is, in fact,
this latter use which provides a personal touch to our discourses,
making them more grasping, more entertaining, and more living.  And it
is often this use which significantly influences our decisions,
preferences, and choices in everyday life.  Therefore, a high-quality
automatic analysis of the subjective part of information is typically
as important as the extraction and analysis of objective facts.

\chapter{Introduction to Sentiment Analysis}

The field of knowledge which deals with the analysis of people's
opinions, sentiments, evaluations, appraisals, attitudes, and emotions
towards particular entities mentioned in discourse is called
\emph{sentiment analysis} (SA) \citep{Liu:12}.  The definition of this
discipline, however, much like the definition of the term
\emph{sentiment} itself, is neither complete nor universally accepted.
The main reasons for this are
\begin{inparaenum}[\itshape a\upshape)]
  \item a frequently unclear delimitation of the subjective and
    objective components of information and
  \item the heterogeneity of the language system to which SA methods
    are applied.
\end{inparaenum}

The former factor, for instance, makes it difficult to delimitate what
kinds of events and expressions should actually belong to the domain
of sentiment analysis, and which ones shall rather be excluded from
it.  A prominent example of such problematic borderline cases are the
so-called subjective facts, such as ``terrorist attacks'' or ``cancer
drugs'', which some people perceive as emotionally laden terms while
others regard them as purely objective statements.

The latter factor complicates a precise definition of SA because
different levels of the language have their own concepts of
subjectivity, which in turn require different approaches and methods
to use.  Depending on the language level being analyzed, researchers
typically distinguish three different subtypes of sentiment analysis:
\begin{itemize}
  \item\emph{subsentential} or \emph{fine-grained} SA, whose task is
    to determine and analyze specific subjective opinions and/or
    evaluations within single clauses,
  \item\emph{sentential} analysis, which tries to ascribe a single
    polarity class to each sentence in a text, and
  \item\emph{suprasentential} or \emph{document-level} sentiment
    classification, which seeks to determine the polarity and
    subjectivity classes of complete discourses.
\end{itemize}

Each of these subtypes has its own characteristic strengths and
weaknesses.  The fine-grained SA, for example, is typically considered
to be the ultimate goal of any opinion mining\footnote{Following
  \citet{Liu:12}, we do not make a distinction between the terms
  \emph{opinion mining} and \emph{sentiment analysis} and use both
  expressions interchangeably in this thesis.}  system as it aims at
the highest possible recall of all subjective expressions occurring in
a text.  At the same time, this task is unfortunately very challenging
even for human beings, as we will show in Section \ref{sec:snt:corpus},
let alone computer programs.  The consequently low results often
intimidate researchers and prevent subsentential SA systems from being
used in industrial applications.

The last two disciplines (sentential and document-level analyses) can
be considered as approximations of the fine-grained SA.  They coarsen
the targeted recall down to the level of sentences or texts
respectively, trying to determine only one (the most prominent)
expression of subjectivity per analyzed unit.  In contrast to the
subsentential SA, these approaches typically yield better results at
the cost of sacrificing important details.

Due to these crucial disparities between different variants of
sentiment analysis, speaking of the difficulty or easiness of this
task for a specific domain in general is in the same way wrong as
making overall judgments about the amenability of this domain to the
whole natural language processing field: one always has to specify the
precise level of sentiment analysis whose difficulty is being analyzed
just as she has to set apart a particular NLP task in order to assess
its complicatedness.

In this chapter, we will primarily concentrate on the most challenging
SA objective -- that of the fine-grained SA.  After a brief summary of
related work done on the opinion mining in general and sentiment
analysis of social media in particular, we will introduce a
comprehensive corpus of German tweets that has been created for the
purpose of this work.  A detailed inter-annotator agreement study of
this dataset will reveal which linguistic and extra-linguistic factors
significantly influence the distribution of sentiments in Twitter, and
which of them cause utter confusion among human experts.  After
obtaining an upper bound on human performance, we will subsequently
compare with it the results attained by automatic sentiment systems,
starting with the most fundamental task of recognizing subjective
expressions and concluding with the ultimate goal of recognizing
textual spans of sentiments, their objects of evaluation
(\emph{targets}), and authors (\emph{sources}).

\section{Prehistory of the Field}

However, before we delve into the odds and ends of contemporary
sentiment analysis, we first would like to make a short digression
into the history of this field in order to understand its modern
trends and theories better.  Like many other subdisciplines of
computational linguistics, opinion mining has emerged from several
other areas of research, including philosophy, psychology, cognitive
sciences, and last but least narratology and linguistics.

In \emph{philosophy}, the questions about the nature of emotions,
their interaction with the human consciousness, and their influence on
people's deeds have occupied the minds of many great scholars,
starting from Plato and Aristotle.  Plato, for instance, argued in his
treatise ``The Republic'' \citep[Book~IV]{Plato:91} that the human
soul presumably consists of three fundamental parts: the rational, the
appetitive, and the passionate.  The last part -- the one by which we
become angry or get into a temper -- determines our notion of justice
by favoring either the rational or the appetitive aspect.

These ideas were further particularized and partly revised by Plato's
most prominent student -- Aristotle.  In Book~II of ``The Rhetoric''
\citep{Aristotle:54}, Aristotle not only provides a precise taxonomy
and definitions of possible emotions that might constitute the
passionate part of the mind, but also contends that these are both the
reasons for the change of human judgements and the consequences of
those changes \cite[cf.][p. 157]{Leighton:82}.

As noted by \citet{Sousa:14}, the sheer variety of phenomena covered
by the term ``emotion'' and its closest neighbors tended to discourage
tidy philosophical theories and daunted researchers for a long period
of time.  A real renaissance of emotional studies happened, however,
in the late 19-th century in \emph{psychology} with the introduction
of the James-Lange theory \citep{James:1884}.  In his groundbreaking
work, \citeauthor{James:1884} proclaimed the naturalistic idea that
bodily changes followed directly the perception of the exciting facts,
and that our feelings of the same changes were, in fact, the emotions.
In other words, our biological processes were the primary and the only
reason for us to perceive sentiments:
\begin{quote}
``If we fancy some strong emotion, and then try to abstract from our
  consciousness of it all the feelings of its characteristic bodily
  symptoms, we find we have nothing left behind, no `mind-stuff' out
  of which the emotion can be constituted, and that a cold and neutral
  state of intellectual perception is all that
  remains.''\citep[p. 193]{James:1884}
\end{quote}
Independently of James' work, similar views were proposed by the
Danish physician Carl~Lange \citep{Lange:1885}, who hypothesized that
emotional phenomena were the consequences of cardiovascular events
\citep[cf.][]{Lang:94}.

In the 1920s, this theory was severely criticized by Philip~Bard and
Walter~B.~Cannon \citep{Bard:28,Cannon:31}, who proposed an
alternative interpretation of emotions, refuting their connection to
the viscera and arguing that both bodily reactions and subjective
feelings were generated simultaneously in the talamic region of the
brain.  Later on, their conception was in turn put into question by
Stanley Schachter and Jerome E. Singer \citep{Schachter:62}, who
claimed that bodily means alone were insufficient to express the full
range of possible feelings and that cognitive factors could be major
determinants of emotional states. %% To prove this hypothesis, the
%% authors conducted a series of experiments in which they administered
%% epinephrine to human subjects and put them into a room with a
%% specially instructed stooge.  Their study showed that the emotional
%% reaction of the participants crucially depended on the behavior of the
%% stooge during the stay.

These advances in psychology reinforced by the nascent appraisal
theory \citep{Arnold:60} had a significant influence on many other
scientific fields, including but not limited to the \emph{literary
  studies}.  Among the greatest pursuers of this research direction in
narratology, we certainly should name Am\'elie Rorty and Ann Banfield.
\citet{Rorty:80} suggested that the directive power which emotions
exerted over perception was partly a function of their essentially
dramatic or narrative structure \citep[cf.][]{Sousa:14}.
\citet{Banfield:82} investigated how author's and characters'
attitudes could be expressed through direct and indirect speech in
narratives.

One of the first who applied \citeauthor{Banfield:82}'s theory to the
needs and wants of \emph{computational linguistics} was
\citet{Wiebe:90a}.  In her work, the author proposed an algorithm for
identifying characters of narratives whose point of view was being
represented by subjective sentences.  Later on, \citet{Wiebe:94}
further enhanced this system with the possibility to predict
subjective clauses in stories automatically.  This work was allegedly
one of the first experiments on the automatic prediction of sentiments
on the level of single sentences.

The real breakthrough in the sentiment analysis field, however,
happened with the introduction of the first sufficiently big manually
annotated corpora.  Notable contributions in this regard were made by
\citet{Pang:04,Pang:05}, who offered a dataset of $\approx2,000$ movie
reviews with automatically derived polarity classes; \citet{Hu:04},
who presented a manually labeled set of Amazon and C|Net product
feedbacks; \citet{Thomas:06}, who created an automatically labeled
corpus of congressional debates; and, finally, \citet{Wiebe:05}, who
developed a manually annotated sentiment corpus of 535 newspaper
texts.

The availability of these resources has fostered a plethora of new
methods and approaches for both fine- and coarse-grained SA, making
opinion mining one of the most challenging and competitive fields
nowadays.  Important cornerstones in this discipline have already been
established by the works of \citet{Pang:02}, \citet{Wiebe:05},
\citet{Wilson:05}, \citet{Breck:07}, \citet{Choi:09,Choi:10},
\citet{Yessenalina:11}, \citet{Socher:11, Socher:12} and many others.
Nevertheless, many challenges of sentiment analysis, such as domain
adaptation or analysis of non-English texts, still pose considerable
difficulties.

\section{Sentiment Analysis of Social Media}

One of the main problems that people working on opinion mining are
typically confronted with in the first place is that of the discourse
domain to be analyzed.  Since sentiment analysis is a highly
domain-dependent task \citep[cf.][]{Aue:05,Blitzer:07,Li:08} -- i.e.,
systems trained for specific topics and on specific text genres do not
neatly generalize to other subjects and other linguistic styles -- a
natural question that arises in this context is which of the domains
should be addressed first in that case.

While earlier sentiment works were primarily concerned with narratives
\citep{Wiebe:90a,Wiebe:94} or newspaper articles
\citep{Wiebe:03,Wiebe:05,Bautin:08}, it soon became clear that social
media provides a much more fertile ground for mining people's
opinions.  The reason for this is the virtual absence of any
moderation on many modern CMC services.  This lack of censorship
allows users to be upfront about their feelings.  Combined with the
great popularity of social networks, such freedom of expressing one's
thoughts makes social media the arguably most prolific channel for
sharing (and mining) personal emotions.

It is therefore not surprising that the rapid emergence of numerous SM
websites was accompanied by an increasing interest in their content
from many NLP practitioners.  One of the first who attempted to
extract users' evaluations automatically from CMC services were
\citet{Das:01}.  The authors investigated the correlation between
users' attitudes on economic chat boards and stock prices for about
eight stocks.  For this purpose, they trained a collection of five
classifiers on a training set of 500 messages that were manually
labeled with three classes: \emph{buy} (positive evaluations),
\emph{sell} (negative evaluations), and \emph{null} (neutral
statements).  With these classifiers, they subsequently annotated the
rest of the downloaded 25,000 posts, taking the majority votes of the
five systems as the final labels, and then scrutinized the
interrelationship between these predictions and the observed stock
indices.
%% This study revealed that user chats strongly correlated with stock
%% developments in a reactive way: changes in stock prices had
%% typically lead to fluctuations of public opinions but not vica
%% versa.

\citet{Glance:05} also used sentiment analysis for business
intelligence purposes by mining users' opinions on Usenet newsgroups
with a set of hand-crafted rules.  Further notable works in this
direction include those of \citet{Antweiler:04}, who investigated how
postings on stock message boards correlated with stock volatility;
\citet{Gruhl:05}, who studied the cross-correlation between online
mentions of products and sales spikes for that commodities, also
proposing an algorithm for tracking such correlations and making
predictions about products' future sales automatically; finally,
\citet{Ghose:07} examined the effect of opinions on the pricing power
of merchants in online marketplaces, claiming that more positive
opinions on the Web entitled traders to increase their selling prices.

A separate line of research that is closely related to both commercial
intelligence and sentiment analysis of social media involves opinion
mining of product reviews.  A pioneering work in this direction was
done by \citet{Turney:02}, who performed a two-class classification of
410 Epinions comments, dividing customers' reviews of automobiles,
banks, movies, and travel destinations into \emph{recommended} (thumbs
up) and \emph{not recommended} (thumbs down) ones.  In order to make
these decisions, the proposed system first computed the sum of the
pointwise mutual information (PMI) scores between the adjectival and
adverbial phrases occurring in a review text and the word
``excellent'', and then subtracted from this the PMIs of the found
phrases in conjunction with the word ``poor''.  Reviews with negative
results were subsequently considered as \emph{thumbs down}, and
comments with non-negative total scores were correspondingly
classified as \emph{thumbs up}.

According to \citet{Turney:02}, the most challenging type of reviews
to analyze in these experiments were the movie critiques (the accuracy
on this subset only reached 66\%, whereas, for banks and automobiles,
it attained 80 and 84\% respectively).  This finding was also
confirmed by \citet{Pang:02} who tried out several machine learning
classifiers on a manually annotated corpus of $\approx2,000$ write-ups
from the Internet Movie Database (IMDb).  The best results (82.9\%
accuracy) for the two-class classification task (\emph{positive}
versus \emph{negative}) were attained by an SVM system which used the
presence of unigrams as classification features.

Due to its high commercial impact, opinion mining of customer reviews
soon became one of the most popular topics in sentiment analysis
research.  \citet{Dave:03}, for example, attempted to classify C|Net
and Amazon reviews of movies and electrical appliances as positive or
negative using the Na\"{\i}ve Bayes and SVM approaches.  \citet{Hu:04}
developed a three-stage application which first identified products'
features in reviews, then looked for opinionated words associated with
those features, and, finally, produced concise summaries of positive
and negative opinions about each particular product aspect.
\citet{Funk:08} proposed a supervised SVM-system to perform two- and
five-way polarity classification of product reviews, also using these
predictions in a business intelligence system.  Other important
milestones in this research direction were set by the works of
\citet{Popescu:05}, \citet{Ding:09}, \citet{Wei:10},
\citet{Mukherjee:12}, and many others.

%% The authors found that replacing certain entities, such as product
%% names, abbreviations, or numerals, with special meta-tokens and
%% using token n-grams of different lengths had a significant positive
%% effect on the accuracy of both classifiers.

Notwithstanding that product reviews continued playing a crucial role
for the success of the e-commerce, with the increasing spread of
blogging services and social networks, sentiment researchers have
gradually shifted the focus of their work from product feedbacks to
these upcoming text genres.  \citet{Mishne:05} and \citet{Mishne:07},
for instance, tried to classify Livejournal blogs according to the
moods of their authors (e.g.  \emph{amused}, \emph{tired},
\emph{happy} etc.).  The gold labels for their $\approx$~815,000 blog
corpus were obtained automatically from blog labels on the Livejournal
website.  On this corpus, the authors trained a supervised SVM
classifier, reaching an average of 8\% improvement over the 50\%
baseline.

\citet{Chesley:06} also applied an SVM system in order to classify
user blogs into objective, positive and negative subjective ones.  The
authors derived their features from the Wiktionary and the InfoXtract
lexicon \citep{Srihari:03} and attained an accuracy increase of up to
15~\% over the majority class baseline.  This research direction was
further continued by \citet{Godbole:07}, who applied a lexicon-based
method to detect users' opinions in newspapers and blogs; and
\citet{Gill:08}, who conducted an inter-annotator agreement study of
mood detection in blogs, finding that raters' consensus on the
expressed emotions strongly increased with the length of blog entries.

Speaking of blog length, it should certainly be said that the
inception of the micro-blogging service Twitter in 2006 was a real
game changer to the opinion mining field: the sudden availability of
huge amounts of data; the presence of all possible social, national,
and age groups combined with a high idiosyncrasy and diversity of the
language used on this service gave rise to numerous scientific
projects, studies, and publications, which we will briefly summarize
in the next subsection.

\section{Sentiment Analysis of Twitter}\label{snt:subsec:intro:saot}

Among the first attempts at an automatic sentiment analysis of Twitter
was the study conducted by \citet{Go:09}.  In their experiments, the
authors collected a set of 1,600,000 microblogs containing smileys.
Based on these emoticons, they automatically derived polarity classes
of these tweets (positive or negative) and then used these
bootstrapped labels to train the Na\"{\i}ve Bayes, MaxEnt, and SVM
classifiers.  The last system produced the best results, reaching an
$F$-score of 82.2\%.

A similar approach was also used by \citet{Pak:10}, who applied the
Na\"{\i}ve Bayes method to differentiate between neutral, positive,
and negative messages.  Likewise, \citet{Davidov:10} obtained the gold
labels for their corpus using distant supervision and then trained a
kNN-like classifier on this dataset.

A different way of collecting the data was proposed by
\citet{Barbosa:10}.  The authors gathered a collection of 200,000
microblogs from three publicly available sentiment web-services.
Using the majority votes of these websites as the gold categories for
their tweets, they trained a supervised SVM system on this corpus and
then let this system predict the subjectivity and polarity classes of
new unseen messages.

The emoticon
dataset\footnote{\url{http://twittersentiment.appspot.com}} of
\citet{Go:09} along with a subset of the Edinburgh Twitter corpus
\cite{Petrovic:10} was later used by \citet{Kouloumpis:11}, who
trained an AdaBoost classifier on these data.  Reaching an average
$F1$-measure of 68\% on a held-out manually labeled dataset, the
authors came to the conclusion that microblog-specific features (such
as the presence or absence of intensifiers or the occurrence of
abbreviations and emoticons) were the most reliable traits for this
classification.  This finding, however, was doubted by
\citet{Agarwal:11}, who contended that POS-specific polarity features
and tree-kernel methods were a better alternative.

Similarly to the opinion mining of customer reviews, sentiment
analysis of Twitter could not go unnoticed by the economic and
sociological communities.  One of the first attempts to analyze tweets
from these perspectives was made by \citet{Jansen:09}, who studied
microblogs' power as online word of mouth.  A manual inspection of
messages mentioning renowned company brands revealed that almost 20\%
of these tweets expressed some polar opinions about their products.
More than a half of these opinionated microblogs were positive
judgements, whereas 35\% of the posts represented negative sentiments.
These findings strongly correlated with the results of an automatic
analysis, suggesting that social media in general and microblogging in
particular could be a valuable source of economic evidence.

An attempt to address sociological problems with SA methods was made
by \citet{Tumasjan:10}, who analyzed public opinions about federal
elections in Germany by automatically extracting sentiments from
users' microblogs.\footnote{We will make a similar analysis in Section
  \ref{sec:snt:fgsa} but will primarily concentrate on the NLP aspect
  of this problem.}  For this purpose, the authors had gathered a
collection of nearly 100,000 messages mentioning major political
parties or prominent German politicians and subsequently translated
these tweets into English.  The resulting translations were analyzed
with the proprietary LIWC software \cite{Pannebaker:07}, which used
relative word counts to compute associations between the given topics
and a pre-defined emotions set.  This study also proved that Twitter
could serve as a platform for political deliberation and that the mere
number of tweets referencing some party strongly corresponded to the
results of political election polls.

As in the previous cases, a key factor for the success and popularity
of this subfield among NLP researchers was the creation and
dissemination of high-quality big annotated corpora.  An important
role in this respect was played by the organizers of the SemEval
competition on sentiment analysis in Twitter
\cite{Nakov:13,Rosenthal:14,Rosenthal:15}.

For the first run of the shared task in \citeyear{Nakov:13},
\citet{Nakov:13} prepared a substantially-sized dataset with more than
15,000 messages (the SemEval Tweet corpus), which were manually
labeled on the crowdsourcing platform Amazon Mechanical Turk.  To that
end, five Turkers were asked to determine the overall polarity of each
collected microblog and to annotate all occurrences of subjective
expressions in those messages with their contextual polarity.

The best performing system in the first iteration -- a supervised SVM
classifier of \citet{Mohammad:13} -- won three out of four subtasks.
This solution relied on an extensive set of hand-crafted surface-form,
semantic, and syntactic features and also actively used multiple
manually and automatically generated sentiment lexica, including the
NRC emotion lexicon of \citeauthor{Mohammad:13a}
(\citeyear{Mohammad:13a}), the MPQA lexicon of \citeauthor{Wilson:05}
(\citeyear{Wilson:05}), and the sentiment list of \citeauthor{Hu:04}
(\citeyear{Hu:04}).  The authors noticed a crucial impact of the
lexicon features on the quality of final predictions (an ablation test
showed that the results of the system worsened by $\approx8.5\%$ when
the lexicon information was removed).

Other competing submissions \cite{Becker:13,Guenther:13,Kokciyan:13}
basically utilized the same approach but used considerably less
information from lexicons.  Nevertheless, all 10 top-scoring systems
in this run could correctly predict the phrase-level polarity of
subjective expressions in more than 80\% of the cases and also
outperformed the 60\% benchmark when classifying the overall polarity
of the tweets.

The success of this shared task, which attracted more than 40
participants, incited the organizers to continue this competition in
the following years.  With slight modifications (addition of new
tweets, inclusion of sarcastic microblogs and LiveJournal sentences),
both subtasks were re-offered in \citeyear{Rosenthal:14}
\cite[cf.][]{Rosenthal:14}.

Here again, the system proposed by \citet{Mohammad:13} with some minor
adjustments \cite{Zhu:14} ranked first in the phrase-polarity task and
got fourth place in predicting the overall polarity of the messages.
The top-scoring solution for the latter track was proposed by
\citet{Miura:14}.  It also relied on a supervised SVM classifier
operating on a set of hand-crafted features but additionally utilized
a dedicated pre-processing pipeline and also imposed special penalties
on the polarity classes that were over-represented in the corpus.

In \citeyear{Rosenthal:15}, three more subtasks were included in the
competition:
\begin{inparaenum}[\itshape a\upshape)]
  \item polarity classification of messages pertaining to specific
    topics,
  \item topic-based trend prediction, and
  \item determining prior polarity of lexical terms found in tweets.
\end{inparaenum}

This time, the best system for the phrase polarity subtask, which also
ranked second in predicting polarities of complete messages, was
proposed by \citet{Severyn:15}.  In their approach, the authors first
unsupervisedly learned neural word embeddings from a big unannotated
Twitter corpus.  These vector representations were then fine-tuned to
the sentiment analysis task on the dataset of \citet{Go:09}.  Finally,
the resulting embeddings were passed as input to a two-layer
convolutional deep neural network and jointly trained on the official
SemEval training set.

A much harder subtask in this run, on which only three of seven
participants could outperform the majority class baseline, turned out
to be the classification of message polarities towards specific topics
(track C).  The winner of this subtask -- a system proposed by
\citet{Boag:15} -- attained a macro-averaged $F1$-score of 50.51\%.
This approach again relied on a supervised SVM classifier and used an
extensive set of hand-crafted features, including raw and normalized
bag-of-words (BOW), the number of positive and negative emoticons, the
number of question and exclamation marks, average and maximum
sentiment scores of single words occurring in a tweet, and many
others.  The BOW features along with the lexicon-derived information
were found to be the most useful classification attributes in these
experiments.  A dedicated preprocessing pipeline with spelling
correction and Twitter-specific tokenization also proved to have a
positive effect on the final results.

As can be seen from the above, even despite its relatively short
history, sentiment analysis of Twitter has already received much of
attention in the NLP community.  A notable fact, however, is that,
with a few exceptions \cite{Basile:13,Araque:15,Cesteros:15}, most of
the works on opinion mining in microblogs were primarily dealing with
either English data only or with English translations of foreign
language tweets.  In the subsequent sections of this chapter, we are
going to investigate whether preliminary findings that have already
been obtained for English (the inherent difficulty of the Twitter
domain for automatic SA methods, the crucial utility of lexicon
features, the necessity of preprocessing the input text before passing
it to a classifier) are equally applicable to German messages too.

%% However, in order to prove or disprove these hypotheses, a vital
%% prerequisite that we need is the existence of a substantial labeled
%% corpus, on which these conjectures could be checked.  Since there were
%% no moderately-sized manually annotated sentiment data for German that
%% we were aware of at the time of writing this thesis, we had to create
%% our own dataset, which we will introduce in the next section of this
%% chapter.

%% After describing the data gathering procedure used to compile our
%% corpus, detailing its annotation scheme, and analyzing the inter-rater
%% agreement, we turn our attention to the evaluation and analysis of
%% existing sentiment resources used for German.  To do so, we first
%% evaluate three German polar lexica which have been semi-automatically
%% obtained from English sources.  We estimate the precision and recall
%% of these dictionaries on our data and also check whether current
%% state-of-the-art approaches for generating such lexica without
%% translation would produce better results.

%% After determining the best-performing lexicon, we look how beneficial
%% this polarity information might be to fine-grained SA objectives, such
%% as determining sources, targets, and the actual spans of subjective
%% opinions in microblogs.  We also evaluate the effects of text
%% preprocessing on this task. Finally, we summarize our findings and
%% draw further conclusions in Section \ref{sec:snt:concl} of this
%% chapter.

\newpage

%% \citet{Sarker:15}, \citet{Dalmia:15}, \citet{Han:15},
%% \citet{Chikersal:15}, \citet{Hagen:15}, \citet{Fernandez:15}

%% Works attempting a more fine-grained sentiment analysis on Twitter
%% usually try to derive a common polarity class for each message with
%% respect to a particular target that is mentioned in that microblog.

%% \citet{Jiang:11}, for instance, tried to classify the polarity of
%% microblogs pertaining to a predefined set of specific topics, like
%% \emph{Obama}, \emph{Google}, \emph{iPad} etc.  To this end, the
%% authors manually labeled a corpus of 1,939 messages and trained a
%% binary SVM model in order to predict the subjectivity and the polarity
%% of the tweets with respect to the given subjects.

%% This classifier could achieve an accuracy of 68.2\% for the
%% subjectivity classification and 85.6\% for the polarity prediction.
%% The $F$-score of this system for the latter task could further be
%% improved from 66\% to 68.3\% by incorporating the information about
%% the predicted polarity class of the re-tweets, replies, and other
%% microblogs posted by the same author.

%% \citet{Mitchell:13} broadened the set of possible targets by allowing
%% any named entities found in microblogs to be associated with a
%% specific polarity.  For that purpose, the authors combined a CRF-based
%% NER system with a sentiment predicting CRF by considering three
%% different possibilities of such combination: a pipeline approach, a
%% joint multi-layer model, and a single classifier with a combined
%% tagset.  The best scores on their corpora of 7,105 Spanish and 2,350
%% English tweets could be achieved with the joint and pipeline
%% approaches.  The accuracy of recognizing the opinionated named
%% entities amounted to 31\% for Spanish and 30.4\% for English.

%% Other notable works in this direction include \citet{Chunping:14} who
%% first applied a Na\"{\i}ve Bayes classifier to predict the
%% subjectivity class of microblogs and then sequentially used two CRF
%% models to predict the particular type of subjectivity (such as anger,
%% fear, happiness etc.) for message sentences.

%% Other notable works in this direction include \citet{Dong:14} who used
%% a recurrent neural network to predict the polarity class associated
%% with the opinion targets.  They, however, assumed the targets of
%% sentiments to be apriori known and only were interested whether a
%% positive or a negative judgement was made about them.

%% The \texttt{SentiStrength} system proposed by \cite{Thelwall:12} used
%% an extensive list of 763 polar terms in order to predict positive and
%% negative scores for MySpace comments.  The manually assigned scores of
%% these terms were automatically fine-tuned during training using a
%% perceptron-like technique.  In addition to the core lexicon, the final
%% implementation of this system also utilized a set of heuristic methods
%% and auxiliary modules such as spelling correction algorithm,
%% dictionaries of booster words and negations as well as special rules
%% for emoticons, repeated letters, and exclamation marks.  It correctly
%% predicted positive emotions in 60.6~\% of the cases and attained
%% 73.5~\% accuracy at predicting negative sentiment scores.  All
%% predictions were made at the level of complete messages.

%% From its very onset in 2006, Twitter has constantly attracted the
%% attention of many NLP-practitioners and scientists, with sentiment
%% researchers being arguably one of the most active of these groups.
%% This trend seems ever exapanding in recent years: For instance, in
%% 2015, a whole section of the international ACL Workshop on
%% Computational Approaches to Subjectivity, Sentiment and Social Media
%% Analysis (WASSA 2015) was solely dedicated to the peculiarities of the
%% opinion mining on Twitter.  Sentiment detection on Twitter also
%% remains an active track of the SegEval Shared Task TODO: cite Nakov.
%% But despite its great popularity, most of this work on this topic
%% still concentrates on English data only.
