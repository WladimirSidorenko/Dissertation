% FILE: sentiment_fgsa.tex  Version 0.0.1
% AUTHOR: Uladzimir Sidarenka

% This is a modified version of the file main.tex developed by the
% University Duisburg-Essen, Duisburg, AG Prof. Dr. Günter Törner
% Verena Gondek, Andy Braune, Henning Kerstan Fachbereich Mathematik
% Lotharstr. 65., 47057 Duisburg entstanden im Rahmen des
% DFG-Projektes DissOnlineTutor in Zusammenarbeit mit der
% Humboldt-Universitaet zu Berlin AG Elektronisches Publizieren Joanna
% Rycko und der DNB - Deutsche Nationalbibliothek

\chapter{Fine-Grained Sentiment Analysis}\label{sec:snt:fgsa}

Even though polar lexicons play a crucial role in opinion-mining
research, they still only serve as a building block for achieving more
challenging and more sophisticated objectives.  One of the most
prominent such objectives is that of fine-grained sentiment analysis
(FGSA), which deals with the identification of subjective evaluative
opinions (\emph{sentiments}), the holders of these opinions
(\emph{sources}), and their respectively evaluated objects
(\emph{targets}) in text.  Since an accurate automatic prediction of
these elements would enable people to track public attitude to
literally any object (e.g., a product, service, or political
decision), FGSA is commonly regarded to be one of the most attractive,
necessary, but, unfortunately, also challenging goals in computational
linguistics.

Researchers usually consider this objective as a sequence labeling
(SL) task, and address it with either of the two popular SL
techniques: conditional random fields (CRFs) or recurrent neural
networks (RNNs).  The former approach represents a discriminative
probabilistic graphical framework, which relies on an extensive set of
hand-crafted features; the latter model utilizes a recursive
computational loop, which can learn feature representations completely
automatically.  In this section, we are going to evaluate each of
these methods in detail in order to find out which of these algorithms
is better suited for the domain of German Twitter.  However, before we
proceed with our evaluation, we should first make a short linguistic
digression and briefly discuss the definition of textual spans, to
which these approaches should assign their labels, and the evaluation
metrics, with which we will estimate the quality of this assignment.

\section{Definition of Sentiment, Target, and Source Spans}
Despite some notable advances and an ongoing active research on
fine-grained opinion extraction, the crucial task of defining the
exact boundaries of sentiment spans and the spans of their respective
targets and sources has not been addressed in the literature with the
due attention yet.  Researchers typically overlook this problem,
leaving its solution to the discretion of their annotators
\cite[cf.][]{Wiebe:05,Klinger:13}.

In contrast to these works, instead of relying on rather intuitive
decisions of our coders, we explicitly provided a rule for determining
opinions' boundaries by telling the experts to assign the
\textsc{sentiment} label to ``\emph{minimal complete syntactic or
  discourse-level units that include both the target of an opinion and
  its actual evaluation}''.

% According to this instruction, during the annotation, linguists first
% had to identify evaluated objects (targets) in text, then find the
% respective evaluative expressions of these objects (usually but not
% necessarily polar terms), and, finally, determine the smallest
% syntactic components (typically noun or verb phrases) or discourse
% units (clauses or sentences) where both of these entities appeared
% together.

A sample annotation analyzed in compliance with this rule is shown in
Example~\ref{snt:fgsa:exmp:sent-anno1}:
\begin{example}[Annotation of a Sentiment Span]\label{snt:fgsa:exmp:sent-anno1}
  \upshape\sentiment{Der neue Papst gilt als
    bescheidener, zur\"uckgenommener Typ.}\\[0.8em]
  \noindent\sentiment{The new Pope is believed to be a sober, modest
    man.}
\end{example}
\noindent In this sentence, an expert had to label the complete clause
as a sentiment, since this unit was the minimal syntactic constituent
which included both the object of the evaluation---``der neue Papst''
(\textit{the new pope})---and the evaluation itself---``bescheidener,
zur\"uckgenommener Typ'' (\textit{a sober, modest man}).

We applied the same principles of minimality and completeness to the
annotation of targets and sources, requiring the main components of
these elements (typically nouns or verbs) to be labeled along with all
their syntactic dependents.  Accordingly, the correct annotation of
the target in the previous example had to look as follows:
\begin{example}[Annotation of a Target Span]\label{snt:fgsa:exmp:sent-anno2}
  \upshape\sentiment{\target{Der neue Papst} gilt als
    bescheidener, zur\"uckgenommener Typ.}\\[0.8em]
  \noindent\sentiment{\target{The new Pope} is believed to be a sober,
    modest man.}
\end{example}
\noindent with the \textsc{target} span assigned to the whole noun
phrase---``der neue Papst'' (\textit{the new pope})---and not only its
main word.

Similarly, source elements had to cover complete syntactic structures
as shown in Example~\ref{snt:fgsa:exmp:src-anno1}:
\begin{example}[Annotation of a Source Span]\label{snt:fgsa:exmp:src-anno1}
  \upshape\sentiment{Die Homosexuellenehe war f\"ur \source{den Kardinal, der jetzt Papst ist,} eine Zerst\"orung von Gottes Plan}\\[0.8em]
  \noindent\sentiment{For \source{the cardinal, who is the Pope now,}
    the same-sex marriage was a destruction of God's plan.}
\end{example}
\noindent This time, again, the whole noun phrase including the
dependent attributive clause---``den Kardinal, der jetzt Papst ist,''
(\textit{the cardinal, who is the Pope now,})---had to be labeled with
the \textsc{source} tag because this constituent was the only
\emph{minimal complete} syntactic node which encompassed both the
immediate holder of the opinion---``Kardinal'' \textit{cardinal}---and
its grammatical dependents, without including any of its parental
elements.

\section{Evaluation Metrics}
The next question which naturally arose after defining the span
boundaries for human coders was that of the best way to compare these
spans with automatically assigned labels.  One possibility to estimate
the quality of such automatic assignment was to compute the precision,
recall, and \F{}-scores using either the binary overlap or exact match
metrics \cite[cf.][]{Choi:06,Breck:07} .  The former method considers
an automatically labeled span as correct if it has at least one token
in common with a labeled entity from the gold annotation.  The latter
metric only regards as true positives those automatic spans which have
absolutely identical boundaries with the expert's assignment.
Unfortunately, both of these approaches are problematic to some
extent: While the binary overlap might be overly optimistic, always
assigning perfect scores to automatic spans which cover the whole
sentence; the exact match metric might, vice versa, be too drastic,
considering the whole assignment as false if only one (possibly
irrelevant) token is missing.

Instead of relying on these measures, we opted for the ``golden mean''
solution to this problem that was proposed by \citet{Johansson:10a}.
In their work, the authors introduced another way of estimating the
quality of automatic label assignments, in which they penalized the
predicted spans proportionally to the number of tokens whose labels
were different from the gold annotation.  More precisely, given a set
of manually annotated entities $\mathcal{S}$ and automatically tagged
spans $\widehat{\mathcal{S}}$, they estimated the precision of an
automatic assignment as:
\begin{equation}\label{eq:fgsa:jmmetric}
  P(\mathcal{S}, \widehat{\mathcal{S}}) = \frac{C(\mathcal{S},
    \widehat{\mathcal{S}})}{|\widehat{\mathcal{S}}|},
\end{equation}
where $C(\mathcal{S},\widehat{\mathcal{S}})$ stands for the \emph{span
  coverage} metric, which is computed as the proportion of overlapping
tokens across all pairs of manually ($s_i$) and automatically ($s_j$)
annotated entities:
\begin{equation*}
  C(\mathcal{S}, \widehat{\mathcal{S}}) = \sum_{s_i \in
    \mathcal{S}}\sum_{s_j \in \widehat{\mathcal{S}}}c(s_i, s_j),
\end{equation*}
and the $|\widehat{\mathcal{S}}|$ term denotes the total number of
spans automatically labeled with the given tag.  Similarly to that,
the recall of the assignment is estimated as:
\begin{equation*}
  R(\mathcal{S}, \widehat{\mathcal{S}}) = \frac{C(\mathcal{S},
    \widehat{\mathcal{S}})}{|\mathcal{S}|},
\end{equation*}
and the \F{}-measure is normally computed as the harmonic mean of the
precision and recall scores:
\begin{equation*}
  F_1 = 2\times\frac{P \times R}{P + R}.
\end{equation*}

Since this proportional estimation could adequately accommodate both
extrema of an automatic annotation---too long and too short
spans---and also penalized for erroneous labels, we decided to use
this measure throughout our subsequent experiments.

\section{Data Preparation}\label{snt:fgsa:subsec:data}

In order to evaluate the CRF and RNN approaches on our data set, we
split the whole corpus into three parts, using 70\% of it as training
data, 10\% as a development set, and the remaining 20\% as a test
corpus.  We tokenized all tweets with an adjusted version of the
Christopher Potts'
tokenizer,\footnote{\url{http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py}}
and preprocessed them using the rule-based normalization technique of
\citet{Sidarenka:13}.
%% During the normalization, Twitter-specific phenomena like @-mentions,
%% retweets, and URIs that were not syntactically integrated in any
%% sentence of the message were removed from the tweets and those
%% elements which played an integral syntactic role were replaced with
%% the special artificial tokens \%User, \%Link etc.  Emoticons like :-),
%% \smiley{}, \frownie{} etc. were also replaced with the placeholders
%% \%PosSmiley, \%NegSmiley, or simply \%Smiley depending on their prior
%% polarity.  Furthemore, out-of-vocabulary words which could be
%% converted to in-vocabulary terms with a pre-defined set of
%% transformations were also normalized.
Afterwards, we labeled the preprocessed data with their part-of-speech
tags using \texttt{TreeTagger}\footnote{We used \texttt{TreeTagger}
  Version 3.2 with the German parameter file UTF-8.} \cite{Schmid:95},
and parsed the resulting sentences with the \texttt{Mate} dependency
parser\footnote{We used \texttt{Mate} Version \texttt{3.61} with the
  German parameter model 3.6.}  \cite{Bohnet:13}.  Finally, since
\texttt{MMAX2} did not provide a straightforward support for the
character offsets of the annotated tokens, and the automatically
tokenized data could disagree with the original corpus tokenization,
we aligned manual annotation with the automatically split words using
the Needleman-Wunsch alignment algorithm~\cite{Needleman:70}.

\section{Fine-Grained Sentiment Analysis with Conditional Random
  Fields}

The first method that we evaluated on the obtained data was that of
the conditional random fields.  First introduced by
\citet{Lafferty:01}, CRFs had rapidly grown in popularity, turning
into one of the most commonly used probabilistic frameworks, which had
been dominating the NLP field for more than a decade.

The main reasons for the huge success of this model are:
\begin{enumerate}[1)]
\item the \emph{structural nature} of CRFs, which, in contrast to
  single-entity classifiers such as logistic regression or SVM, make
  their predictions over a sequence of covariates, trying to find the
  most likely label assignment for the whole input chain and not only
  its individual elements;
\item the \emph{discriminative power} of this framework, which, in
  contrast to generative probabilistic models such as HMM
  \cite{Rabiner:86}, optimizes the conditional probability
  $P(\boldsymbol{Y}|\boldsymbol{X})$ instead of maximizing the joint
  distribution $P(\boldsymbol{X},\boldsymbol{Y})$ and consequently can
  efficiently deal with overlapping and correlated features;
\begin{example}[Overlapping and Correlated Features]
  In order to demonstrate the different effects of correlated and
  overlapping features on generative and discriminative models, let us
  go through an example where we need to predict whether a tweet
  mentioning ``Merkel'' and ``Steinmeier'' is about the Christian
  Democratic Union (\texttt{CDU}) or Social Democratic Party of
  Germany (\texttt{SPD}).

  As features for this task, we will use lexical unigrams appearing in
  the training data.  Assuming that our training set consists of three
  messages mentioning ``Merkel'' and one microblog mentioning
  ``Steinmeier'' which are labeled as \texttt{CDU}, plus one tweet
  mentioning ``Merkel'' and three posts mentioning ``Steinmeier''
  which are annotated as \texttt{SPD}, the generative Na\"{i}ve Bayes
  model would estimate the probability of the two competing classes
  as:
  \begin{align*}
    P(\mathbf{x}, CDU) =& P(\textrm{Merkel},\textrm{Steinmeier}|CDU)\times P(CDU)\\
    =& P(\textrm{Merkel}|CDU)\times P(\textrm{Steinmeier}|CDU) \times P(CDU)\\
    =&\frac{3}{4}\times\frac{1}{4}\times\frac{4}{8}\approx 0.0938\\
    P(\mathbf{x}, SPD) =& P(\textrm{Merkel},\textrm{Steinmeier}|SPD)\times P(SPD)\\
    =& P(\textrm{Merkel}|SPD)\times P(\textrm{Steinmeier}|SPD) \times P(SPD)\\
    =&\frac{1}{4}\times\frac{3}{4}\times\frac{4}{8}\approx 0.0938.\\
  \end{align*}
  After normalizing these probabilities, we would get equal 50\%
  chances for each of the parties, which is fair regarding the token
  distribution in our corpus.  However, if we replace ``Merkel'' with
  ``von der Leyen'' both in the training data and test example and
  rerun this experiment once again, the probability would get
  significantly skewed towards the CDU class:
  \begin{align*}
    P(\mathbf{x}, CDU) =& P(\textrm{von},\textrm{der},\textrm{Leyen},\textrm{Steinmeier}|CDU)\times P(CDU)\\
    =& P(\textrm{von}|CDU)\times P(\textrm{der}|CDU)\times P(\textrm{Leyen}|CDU)\\
    &\times P(\textrm{Steinmeier}|CDU) \times P(CDU)\\
    =&\frac{3}{4}\times\frac{3}{4}\times\frac{3}{4}\times\frac{1}{4}\times\frac{4}{8}\approx 0.0527\\
    P(\mathbf{x}, SPD) =& P(\textrm{von},\textrm{der},\textrm{Leyen},\textrm{Steinmeier}|SPD)\times P(SPD)\\
    =& P(\textrm{von}|SPD)\times P(\textrm{der}|SPD)\times P(\textrm{Leyen}|SPD)\\
    &\times P(\textrm{Steinmeier}|SPD) \times P(SPD)\\
    =&\frac{1}{4}\times\frac{1}{4}\times\frac{1}{4}\times\frac{3}{4}\times\frac{4}{8}\approx 0.0059,\\
  \end{align*}
  which, after normalization, would result in 90\% chances for
  \texttt{CDU}, and a 10\% score for \texttt{SPD}, even though we only
  changed the name of the politician.

  A different situation can be observed for discriminative models such
  as maximum entropy classifier: Instead of optimizing the joint
  distribution $P(\mathbf{x}, y)$ as it is done by generative
  frameworks, discriminative systems seek to optimize the conditional
  likelihood $P(y|\mathbf{x})$ by maximizing the total probability of
  the training set $\sum_{i=1}^N\log P(y_i|\mathbf{x}_i, \mathbf{w})$.
  This probability is usually estimated using the sigmoid function
  $\frac{1}{1 + e^{-(\mathbf{x}_i, \mathbf{w})}}$, where
  $\mathbf{x}_i$ denotes the input features of the $i$-th training
  instance, and the vector $\mathbf{w}$ stands for the respective
  weights of these features.  By optimizing this function using
  gradient descent, we will arrive at the optimal solution
  $w_1 \approx 0.5$ for the feature ``Merkel'' and $w_2 \approx -0.5$
  for the feature ``Steinmeier'' for the first example, which would
  again result in equal 50\% chances for both classes.  In the second
  example, however, all three features ``von'', ``der'', and ``Leyen''
  would get an equal weight of $\approx 0.3$, and the ``Steinmeier''
  feature would receive a coefficient of $\approx -0.4$, which would
  result in 60\% probability for the test message being about the CDU,
  and 40\% that the tweet is about the SPD.  Even though this still
  means a slight skewness towards \texttt{CDU}; this time, the effect
  of correlated features is much less dramatic than in the generative
  case.
\end{example}
\item and, finally, the \emph{avoidance of the label bias problem},
  which other discriminative classifiers, such as maximum entropy
  Markov networks \cite{McCallum:00}, are known to be susceptible to.
  \begin{example}[Label Bias Problem]
    The label bias problem arises in the cases where a locally optimal
    decision outweighs globally superior solutions.  Consider, for
    example, the sentence ``Aber gerade Erwachsene haben damit
    Schwierigkeiten.'' (\textit{But especially adults have
      difficulties with it.}), for which we need to compute the most
    probable sequence of part-of-speech tags.

    \begin{center}
      \begin{tikzpicture}[node distance=5cm]
        \tikzstyle{tag}=[circle split,draw=gray!50,%
          minimum size=2.5em,inner ysep=2,inner xsep=0,%
          circle split part fill={yellow!20,blue!30}]
      \tikzstyle{word}=[draw=none,inner sep=10pt]

      \node[word] (A) at (1, 1) {Aber};
      \node[tag] (B) at (1, 3) {\footnotesize KON \nodepart{lower} 1.};
      \node[word] (D) at (3, 1) {gerade};
      \node[tag] (E) at (3, 2) {\footnotesize ADJA \nodepart{lower} .5};
      \node[tag] (F) at (3, 4) {\footnotesize ADV \nodepart{lower} .5} ;
      \node[word] (G) at (7, 1) {Erwachsene};
      \node[tag] (I) at (7,2) {\footnotesize ADJA \nodepart{lower} .5} ;
      \node[tag] (H) at (7,4) {\footnotesize NN \nodepart{lower} .5};
      \node[word] (J) at (9,1) {haben};
      \node[tag] (K) at (9,3) {\footnotesize VA \nodepart{lower}\small 1.};
      \node[word] (J) at (11,1) {\ldots};

      \path [-] (B) edge node[below] {$.5$} (E);
      \path [-] (B) edge node[above] {$.5$} (F);

      \path [-] (E) edge node[below] {$.3$} (I);
      \path [-] (E) edge node[below left=0.4] {$.7$} (H);
      \path [-] (F) edge node[above left=0.4] {$.8$} (I);
      \path [-] (F) edge node[above] {$.2$} (H);

      \path [-] (I) edge node[below] {$.1$} (K);
      \path [-] (H) edge node[above] {$.9$} (K);
    \end{tikzpicture}
    \captionof{figure}{\emph{Feature weights for states and
        transitions of the part-of-speech example.}\label{fig:snt:memm-crf}}
    \end{center}
    Assuming that feature weights are distributed as shown in
    Figure~\ref{fig:snt:memm-crf}, we will first estimate the
    probability of the correct label sequence for the initial part of
    this sentence using the Maximum Entropy Markov Model (MEMM)---the
    predecessor of the conditional random fields.  According to the
    MEMM's definition, the probability of the correct labeling
    ($KON-ADV-NN-VA$) is equal to:
    \begin{align*}
      P(KON, ADV, NN, VA) &= P(KON)\times P(ADV|KON)\\
      &\times P(NN|ADV)\times P(VA|NN)\\
      &=\frac{\exp(1)}{\exp(1)}\times\frac{\exp(0.5 + 0.5)}{\exp(0.5 + 0.5) + \exp(0.5 + 0.5)}\\%
      &\times\frac{\exp(0.2 + 0.5)}{\exp(0.2 + 0.5) + \exp(0.8 + 0.5)}\times\frac{\exp(0.9 + 1.)}{\exp(0.9 + 1.)} \approx 0.177
    \end{align*}
    At the same time, the probability of the incorrect variant
    ($KON-ADV-ADJA-VA$) amounts to $\approx$ 0.323 and will therefore
    be preferred during automatic tagging.

    A different situation is observed with the CRFs, where the
    normalizing factor in the denominator is computed over the whole
    input sequence without factorizing into individual terms for each
    transition as it is done in the MEMM case.  That way, the
    probability of the correct labels would run up to:
    \begin{align*}
      P(KON, ADV, NN, VA) =& P(KON)\times
      P(ADV|KON)\times P(NN|ADV)\\
      &\times P(VA|NN)\\ =&\frac{\exp(1 + 0.5
        \times 3 + 0.2 + 0.9 + 1)}{Z} \approx 0.252,
    \end{align*}
    where
    $Z = \exp(1 + 0.5 \times 3 + 0.2 + 0.9 + 1) + \exp(1 + 0.5 \times
    3 + 0.8 + 0.1 + 1) + \exp(1 + 0.5 \times 3 + 0.7 + 0.9 + 1) +
    \exp(1 + 0.5 \times 3 + 0.3 + 0.1 + 1)$ is the total score of all
    possible label assignments.  The incorrect alternative
    ($KON-ADV-ADJA-VA$), however, would get a probability score of
    $\approx$ 0.207, which is less than the likelihood of the correct
    labeling.
  \end{example}
\end{enumerate}
\textbf{Training} CRFs get these useful properties thanks to a neatly
formulated objective function in which they seek to optimize the
global log-likelihood of the gold labels $\mathbf{Y}$ conditioned on
the training data $\mathbf{X}$.  In particular, given a set of
training instances
$\mathcal{D} = \{(\mathbf{x}^{(n)}, \mathbf{y}^{(n)})\}_{n=1}^N$,
where $\mathbf{x}^{(n)}$ stands for the input variables of the $n$-th
instance, and $\mathbf{y}^{(n)}$ denotes its respective gold labels,
CRF's training adds up to finding feature coefficients $\mathbf{w}$
which maximize the log-probabilities $\ell$ of $\mathbf{y}^{(i)}$
given their covariates $\mathbf{x}^{(i)}$ over the whole corpus:
\begin{equation}\label{snt:fgsa:eq:crf-w}
  \mathbf{w} = \argmax_{\mathbf{w}}\sum_{n=1}^N\ell
  \left(\mathbf{y}^{(n)}|\mathbf{x}^{(n)}\right).
\end{equation}
The likelihood term $\ell(\mathbf{y}^{(n)}|\mathbf{x}^{(n)})$ in this
equation is commonly estimated using a globally normalized softmax
function:
\begin{equation}\label{snt:fgsa:eq:crf-ell}
  \ell\left(\mathbf{y}^{(n)}|\mathbf{x}^{(n)}\right) =
  \ln\left(P(\mathbf{y}^{(n)}|\mathbf{x}^{(n)})\right) =
  \ln\left(\frac{ \exp\left(\sum_{m=1}^{M}\sum_jw_{j} \cdot f_j(x_{m},
    y_{m-1}, y_{m})\right)}{Z}\right),
\end{equation}
where $M$ stands for the length of the $n$-th training instance,
$f_j(x_{m}, y_{m-1}, y_{m})$ denotes the value of the $j$-th feature
function $f$ at the sequence position $m$, $w_j$ represents the
corresponding weight of this feature, and $Z$ is a normalization
factor calculated over all possible label assignments:
\begin{equation}
  Z =
  \sum_{y'\in\mathcal{Y},y''\in\mathcal{Y}}\exp\left(\sum_{m=1}^{M}\sum_jw_{j}
  \times f_j(x_{m}, y'_{m-1}, y''_{m})\right).
\end{equation}
Since this normalizing term appears in the denominator, and couples
together all feature weights that need to be optimized, it becomes
prohibitively expensive to find the best solution to
Equation~\ref{snt:fgsa:eq:crf-w} analytically with a single shot.  A
possible remedy to this problem is to resort to other optimization
techniques such as gradient descent, in which the weights of features
are successively changed towards the direction of the gradient until
the global minimum of the loss function is reached.

From Equation~\ref{snt:fgsa:eq:crf-ell}, we can see that the partial
derivative of the log-likelihood function $\ell$ w.r.t. a single
feature weight $w_j$ amounts to the following solution:
\begin{equation}
  \frac{\partial}{\partial w_j}\ell =%
  \sum_{n=1}^N\sum_{m=1}^Mf_j(x_{m}, y_{m-1}, y_{m}) -%
  \sum_{n=1}^N\sum_{m=1}^{M}\sum_{y'\in\mathcal{Y},y''\in\mathcal{Y}}f_j(x_{m},%
  y'_{m-1}, y''_{m})P(y',y''|\mathbf{x}^{(n)}),
\end{equation}
which, after dividing both parts of the equation with the constant
term $N$---the size of the corpus---can in turn be transformed into:
\begin{equation}
  \frac{1}{N}\frac{\partial}{\partial w_j}\ell = \E[f_j(\mathbf{x},
  \mathbf{y})] - \E_{\mathbf{w}}[f_j(\mathbf{x}, \mathbf{y})],
\end{equation}
where the first term ($\E[f_j(\mathbf{x}, \mathbf{y})]$) is the
expectation of the feature $f_j$ under the empirical distribution, and
the second term ($\E_{\mathbf{w}}[f_j(\mathbf{x}, \mathbf{y})]$) is
the same expectation under the model's parameters $\mathbf{w}$.  That
way, the optimal solution to the log-likelihood objective in
Equation~\ref{snt:fgsa:eq:crf-ell} is the one, where the model's
expectation of the features matches their (true) empirical expectation
in the corpus.

The marginal probabilities of the features, which are required for
computing their expectation, can be estimated dynamically using the
forward-backward (FB) algorithm \cite{Rabiner:90}, which is a
particular case of the more general belief propagation method
\cite[cf.][p.~81]{Barber:12}.

\noindent\textbf{Inference} Once the optimal feature weights are
learned, one can unproblematically compute the most likely label
assignment for a new instance by using the Viterbi algorithm
\cite{Viterbi:67}, which effectively corresponds to the forward pass
of the FB method with the summation over the alternative preceding
states replaced by the maximum operator (hence the other name for this
algorithm---``max-product'').

\noindent\textbf{Features} A crucial component which accounts for a
huge part of the success (or failure) of a CRF system are feature
attributes which are defined by its developer.

Traditionally, feature functions used in CRFs are divided into
transition- and state-based ones.  The former attributes represent
real- or binary-valued functions
$f(\mathbf{x}, y'', y')\rightarrow\mathbb{R}$ associated with some
data predicate $\phi(\mathbf{x})\rightarrow\mathbb{R}$ and the labels
$y''$ and $y'$.  The value of such attribute at position $m$ in
sequence $\mathbf{x}$ is usually defined as follows:
\begin{equation}
  f(\mathbf{x}_m, y'', y') = \begin{cases} \phi(\mathbf{x}_m), &
    \mbox{if } \mathbf{y}_{m-1} = y''\mbox{ and }\mathbf{y}_{m} =
    y'\\ 0, & \mbox{otherwise;}
  \end{cases}
\end{equation}
where the predicate~$\phi$ typically represents a simple unit
function: $\phi(\mathbf{x}_m)\mapsto 1$, $\forall\mathbf{x}_m$.

In contrast to the ternary transition features, state attributes are
associated with binary predicates, whose output depends on the input
data at the given position and the label $y'$ at the respective state:
\begin{equation}
  f(\mathbf{x}_m, y') = \begin{cases} \phi(\mathbf{x}_m), & \mbox{if }
    \mathbf{y}_{m} = y'\\ 0, & \mbox{otherwise.}
  \end{cases}
\end{equation}
This time, the predicate~$\phi$ is usually much more sophisticated, as
it reflects various properties of the input sequence at the respective
position, such as whether the current token is capitalized or whether
it begins with a specific prefix or ends with a specific suffix.  This
type of features commonly accounts for the overwhelming majority of
all attributes used in a CRF system.

As state attributes in our experiments, we used the following types of
predicates (which, for simplicity, are listed here in groups):
\begin{itemize}
\item\emph{formal}, which included the initial three characters of
  each token (e.g., $\phi_{abc}(\mathbf{x}_m) = 1\mbox{ if
  }\mathbf{x}_m\sim\mbox{ /\textasciicircum abc/ else } 0$), its last
  three characters, and the general spelling class of the token (e.g.,
  alphanumeric, digit, or punctuation);

\item\emph{morphological}, which encompassed the part-of-speech tags
  of the analyzed tokens as well as case and gender values for
  inflectable PoS types, the degree of comparison for adjectives, and
  mood, tense, and person forms for verbs;

\item\emph{lexical}, which comprised the actual lemma and form of the
  tokens (using one-hot encoding), their polarity classes (positive,
  negative, or neutral), which we obtained from the Zurich Polarity
  Lexicon~\cite{Clematide:10};

\item and, finally, \emph{syntactic}, which included the dependency
  relation via which token $x_m$ was connected to its parent; two
  binary features reflecting whether the previous token in the sentence
  was the parent or child of the current word; as well as two other
  features, one of which encoded the dependency relation of the
  previous token in the sentence to its parent + dependency relation
  of the current token to its ancestor, and the other reflected the
  dependency link of the next token + dependency relation of the
  current token to its parent.
\end{itemize}

In addition to the above attributes, we also used a set of complex
lexico-syntactic features, which simultaneously combined several
semantic and syntactic traits.  These included:
\begin{itemize}
\item the lemma of the syntactic parent;
\item the part-of-speech tag and the polarity class of the
  grandparent;
\item the lemma of the child node + dependency relation connecting the
  parent with this child;
\item the PoS tag of the child node + its dependency relation + PoS
  tag of the current token;
\item the lemma of the child node + its dependency relation + lemma of
  the current token;
\item the overall polarity of the children, which was computed by
  summing up the polarity scores of all immediate syntactic
  descendants, and checking whether the resulting value was greater,
  less than or equal to zero.\footnote{We again used the Zurich
    Polarity Lexicon of~\citet{Clematide:10} for computing these
    scores.}
\end{itemize}

\textbf{Results} The results of our experiments are shown in
Table~\ref{snt-fgsa:tbl:crf-res}.  As we can see from the table, with
the given set of features, the model can perfectly well fit the
training data, achieving a macro-averaged \F-score of~0.904.  The
learned parameters, however, only partially generalize to unseen
tweets, leading to notably lower \F-results on the test corpus
(0.287).  This disbalance indicates a strong ``overfitting'' of the
model to the training data (i.e., the assignment of unreasonably high
weights to rather sporadic, noisy features, which only accidentally
co-occurred with the target classes in the corpus).

Another notable tendency, which can be observed both on the training
and test splits, is that the recall of the CRF system is generally
lower than its precision.  This again can be attributed to the
overfitting effect, due to which, less indicative features become more
important than attributes which actually give rise to subjective
evaluations.  Since the former features might not to appear in the
test data or, even if they do, are unlikely to correlate with the
sentiment entities, the model often fails to recognize sentiments in
new contexts which do have important but underweight traits.

\begin{table*}
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{1}{>{\centering\arraybackslash}p{0.136\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Data Set} & \multicolumn{3}{c}{\bfseries Sentiment} & %
      \multicolumn{3}{c}{\bfseries Source} & %
      \multicolumn{3}{c}{\bfseries Target} & %
      \multirow{2}{0.136\columnwidth}{\bfseries\centering Macro\newline \F{}}\\\cline{2-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} &\\\midrule

      Training Set & 0.949 & 0.908 & 0.928 & 0.903 & 0.87 & 0.886 & %
      0.933 & 0.865 & 0.898 & 0.904\\
      Test Set & 0.37 & 0.28 & 0.319 & 0.305 & 0.244 & 0.271 & 0.304 & %
      0.244 & 0.271 & 0.287\\\bottomrule
    \end{tabular}
    \egroup
    \caption{Results of fine-grained sentiment analysis with the
      first-order linear-chain CRFs.}
    \label{snt-fgsa:tbl:crf-res}
  \end{center}
\end{table*}

\section{Fine-grained Sentiment Analysis with Recurrent Neural
  Networks}

A competitive alternative to the conditional random fields are deep
recurrent neural networks (RNNs).  Introduced in the
mid-nineties~\cite{Hochreiter:97}, RNNs have become one of the most
popular trends in the recent surge of deep learning research, showing
superior performance on many important NLP tasks including
part-of-speech tagging~\cite{Wang:15:pos}, dependency
parsing~\cite{Kiperwasser:16a}, machine
translation~\cite{Kalchbrenner:13,Bahdanau:14,Sutskever:14} etc.  The
key factors which account for this success are
\begin{enumerate}[1)]
\item \emph{the ability of these systems to learn optimal feature
    representations automatically}, which favorably sets them apart
  from traditional supervised machine learning frameworks such as SVMs
  or CRFs where all features need to be defined by user; and
\item \emph{the ability to deal with arbitrary sequence lengths},
  which advantageously distinguishes these approaches from other NN
  architectures such as convolutional or standard feed-forward
  networks where the size of all layers has to be constant.
\end{enumerate}

The main component which underlies any modern RNN system is a
fixed-size hidden vector $\vec{h}$, which gets recurrently updated
over the input sequence $\mathbf{x}$, and is supposed to encode the
meaning of the sequence seen so far.  The general form of this vector
at input position $t$ is usually defined as:
\begin{align}
  \vec{h}^{(t)} = f(\vec{h}^{(t-1)}, \mathbf{x}^{(t)});
\end{align}
where $f$ represents some pointwise non-linear transformation
function, $\vec{h}^{(t-1)}$ denotes the state of the hidden vector at
the previous time step, and $\mathbf{x}^{(t)}$ is the input vector at
position $t$.

\textbf{LSTM} A fundamental problem which arises from the above
formula is that the gradients of the trained parameters (the ones
involved in computing the $\vec{h}$ vector), rapidly vanish to zero or
explode to infinity (depending on whether the absolute values of
$\vec{h}$ are less or greater than one) as the length of the input
sequence increases.  In order to solve this issue,
\citet{Hochreiter:97} proposed the long short-term memory mechanism
(LSTM), in which they explicitly incorporated the goal of dropping
parts of the input which appeared to be irrelevant for the final
outcome.  In particular, given an input sequence $\mathbf{x}$, they
introduced a special \emph{activation unit} $\vec{i}^{(t)}$:
\begin{align*}
  \vec{i}^{(t)} &= \sigma\left(W_i\cdot \mathbf{x}^{(t)} + U_i \cdot \vec{h}^{(t-1)} + \vec{b}_i\right);
\end{align*}
where $\sigma$ denotes the sigmoid function; $W_i$, $U_i$, and
$\vec{b_i}$ represent the optimized model's matrices and vector; and
$\mathbf{x}^{(t)}$ and $\vec{h}^{(t-1)}$ stand for the input and
previous hidden states respectively.  In addition to that, the authors
also estimated a dedicated \emph{forget gate}~$\vec{f}^{(t)}$:
\begin{align*}
  \vec{f}^{(t)} &= \sigma\left(W_f\cdot \mathbf{x}^{(t)} + U_f
  \cdot \vec{h}^{(t-1)} + \vec{b}_f\right),
\end{align*}
which was then used to erase parts of the previous input.

After computing an \emph{intermediate update value}
$\widetilde{c}^{(t)}$ for the current time step~$t$:
\begin{align*}
  \widetilde{c}^{(t)} &= tanh\left(W_c\cdot \mathbf{x}^{(t)} + U_c
  \cdot \vec{h}^{(t-1)} + \vec{b}_c\right),
\end{align*}
they estimated the \emph{final update} state~$\vec{c}^{(t)}$ by taking a
weighted sum of the candidate update vector~$\widetilde{c}^{(t)}$ and
the previous update value~$\vec{c}^{(t-1)}$:
\begin{align*}
  \vec{c}^{(t)} &= \vec{i}^{(t)} \odot \widetilde{c}^{(t)} + \vec{f}^{(t)} \odot \vec{c}^{(t-1)};
\end{align*}
from which, they finally computed the output vector
$\vec{o}^{(t)}$ and the new value of the hidden state $\vec{h}^{(t)}$:
\begin{align*}
  \vec{o}^{(t)} &= \sigma\left(W_o\cdot \mathbf{x}^{(t)} + U_o \cdot \vec{h}^{(t-1)} + V_o \cdot \vec{c}^{(t)} + \vec{b}_o\right),\\
  \vec{h}^{(t)} &= \vec{o}^{(t)} \odot tanh(\vec{c}^{(t)}).
\end{align*}

\textbf{GRU} Despite their enormous popularity and many successful
practical applications~\cite[cf.][]{Filippova:15,Ghosh:16,Rao:16},
LSTMs have often been criticized for the high complexity of the
recurrent unit.  In order to overcome this problem while still keeping
the gradients within an appropriate range, \citet{Cho:14a} introduced
an alternative architecture called Gated Recurrent Units (GRU).  In
this framework, the authors also used activation and forget
gates---$\vec{i}^{(t)}$ and $\vec{f}^{(t)}$---similar to the ones
defined by~\citet{Hochreiter:97}:
\begin{align*}
  \vec{i}^{(t)} &= \sigma\left(W_i\cdot \mathbf{x}^{(t)} + U_i \cdot \vec{h}^{(t-1)} + \vec{b}_i\right),\\
  \vec{f}^{(t)} &= \sigma\left(W_f\cdot \mathbf{x}^{(t)} + U_f \cdot \vec{h}^{(t-1)} + \vec{b}_f\right).
\end{align*}
The candidate activation $\widetilde{c}^{(t)}$ was then estimated as:
\begin{align*}
  \widetilde{c}^{(t)} &= tanh\left(W_c\cdot \mathbf{x}^{(t)} + U_c
  \cdot \left(\vec{f}^{(t)} \odot \vec{h}^{(t-1)}\right)  + \vec{b}_c\right),
\end{align*}
and the final hidden state $\vec{h}^{(t)}$ was computed as follows:
\begin{align*}
  \vec{h}^{(t)} &= \vec{i}^{(t)} \odot \vec{h}^{(t-1)} + \left(\vec{1} -
  \vec{i}^{(t)}\right) \odot \widetilde{c}^{(t)}.
\end{align*}
After obtaining the output of the recurrence ($\vec{o}^{(t)}$ in the
case of LSTM, and $\vec{h}^{(t)}$ in the case of GRU), we calculated
the final probability of the labels $\vec{p}^{(t)}$ by computing the
dot product of the output vector with the matrix $O$, and estimating
the softmax of this product:
\begin{align*}
  \vec{p}^{(t)} &= softmax\left(O\cdot\vec{o}^{(t)}\right).
\end{align*}

\textbf{Training} A neat property of both of these approaches is that
the final equation, which is obtained after unrolling the loop, is
differentiable with respect to all of its parameters, and can
therefore be optimized using the standard gradient update techniques.
Since most of these parameters, however, represent high-dimensional
matrices or vectors, finding an optimal learning rate (i.e., the size
of the update step taken in the direction of the gradient) might pose
considerable difficulties, leading either to prohibitively large
training times (if the steps are too small) or a complete divergence
of the trained model (if the steps are too big).

Several algorithms have been proposed for solving this problem
including the method of momentum~\cite{Rumelhart:88},
AdaGrad~\cite{Duchi:11}, AdaDelta~\cite{Zeiler:12},
RmsProp~\cite{Tieleman:12} etc.  In our RNN experiments, we chose the
last of these options---the RmsProp technique
of~\citet{Tieleman:12}---as this algorithm showed both a faster
convergence and superior classification results.

Another important factor, which could significantly affect the
training results, were the initial values of the model's parameters.
As shown by~\citet{He:15}, an inappropriate initialization of neural
network might lead to a complete stalling of the whole learning
process.  Following the recommended practices~\cite{Saxe:13}, we used
an orthogonal initialization for all linear transformation matrices,
and applied the uniform He sampling~\cite{He:15} for setting the
initial values of the bias vectors.

Finally, due to a high imbalance of the target classes in the training
set (where most of the instances represented objective statements
without any sentiment tags), we ``over-sampled'' opinionated tweets
(i.e., we randomly repeated some of the training microblogs containing
sentiments until we reached an equal proportion of subjective and
objective messages), and chose \emph{hinge-loss} as the optimized
objective function $L$:\footnote{Since most of the tokens in the
  over-sampled training set still had the \textsc{NONE} tag, the
  easiest way for a classifier to minimize the training error would be
  to always predict this tag with a very high confidence.  We hoped to
  mitigate this effect by using hinge-loss, since this objective
  function only penalizes incorrectly predicted labels or correct tags
  whose probability is insufficiently high (less than $c$), but does
  not reward any over-confident decisions.}
\begin{align}
  L &= \sum_{i}^{N}\sum_{t=0}^{\lvert\mathbf{x}_i\rvert}\max\left(0, %
  c + \max\limits_{y'\neq y}\vec{p}_{t,y'} - \vec{p}_{t,y}\right) + \alpha \norm{O}^2_2,
\end{align}
where $\vec{p}_{t,y'}$ stands for the probability of the most likely
wrong tag $y'$ at position $t$ in the training instance
$\mathbf{x}_i$, $\vec{p}_{t,y}$ represents the probability of the gold
label, and $\norm{O}^2_2$ stands for the $L2$-norm of the $O$ matrix.

We optimized the scalar hyper-parameters $c$ and $\alpha$ on the
development set, and trained the final model for 256 epochs, choosing
parameter values which maximized the macro-averaged \F-score on the
development data.

\textbf{Inference} Since each of the above approaches (LSTM and GRU)
explicitly defines an output unit, the inference of the most likely
label assignment for an input instance $\mathbf{x}$ is straightforward
and amounts to finding the $\argmax$ value of the output vector at
each time step of the recurrence:
\begin{equation}
  \mathbf{\hat{y}} =
  \argmax{\vec{p}^{(1)}},\argmax{\vec{p}^{(2)}},\ldots,\argmax{\vec{p}^{(|\mathbf{x}|)}}.
\end{equation}

\textbf{Results} To account for the random factors in the
initialization, we repeated each training experiment three times, and
show the mean and standard deviation of these results in
Table~\ref{snt-fgsa:tbl:rnn-res}.

\begin{table*}
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{1}{>{\centering\arraybackslash}p{0.136\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Data Set} & \multicolumn{3}{c}{\bfseries Sentiment} & %
      \multicolumn{3}{c}{\bfseries Source} & %
      \multicolumn{3}{c}{\bfseries Target} & %
      \multirow{2}{0.136\columnwidth}{\bfseries\centering Macro\newline \F{}}\\\cline{2-10}
      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} &\\\midrule

      \multicolumn{11}{c}{\cellcolor{cellcolor}LSTM}\\
      %%  Tag        Precision    Recall F-Measure
      %% O             86.60%    86.63%    86.62%
      %% SENTIMENT     55.50%    74.72%    63.69%
      %% SOURCE        46.27%    68.82%    55.33%
      %% TARGET        43.12%    77.99%    55.53%

      %% Tag        Precision    Recall F-Measure
      %% O             88.76%    67.71%    76.82%
      %% SENTIMENT     30.76%    76.51%    43.88%
      %% SOURCE        38.84%    49.82%    43.65%
      %% TARGET        29.45%    67.18%    40.95%

      %% Tag        Precision    Recall F-Measure
      %% O             86.38%    89.91%    88.11%
      %% SENTIMENT     60.90%    74.87%    67.16%
      %% SOURCE        48.96%    71.18%    58.02%
      %% TARGET        50.61%    74.58%    60.30%

      Training Set & 0.49\stddev{0.16} & 0.75\stddev{0.01} & 0.58\stddev{0.13} & %
      0.45\stddev{0.05} & 0.63\stddev{0.12} & 0.52\stddev{0.08} %
      & 0.41\stddev{0.11} & 0.73\stddev{0.06} & 0.52\stddev{0.11} %
      & 0.54\stddev{0.11}\\

      %% Tag        Precision    Recall F-Measure
      %% O             77.77%    82.91%    80.26%
      %% SENTIMENT     31.69%    28.00%    29.73%
      %% SOURCE        23.05%    31.25%    26.53%
      %% TARGET        21.77%    23.57%    22.63%

      %% Tag        Precision    Recall F-Measure
      %% O             79.83%    71.62%    75.50%
      %% SENTIMENT     26.23%    43.07%    32.60%
      %% SOURCE        26.07%    30.68%    28.19%
      %% TARGET        21.53%    30.16%    25.13%

      %% Tag        Precision    Recall F-Measure
      %% O             77.58%    86.08%    81.61%
      %% SENTIMENT     30.16%    22.60%    25.84%
      %% SOURCE        24.42%    30.60%    27.16%
      %% TARGET        24.99%    21.20%    22.94%

      Test Set & 0.29\stddev{0.03} & \textbf{0.31}\stddev{0.11} & \textbf{0.29}\stddev{0.03} &%
      \textbf{0.25}\stddev{0.02} & \textbf{0.31}\stddev{0.0} & \textbf{0.27}\stddev{0.01} & %
      \textbf{0.23}\stddev{0.02} & \textbf{0.25}\stddev{0.05} & \textbf{0.24}\stddev{0.01} & %
      \textbf{0.27}\stddev{0.02}\\

      \multicolumn{11}{c}{\cellcolor{cellcolor}GRU}\\

      %% Tag        Precision    Recall F-Measure
      %% O             82.40%    89.44%    85.77%
      %% SENTIMENT     60.16%    60.25%    60.21%
      %% SOURCE        44.77%    66.74%    53.59%
      %% TARGET        46.36%    63.22%    53.49%

      %% Tag        Precision    Recall F-Measure
      %% O             87.39%    80.32%    83.71%
      %% SENTIMENT     44.88%    70.75%    54.92%
      %% SOURCE        39.69%    63.31%    48.79%
      %% TARGET        35.37%    73.96%    47.86%

      %% Tag        Precision    Recall F-Measure
      %% O             81.03%    87.80%    84.28%
      %% SENTIMENT     47.95%    66.84%    55.84%
      %% SOURCE        42.49%    56.86%    48.64%
      %% TARGET        58.22%    51.08%    54.42%

      Training Set & 0.51\stddev{0.08} & 0.66\stddev{0.05} & 0.57\stddev{0.03} & %
      0.42\stddev{0.03} & 0.62\stddev{0.05} & 0.5\stddev{0.03} & %
      0.47\stddev{0.11} & 0.63\stddev{0.11} & 0.52\stddev{0.04} & 0.53\stddev{0.03}\\

      %% Tag        Precision    Recall F-Measure
      %% O             76.78%    86.47%    81.34%
      %% SENTIMENT     30.77%    19.68%    24.01%
      %% SOURCE        20.71%    26.44%    23.22%
      %% TARGET        24.20%    20.93%    22.45%

      %% Tag        Precision    Recall F-Measure
      %% O             78.15%    79.14%    78.64%
      %% SENTIMENT     28.61%    30.25%    29.41%
      %% SOURCE        19.71%    29.45%    23.62%
      %% TARGET        21.54%    27.49%    24.15%

      %% Tag        Precision    Recall F-Measure
      %% O             77.52%    86.03%    81.56%
      %% SENTIMENT     30.66%    28.43%    29.50%
      %% SOURCE        24.46%    29.09%    26.58%
      %% TARGET        27.17%    14.15%    18.61%

      Test Set & \textbf{0.3}\stddev{0.01} & 0.26\stddev{0.06} & 0.28\stddev{0.03} & %
      0.22\stddev{0.03} & 0.28\stddev{0.02} & 0.24\stddev{0.02} & %
      0.24\stddev{0.03} & 0.21\stddev{0.07} & 0.22\stddev{0.03} & 0.25\stddev{0.01}\\\bottomrule
    \end{tabular}
    \egroup
    \caption{Results of fine-grained sentiment analysis with recurrent
      neural networks.}
    \label{snt-fgsa:tbl:rnn-res}
  \end{center}
\end{table*}

As we can see from the table, the LSTM model shows generally better
scores than the GRU system on both training and test data.  The only
aspect at which it yields slightly worse results than the latter
approach is the precision of sentiment spans, which, however, is more
than compensated for by a much higher recall.  Moreover, the
overfitting effect is significantly less pronounced than in the CRF
case (where the \F scores on the training and test data differed by a
factor of three).  Nonetheless, both RNN systems achieve lower results
than the linear-chain CRFs, which indicates the fact that deeply
learned features still cannot capture the full extent of information
which a human expert can encode with manually defined attributes.

\section{Evaluation}

After estimating the results of the most popular FGSA approaches with
the (mostly) standard settings, we decided to investigate the impact
of different external factors on the net scores of these methods.  For
this purpose, we reran the evaluation, changing one aspect of the
training procedure at a time, and re-estimated the scores on the
development set (in order to keep the test corpus undisclosed).  The
results of these experiments are presented below.

\subsection{Effect of the Annotation Scheme}

As the first factor which could significantly affect the quality of
the automatic approaches, we considered the annotation scheme that we
used to create the corpus.  As described in
Section~\ref{subsec:snt:ascheme}, we initially asked our experts to
assign the \textsc{sentiment} label to complete syntactic or
discourse-level units which encompassed both the target of an opinion
and its immediate evaluation.  Even though this decision was
linguistically plausible and extremely helpful for determining the
boundaries of opinions, it also posed considerable difficulties for
sequence labeling approaches, since the same tag got assigned not only
to immediately subjective words, but also to objective terms which
resided within the same syntactic constituent as the polar term and
its target.  Since none of the tested methods could explicitly
incorporate this logic, we decided to check whether an alternative
interpretation of the annotation scheme could alleviate their
inference.

In particular, instead of unconditionally labeling all words belonging
to a sentiment span in the original annotation with the \textsc{SNT}
tag as we did previously (which we call a \emph{broad} interpretation
of the annotation scheme), we only assigned this label to the
emotional expressions found in the corpus (which we term a
\emph{narrow} interpretation of the scheme).  The difference between
these two ways of interpreting the annotation scheme is shown in
Examples~\ref{snt:fgsa:exmp:wide} and~\ref{snt:fgsa:exmp:narrow}.
\begin{example}[Broad Sentiment
  Interpretation]\label{snt:fgsa:exmp:wide}
  \noindent\sentiment{\target{Francis} makes a \intensifier{very}
    \emoexpression{good} impression on\\ \source{me}!
    \emoexpression{:)}}

  $\rightarrow$

  \noindent Francis/TRG makes/SNT a/SNT very/SNT good/SNT
  impression/SNT on/SNT\\ me/SRC !/SNT :)/SNT
\end{example}

\begin{example}[Narrow Sentiment Interpretation]\label{snt:fgsa:exmp:narrow}
  \noindent\sentiment{\target{Francis} makes a \intensifier{very}
    \emoexpression{good} impression on\\ \source{me}!
    \emoexpression{:)}}

  $\rightarrow$

  \noindent Francis/TRG makes/NON a/NON very/NON good/SNT
  impression/NON on/NON\\ me/SRC !/NON :)/SNT
\end{example}
\noindent In the former (broad) case, we labeled the whole opinionated
sentence with the \textsc{SNT} tag except for the words which denoted
the target and source of the opinion.  In the latter (narrow) case, we
only assigned the \textsc{SNT} tag to the emotional expression
``good'' and the emoticon ``:)'', which, however, were expressive
enough to convey the main evaluative sense of the whole subjective
statement.

The results of the automatic systems for these two interpretations are
given in Table~\ref{snt-fgsa:tbl:broad-narrow}.
\begin{table*}[hbt!]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{1}{>{\centering\arraybackslash}p{0.136\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Data Set} & \multicolumn{3}{c}{\bfseries Sentiment} & %
      \multicolumn{3}{c}{\bfseries Source} & %
      \multicolumn{3}{c}{\bfseries Target} & %
      \multirow{2}{0.136\columnwidth}{\bfseries\centering Macro\newline \F{}}\\\cline{2-10}
      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} &\\\midrule

      \multicolumn{11}{c}{\cellcolor{cellcolor}Broad Interpretation}\\

      %% SENTIMENT     37.62%    31.85%    34.49%
      %% SOURCE        29.75%    33.00%    31.29%
      %% TARGET        29.25%    23.06%    25.79%

      CRF & 0.38 & 0.32 & 0.34 & %
      \textbf{0.3} & 0.33 & 0.31 & %
      \textbf{0.29} & 0.23 & \textbf{0.26} & 0.31\\

      % Tag        Precision    Recall F-Measure
      % O             77.89%    70.02%    73.75%
      % SENTIMENT     24.03%    37.51%    29.29%
      % SOURCE        28.52%    37.43%    32.37%
      % TARGET        22.73%    30.54%    26.06%

      % Tag        Precision    Recall F-Measure
      % O             76.44%    86.10%    80.98%
      % SENTIMENT     30.53%    23.22%    26.38%
      % SOURCE        27.74%    36.33%    31.46%
      % TARGET        28.76%    23.59%    25.92%

      % Tag        Precision    Recall F-Measure
      % O             76.87%    83.75%    80.16%
      % SENTIMENT     30.39%    25.69%    27.84%
      % SOURCE        31.59%    37.88%    34.45%
      % TARGET        24.59%    26.94%    25.71%

      % Summary:
      % Tag             Precision    Recall        F1
      % SENTIMENT           28.32     28.81     27.84
      % SOURCE              29.28     37.21     32.76
      % TARGET              25.36     27.02     25.90
      % Macro-F1  28.8311

      LSTM & 0.28 & 0.29 & 0.28 & %
      0.29 & \textbf{0.37} & \textbf{0.33} & %
      0.25 & \textbf{0.27} & \textbf{0.26} & 0.29\\

      % Tag        Precision    Recall F-Measure
      % O             76.64%    86.78%    81.39%
      % SENTIMENT     30.31%    19.73%    23.90%
      % SOURCE        27.54%    39.00%    32.28%
      % TARGET        23.10%    19.12%    20.93%

      % Tag        Precision    Recall F-Measure
      % O             77.05%    78.92%    77.97%
      % SENTIMENT     28.27%    28.35%    28.31%
      % SOURCE        27.30%    43.05%    33.41%
      % TARGET        22.68%    29.29%    25.56%

      % Tag        Precision    Recall F-Measure
      % O             76.82%    85.63%    80.98%
      % SENTIMENT     27.49%    25.78%    26.60%
      % SOURCE        31.34%    39.30%    34.87%
      % TARGET        29.86%    13.15%    18.26%

      % Summary
      % Tag             Precision    Recall        F1
      % SOURCE              28.73     40.45     33.52
      % SENTIMENT           28.69     24.62     26.27
      % TARGET              25.21     20.52     21.58
      % Macro-F1  27.1244

      GRU & 0.29 & 0.25 & 0.26 & %
       0.29 & 0.4 & 0.34 & %
       0.25 & 0.21 & 0.22 & 0.27\\

      \multicolumn{11}{c}{\cellcolor{cellcolor}Narrow Interpretation}\\

      %% Tag        Precision    Recall F-Measure
      %% O             85.93%    85.69%    85.81%
      %% SENTIMENT     58.84%    64.49%    61.54%
      %% SOURCE        26.13%    23.00%    24.47%
      %% TARGET        22.14%    20.14%    21.09%

      CRF & 0.59 & 0.64 & 0.62 & %
      0.26 & 0.23 & 0.24 & %
      0.22 & 0.20 & 0.21 & 0.36\\

      % Tag        Precision    Recall F-Measure
      % O             85.08%    90.61%    87.76%
      % SENTIMENT     57.18%    66.52%    61.50%
      % SOURCE        27.22%    40.25%    32.48%
      % TARGET        25.54%    12.56%    16.84%

      % Tag        Precision    Recall F-Measure
      % O             84.57%    91.93%    88.10%
      % SENTIMENT     69.83%    60.92%    65.07%
      % SOURCE        32.28%    35.55%    33.84%
      % TARGET        26.04%    16.18%    19.96%

      % Tag        Precision    Recall F-Measure
      % O             84.05%    91.82%    87.77%
      % SENTIMENT     58.79%    67.52%    62.85%
      % SOURCE        30.90%    27.85%    29.30%
      % TARGET        27.07%    13.31%    17.85%

      LSTM & \textbf{0.62} & \textbf{0.65} & \textbf{0.63} & %
      \textbf{0.3} & 0.35 & 0.32 & %
      0.26 & 0.14 & 0.18 & \textbf{0.38}\\

      %% Tag        Precision    Recall F-Measure
      %% O             84.78%    87.61%    86.17%
      %% SENTIMENT     59.02%    61.71%    60.34%
      %% SOURCE        27.35%    37.83%    31.75%
      %% TARGET        22.37%    21.18%    21.76%

      %% Tag        Precision    Recall F-Measure
      %% O             84.38%    90.46%    87.31%
      %% SENTIMENT     60.14%    64.76%    62.36%
      %% SOURCE        27.93%    29.53%    28.71%
      %% TARGET        27.40%    20.07%    23.17%

      % Tag        Precision    Recall F-Measure
      % O             84.98%    85.60%    85.29%
      % SENTIMENT     65.71%    61.64%    63.61%
      % SOURCE        29.91%    31.20%    30.54%
      % TARGET        19.18%    31.63%    23.88%

      GRU & \textbf{0.62} & 0.63 & 0.62 & %
      0.28 & 0.33 & 0.3 & %
      0.23 & 0.24 & 0.23 & \textbf{0.38}\\\bottomrule

    \end{tabular}
    \egroup
    \caption{Results of fine-grained analysis with the broad and
      narrow sentiment interpretations.}
    \label{snt-fgsa:tbl:broad-narrow}
  \end{center}
\end{table*}

As we can see from the table, the broad approach generally leads to
notably lower scores for sentiment spans, but yields much better
results for sources and targets of the opinions.  An opposite
situation is observed with the narrow interpretation: even though the
\F-values for sentiments are twice as high as in the broad case, the
scores for the remaining opinion elements are up to seven percent
lower~(consider, for instance, 0.31~\F{} attained by the linear-chain
CRFs with the broad interpretation versus 0.24~\F{} achieved by this
model with the narrow mapping).

An obvious explanation for these results is the expectedly better
amenability of the narrow scheme to the prediction of sentiment
labels: since \textsc{sentiment} tags only get assigned to
unequivocally polar terms, it becomes easier for the models to infer
this class using their state features---especially morphological or
lexical ones---or the learned word embeddings.  However, on the other
hand, such short spans lead to disrupted label chains for other
opinion-related elements, making sentiment tags be far apart from the
spans of the respective sources and targets.  Consequently, these
latter classes suffer from the lack of context and become heavily
dependent on the state attributes too.  Although, this time, the
effect of these attributes is rather negative, since, in contrast to
polar terms, being a holder or target of an opinion is not an inherent
property of a lexical term, but arises solely from the context which
this term appears in.

Consider, for instance, the name ``Silvio Berlusconi'' in
Example~\ref{snt:fgsa:trg-ctxt}, where it appears as a target of an
evaluation in the first sentence, but serves as a normal subject of an
objective clause in the second case.  The decision about the role of
this name depends primarily on the sense of the whole statement rather
than the name itself.  Consequently, state attributes might only
increase our prior belief that certain words would rather appear in a
subjective context, but cannot tell for sure whether they actually do
so or not.  Therefore, the recognition of sources and targets of
opinions becomes much harder, once the context information is deprived
(as in the narrow case).\footnote{The negative effect of state
  features on the prediction of sources and targets was actually
  observed in our corpus, where one of the most frequently made
  mistakes was the uncoditional assignment of the TRG tag to the word
  ``Nordkorea'' (\emph{North Korea}) regardless of whether its
  surrounding context was polar or not.}

\begin{example}[Contextual Dependence of Target
  Elements]\label{snt:fgsa:trg-ctxt}
  Hoffentlich ist es nicht \target{Silvio Berlusconi}. \#Papst\\[0.5em]
  \noindent Hopefully, this won't be \target{Silvio Berlusconi}. \#Pope\\[1em]
  Silvio Berlusconi ist ein italienischer Medienmagnat und Politiker.\\[0.5em]
  \noindent Silvio Berlusconi is an Italian media tycoon and politician.\\
\end{example}

\subsection{Effect of Graph Topologies}

Since the lack of contextual links appeared to have a strong negative
effect on the prediction of sources and targets, we decided to
investigate whether redefining the way these links were established in
the model would improve the results.  For this purpose, we implemented
three possible extensions to the traditional first-order linear-chain
CRF model, which are shown in Figure~\ref{fig:snt:ho-crf}:
\begin{itemize}
  \item higher-order linear-chain CRFs,
  \item first- and higher-order semi-Markov models, and
  \item tree-structured CRFs.
\end{itemize}

\begin{figure*}[thb]
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
      \input{linchain-crf-1}
    \caption{First-order linear-chain CRFs}
  \end{subfigure}
  ~
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
      \input{linchain-crf-ho}
    \caption{Second-order linear-chain CRFs}
  \end{subfigure}\\[1em]
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
      \input{semim-crf}
    \caption{Semi-Markov CRFs}
  \end{subfigure}
  ~
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
      \input{tree-crf}
    \caption{Tree-structured CRFs}
  \end{subfigure}
  \caption{\emph{Factor graphs of different CRF topologies.}\\Blank
    nodes represent unobserved variables; gray circles denote observed
    input; feature functions are shown as squares (with black squares
    representing state features, and red rectangles denoting
    transition functions).\label{fig:snt:ho-crf}}
\end{figure*}

In the first (higher-order) variant, instead of estimating the
likelihood of a single tag~$y$ at the given
position~$i$~(e.g.,~$P(y_i)=$ SNT), we kept a separate track of the
probability of each possible label
sequence~$P(y_{i-n+1},\ldots,y_{i})$, where $n$ denotes the order of
the model.  In compliance with this structure, we established
transition functions only between those pairs of adjacent nodes where
the label suffix of the preceding unobserved state matched the tag
prefix of the successor.  (For example, in the second-order case, we
only connected the node TRG|SNT to the preceding nodes SNT|TRG,
SRC|TRG, TRG|TRG, and NON|TRG via transition factors; since these were
the only unobserved states whose last label matched the first tag of
the former node.)  Furthermore, instead of considering just one
transition attribute between those states, as we did in the
first-order case (e.g.,
$f(\mathbf{x}_i, TRG, SNT) = 1.\text{ if }y_{i-1}=TRG\text{ and
}y_i=SNT\text{ else }0.$), we took a sum of different transition
features for each possible prefix length.  For instance, in the case
of transition between the states NON|TRG and TRG|SNT, we took a sum of
two transition functions:
\begin{equation*}
  f_1(\mathbf{x}_i, NON|TRG, SNT) = \begin{cases} 1, &
    \mbox{if } \mathbf{y}_{i-2} = NON\mbox{ and }\mathbf{y}_{i-1} = TRG\mbox{ and }\mathbf{y}_{i} =
    SNT\\ 0, & \mbox{otherwise;}\\
  \end{cases}
\end{equation*}
and
\begin{equation*}
  f_2(\mathbf{x}_i, TRG, SNT) = \begin{cases} 1, &
    \mbox{if } \mathbf{y}_{i-1} = TRG\mbox{ and }\mathbf{y}_{i} =
    SNT\\ 0, & \mbox{otherwise.}
  \end{cases}
\end{equation*}

Since the number of states in this extension increased exponentially
with the order of the model, we applied the heuristic inference
algorithm of~\citet{Nguyen:14} by only considering those label
sequence which were actually observed in the training data and
pre-caching valid prefix and suffix transitions while doing the belief
propagation.

The same optimization was also applied to higher-order semi-Markov
CRFs, which, in contrast to the linear-chain models, operate on whole
chunks of text, simultaneously trying to predict both the most
probably segmentation of the input and the best possible label
assignment to these segments.  In particular, instead of simply
optimizing the conditional probability of the labels, as it is done by
the linear CRFs:
\begin{equation*}
  P(\mathbf{y}|\mathbf{x}) = \frac{\exp\left(\sum_{m=1}^{M}\sum_jw_{j}
      \times f_j(x_{m}, y_{m-1},
      y_{m})\right)}{Z\left(\mathbf{x}\right)},
\end{equation*}
semi-Markov models seek to maximize the conditional likelihood of the
segments and their labels over the training data:
\begin{equation*}
  P(\mathbf{s}|\mathbf{x}) = \frac{\exp\left(\sum_{n=1}^{N}\sum_jw_{j}
      \cdot f_j(s_{n}, y_{s_{n-1}},
      y_{s_n})\right)}{Z\left(\mathbf{x}\right)}.
\end{equation*}
The $\mathbf{s}$ term in the latter formula stands for the total
segmentation of an input instance, $s_{n}$ denotes the $n$-the
segment, and $y_{s_{n-1}}$ and $y_{s_n}$ represent the labels of the
previous and current segments respectively.  The normalization factor
$Z\left(\mathbf{x}\right)$ here is computed over all possible label
and segment assignments, with segments' length ranging from 1 to $K$,
where $K$ is the maximum length of a contiguous tag span observed in
the training data.

Finally, tree-structured CRFs represent another generalization of the
linear-chain model, in which transition functions are established
between syntactic dependents instead of adjacent tokens.  As the
underlying graph structure for this variant of CRFs, we used the
automatically derived dependency trees, getting these analyses from
the state-of-the-art dependency parser~\cite[Mate;][]{Bohnet:09}.
Since these graphs were guaranteed to be acyclic, we could still apply
the normal belief propagation method with exact inference, getting the
same convergence guarantees as in the linear-chain case.

The results of these systems on the training and development sets are
shown in Table~\ref{fgsa:tbl:crf-topologies}.
\begin{table*}[hbt!]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.12\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.094\columnwidth}}} % next nine columns
      \toprule
      \multirow{2}*{\bfseries Element} & %
      \multicolumn{9}{c}{\bfseries Topology}\\\cline{2-10}
      & lcCRF$^1$ & lcCRF$^2$ & lcCRF$^3$ & lcCRF$^4$ & %
      smCRF$^1$ & smCRF$^2$ & smCRF$^3$ & smCRF$^4$ & trCRF$^1$\\\midrule

      \multicolumn{10}{c}{\cellcolor{cellcolor}Training Set}\\

      Sentiment & 0.928 & 0.919 & 0.922 & 0.925 & 0.931 & 0.931 & 0.933 & 0.931 & 0.906\\
      Source & 0.887 & 0.876 & 0.89  & 0.901 & 0.869 & 0.886 & 0.874 & 0.878 & 0.881\\
      Target & 0.898 & 0.811 & 0.816 & 0.827 & 0.813 & 0.827 & 0.815 & 0.817 & 0.876\\

      \multicolumn{10}{c}{\cellcolor{cellcolor}Development Set}\\

      Sentiment & 0.345 & 0.334 & 0.332 & 0.335 & \textbf{0.395} & 0.385 & 0.389 & 0.378 & 0.331\\
      Source & 0.313 & \textbf{0.32} & 0.272 & 0.304 & 0.298 & 0.282 & 0.287 & 0.291 & 0.223\\
      Target & 0.258 & 0.235 & 0.24 & 0.229 & 0.287 & \textbf{0.309} & 0.301 & 0.292 & 0.243\\\bottomrule
    \end{tabular}
    \egroup
    \caption{Results of fine-grained sentiment analysis with different
      CRF topologies.\\ {\small lcCRF---linear-chain CRFs,
        smCRF---semi-Markov CRFs, trCRF---tree-structured CRFs;\\1, 2,
        3, and 4 in the superscripts denote the order}}
    \label{fgsa:tbl:crf-topologies}
  \end{center}
\end{table*}

As we can see from the scores, semi-Markov CRFs achieve better results
at predicting sentiments and targets, but show a degradation in
classifying sources of the opinions.  Moreover, second-order
semi-Markov and linear-chain topologies outperform first-order models
on targets and sources.  However, further increasing the order of
these structures does not bring about any further improvements.
Somewhat surprisingly, tree-structured CRFs show worse scores than
their linear counterparts.

In order to see whether the same tendencies would hold for the deep
learning methods, we also implemented higher-order and tree-structured
extensions to the LSTM and GRU systems.  In the former case, we passed
a concatenation of $n$ preceding $\vec{h}$ vectors (where $n$ was the
order of the model) as input to the reccurrence loop.  In the
tree-structure modification, we followed the approach
of~\citet{Tai:15} and defined the LSTM unit as follows:
\begin{align*}
  \tilde{h}^{(t)} &= \sum_{k \in C\left(t\right)}\vec{h}^{(k)},\\
  \vec{i}^{(t)} &= \sigma\left(W_i\cdot\vec{x}^{(t)} + U_i\cdot\tilde{h}^{(t)} + \vec{b}_i\right),\\
  \vec{o}^{(t)} &= \sigma\left(W_o\cdot\vec{x}^{(t)} + U_o\cdot\tilde{h}^{(t)} + \vec{b}_o\right),\\
  \vec{u}^{(t)} &= \sigma\left(W_o\cdot\vec{x}^{(t)} + U_o\cdot\tilde{h}^{(t)} + \vec{b}_u\right),\\
  \vec{f}^{(t,k)} &= \sigma\left(W_f\cdot\vec{x}^{(t)} + U_f\cdot\vec{h}^{(k)} + \vec{b}_f\right),\\
  \vec{c}^{(t)} &= \vec{i}^{(t)}\odot\vec{u}^{(t)} + \sum_{k \in C(t)}f^{(t,k)}\odot c^{(k)},\\
  \vec{h}^{(t)} &= \vec{o}^{(t)}\odot tanh\left(\vec{c}^{(t)}\right);
\end{align*}
where $C\left(t\right)$ stands for the indices of all child nodes of
the token $t$.

In a similar way, we also redefined the GRU unit to the following
solutions:
\begin{align*}
  \tilde{h}^{(t)} &= \sum_{k \in C\left(t\right)}\vec{h}^{(k)},\\
  \vec{i}^{(t)} &= \sigma\left(W_i\cdot \mathbf{x}^{(t)} + U_i \cdot \tilde{h}^{(t)}\right),\\
  \vec{f}^{(t,k)} &= \sigma\left(W_f\cdot \mathbf{x}^{(t)} + U_f \cdot \vec{h}^{(t,k)}\right),\\
  \widetilde{c}^{(t)} &= tanh\left(W_c\cdot \mathbf{x}^{(t)} + U_c
  \cdot \sum_{k\in C(t)}\left(\vec{f}^{(t,k)} \odot \vec{h}^{(k)}\right)\right),\\
  \vec{h}^{(t)} &= \vec{i}^{(t)} \odot \tilde{h}^{(t)} + \left(\vec{1} -
  \vec{i}^{(t)}\right) \odot \widetilde{c}^{(t)}.
\end{align*}

The results of these modifications are shown in
Table~\ref{fgsa:tbl:nn-topologies}, in which we can see that the
first-order LSTM model still outperforms all higher-order LSTM and GRU
extensions on predicting targets and sources of the opinions.
Moreover, first-order GRU also achieves the best scores on predicting
sentiment spans among all compared models.  This time, again, none of
the tree-structured extensions could outperform their linear-chain
counterparts, which might be partially explained by the errors
produced by the parser whose original target domain are
standard-language news texts.

\begin{table*}
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.12\columnwidth} % first columm
        *{8}{>{\centering\arraybackslash}p{0.10575\columnwidth}}} % next nine columns
      \toprule
      \multirow{2}*{\bfseries Element} & %
      \multicolumn{8}{c}{\bfseries Topology}\\\cline{2-9}
      & lcLSTM$^1$ & lcLSTM$^2$ & lcLSTM$^3$ & lcGRU$^1$ & %
      lcGRU$^2$ & lcGRU$^3$ & trLSTM$^1$ & trGRU$^1$\\\midrule

      \multicolumn{9}{c}{\cellcolor{cellcolor}Training Set}\\

      Sentiment & 0.584 & 0.559 & 0.54 & 0.57 & 0.587 & 0.606 & 0.43 & 0.518\\
      Source & 0.525 & 0.458 & 0.424  & 0.503 & 0.546 & 0.548 & 0.317 & 0.372 \\
      Target & 0.521 & 0.513 & 0.501 & 0.519 & 0.544 & 0.605 & 0.305 & 0.425\\

      \multicolumn{9}{c}{\cellcolor{cellcolor}Development Set}\\

      Sentiment & 0.278 & 0.285 & 0.281 & \textbf{0.335} & 0.252 & 0.253 & 0.314 & 0.292\\
      Source & \textbf{0.328} & 0.314 & 0.303  & 0.263 & 0.298 & 0.306 & 0.256 & 0.262\\
      Target & \textbf{0.259} & 0.218 & 0.222 & 0.216 & 0.219 & 0.188 & 0.205 & 0.193\\\bottomrule
    \end{tabular}
    \egroup
    \caption{Results of fine-grained sentiment analysis with different
      neural network topologies.\\ {\small lcLSTM---linear-chain LSTM,
        lcGRU---linear-chain GRU, trLSTM---tree-structured LSTM,
        trGRU---tree-structured GRU;\\1,
        2, and 3 in the superscripts denote the order}}
    \label{fgsa:tbl:nn-topologies}
  \end{center}
\end{table*}

\subsection{Effect of Features}

Since redefining the graph topologies of the presented models did not
bring the expected improvements, we decided to investigate the effect
of the input provided to these systems on their net results, and also
analyze more thoroughly what these models have actually learned from
this input.  For this purpose, we first performed an ablation test of
the CRF's state features, removing one feature group at a time and
rechecking the performance of the model on the development set.

\begin{table}[hbt]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.47\tabcolsep}\scriptsize
    \begin{tabular}{p{0.14\columnwidth} % first columm
        *{6}{>{\centering\arraybackslash}p{0.13\columnwidth}}} % next five columns
      \toprule
          \multirow{2}{0.2\columnwidth}{\bfseries Element} &
          \multirow{2}{0.1\columnwidth}{\bfseries Original\newline \F-Score} &
          \multicolumn{5}{c}{\bfseries \F-Score after Feature Removal}\\\cline{3-7}
          & & Formal & Morphological & Lexical & Syntactic & Complex\\\midrule

          Sentiment & 0.346 & 0.343\negdelta{0.003} & 0.344\negdelta{0.002} & 0.326\negdelta{0.02} & 0.345\negdelta{0.001} & 0.324\negdelta{0.022}\\
          Source & 0.309 & 0.321\posdelta{0.012} & 0.313\posdelta{0.004} & 0.265\negdelta{0.044} & 0.359\posdelta{0.05} & 0.271\negdelta{0.038}\\
          Target & 0.26 & 0.282\posdelta{0.022} & 0.252\negdelta{0.008} & 0.263\posdelta{0.003} & 0.233\negdelta{0.027} & 0.263\posdelta{0.003}\\\bottomrule
    \end{tabular}
    \egroup
    \caption[Results of the feature ablation tests for the CRF
    model.]{Results of the feature ablation tests for the CRF
      model.\\{\small\itshape (negative changes w.r.t. the original
        scores on the development set are shown in
        \textsuperscript{\textcolor{red3}{red}}; positive changes are
        depicted in \textsuperscript{\textcolor{seagreen}{green}}
        superscripts)\footnotemark}}
    \label{tbl:ablation}
  \end{center}
\end{table}

The results of this test are shown in
Table~\ref{tbl:ablation}.\footnotetext{Negative changes indicate good
  features in this context, since their removal leads to a degradation
  of results.}  As we can see from the table, all feature types turn
out to be useful for predicting sentiments as their removal
unequivocally leads to a degradation of the scores.  This quality
drop, however, is usually quite small, suggesting that other attribute
types can easily make up for the removed ones.  A different situation
is observed for sources and targets though.  In the first case,
removing formal, morphological, and syntactic features shows a
strongly positive effect, improving the \F-results for sources by up
to five percent.  However, removing lexical and lexico-syntactic
features, on the contrary, worsens these results, tearing the
\F-scores down by up to 4.4\%.  Except for the formal group, all these
attributes behave completely differently when applied to targets,
which seem to benefit from morphological and syntactic features, while
suffering a slight degradation from lexical and complex attributes.

\begin{table}[hbt]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.47\tabcolsep}\scriptsize
    \begin{tabular}{>{\centering\arraybackslash}p{0.05\columnwidth} % first columm
        *{4}{>{\centering\arraybackslash}p{0.22\columnwidth}}} % next four columns
      \toprule
          \multirow{2}{0.2\columnwidth}{Rank} &
          \multicolumn{2}{c}{\bfseries Top-10 State Features} &
          \multicolumn{2}{c}{\bfseries Top-10 Transition Features}\\\cline{2-5}
          & Feature & Score & Feature & Score\\\midrule

          1 & prntLemma=meiste $\rightarrow$ TRG & 18.68 & NON $\rightarrow$ TRG & -7.01\\
          2 & prntLemma=rettungsschirme $\rightarrow$ TRG & 18.3 & NON $\rightarrow$ SRC & -6.85\\
          3 & initChar=sty $\rightarrow$ NON & -16.04  & NON $\rightarrow$ SNT & -5.39\\
          4 & form=meisten $\rightarrow$ NON & 15.99 & TRG $\rightarrow$ SRC & -2.99\\
          5 & prntLemma=urlauberin $\rightarrow$ SNT & 14.74 & NON $\rightarrow$ NON & 2.69\\
          6 & lemma=anfechten  $\rightarrow$ SNT & 14.07 & SRC $\rightarrow$ NON & -2.59\\
          7 & form=thomasoppermann  $\rightarrow$ TRG & 13.44 & SNT $\rightarrow$ SNT & 2.54\\
          8 & form=bezeichnete $\rightarrow$ SNT & 13.25 & TRG $\rightarrow$ TRG & 2.31\\
          9 & deprel[0]|deprel[1]=NK|AMS $\rightarrow$ NON & 12.92 & SRC $\rightarrow$ SRC & 2.19\\
          10 & trailChar=te. $\rightarrow$ NON & 12.77 & SRC $\rightarrow$ TRG & -2.07\\\bottomrule
    \end{tabular}
    \egroup
    \caption{Top ten state and transition features learned by the CRF
      model.\\{\small (sorted by the absolute values of their
        weights)}}
    \label{fgsa:tbl:ablation}
  \end{center}
\end{table}

In order to get a better overview of the learned model's parameters,
we also extracted top ten state and transition features ranked by the
absolute values of their learned weights (see
Table~\ref{fgsa:tbl:ablation}).  As can be seen from the statistics,
three of the five highest ranked state attributes are complex features
reflecting the lemma of the parent token: ``meiste'' (\emph{most}) and
``rettungsschirme'' (\emph{bailout}), which typically indicate a
target, and ``urlauberin'' (\emph{holiday}), which frequently
correlates with sentiments.  Another common group of features are the
lemma and form of the current token: here, we again can encounter
``meisten'' (\emph{most}), which, however, indicates the absence of
any sentiment entities at the current position; two other
attributes---``anfechten'' (\emph{doubt}) and ``bezeichnete''
(\emph{called})---represent the so-called \emph{direct speech events}
and expectedly correlate with the sentiment tags; the remaining
feature---``thomasopperman''---is a person name, which frequently
serves as a target of an opinion.

An interesting pattern can be observed for transition attributes: As
we can see from the results, the top-three transition features
indicate a strong belief that an objective token is highly unlikely to
be followed by a sentiment entity (hence, the high negative weights of
the transitions emanating from \textsc{NON}).  It is, however, quite
common that a \textsc{NON} tag will precede another \textsc{NON}
instance (cf. line 5 of the table).  Other transition attributes also
mainly reflect plausible regularities: It is, for instance, uncommon
that a target of an opinion will appear immediately before a source
(\textsc{TRG}$\rightarrow$\textsc{SRC} $= -2.99$); in the same vein,
it is rather uncommon that a source tag will precede a target
(\textsc{SRC}$\rightarrow$\textsc{TRG} $= -2.07$); nonetheless, is is
perfectly acceptable that the same tag will continue over multiple
words (e.g., \textsc{SNT}$\rightarrow$\textsc{SNT} $= 2.54$,
\textsc{TRG}$\rightarrow$\textsc{TRG} $= 2.31$).

\subsection{Effect of Word Embeddings}

Similarly to the CRF features, we also investigated the impact of
input embeddings on the net results of the RNN approaches.  For this
purpose, instead of learning task-specific word representations we did
in the initial experiments, we re-ran the training using two other
embedding types:
\begin{itemize}
\item\emph{least-squares embeddings}, in which we again learned
  task-specific word vectors, but additionally computed an optimal
  transformation matrix $W$ using the method of ordinary least
  squares:
  \begin{equation}\label{eq:fgsa:least-sq}
    W = \argmin_{W}\lVert V_{TS} - W^T\cdot V_{W2V}\rVert_F,
  \end{equation}
  where $V_{TS}$ stands for the matrix of task-specific word
  representations learned by the system, $V_{W2V}$ represents the
  respective matrix of word2vec embeddings, and $\left\lVert\cdot\right\rVert_F$
  is the Frobenius norm of the resulting difference.  With this
  matrix, we then obtained the best possible approximations of
  task-specific vectors for unknown words that were encountered in the
  test set;
\item finally, we also experimented with the normal \emph{word2vec
    embeddings} which were learned with the standard options on the
  German Twitter snapshot~\cite{Scheffler:14}.
\end{itemize}

The results of these modifications are given in
Table~\ref{snt-fgsa:tbl:embeddings}.  As we can see from the scores,
the least-squares method significantly boosts the recall, which, in
turn, leads to much higher macro-averaged \F-measures, outperforming
all other compared approaches. The task-specific variant shows
second-best results, mainly due to a higher precision of the targets
and sources.  Last but not least, the word2vec approach still brings
an improvement for the prediction of sentiment spans, but otherwise
shows a notable degradation at literally every other opinion-related
aspect.

\begin{table*}
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{1}{>{\centering\arraybackslash}p{0.136\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Data Set} & \multicolumn{3}{c}{\bfseries Sentiment} & %
      \multicolumn{3}{c}{\bfseries Source} & %
      \multicolumn{3}{c}{\bfseries Target} & %
      \multirow{2}{0.136\columnwidth}{\bfseries\centering Macro\newline \F{}}\\\cline{2-10}
      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} &\\\midrule

      \multicolumn{11}{c}{\cellcolor{cellcolor}Task-Specific Embeddings}\\

      LSTM & 0.283 & 0.288 & 0.278 & %
       \textbf{0.293} & 0.372 & 0.328 & %
       \textbf{0.254} & 0.27 & \textbf{0.259} & 0.288\\

      GRU & 0.287 & 0.246 & 0.263 & %
       0.287 & 0.405 & \textbf{0.335} & %
       0.252 & 0.205 & 0.216 & 0.271\\

      \multicolumn{11}{c}{\cellcolor{cellcolor}Least-Squares Embeddings}\\

      LSTM & 0.268 & \textbf{0.37} & 0.307 & %
      0.261 & \textbf{0.414} & 0.314 & %
      0.223 & \textbf{0.275} & 0.245 & \textbf{0.289}\\

      GRU & 0.256 & 0.341 & 0.291 & %
       0.267 & 0.395 & 0.318 & %
       0.229 & 0.262 & 0.245 & 0.285\\

      \multicolumn{11}{c}{\cellcolor{cellcolor}word2vec Embeddings}\\

      LSTM & \textbf{0.291} & 0.329 & \textbf{0.309} & %
       0.2 & 0.311 & 0.244 & %
       0.221 & 0.219 & 0.22 & 0.257\\

      GRU & 0.273 & 0.355 & 0.301 & %
       0.207 & 0.353 & 0.257 & %
       0.213 & 0.26 & 0.233 & 0.264\\\bottomrule
    \end{tabular}
    \egroup
    \caption{Results of fine-grained sentiment analysis with different
      types of word embeddings.}
    \label{snt-fgsa:tbl:embeddings}
  \end{center}
\end{table*}

\subsection{Effect of Text Normalization}

Another question which remained open in the foregoing experiments was
whether the input passed to the models actually had to be normalized
or not.  As mentioned in Section~\ref{snt:fgsa:subsec:data}, when
preparing the data, we preprocessed all corpus tweets using the
rule-based normalization procedure of~\citet{Sidarenka:13}.  In this
approach, we
\begin{itemize}
\item replaced syntactically integrated Twitter-specific phenomena
  (@-mentions, hyperlinks, e-mail addresses etc.) with special unified
  tokens representing their semantic classes (e.g., ``\%Username'' for
  @-mentions, and ``\%URI'' for hyperlinks);
\item removed these phenomena if they were syntactically independent
  and did not play a potential role in the expression of sentiments
  (e.g., we stripped off all retweet mentions and hyperlinks appearing
  at the very end of the tweets if they were not preceded by a
  preposition);
\item substituted all emoticons with unified placeholders representing
  their polarities (e.g., \smiley{} $\rightarrow$ ``\%PosSmiley'',
  \frownie{} $\rightarrow$ ``\%NegSmiley'', \texttt{:-O} $\rightarrow$
  ``\%Smiley'' etc.);
\item stripped off the number sign (\#) from the beginning of hashtags
  (e.g., ``\#gl\"ucklich'' $\rightarrow$ ``gl\"ucklich'');
\item and, finally, restored some common forms of misspellings (e.g.,
  ``zuguckn'' $\rightarrow$ ``zugucken'' (\emph{to watch}), ``Tach''
  $\rightarrow$ ``Tag'' (\emph{day}) etc.) using a set of
  manually-defined rules.
\end{itemize}

Even though these transformations were supposed to improve the
grammaticality of the sentences, an opposite consequence of this
normalization could be the loss of (potentially valuable) surface
features.  In order to check, which of these effects had a stronger
impact on the FGSA results, we repeated the evaluation once again,
turning the preprocessing pipeline off this time.

\begin{table*}[bht!]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{1}{>{\centering\arraybackslash}p{0.136\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Data Set} & \multicolumn{3}{c}{\bfseries Sentiment} & %
      \multicolumn{3}{c}{\bfseries Source} & %
      \multicolumn{3}{c}{\bfseries Target} & %
      \multirow{2}{0.136\columnwidth}{\bfseries\centering Macro\newline \F{}}\\\cline{2-10}
      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} &\\\midrule

      \multicolumn{11}{c}{\cellcolor{cellcolor}w Normalization}\\

      CRF & \textbf{0.376} & \textbf{0.319} & \textbf{0.345} & %
       \textbf{0.298} & 0.33 & 0.313 & %
       \textbf{0.293} & 0.231 & 0.258 & \textbf{0.305}\\

      LSTM & 0.283 & 0.288 & 0.278 & %
       0.293 & 0.372 & 0.328 & %
       0.254 & \textbf{0.27} & \textbf{0.259} & 0.288\\

      GRU & 0.287 & 0.246 & 0.263 & %
       0.287 & \textbf{0.405} & \textbf{0.335} & %
       0.252 & 0.205 & 0.216 & 0.271\\

      \multicolumn{11}{c}{\cellcolor{cellcolor}w/o Normalization}\\

      CRF & 0.301 & 0.278 & 0.289 & %
       0.276 & 0.3  & 0.287 & %
       0.255 & 0.23 & 0.242 & 0.273\\

      LSTM & 0.274 & 0.252 & 0.261 & %
       0.284 & 0.367 & 0.32 & %
       0.237 & 0.241 & 0.237 & 0.273\\

      GRU & 0.266 & 0.245 & 0.252 & %
       0.296 & 0.369 & 0.328 & %
       0.232 & 0.268 & 0.245 & 0.275\\\bottomrule

    \end{tabular}
    \egroup
    \caption{Results of fine-grained sentiment analysis with (w) and
      without (w/o) text normalization.}
    \label{snt-fgsa:tbl:normalization}
  \end{center}
\end{table*}

As we can see from the results in
Table~\ref{snt-fgsa:tbl:normalization}, text preprocessing was
unequivocally beneficial to the sentiment classification as all of the
best observed results were achieved exclusively with normalized text.
The only aspect which benefited from keeping the input unchanged was
the precision of target classification with GRU, which, in turn, led
to a slightly higher (+0.4\%) macro-averaged \F-scores for this
system.  Apart from that, all other aspects and classifiers showed a
notable degradation when the preprocessing module was switched off.

\subsection{Effect of the Sentiment Lexicon}

Finally, to answer the question pointed out at the beginning of this
chapter (how useful sentiment lexicons were extrinsically), we
scrutinized the effect of the lexicon that we used in the CRF system
\cite[Zurich Polarity List;][]{Clematide:10} in more detail.  For this
purpose, we repeated the CRF experiments with all lexicon features
removed from the input.  In addition to that, in order to see whether
deep RNN methods would benefit from the lexicon data, we also
re-evaluated the GRU and LSTM systems, appending the lexicon scores of
the input words to their respective embedding vectors.  (This
additional part of the embeddings was kept fixed and did not get
updated during training.)

\begin{table*}
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{1}{>{\centering\arraybackslash}p{0.136\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Data Set} & \multicolumn{3}{c}{\bfseries Sentiment} & %
      \multicolumn{3}{c}{\bfseries Source} & %
      \multicolumn{3}{c}{\bfseries Target} & %
      \multirow{2}{0.136\columnwidth}{\bfseries\centering Macro\newline \F{}}\\\cline{2-10}
      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} &\\\midrule

      \multicolumn{11}{c}{\cellcolor{cellcolor}w Lexicon}\\

      CRF & 0.379 & 0.309 & 0.34 & %
        0.291 & 0.32 & 0.305 & %
        0.284 & 0.226 & 0.252 & 0.299\\

      LSTM &  &  &  & %
        &  &  & %
        &  &  & \\

      GRU & 0.268 & 0.235 & 0.249 & %
        0.28 & 0.383 & 0.323 & %
        0.253 & 0.193 & 0.217 & 0.263\\

      \multicolumn{11}{c}{\cellcolor{cellcolor}w/o Lexicon}\\

      CRF & 0.382 & 0.303 & 0.338 & %
       0.288 & 0.3 & 0.294 & %
       0.291 & 0.221 & 0.251 & 0.294\\

      LSTM & 0.283 & 0.288 & 0.278 & %
       0.293 & 0.372 & 0.328 & %
       0.254 & 0.27 & 0.259 & 0.288\\

      GRU & 0.287 & 0.246 & 0.263 & %
       0.287 & 0.405 & 0.335 & %
       0.252 & 0.205 & 0.216 & 0.271\\\bottomrule

    \end{tabular}
    \egroup
    \caption{Results of fine-grained sentiment analysis with (w) and
      without (w/o) sentiment lexicons.}
    \label{snt-fgsa:tbl:lexicons}
  \end{center}
\end{table*}

The results of these approaches with both configurations (with and
without the lexicon) are shown in Table~\ref{snt-fgsa:tbl:lexicons}.
Somewhat surprisingly, the loss of the lexicon information did not
cause a severe drop of the CRF results: the biggest degradation here
was observed for the \F-score of sentiment sources and amounted to
only 0.9~\%.  Other sentiment-related aspects showed even smaller
deteriorations, so that the final macro-averaged \F-measure without
the lexicon features was only 0.5~\% lower than the original score
attained with the full feature set.

\section{Summary and Conclusions}\label{fgsa:subsec:conclusions}

TODO

% We also note that all our obtained results are significantly lower
% than the corresponding figures achieved for other domains and other
% languages.  A closer look into the errors showed us that the main
% reason for such severe accuracy drop was an insufficiently good
% recognition of opinionated words and phrases.  This can be partially
% explained by the creativity of the users in expressing their thoughts
% but also by the lack of appropriate sentiment dictionaries which would
% not only contain standard language expressions but also slang words
% and cusses.

% An additional challenge was posed by emoticons which could not only
% convey an evaluative meaning but also serve as politeness expressions
% and were therefore ambiguous sentiment markers.  Furthermore, we
% should also mention that the German language itself is more difficult
% for automatic processing due to its relatively free word order,
% ambiguous inflections, and a rich morphology.  And these problems
% become even more aggravated when dealing with Twitter.

% We also saw that the outcomes of our experiments crucially depended on
% the definition of the sentiments that we applied and that different
% CRF variants had different influence on the classification accuracy
% for either interpretation.  Unfortunately, none of these models could
% clearly outperform its rivals, but we could detect some general trends
% for each of the proposed interpretations.  If these trends would also
% hold whe using other feature sets and other corpora is one of the
% questions that we want to address in future research.

\section{Related Work}

Due to its high social impact and big economic importance,
fine-grained sentiment analysis has always been an area of active
research in the opinion mining community.  One of the first
groundbreaking steps in this direction was made
by~\citet{Nasukawa:03}, who proposed a lexicon-based approach for
determining phrase-level sentiments towards particular pre-defined
objects.  More precisely, in the initial phase of their method, the
authors determined occurrences of a priori known targets in text.
Afterwards, they analyzed the immediate context of these occurrences,
looking for subjective terms from their manually compiled lexicon;
and, finally, determined the overall orientation of these opinions,
judging by the polarity scores of the lexicon terms and a set of
heuristic rules which accounted for contextual changes.

Similarly to this approach, \citet{Bethard:04} proposed a method for
identifying subjective opinions and their holders, in which they first
determined polar expressions using the lexicon of~\citet{Wiebe:03} and
then used these expressions as features for a set of SVM classifiers.
The authors applied the resulting ensemble to the nodes of syntactic
constituency trees, trying to predict which of these nodes were the
heads of opinionated clauses or sources of sentiments.  In the same
vein, \citet{Kobayashi:07} used a lexicon-based system for finding
subjective expressions pertaining to some pre-determined product
aspects.

One of the first attempts at automatically analyzing subjective
opinions with conditional random fields was made by \citet{Choi:05},
who combined a linear-chain CRF system with the unsupervised pattern
extractor AutoSlog-TS~\cite{Riloff:96} in order to predict holders of
opinions.  This approach showed a significant improvement over
compared baselines (noun phrases pre-selected by heuristic rules).
Later on, \citet{Choi:06} further enhanced their system,
simultaneously trying to identify both sources of sentiments and their
respective polar expressions, also establishing links between these
entities.  For this purpose, they applied two CRF classifiers (one for
each entity type), getting top-10 results from each of these systems.
After obtaining these predictions, they pruned off invalid suggestions
using a set of hard and soft constraints, whose weights were optimized
through the integer linear programming.  This two-stage procedure not
only improved the automatic recognition of sources and polar
expressions, but also yielded an impressive \F-score of 0.689 on
predicting inter-entity links.

With the release of the MPQA corpus \cite[cf.][]{Wiebe:05},
fine-grained opinion-mining researchers have gradually shifted the
focus of their work to the analysis of newspaper texts: For example,
\citet{Breck:07} addressed the problem of predicting \emph{direct
  speech events} (DSEs) and \emph{expressive subjective elements}
(ESE), for which they also adopted the CRF approach, getting a
substantial improvement over dictionary-based baselines.
\citet{Choi:10} attempted to jointly predict boundaries, polarity, and
intensity of subjective elements (DSEs and ESEs) with a single CRF
classifier.  To this end, they applied the hierarchical parameter
sharing technique \cite{Zhao:08}, letting their system differentiate
between ten different sentiment classes (two classes for the presence
or absence of an opinion times three polarities times three
intensities), ensuring, however, that the parameters belonging to the
same group (e.g., positive polarity) were shared among all the
respective subclasses (e.g., positive sentiments with high, low, and
middle intensities).

Instead of relying on just one most likely label assignment when
predicting subjective expressions, \citet{Johansson:10a} explored the
possibility of reranking the initial decisions by first obtaining
top-$k$ alternative predictions (with $k = 8$) and then reweighting
these labelings with a different classifier.  In particular, they
experimented with linear and tree-based preference kernels as well as
structured perceptron and passive-aggressive algorithms for doing this
reweighting, finding that the last option---the passive-aggressive
method---performed best on the MPQA dataset, yielding 4\% improvement
over the standard linear-chain CRF baseline.
\citet[cf.][]{Johansson:10b} also used this idea for the joint
extraction of subjective expressions and their holders from newspaper
articles.

\citet{Yang:12} were allegedly the first who used semi-Markov CRFs in
an opinion mining application.  In order to determine the boundaries
of DSEs and ESEs, the authors derived a set of potential segments from
shallow syntactic parses and then let a semi-Markov model determine
the most likely segmentation and label assignment over these
hypotheses.

A veritable milestone in the fine-grained sentiment analysis research
was set by~\citet{Socher:13}, who first introduced a deep learning
method for predicting the polarity of syntactic constituents.  In
their work, the authors computed a vector representation of a given
constituency node ($\vec{w}_c \in \mathbb{R}^n$) by recursively
multiplying the embeddings of its descendants (e.g.,
$\vec{w}_1 \in \mathbb{R}^n$ and $\vec{w}_2 \in \mathbb{R}^n$) with a
compositional tensor $V \in \mathbb{R}^{2n\times2n\times2n}$ and
applying the softmax non-linearity afterwards:
\begin{align*}
  \vec{w}_c = softmax\left(%
  \begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}^T \cdot V^{1:n}\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}%
  + W\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}%
  \right),
\end{align*}
The $W\in\mathbb{R}^{n\times n}$ term in the above equation stands for
an additional shared compositionality matrix, and the tensor
product for a single element $\vec{v}_i$ is defined as follows:
\begin{align*}
  \vec{v}_i = \begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}^T \cdot V^{i}\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}.
\end{align*}
The embeddings of the underlying words as well as the compositionality
matrix $W$ and the tensor $V$ were learned automatically during
training.

A recurrent approach to the FGSA problem was proposed
by~\citet{Irsoy:14a} \cite[see also][]{Irsoy:14b}, who explored deep
bidirectional neural networks for predicting DSEe and ESEs on the MPQA
corpus.  In particular, they estimated the label probabilities
$\vec{y}$ for the $t$-th token as:
\begin{align*}
  y_t = softmax\left(U_{\rightarrow}\cdot \vec{h}_t^{(L)} + U_{\leftarrow}\cdot \cev{h}_t^{(L)} + \vec{c}\right),
\end{align*}
where $\vec{h}_t^{(L)}$ and $\cev{h}_t^{(L)}$ were the outputs of the
top-$L$ LSTM-levels computed from left to right and from right to left
respectively; and $U_{\rightarrow}$, $U_{\leftarrow}$, and the bias
term $\vec{c}$ were the other learned parameters which were used to
reduce the dimensions.  This system could outperform the previous
state of the art set by \citet{Yang:12}, reaching a proportional
\F-score of 66.01 for DSEs and 56.26 for ESEs.

Other notable works on fine-grained opinion mining with deep neural
networks include those of \citet{Irsoy:14c}, who used a tensor
multiplication within a recursive loop; \citet{Tang:16}, who proposed
a memory-based multilevel network for classifying opinions about the
given aspects; and, finally, \citet{Wang:16}, who used the output of a
deep recursive network as input to a CRF system, updating the weights
of the network along with the CRF parameters.

\newpage
