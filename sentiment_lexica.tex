% FILE: sentiment_lexica.tex  Version 0.01
% AUTHOR: Uladzimir Sidarenka

% This is a modified version of the file main.tex developed by the
% University Duisburg-Essen, Duisburg, AG Prof. Dr. Günter Törner
% Verena Gondek, Andy Braune, Henning Kerstan Fachbereich Mathematik
% Lotharstr. 65., 47057 Duisburg entstanden im Rahmen des
% DFG-Projektes DissOnlineTutor in Zusammenarbeit mit der
% Humboldt-Universitaet zu Berlin AG Elektronisches Publizieren Joanna
% Rycko und der DNB - Deutsche Nationalbibliothek

\section{Sentiment Lexica}\label{sec:snt:lex}

The first avenue that we are going to explore with the help of the
obtained corpus data is the automatic prediction of polar lexical
terms or, in other words, emotional expressions.  To this end, we will
first estimate the recall, precision, and $F1$-score of the sentiment
lexica that currently exist German.  Since all of these resources were
created in a semi-supervised way by automatically translating lists of
opinionated English terms and then manually revising these
translations, we will also recheck whether the original methods that
were initially used for creating the English sources would yield
better results when applied to German data directly.  In the
concluding step, we will finally investigate whether one of the most
popular machine learning trends of the the recent time -- the neural
word embeddings \cite{Mikolov:13} -- would provide a valuable basis
for obtaining domain-specific sentiment lexica for a new language
automatically.

\subsection{Data}

In the following experiments, we will use version 0.1.0 of the Potsdam
Twitter Sentiment Corpus (PotTS) presented in the previous section.
The main changes included in this version as compared to the initial
corpus state are:
\begin{inparaenum}
  \item the revision of the existing annotations of the emotional
    expressions,

  and \item the introduction of the boolean flags
  \emph{subjective-fact} and \emph{uncertain} for these elements into
  the annotation scheme.
\end{inparaenum}

To achieve the first goal, we automatically highlighted the
differences between the initial corpus annotations and the existing
German sentiment lexica, letting our experts revise these
contradictions.

The aforementioned attributes were introduced on request of our
experts as they could not come to a firm conclusion whether they had
to consider subjective facts as normal opinionated terms (hence the
\emph{subjective-fact} feature) and whether they had to annotate with
this tag some boundary cases.

The statistics on the total number of the labeled elements and their
updated agreement is shown in Table~\ref{tbl:snt-lex:agrmnt}:

\begin{table*}[thb!]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.7\tabcolsep} \scriptsize
    \begin{tabular}{|p{0.15\textwidth}| % first columm
        *{10}{>{\centering\arraybackslash}p{0.05\textwidth}|}} % next ten columns
      \hline
          \multirow{2}{0.2\textwidth}{\bfseries Element} &
          \multicolumn{5}{c|}{Binary $\kappa$} & %
          \multicolumn{5}{c|}{Proportional $\kappa$}\\\cline{2-11}
          & $M_1$ & $A_1$ & $M_2$ & $A_2$ & $\mathbf{\kappa}$ %
          & $M_1$ & $A_1$ & $M_2$ & $A_2$ & $\mathbf{\kappa}$\\\hline

          EExpression &  &  &  & & \textbf{} & &  &  &  & \textbf{}\\\hline
    \end{tabular}
    \egroup
  \end{center}
  \captionof{table}{Inter-annotator agreement on the emotional
    expression after the corpus update.\\ {\small ($M1$ -- number of
      tokens with matching labels in the first annotation, $A1$ --
      total number of tokens labeled with that class in the first
      annotation, $M2$ -- number of tokens with matching labels in the
      second annotation, $A2$ -- total number of tokens labeled with
      that class in the second annotation)}}
  \label{tbl:snt-lex:agrmnt}
\end{table*}

\subsection{Baseline: Existing German Lexica}

To obtain a raw estimate of the expected scores for the recognition of
subjective expressions, we will first evaluate the quality of the
existing sentiment resources for German on our corpus.  The most
prominent of these resources are:

The German Polarity Clues of \citet{Waltinger:10}

The SentiWS Lexicon of \citet{Remus:10}

The Zurich Polarity Word List of \citet{Clematide:10}

Since all of these lexica were created in a similar way but from
different sources we also decided to check whether the union or the
intersection of their entries would yield competitive results.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize \small
    \begin{tabular}{|p{0.21\columnwidth}| % first columm
        *{8}{>{\centering\arraybackslash}m{0.1\columnwidth}|}} % next nine columns
      \hline
          \multirow{2}*{\bfseries Element} & \multicolumn{3}{c|}{Positive
        Expressions} & %
      \multicolumn{3}{c|}{Negative Expressions} & %
      \multirow{2}*{Macro-$F1$} & %
      \multirow{2}*{Micro-$F1$}\\\cline{2-7}

      & Precision & Recall & $F1$ & Precision & Recall & $F1$ & & \\\hline
      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Existing Lexica}\\\hline

      GPC & 17.7\stddev{6.07} & 48.35\stddev{13.26} &%
      12.66\stddev{3.59} & 9.42\stddev{4.37} & 45.99\stddev{15.58} &
      7.6\stddev{3.12} & 22.51\stddev{1.9} & 89.56\stddev{1.28}\\

      SWS & 29.26\stddev{11.3} & 29.26\stddev{11.3} &
      16.39\stddev{4.95} & 36.03\stddev{14.42} & 36.45\stddev{15.29} &
      \textbf{17.16}\stddev{5.49} & 27.48\stddev{2.95} &
      95.61\stddev{0.78}\\

      ZPL & 33.00\stddev{11.69} & 31.14\stddev{10.53} &
      15.60\stddev{4.81} & 23.68\stddev{12.12} & 25.15\stddev{11.85} &
      11.76\stddev{5.37} & 25.45\stddev{2.71} & 95.93\stddev{0.66}\\

      GPC $\cap$ SWS $\cap$ ZPL & \textbf{46.22}\stddev{13.44} &
      36.43\stddev{13.46} & \textbf{19.77}\stddev{5.87} &
      \textbf{48.06}\stddev{17.22} & 26.93\stddev{12.99} &
      16.35\stddev{5.96} & \textbf{28.45}\stddev{3.16} &
      \textbf{96.94}\stddev{0.6}\\

      GPC $\cup$ SWS $\cup$ ZPL & 16.89\stddev{5.77} &
      \textbf{49.98}\stddev{12.96} & 12.35\stddev{3.49} &
      9.36\stddev{4.4} & \textbf{49.05}\stddev{15.9} &
      7.66\stddev{3.18} & 22.37\stddev{1.91} &
      88.97\stddev{1.26}\\\hline
      \multicolumn{9}{|c|}{\cellcolor{cellcolor}State-of-the-art Approaches}\\\hline

      SentiWordNet$^{\mathrm{ternary}}_{\mathrm{Rocchio}}$ & 67.09\stddev{22.16} &
      14.74\stddev{7.79} & 11.73\stddev{5.59} & 4.57\stddev{4.57} &
      5.88\stddev{5.4} & 2.48\stddev{2.37} & 21.08\stddev{2.15} &
      96.04\stddev{0.72}\\

      Ising Spin Model & \stddev{} & \stddev{} & \stddev{} & \stddev{}
      & \stddev{} & \stddev{} & \stddev{} & \stddev{}\\\hline

      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Our Method}\\\hline
    \end{tabular}
    \egroup
    \caption{Classification results.\\ {\small (GPC -- German Polarity
        Clues \cite{Waltinger:10}, SWS -- SentiWS \cite{Remus:10}, ZPL
        -- Zurich Polarity Lexicon \cite{Clematide:10})}}
    \label{snt-lex:tbl:gsl-res}
  \end{center}
\end{table}

\subsection{Ontology-based Lexicon Generation}

Since SentiWS \cite{Remus:10} was the best compared stand-alone
lexicon, a natural question that arises in this case is whether the
original method used for creating its underlying Engish source data --
the SentiWordNet lexicon of \citet{Esuli:06b} -- would also produce a
comparatively good polar term list when applied to German data
directly.

To check this hypothesis, we have re-implemented the original system
of the authors of SentiWordNet and applied it to the German equivalent
of the English WordNet \cite{Miller:95} -- the GermaNet database
\cite{Hamp:97}.

The results of this method are shown in Table~\ref{snt-lex:tbl:swn-res}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize \small
    \begin{tabular}{|p{0.21\columnwidth}| % first columm
        *{8}{>{\centering\arraybackslash}m{0.1\columnwidth}|}} % next nine columns
      \hline
          \multirow{2}*{\bfseries Element} & \multicolumn{3}{c|}{Positive
        Expressions} & %
      \multicolumn{3}{c|}{Negative Expressions} & %
      \multirow{2}*{Macro-$F1$} & %
      \multirow{2}*{Micro-$F1$}\\\cline{2-7}

      & Precision & Recall & $F1$ & Precision & Recall & $F1$ & & \\\hline
      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Existing Lexica}\\\hline

      SentiWordNet$^{\mathrm{ternary}}_{\mathrm{Rocchio}}$ & 67.09\stddev{22.16} &
      14.74\stddev{7.79} & 11.73\stddev{5.59} & 4.57\stddev{4.57} &
      5.88\stddev{5.4} & 2.48\stddev{2.37} & 21.08\stddev{2.15} &
      96.04\stddev{0.72}\\

      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Our Method}\\\hline
    \end{tabular}
    \egroup
    \caption{Classification results.\\ {\small (GPC -- German Polarity
        Clues \cite{Waltinger:10}, SWS -- SentiWS \cite{Remus:10}, ZPL
        -- Zurich Polarity Lexicon \cite{Clematide:10})}}
    \label{snt-lex:tbl:swn-res}
  \end{center}
\end{table}

\subsection{Corpus-based Lexicon Induction}

Another popular alternative to the Ontology-based methods is lexicon
induction on the basis of actual corpus data.  Considering that
Twitter vocabulary is typically very different from the entries that
are usually included into standard-language dictionaries, applying
this strategy directly to tweets might potentially significantly
outperform the results of both translated resources and polar term
lists generated from GermaNet.

To check this hypothesis, we have re-implemented the Ising Spin system
of \citet{Takamura:05} -- one of the arguably most competitive methods
for unsupervised lexicon induction -- and applied it to the German
Twitter snapshot of \cite{Scheffler:14}.

The results of this method are shown in Table~\ref{snt-lex:tbl:ispn-res}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize \small
    \begin{tabular}{|p{0.21\columnwidth}| % first columm
        *{8}{>{\centering\arraybackslash}m{0.1\columnwidth}|}} % next nine columns
      \hline
          \multirow{2}*{\bfseries Element} & \multicolumn{3}{c|}{Positive
        Expressions} & %
      \multicolumn{3}{c|}{Negative Expressions} & %
      \multirow{2}*{Macro-$F1$} & %
      \multirow{2}*{Micro-$F1$}\\\cline{2-7}

      & Precision & Recall & $F1$ & Precision & Recall & $F1$ & & \\\hline
      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Existing Lexica}\\\hline

      Ising Spin Model & \stddev{} & \stddev{} & \stddev{} & \stddev{}
      & \stddev{} & \stddev{} & \stddev{} & \stddev{}\\\hline

      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Our Method}\\\hline
    \end{tabular}
    \egroup
    \caption{Classification results.\\ {\small (GPC -- German Polarity
        Clues \cite{Waltinger:10}, SWS -- SentiWS \cite{Remus:10}, ZPL
        -- Zurich Polarity Lexicon \cite{Clematide:10})}}
    \label{snt-lex:tbl:ispn-res}
  \end{center}
\end{table}

\subsection{Lexicon Generation Using Neural Word Embeddings}

A new family of lexicon induction methods builds on learned vector
representations of words -- the neural word embeddings
\cite{Mikolov:13}.

The results of this method are shown in Table~\ref{snt-lex:tbl:w2v}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize \small
    \begin{tabular}{|p{0.21\columnwidth}| % first columm
        *{8}{>{\centering\arraybackslash}m{0.1\columnwidth}|}} % next nine columns
      \hline
          \multirow{2}*{\bfseries Element} & \multicolumn{3}{c|}{Positive
        Expressions} & %
      \multicolumn{3}{c|}{Negative Expressions} & %
      \multirow{2}*{Macro-$F1$} & %
      \multirow{2}*{Micro-$F1$}\\\cline{2-7}

      & Precision & Recall & $F1$ & Precision & Recall & $F1$ & & \\\hline
      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Existing Lexica}\\\hline

      SentiWordNet$^{\mathrm{ternary}}_{\mathrm{Rocchio}}$ & 67.09\stddev{22.16} &
      14.74\stddev{7.79} & 11.73\stddev{5.59} & 4.57\stddev{4.57} &
      5.88\stddev{5.4} & 2.48\stddev{2.37} & 21.08\stddev{2.15} &
      96.04\stddev{0.72}\\

      Ising Spin Model & \stddev{} & \stddev{} & \stddev{} & \stddev{}
      & \stddev{} & \stddev{} & \stddev{} & \stddev{}\\\hline

      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Our Method}\\\hline
    \end{tabular}
    \egroup
    \caption{Classification results.\\ {\small (GPC -- German Polarity
        Clues \cite{Waltinger:10}, SWS -- SentiWS \cite{Remus:10}, ZPL
        -- Zurich Polarity Lexicon \cite{Clematide:10})}}
    \label{snt-lex:tbl:w2v}
  \end{center}
\end{table}

\subsection{Related Work}

Among the first who addressed the problem of the automatic generation
of sentiment lexica were \citet{Hatzivassi:97}.  In their work, the
authors relied on the hypothesis that coordinatively conjoined terms
often share the same semantic orientation while adversatively linked
words rather express opposite polarities.  To test this conjecture,
they automatically extracted all pairs of conjoined adjectives from
the Wall Street Journal (WSJ) corpus and represented those adjectives
as nodes in a graph.  The arcs weights of this graph were to show the
strength and direction by which two coordinatively conjoined terms
influenced each others' polarity.  To derive these weights,
\citeauthor{Hatzivassi:97} trained a log-linear regression model on
those pairs of terms in which both nodes belonged to a manually
labeled seed set of 1,336 adjectives and then let this model predict
the weights for the rest of the arcs.  In the final step, the
resulting graph was partitioned into two clusters -- that of positive
and that of negative terms -- which were subsequently used to enrich
the initial seed set.
%% This method, enhanced by the possibility of recognizing gradable
%% adjectives, was later used in the classification experiments of
%% \citet{Hatzivassi:00} to predict subjective and objective sentences in
%% the WSJ.

A different way of bootstrapping polarity terms from large text
corpora was proposed by \citet{Turney:03}.  Following
\citeauthor{Turney:02}'s original approach for classifying reviews
\citep{Turney:02}, the authors generated a polarity lexicon by first
taking a seed set of 14 a priory known polar adjectives (seven
negative and seven positive ones) and then expading this set with the
words that had the strongest pointwise mutual information associations
with the chosen seeds.  The PMI scores were computed as the log ratio
between the number of times a new word $w$ appeared with any of the
seed terms, divided by the total number of search hits for the
complete seed set.  As search hits, the authors considered the number
of relevant documents returned by the \texttt{AltaVista} search engine
for the given queries.  This system attained an accuracy of 82.84\% on
the General Inquirer Lexicon \citep{Stone:66} and correctly predicted
polar terms from the lexicon of \cite{Hatzivassi:97} in 87.13\% of the
cases. %% This method could also be further improved by using cosine
%% similarities between word vectors from an LSA matrix instead of
%% web-based PMIs.

Other notable works on corpus-based lexicon induction include
\citet{Kanayama:06}, who enhanced the method of
\citeauthor{Hatzivassi:97} by incorporating iter-sentential coherence
relations.  \citet{Kaji:07} also followed a corpus-based approach as
they mined opinionated sentences from HTML pages using structural and
linguistic clues and then extracted new polar terms from these
sentences, considering words having the highest PMI association scores
with the rest of the sentences as subjective.

One of the allegedly first attempts to derive a sentiment lexicon from
a lexical ontology instead of a corpus was made by \citet{Kim:04}.  In
their work, the authors expanded an initial seed set of 34 adjectives
and 44 verbs to a list of 18,192 presumably polar terms by iteratively
enriching the seed words with their synonyms and antonyms from the
\texttt{WordNet} database \citep{Miller:95}. In a similar way,
\citet{Godbole:07} enriched their topic-specific seed sets of polar
expressions by following the synonymy and antonymy links in
\texttt{WordNet} and exponentially decreasing the probability of a new
word being polar with the growing path length from a seed term.

%% Another interesting finding of this paper was surpassed by using
%% the cosine similarities between word vectors in an LSA matrix
%% instead of PMI scores from web crawlers.

%% More precisely, the social orientation PMI (SO-PMI) of a new word
%% $w$ was defined as: \begin{equation*} \text{SO-PMI}(w) =
%% \log_2\bigg(\frac{\prod_{p\in\mathcal{P}}\text{hits}(w\text{NEAR}p)%
%% \prod_{n\in\mathcal{N}}\text{hits}(w\text{NEAR}n)}%
%% {\prod_{p\in\mathcal{P}}\text{hits}(p)\prod_{n\in\mathcal{N}}\text{hits}(n)}\bigg) \end{equation*}
%% where $\mathcal{P}$ and $\mathcal{N}$ are the seed sets of positive
%% and negative adjectives respectively and
%% $\text{hits}(\cdot\text{NEAR}\cdot)$ is the number

%% The authors evaluated the accuracy of their model on the General
%% Inquirer lexicon \cite{Stone:66}.  Its final results (81.9\%) were
%% comparable to the figures obtained by \citet{Turney:03} in their
%% method (82.84\%) and significantly outperformed the precision of
%% the approach proposed by \citet{Kamps:04} (73.4 versus 70.8).

Another purely dictionary-based method was described by
\citet{Esuli:06b}.  The authors automatically assigned positive,
negative, and neutral polarity scores to synsets in \texttt{WordNet}
by iteratively expanding seed sets for each of these three polarity
classes using a committee of binary and multiclass classifiers.  At
each step of this process, these classifiers were trained on the seed
sets obtained from the previous runs and then used to predict whether
further synsets immediately accessible from the recently expanded
seeds would belong the same polarity class as their seed term.

\citet{Takamura:05} attempted to unite corpus- and dictionary-based
approaches into a single framework.  To this end, the authors adopted
the Ising spin model from the statistical mechanics and represented
all words found in \texttt{WordNet}, in the Wall Street Journal, and
the Brown corpus as nodes in a graph.  The edges of this graph
represented associativity links and were established between any two
words, if one of these word was a synonym, an antonym, or a hyponym of
the other one or appeared in its gloss or context.  Taking into
account the a priori known polarities of some of the terms, the Ising
model then tried to find an approximation of the most likely polarity
combination of all terms in the graph over all possible polarity
assignments.

\subsubsection{Domain-Specific Sentiment Lexica}

\citet{Chetviorkin:14} obtained a set of possible subjective terms
from English and Russian microblogs by using an ensemble of supervised
machine learning classifiers that had previously been trained on a
manually annotated corpus of movie reviews.  In order to determine the
prior polarity of the extracted terms, the authors first calculated
approximate polarity scores of the processed messages using general
polarity lexicons and then took these rough estimates as prior
polarity expectations of the candidate expressions.  The posterior
scores of these expressions were computed using the Ising spin model
in a similar way to the approach proposed by \citet{Takamura:05}.  The
resulting lexicon comprised 2,772 words for Russian and 2,786 lexical
items for English.

\subsubsection{Sentiment Lexica for German}

One of the first attempts to create a sentiment dictionary for German
was made by \citet{Remus:10}.  In their work, the authors
automatically translated the General Inquirer lexicon \citep{Stone:66}
into German.  In order to incorporate domain-specific knowledge into
this translated resource, they also collected a set of terms that were
strongly associated with positive or negative product reviews and
added this set to their translations.  Finally, the resulting lexicon
was enriched with inflection forms and frequent collocations of the
translated expressions.  The authors automatically estimated polarity
scores of their obtained 3,648 lexicon lemmas as the difference
between their positive and negative PMIs (i.e. PMI scores between the
collected terms and the pre-defined sets of positive or negative
expressions).

Similar work was also done by \citet{Waltinger:10} who compiled a list
of 10,141 subjective terms by first automatically translating the
Subjectivity Clues lexicon \citep{Wilson:05} and then manually
reassessing polarity scores of the translated items.  The author
expanded this list with the most frequent German synonyms of the
obtained terms and also added a set of negated opinionated phrases
(e.g. \emph{nicht schlecht} (\emph{not bad})).  The resulting
dictionary was used as a source of features for an automatic
classification of the polarity of Amazon reviews and showed superior
accuracy in comparison with other automatically translated resources.

A slightly different approach was taken by \citet{Clematide:10} who
manually annotated synsets from \emph{GermaNet} \cite{Hamp:97} with
their prior polarities and polarity strengths.  A subsequently
conducted study revealed, however, that the stock of polar adjectives
obtained in this way was insufficient for doing proper subjectivity
analysis of literary texts.  In order to overcome this problem, the
authors followed the idea of \citet{Hatzivassi:97} and extracted an
additional set of 918 adjectives which frequently co-occurred in
conjoined phrases with subjective terms from their dictionary.  After
manually inspecting each of the candidate terms, the authors added
this pruned set of new adjectives to their original lexicon.  The
total size of this final dictionary run up to 7,432 entries, 2,779 of
which were positive.

\subsection{Summary and Conclusions}

In this section, we presented the first attempt of a practical
evaluation of our corpus.  In doing so, we addressed the task of
automatic prediction of polar terms (emotional expressions) with the
help of sentiment dictionaries.  To obtain a rough baseline estimate,
we first evaluated the quality of the existing sentiment lists for
German: German Polarity Clues~\cite{Waltinger:10},
SentiWS~\cite{Remus:10}, and Zurich Polarity List~\cite{Clematide:10}.
We showed that \ldots achieved the best quality, reaching an average
$F1$-score of \cdots on recognizing positive expressions and \cdots on
predicting negative polar terms.

In the next step, we analyzed whether the methods that were used for
creating the original English resources whose translations formed the
basis of the German lexica could yield better results than the
manually revised tranlated lists when applied to German data directly.

The first family of lexicon generation methods that we looked into
used Ontology information about lexical items: words' definitions,
examples, and lexical links, to find subjectively connotated terms.
In particular, we reimplemented the original method of
\citet{Esuli:06b} and applied it to the German lexical database --
GermaNet \cite{Hamp:97}.  The results of this approach turned out to
be relatively low, only achievening \cdots for positive expressions
and \cdots for negative polar terms.  These rather low scores can be
explained by the scarceness of GermaNet definitions on the one hand
and the unconventional vocabulary used on Twitter.

Another popular approach to an unsupervised induction of sentiment
lexica relies on the cooccurrence information about the words taken
directly from corpus.  One of the most popular methods from this
category is the Ising spin model adopted from the statistical
mechanics which interprets words as magnetic spins in a crystal grid
and tries to derive the most probable orientation of these spins in a
magnetic field.  This model was first applied to the needs of
computational linguistics by \citet{Takamura:05}, who induced a
sentiment lexicon for English using \cdots corpus data.  We have
reimplemented this approach in our program suite and applied to the
German Twitter snapshop of \citet{Scheffler:14}.  The results of this
approach are shown in Table~\ref{snt-lex:tbl:ispn-res}.

A different way of incorporating corpus data is to encode the
cooccurrence statistics directly into word information, representing
the latter as vectors.  We explored this direction in the final part
of this section, first obtaining word2vec embeddings for tokens from
the aforementioned snapshot and then applying clustering algorithms to
these representations.

Our results show that \cdots.

\newpage
