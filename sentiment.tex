% FILE: chapter-sentiment.tex  Version 0.01
% AUTHOR: Uladzimir Sidarenka

% This is a modified version of the file main.tex developed by the
% University Duisburg-Essen, Duisburg, AG Prof. Dr. Günter Törner
% Verena Gondek, Andy Braune, Henning Kerstan Fachbereich Mathematik
% Lotharstr. 65., 47057 Duisburg entstanden im Rahmen des
% DFG-Projektes DissOnlineTutor in Zusammenarbeit mit der
% Humboldt-Universitaet zu Berlin AG Elektronisches Publizieren Joanna
% Rycko und der DNB - Deutsche Nationalbibliothek

\chapter{Sentiment Analysis}

\section{Introduction to Sentiment Analysis}

Interpersonal communication is not only a way to share objective
information with other people but also a vibrant channel to convey
one's subjective thoughts, attitudes, and feelings.  It is, in fact,
this latter use which provides a personal touch to our discourses,
making them more grasping, more entertaining, and more living.  And it
is often this use which significantly influences our decisions,
preferences, and choices in everyday life.  Therefore, a high-quality
automatic analysis of the subjective part of information is typically
not less important than the extraction and analysis of objective
facts.

The field of knowledge which deals with the analysis of people's
opinions, sentiments, evaluations, appraisals, attitudes, and emotions
towards particular entities mentioned in discourse is called
\emph{sentiment analysis} (SA) \citep{Liu:12}.  The definition of this
discipline, however, much like the definition of the term
\emph{sentiment} itself, is neither complete nor universally accepted.
The main reasons for these controversies are
\begin{inparaenum}[\itshape i)\upshape]
  \item a frequently unclear delimitation of the subjective and
    objective components of information and
  \item the heteroginity of the language system to which SA methods
    are applied.
\end{inparaenum}

The former factor, for instance, makes it difficult to delimitate what
kinds of events and expressions should actually belong to the domain
of sentiment analysis and which ones shall rather be excluded from it.
A prominent example of such problematic borderline cases are the
so-called subjective facts, such as \emph{terrorist attacks} or
\emph{cancer drugs}, which some people perceive as emotionally laden
terms while others regard them as purely objective statements.

The latter factor complicates a precise definition of SA because
different levels of language have their own concepts of subjectivity,
which in turn require different approaches and methods of analysis.
Depending on the language level being analyzed, researchers typically
distinguish three different subtypes of sentiment analysis:
\begin{itemize}
  \item\emph{subsentential} of \emph{fine-grained} SA whose task is to
    determine and analyze specific subjective opinions and/or
    evaluations within single clauses,
  \item\emph{sentential} analysis which tries to ascribe a single
    polarity class to each sentence in a text, and
  \item\emph{suprasentential} or \emph{document-level} sentiment
    classification which seeks to determine the polarity and
    subjectivity classes of complete discourses.
\end{itemize}

Each of these subtypes has its own characteristic strengths and
weaknesses.  The fine-grained SA, for example, is typically considered
to be the ultimate goal of any opinion mining\footnote{Following
  \citet{Liu:12}, we do not make a distinction between the terms
  \emph{opinion mining} and \emph{sentiment analysis} and use both
  expressions interchangeably in this thesis.}  system as it aims at
the highest possible recall of all subjective expressions occurring in
text.  At the same time, this task is unfortunately very challenging
even for human beings, as we will show in Section \ref{sec:corpus},
let alone computer programs.  The consequently low results often
intimidate researchers and prevent subsentential SA systems from being
used in industrial applications.

The last two disciplines (sentential and document-level analyses) can
be considered as approximations of the fine-grained SA.  They coarsen
the targeted recall down to the level of sentences or texts
respectively, trying to determine only one (the most prominent)
expression of subjectivity per analyzed unit.  In contrast to the
subsentential SA, these approaches typically yield better results but
sacrifice important information.

As can be seen, due to these crucial distinctions between different
variants of sentiment analysis, speaking of the difficulty or easiness
of this analysis for a specific domain in general is in the same way
wrong as making overall judgments about the amenability of that domain
to natural language processing: one always has to specify a particular
NLP task and the precise level of sentiment granularity whose
difficulty is being estimated before drawing any conclusions about
their complexity.

In this chapter, we will primarily concentrate on the most challenging
SA objective -- that of the fine-grained SA.  After a brief summary of
related work done on the opinion mining for social media so far, we
will introduce a comprehensive corpus of German tweets that has been
created for the purpose of this work.  A detailed inter-annotator
agreement study of this dataset will reveal which linguistic and
extra-linguistic factors significantly influence the distribution of
sentiments in Twitter and which of them cause utter confusion among
human experts.  After obtaining an upper bound on human performance,
we will successively compare with it the results attained by automatic
sentiment systems, starting with the most fundamental task of
recognizing subjective expressions and concluding with the ultimate
goal of recognizing textual spans of sentiments, their objects of
evaluation (\emph{targets}), and authors (\emph{sources}).

\section{Sentiment Analysis of Social Media}
As noted by many researchers \citep{Aue:05,Blitzer:07,Jakob:10},
sentiment analysis is a highly domain-dependent task: systems trained
for specific topics and on specific domains do not neatly generalize
to other subjects and other text genres.  A natural question that
arises in this context is which of the domains should be addressed
first in this case.

While earlier sentiment works were primarily concentrating on
narratives \citep{Wiebe:90a,Wiebe:94} or formal newspaper texts
\citep{Wiebe:03,Wiebe:05}, it soon became clear that social media
provides a much more fertile ground for mining people's opinions.  The
reason for this is the virtual absence of any moderation on many
modern CMC services.  This lack of censorship allows users to be
forthright about their feelings when confabulating with their friends.
Combined with the great popularity of social networks, such freedom of
expressing one's thoughts makes social media the arguably most
suitable channel for sharing emotions.

It is, therefore, no wonder that the rapid growth of the number of SM
websites was accompanied by an increasing interest in their content
from the side of NLP community.  One of the first reported attempts to
mine users' evaluations automatically from CMC services was made by
\citet{Turney:02}.  The author performed a two-class classification of
410 Epinions comments, dividing customers' reviews of automobiles,
banks, movies, and travel destinations into \emph{recommended} (thumbs
up) and \emph{not recommended} (thumbs down) ones.  To determine the
polarity class of each message, the classifier first extracted
adjectival and adverbial phrases from the text of that message and
then computed the difference between the pointwise mutual information
(PMI) of the obtained phrase and the word ``excellent'' and the PMI of
that phrase and the word ``poor''.  The review was classified as
\emph{thumbs up} if the sum of these differences over the whole text
was positive and as \emph{thumbs down} otherwise.

In the same year, \citet{Pang:02} published an almost eponymous work
in which they described their experiments with classifying movie
reviews from the Internet Movie Database (IMDb) into positive and
negative.  For this purpose, the authors applied several machine
learning classifiers, including Na\"{\i}ve Bayes, MaxEnt, and SVM,
considering different combinations of features.  The best results
(82.9\% accuracy) were attained by the SVM system which used the
presence of unigrams as classification traits.

One of the first attempts to analyze sentiments on Twitter was made by
\citet{Go:09}.  For their experiments, the authors collected a set of
1,600,000 tweets containing smileys.  Based on these emoticons, they
automatically derived polarity classes for these messages (positive or
negative) and used them to train a Na\"{\i}ve Bayes, a MaxEnt, and an
SVM classifier.  The best $F$-score for this two-class classification
problem could be achieved by the last system and run up to 82.2\%.

Similar work was done later by \citet{Pak:10} who used a Na\"{\i}ve
Bayes approach to differentiate between neutral, positive, and
negative microblogs. \citet{Barbosa:10} also gathered a collection of
200,000 tweets from three publicly available sentiment web-services
and then trained an SVM classifier to predict the subjectivity and the
polarity class of new unseen messages.

Works attempting a more fine-grained sentiment analysis on Twitter
usually try to derive a common polarity class for each message with
respect to a particular target that is mentioned in that microblog.

\citet{Jiang:11}, for instance, tried to classify the polarity of
microblogs pertaining to a predefined set of specific topics, like
\emph{Obama}, \emph{Google}, \emph{iPad} etc.  To this end, the
authors manually labeled a corpus of 1,939 messages and trained a
binary SVM model in order to predict the subjectivity and the polarity
of the tweets with respect to the given subjects.

This classifier could achieve an accuracy of 68.2\% for the
subjectivity classification and 85.6\% for the polarity prediction.
The $F$-score of this system for the latter task could further be
improved from 66\% to 68.3\% by incorporating the information about
the predicted polarity class of the re-tweets, replies, and other
microblogs posted by the same author.

\citet{Mitchell:13} broadened the set of possible targets by allowing
any named entities found in microblogs to be associated with a
specific polarity.  For that purpose, the authors combined a CRF-based
NER system with a sentiment predicting CRF by considering three
different possibilities of such combination: a pipeline approach, a
joint multi-layer model, and a single classifier with a combined
tagset.  The best scores on their corpora of 7,105 Spanish and 2,350
English tweets could be achieved with the joint and pipeline
approaches.  The accuracy of recognizing the opinionated named
entities amounted to 31\% for Spanish and 30.4\% for English.

Other notable works in this direction include \citet{Chunping:14} who
first applied a Na\"{\i}ve Bayes classifier to predict the
subjectivity class of microblogs and then sequentially used two CRF
models to predict the particular type of subjectivity (such as anger,
fear, happiness etc.) for message sentences.

Other notable works in this direction include \citet{Dong:14} who used
a recurrent neural network to predict the polarity class associated
with the opinion targets.  They, however, assumed the targets of
sentiments to be apriori known and only were interested whether a
positive or a negative judgement was made about them.

\citet{Derks:08}

The \texttt{SentiStrength} system proposed by \cite{Thelwall:12} used
an extensive list of 763 polar terms in order to predict positive and
negative scores for MySpace comments.  The manually assigned scores of
these terms were automatically fine-tuned during training using a
perceptron-like technique.  In addition to the core lexicon, the final
implementation of this system also utilized a set of heuristic methods
and auxiliary modules such as spelling correction algorithm,
dictionaries of booster words and negations as well as special rules
for emoticons, repeated letters, and exclamation marks.  It correctly
predicted positive emotions in 60.6~\% of the cases and attained
73.5~\% accuracy at predicting negative sentiment scores.  All
predictions were made at the level of complete messages.

\section{Sentiment Corpus}\label{sec:corpus}
\subsection{Selection Criteria}
\subsubsection{Topic Selection}
\subsubsection{Formal Selection}

\subsection{Annotation Scheme}
\subsection{Statistics and Preliminary Results}
\begin{table}[h]
  \centering\small
  \caption[Sentiment corpus statistics]{Statistics on the annotatedwith
    sentiment corpus.\\ POL = corpus part with discussions about
    general politic topics; FE = corpus part describing the federal
    election 2013; PE = corpus part with discussions about the Pope
    election 2013; GEN = part of the corpus containing tweets with no
    particular topic}
  \begin{tabular}{|>{\centering}p{0.15\textwidth}|*{4}{>{\centering}p{\oosixthClmnWidth}|}
      >{\centering\bfseries}p{\oosixthClmnWidth}|*{4}{>{\centering}p{\oosixthClmnWidth}|}
      >{\centering\bfseries}p{\oosixthClmnWidth}|}
    \hline

    \multirow{2}{*}{\parbox{0.13\textwidth}{\centering Markable Type}}
    & \multicolumn{5}{>{\centering}p{7\oosixthClmnWidth}|}{Annotator
      1} &
    \multicolumn{5}{>{\centering}p{7\oosixthClmnWidth}|}{Annotator
      2}\tabularnewline\cline{2-11}

    & POL & FE & PE & GEN & Total & POL & FE & PE & GEN &
    Total\tabularnewline\hline

    Sentiment & 212 & 222 & 163 & 131 & 728 & 317 & 335 & 314 & 305 & 1271
    \tabularnewline\hline

    Source & 101 & 119 & 68 & 73 & 361 & 114 & 109 & 94 & 85 & 402
    \tabularnewline\hline

    Target & 229 & 279 & 184 & 151 & 843 & 342 & 369 & 328 & 324 & 1363
    \tabularnewline\hline

    Emotional Expression & 727 & 689 & 581 & 811 & 2808 & 662 & 669 & 671 & 768 & 2770
    \tabularnewline\hline

    Intensifier & 16 & 32 & 14 & 44 & 106 & 31 & 35 & 31 & 58 & 155
    \tabularnewline\hline

    Diminisher & 2 & 4 & 3 & 2 & 11 & 2 & 9 & 4 & 2 & 17
    \tabularnewline\hline

    Negation & 18 & 15 & 23 & 14 & 70 & 33 & 33 & 31 & 23 & 120
    \tabularnewline\hline
  \end{tabular}
  \label{table:sentiment-agreement-topics}
\end{table}

\subsection{Inter-Annotator Agreement}
\subsubsection{Inter-Annotator Agreement for Topics}
\begin{table}[h]
  \centering\small
  \caption[Inter-annotator agreement for the sentiment corpus across
    topics]{Inter-annotator agreement for the sentiment corpus across
    topics.\\ POL = corpus part with discussions about general politic
    topics; FE = corpus part describing the federal election 2013; PE
    = corpus part with discussions about the Pope election 2013; GEN =
    part of the corpus containing tweets with no particular topic}
  \begin{tabular}{|>{\centering}p{0.15\textwidth}|*{4}{>{\centering}p{\oosixthClmnWidth}|}
      >{\centering\bfseries}p{\oosixthClmnWidth}|*{4}{>{\centering}p{\oosixthClmnWidth}|}
      >{\centering\bfseries}p{\oosixthClmnWidth}|}
    \hline

    \multirow{2}{*}{\parbox{0.13\textwidth}{\centering Markable Type}}
    &
    \multicolumn{5}{>{\centering}p{7\oosixthClmnWidth}|}{$\kappa$-Agreement
      for Binary Overlap} &
    \multicolumn{5}{>{\centering}p{7\oosixthClmnWidth}|}{$\kappa$-Agreement
      for Proportional Overlap}\tabularnewline\cline{2-11}

    & POL & FE & PE & GEN & Total & POL & FE & PE & GEN &
    Total\tabularnewline\hline

    Sentiment & 0.35 & 0.35 & 0.45 & 0.41 & 0.39 & 0.27 & 0.29 & 0.36 & 0.34 & 0.32
    \tabularnewline\hline

    Source & 0.39 & 0.27 & 0.41 & 0.41 & 0.37 & 0.38 & 0.28 & 0.4 & 0.4 & 0.36
    \tabularnewline\hline

    Target & 0.32 & 0.38 & 0.4 & 0.39 & 0.38 & 0.26 & 0.28 & 0.31 & 0.32 & 0.3
    \tabularnewline\hline

    Emotional Expression & 0.64 & 0.57 & 0.68 & 0.66 & 0.64 & 0.6 & 0.54 & 0.65 & 0.63 & 0.61
    \tabularnewline\hline

    Intensifier & 0.46 & 0.48 & 0.21 & 0.62 & 0.52 & 0.46 & 0.48 & 0.21 & 0.6 & 0.51
    \tabularnewline\hline

    Diminisher & 0.67 & 0.44 & 0.0 & 0.4 & 0.37 & 0.67 & 0.44 & 0.0 & 0.4 & 0.37
    \tabularnewline\hline

    Negation & 0.44 & 0.1 & 0.36 & 0.21 & 0.28 & 0.44 & 0.1 & 0.36 & 0.21 & 0.28
    \tabularnewline\hline
  \end{tabular}
  \label{table:sentiment-agreement-topics}
\end{table}

\subsubsection{Inter-Annotator Agreement for Formal Criteria}

\subsection{Analysis of Annotator Mistakes}
\subsection{Related Work}
\subsection{Conclusions}

\section{Identification of Subjective Expressions}
\subsection{Ontology-based Identification of Subjective Elements}
\subsection{Corpus-based Identification of Subjective Elements}
\subsection{Machine Learning Approaches to Identification of Subjective Expressions}
%% \subsection{Hybrid Methods for Polarity Identification}
\subsection{Related Work}
\subsection{Conclusions}

\section{Identification of Targets, Sources, and Sentiments}
\subsection{Rule-based Approaches to Sentiment Tagging}
\subsection{Machine Learning Approaches to Sentiment Tagging}
\subsubsection{Compared Systems}
\subsubsection{Features}
\subsubsection{Ablation Tests}
\subsubsection{Evaluation}
\subsection{Related Work}
\subsection{Conclusions}

\section{Coarse-grained Sentiment Analysis}
\subsection{Sentiment Analysis on the Level of Discussions}
\subsection{Sentiment Analysis on the Level of Messages}
\subsection{Sentiment Identification on the Level of Sentences}
\subsection{Related Work}
\subsection{Conclusions}

\section{Effects of Text Normalization and Domain Adaptation}
\subsection{Text Normalization Impact on Fine-grained Sentiment Analysis}
\subsection{Text Normalization Effect on Coarse-grained Sentiment Analysis}
\subsection{Comparison with other Corpora and other Domains}
\subsection{Related Work}
\subsection{Conclusions}
