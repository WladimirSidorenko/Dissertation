% FILE: abstract.tex  Version 2.1
% AUTHOR:
% Universität Duisburg-Essen, Standort Duisburg
% AG Prof. Dr. Günter Törner
% Verena Gondek, Andy Braune, Henning Kerstan
% Fachbereich Mathematik
% Lotharstr. 65., 47057 Duisburg
% entstanden im Rahmen des DFG-Projektes DissOnlineTutor
% in Zusammenarbeit mit der
% Humboldt-Universitaet zu Berlin
% AG Elektronisches Publizieren
% Joanna Rycko
% und der
% DNB - Deutsche Nationalbibliothek
%% \begin{abstract}
%% Here is the english abstract.
%% \end{abstract}
%-englische-Zusammenfassung---------------------------------------
\selectlanguage{english}
\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

The growing popularity of online conversation platforms over the last few
years has lead to a dramatic increase in the amount of unstructured text data
communicated through these platforms.  Dealing with this abundance of data in
manual way becomes fairly impossible nowadays.  But, unfortunately, existing
natural language processing (NLP) applications -- most of which were created
for standard language texts -- cannot handle the relatively peculiar genre of
casual online conversations sufficiently good either.  Therefore, we urgently
need other, more robust and more high-quality NLP programs which are able to
process even such ``unconventional'' linguistic variants as the Web language.
But in order to build them, we first need to understand the nature of these
variants better.

%% But in order to build a high-quality NLP application for a particular language
%% form, one should first understand the nature of this

%% This thesis presents a linguistic and computational study of one of the most
%% proliferating variants of the German online language -- namely, users'
%% microblogs (tweets) posted on Twitter.\footnote{\url{https://twitter.com/}}
%% From the linguistic perspective, we investigate in our work what language
%% phenomena actually account for the notorious language style of tweets making
%% it different from the standard German variant, how frequent those phenomena
%% are,

In this thesis, we present a study of one of the most proliferating forms of
the German online language -- namely, users' conversations on the Twitter
platform.\footnote{\url{https://twitter.com/}} We investigate the specific
characteristics of Twitter messages (tweets) from both linguistic and
computational perspectives.  From the linguistic point of view, we analyze
which language phenomena actually account for the notoriously peculiar style
of tweets, how frequent those phenomena are, and what role they play in the
online communication.  From the computational perspective, we evaluate to what
extent differences in the language style can affect the performance of the
existing NLP approaches and what techniques could help improve this
performance.

We conduct our study on three different levels of analysis granularity:
\begin{inparaenum}[\itshape a\upshape)]
    \item the \emph{subsentential},
    \item the \emph{sentential}, and
    \item the \emph{suprasentential} one.
\end{inparaenum}

In the first (\emph{subsentential}) stage of our research, we investigate how
difficult the task of automatic low-level segmentation (i.e. automatic
detection of word and sentence boundaries) is for tweets.  After that, we turn
to the analysis of spelling deviations which are knowingly ubiquituous in
Twitter messages.  In order to estimate how aggravated this problem is, we
collect statistics of the most frequent out-of-vocabulary words drawn from a
sample of users' posts; classify these words into three major classes (regular
German words omitted from dictionaries, stylistic specifics of text genre, and
true spelling deviations), and calculate how frequent each of these classes
are.  We then present possible strategies for handling the latter two groups
(rule-based normalization of stylistic specifics and hybrid correction of
spelling mistakes).  In the description of related work, we also juxtapose two
general approaches to handling noisy texts -- text normalization and domain
adaptation -- and analyze the pros and cons of these approaches.

On the \emph{sentential} level, we concentrate on the task of automatic
sentiment analysis in sentences.  Here, we first present a comprehensive
corpus of German tweets which were manually annotated with fine-grained
sentiment relations.  We then conduct an agreement study of this corpus in
order to evaluate how difficult the sentiment analysis task is for humans and
what sentiment elements, text topics, and tweet types are most difficult to
annotate.  After that, we demonstrate how well the sentiment classification
task can be done automatically using machine learning and lexicon-based
techniques.  In our experiments, we consider three related subtasks:
\begin{inparaenum}[\itshape a\upshape)]
 \item automatic classification of polar words,
 \item automatic detection of sentiment constituents (i.e. opinion holders,
 targets, and evaluation spans), and, finally,
 \item automatic prediction of the intensity and polarity of sentiments.
\end{inparaenum}

On the \emph{suprasentential} or \emph{discourse} level of the analysis, we
focus our attention on the task of automatic discourse segmentation and
relation extraction.  We first try to apply the ideas of the classical
Rhetorical Structure Theory \citep{Mann-88} to Twitter conversations.  Since
this theory, however, was primarily developed for official monologue texts
rather than casual multilogues like the ones found on Twitter, several
refinements and extensions to this framework turned out to be necessary.
These refinements primarily concern the definition of elementary discourse
units (EDUs) for such Twitter-specific elements as @-mentions, hyperlinks,
hashtags etc.  Furthermore, as we show, several additions to the discourse
relations set are needed in order to cover the inter-tweet relationships.
After defining these extensions, we evaluate different methods and strategies
for automatically detecting EDU boundaries and doing discourse parsing on
Twitter.

In the concluding step, we investigate how using text normalization, on the
one hand, and incorporating discourse information, on the other, can help us
improve the results of sentence-level sentiment analysis.
