% FILE: sentiment_fgsa.tex  Version 0.0.1
% AUTHOR: Uladzimir Sidarenka

% This is a modified version of the file main.tex developed by the
% University Duisburg-Essen, Duisburg, AG Prof. Dr. Günter Törner
% Verena Gondek, Andy Braune, Henning Kerstan Fachbereich Mathematik
% Lotharstr. 65., 47057 Duisburg entstanden im Rahmen des
% DFG-Projektes DissOnlineTutor in Zusammenarbeit mit der
% Humboldt-Universitaet zu Berlin AG Elektronisches Publizieren Joanna
% Rycko und der DNB - Deutsche Nationalbibliothek

\chapter{Fine-Grained Sentiment Analysis}\label{sec:snt:fgsa}

The main goal of fine-grained sentiment analysis (FGSA) is
traditionally defined as the identification of subjective evaluative
opinions (\emph{sentiments}), the holders of these opinions
(\emph{sources}), and their respectively evaluated objects
(\emph{targets}) in text.  Since an accurate automatic prediction of
these elements would enable us to track public attitude to literally
any object (e.g., a product, a service, or a political decision), FGSA
is commonly regarded to be one of the most attractive, necessary, but,
unfortunately, also challenging goals in computational linguistics.

Researchers usually consider this objective as a sequence labeling
(SL) task and address it with either of the two popular SL techniques:
conditional random fields (CRFs) or recurrent neural networks (RNNs).
The former approach represents a discriminative probabilistic
graphical model, which relies on hand-crafted features; the latter
framework utilizes a recursive computational loop, which can learn
feature representations completely automatically.  In this section, we
are going to evaluate each of these methods in detail in order to find
out which of these algorithms is better suited for the domain of
German Twitter.  However, before we proceed with our evaluation, we
should first make a short linguistic digression and briefly discuss
the definition of textual spans, to which these approaches should
assign their labels, and the evaluation metrics, with which we will
estimate the quality of this assignment.

\section{Definition of Sentiment, Target, and Source Spans}
Despite some notable advances and an ongoing active research on
fine-grained opinion extraction, the crucial task of defining the
exact boundaries of sentiment spans and the spans of their respective
targets and sources has not been addressed in the literature with the
due attention yet.  Researchers typically overlook this problem,
leaving its solution to the discretion of their annotators
\cite[cf.][]{Wiebe:05,Klinger:13}.

In contrast to these works, instead of relying on rather intuitive
decisions of our coders, we explicitly provided a rule for determining
opinions' boundaries by telling the experts to assign the
\texttt{sentiment} tag to ``\emph{minimal complete syntactic or
  discourse-level units that included both the target of an opinion
  and its actual evaluation}''.

According to this instruction, during the annotation, a linguist first
had to identify evaluated objects (targets) in text, then find the
respective evaluative expressions of these objects (usually but not
necessarily polar terms), and, finally, determine the smallest
syntactic components (typically noun or verb phrases) or discourse
units (clauses or sentences) where both of these entities appeared
together.

An annotation example labeled in compliance with this rule is provided
below:
\begin{example}\label{snt:fgsa:exmp:sent-anno1}
  \upshape\sentiment{Der neue Papst gilt als
    bescheidener, zur\"uckgenommener Typ.}\\[0.8em]
  \noindent\sentiment{The new Pope is believed to be a sober, modest
    man.}
\end{example}
\noindent In this sentence, the experts had to label the complete
clause as a sentiment, since this unit was the minimal syntactic
constituent which included both the object of the evaluation---``der
neue Papst'' (\textit{the new pope})---and the evaluation
itself---``bescheidener, zur\"uckgenommener Typ'' (\textit{a sober,
  modest man}).

We applied the same principles of minimality and completeness to the
annotation of targets and sources, requiring the main components of
these elements (typically nouns or verbs) to be labeled along with all
their syntactic dependants.

Accordingly, the correct annotation of the target in the previous
example had to look as follows:
\begin{example}\label{snt:fgsa:exmp:sent-anno2}
  \upshape\sentiment{\target{Der neue Papst} gilt als
    bescheidener, zur\"uckgenommener Typ.}\\[0.8em]
  \noindent\sentiment{\target{The new Pope} is believed to be a sober,
    modest man.}
\end{example}
\noindent with the \texttt{target} tag encompassing the whole noun
phrase---``der neue Papst'' (\textit{the new pope})---not only its
main noun.

Similarly, source elements had to cover complete syntactic structures
as shown in Example~\ref{snt:fgsa:exmp:src-anno1}:
\begin{example}\label{snt:fgsa:exmp:src-anno1}
  \upshape\sentiment{Die Homosexuellenehe war f\"ur \source{den Kardinal, der jetzt Papst ist,} eine Zerst\"orung von Gottes Plan}\\[0.8em]
  \noindent\sentiment{For \source{the cardinal, who is the Pope now,}
    the same-sex marriage was a destruction of God's plan.}
\end{example}
\noindent This time, again, the whole noun phrase including its
dependent attributive clause---``den Kardinal, der jetzt Papst ist,''
(\textit{the cardinal, who is the Pope now,})---had to be labeled with
the \texttt{source} tag because this constituent was the only
\emph{minimal complete} syntactic node which encompassed both the
immediate holder of the opinion---``Kardinal'' \textit{cardinal}---and
its grammatical dependants without including any of its parental
elements.

\section{Evaluation Metrics}
The next question which naturally arises after defining the span
boundaries for human coders is that of the best way to compare these
spans with automatically assigned labels.  One possibility to estimate
the quality of such automatic assignment is to compute the precision,
recall, and \F{}-scores using either the binary overlap or exact match
metrics \cite{Choi:06,Breck:07} .  The former method considers an
automatically labeled span as correct if it has at least one token in
common with a labeled entity from the gold annotation.  The latter
metric only regards as true positives those automatic spans which have
absolutely identical boundaries with the gold assignment.
Unfortunately, both of these approaches are problematic to some
extent: While the binary overlap might be overly optimistic, always
assigning perfect scores to automatic spans which cover the whole
sentence; the exact match metric might, vice versa, be too drastic,
considering the whole assignment as false if only one (possibly
irrelevant) token is missing.

Instead of relying on these measures, we opted for the ``golden mean''
solution to this problem that was proposed by \citet{Johansson:10a}.
In their work, the authors introduced another way of estimating the
quality of an automatic assignment, in which they penalized the
predicted spans proportionally to the number of tokens whose labels
were different from the gold annotation.  More precisely, given a set
of manually annotated gold entities $\mathcal{S}$ and automatically
tagged spans $\widehat{\mathcal{S}}$, they estimated the precision of
the automatic assignment as:
\begin{equation}\label{eq:fgsa:jmmetric}
  P(\mathcal{S}, \widehat{\mathcal{S}}) = \frac{C(\mathcal{S}, \widehat{\mathcal{S}})}{|\widehat{\mathcal{S}}|},
\end{equation}
where $C(\mathcal{S},\widehat{\mathcal{S}})$ means the \emph{span
  coverage} metric and is computed as the proportion of overlapping
tokens across all pairs of manually ($s_i$) and automatically ($s_j$)
annotated entities:
\begin{equation*}
 C(\mathcal{S}, \widehat{\mathcal{S}}) = \sum_{s_i \in
   \mathcal{S}}\sum_{s_j \in \widehat{\mathcal{S}}}c(s_i, s_j);
\end{equation*}
The $|\widehat{\mathcal{S}}|$ term in Equation~\ref{eq:fgsa:jmmetric}
stands for the total number of spans automatically labeled with the
given tag.  Similarly to precision, the recall of the assignment was
estimated as:
\begin{equation*}
  R(\mathcal{S}, \widehat{\mathcal{S}}) = \frac{C(\mathcal{S}, \widehat{\mathcal{S}})}{|\mathcal{S}|},
\end{equation*}
and the \F{}-measure was normally computed as the harmonic mean of the
precision and recall scores:
\begin{equation*}
  F_1 = 2\times\frac{P \times R}{P + R}.
\end{equation*}

Since this proportional estimation adequately accommodates both
extrema of an automatic labeling---too long and too short spans---and
also penalizes for erroneous and spurious tags, we will primarily rely
on this measure throughout our subsequent experiments.

\section{Data Preparation}\label{snt:fgsa:subsec:data}

In order to evaluate the CRF and RNN approaches on our data set, we
split the whole corpus into three parts, using 70\% of it as training
data, 10\% as a development set, and the remaining 20\% as a test
corpus.  We tokenized all tweets with an adjusted version of the
Christopher Potts' Twitter-aware
tokenizer\footnote{\url{http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py}}
and preprocessed them using the rule-based normalization technique of
\citet{Sidarenka:13}.
%% During the normalization, Twitter-specific phenomena like @-mentions,
%% retweets, and URIs that were not syntactically integrated in any
%% sentence of the message were removed from the tweets and those
%% elements which played an integral syntactic role were replaced with
%% the special artificial tokens \%User, \%Link etc.  Emoticons like :-),
%% \smiley{}, \frownie{} etc. were also replaced with the placeholders
%% \%PosSmiley, \%NegSmiley, or simply \%Smiley depending on their prior
%% polarity.  Furthemore, out-of-vocabulary words which could be
%% converted to in-vocabulary terms with a pre-defined set of
%% transformations were also normalized.
In the next step, we labeled the preprocessed data with their
part-of-speech tags using \texttt{TreeTagger}\footnote{We used
  \texttt{TreeTagger} Version 3.2 with the German parameter file
  UTF-8.}  \cite{Schmid:95} and parsed the obtained sentences with the
\texttt{Mate} dependency parser\footnote{We used \texttt{Mate} Version
  \texttt{3.61} with the German parameter model 3.6.}
\cite{Bohnet:13}.  Finally, since \texttt{MMAX2} did not provide a
straightforward support for character offsets of the annotated tokens
and the automatically tokenized data could disagree with the original
corpus tokenization, we aligned the manual annotation with the
automatically split words using the Needleman-Wunsch alignment
algorithm~\cite{Needleman:70}.

\section{Fine-Grained Sentiment Analysis with Conditional Random
  Fields} The first method that we are going to evaluate on the
obtained data is that of the conditional random fields.  First
introduced by \citet{Lafferty:01}, CRFs rapidly grew in popularity,
turning into one of the most commonly used probabilistic frameworks,
which dominated the NLP field for more than a decade.  The main resons
for this huge success are:
\begin{enumerate}[1)]
\item the \emph{structural nature} of CRFs, which, in contrast to
  single-entity classifiers such as logistic regression or SVM, make
  their predictions over a sequence of covariates, trying to find the
  most likely label assignment for the whole input chain and not only
  its individual elements;
\item the \emph{discriminative power} of this framework, which, in
  contrast to generative probabilistic models such as HMM
  \cite{Rabiner:86}, optimizes the conditional probability
  $P(\boldsymbol{Y}|\boldsymbol{X})$ instead of maximizing the joint
  distribution $P(\boldsymbol{X},\boldsymbol{Y})$ and consequently can
  efficiently deal with overlapping and correlated features;
\begin{example}[Overlapping and Correlated Features]
  In order to demonstrate the different effects of correlated and
  overlapping features on generative and discriminative models, let us
  go through an example where we need to predict whether a tweet
  mentioning ``Merkel'' and ``Steinmeier'' is about the Christian
  Democratic Union (\texttt{CDU}) or Social Democratic Party of
  Germany (\texttt{SPD}).

  As features for this task, we will use lexical unigrams appearing in
  the training data.  Assuming that our training set consists of three
  messages mentioning ``Merkel'' and one microblog mentioning
  ``Steinmeier'' which are labeled as \texttt{CDU} plus one tweets
  mentioning ``Merkel'' and three posts mentioning ``Steinmeier''
  which are annotated as \texttt{SPD}, the generative Na\"{i}ve Bayes
  model would estimate the probability of the two competing classes
  as:
  \begin{align*}
    P(\mathbf{x}, CDU) =& P(\textrm{Merkel},\textrm{Steinmeier}|CDU)\times P(CDU)\\
    =& P(\textrm{Merkel}|CDU)\times P(\textrm{Steinmeier}|CDU) \times P(CDU)\\
    =&\frac{3}{4}\times\frac{1}{4}\times\frac{4}{8}\approx 0.0938\\
    P(\mathbf{x}, SPD) =& P(\textrm{Merkel},\textrm{Steinmeier}|SPD)\times P(SPD)\\
    =& P(\textrm{Merkel}|SPD)\times P(\textrm{Steinmeier}|SPD) \times P(SPD)\\
    =&\frac{1}{4}\times\frac{3}{4}\times\frac{4}{8}\approx 0.0938.\\
  \end{align*}
  After normalizing these probabilities, we would get equal 50\%
  chances for each of the parties, which is fair regarding the token
  distribution in our corpus.  However, if we replace ``Merkel'' with
  ``von der Leyen'' both in the training data and test example and
  rerun this experiment once again, the probability would get
  significantly skewed towards the CDU class:
  \begin{align*}
    P(\mathbf{x}, CDU) =& P(\textrm{von},\textrm{der},\textrm{Leyen},\textrm{Steinmeier}|CDU)\times P(CDU)\\
    =& P(\textrm{von}|CDU)\times P(\textrm{der}|CDU)\times P(\textrm{Leyen}|CDU)\\
    &\times P(\textrm{Steinmeier}|CDU) \times P(CDU)\\
    =&\frac{3}{4}\times\frac{3}{4}\times\frac{3}{4}\times\frac{1}{4}\times\frac{4}{8}\approx 0.0527\\
    P(\mathbf{x}, SPD) =& P(\textrm{von},\textrm{der},\textrm{Leyen},\textrm{Steinmeier}|SPD)\times P(SPD)\\
    =& P(\textrm{von}|SPD)\times P(\textrm{der}|SPD)\times P(\textrm{Leyen}|SPD)\\
    &\times P(\textrm{Steinmeier}|SPD) \times P(SPD)\\
    =&\frac{1}{4}\times\frac{1}{4}\times\frac{1}{4}\times\frac{3}{4}\times\frac{4}{8}\approx 0.0059,\\
  \end{align*}
  which, after normalization, would result in 90\% chances for
  \texttt{CDU}, and a 10\% score for \texttt{SPD}, even though we only
  changed the name of the politician.

  A different situation can be observed for discriminative models such
  as a maximum entropy classifier: Instead of optimizing the joint
  distribution $P(\mathbf{x}, y)$ as it is done by generative
  frameworks, discriminative systems seek to optimize the conditional
  likelihood $P(y|\mathbf{x})$ by maximizing the probability of the
  training set $\sum_{i=1}^N\log P(y_i|\mathbf{x}_i, \mathbf{w})$.
  This probability is usually estimated using the sigmoid function
  $\frac{1}{1 + e^{-(\mathbf{x}_i, \mathbf{w})}}$, where
  $\mathbf{x}_i$ denotes the input features of the $i$-th training
  instance, and the vector $\mathbf{w}$ stands for the respective
  weights of these features.  By optimizing this function using
  gradient descent, we will arrive at the optimal solution $w_1
  \approx 0.5$ for the feature ``Merkel'' and $w_2 \approx -0.5$ for
  the feature ``Steinmeier'' for the first example, which would again
  result in equal 50\% chances for both classes.  In the second
  example, however, all three features ``von'', ``der'', and ``Leyen''
  would get an equal weight of $\approx 0.3$, and the ``Steinmeier''
  feature would receive a coefficient of $\approx -0.4$, which would
  result in 60\% probability for the test message being about the CDU
  and 40\% that the tweet is about the SPD.  Even though this still
  means a slight skewness towards \texttt{CDU}; this time, the effect
  of correlated features is much less dramatic than in the generative
  case.
\end{example}
\item and, finally, the \emph{avoidance of the label bias problem},
  which other discriminative classifiers, such as maximum entropy
  Markov networks \cite{McCallum:00}, are known to be susceptible to.
  \begin{example}[Label Bias Problem]
    The label bias problem arises in the cases where a locally optimal
    decision outweights globally superior solutions.  Consider, for
    example, the sentence ``Aber gerade Erwachsene haben damit
    Schwierigkeiten.'' (\textit{But especially adults have
      difficulties with it.}), for which we need to compute the most
    probable sequence of part-of-speech tags.

    \begin{center}
      \begin{tikzpicture}[node distance=5cm]
        \tikzstyle{tag}=[circle split,draw=gray!50,%
          minimum size=2.5em,inner ysep=2,inner xsep=0,%
          circle split part fill={yellow!20,blue!30}]
      \tikzstyle{word}=[draw=none,inner sep=10pt]

      \node[word] (A) at (1, 1) {Aber};
      \node[tag] (B) at (1, 3) {\footnotesize KON \nodepart{lower} 1.};
      \node[word] (D) at (3, 1) {gerade};
      \node[tag] (E) at (3, 2) {\footnotesize ADJA \nodepart{lower} .5};
      \node[tag] (F) at (3, 4) {\footnotesize ADV \nodepart{lower} .5} ;
      \node[word] (G) at (7, 1) {Erwachsene};
      \node[tag] (I) at (7,2) {\footnotesize ADJA \nodepart{lower} .5} ;
      \node[tag] (H) at (7,4) {\footnotesize NN \nodepart{lower} .5};
      \node[word] (J) at (9,1) {haben};
      \node[tag] (K) at (9,3) {\footnotesize VA \nodepart{lower}\small 1.};
      \node[word] (J) at (11,1) {\ldots};

      \path [-] (B) edge node[below] {$.5$} (E);
      \path [-] (B) edge node[above] {$.5$} (F);

      \path [-] (E) edge node[below] {$.3$} (I);
      \path [-] (E) edge node[below left=0.4] {$.7$} (H);
      \path [-] (F) edge node[above left=0.4] {$.8$} (I);
      \path [-] (F) edge node[above] {$.2$} (H);

      \path [-] (I) edge node[below] {$.1$} (K);
      \path [-] (H) edge node[above] {$.9$} (K);
    \end{tikzpicture}
    \captionof{figure}{\emph{Feature weights for states and
        transitions of the part-of-speech example.}\label{fig:snt:memm-crf}}
    \end{center}
    Assuming that feature weights are distributed as shown in
    Figure~\ref{fig:snt:memm-crf}, we will first estimate the
    probability of the correct label sequence for the initial part of
    this sentence using the Maximum Entropy Markov Model (MEMM)---the
    predecessor of Conditional Random Fields.  According to the MEMM's
    definition, the probability of the correct labeling
    ($KON-ADV-NN-VA$) is equal to:
    \begin{align*}
      P(KON, ADV, NN, VA) =& P(KON)\times P(ADV|KON)\\
      &\times P(NN|ADV)\times P(VA|NN)\\
      &=\frac{\exp(1)}{\exp(1)}\times\frac{\exp(0.5 + 0.5)}{\exp(0.5 + 0.5) + \exp(0.5 + 0.5)}\\%
      \times&\frac{\exp(0.2 + 0.5)}{\exp(0.2 + 0.5) + \exp(0.8 + 0.5)}\times\frac{\exp(0.9 + 1.)}{\exp(0.9 + 1.)} \approx 0.177
    \end{align*}
    At the same time, the probability of the incorrect variant
    ($KON-ADV-ADJA-VA$) amounts to $\approx$ 0.323 and will therefore
    be preferred during automatic tagging.

    A different situation is observed with the CRFs, where the
    normalizing factor in the denominator is computed over the whole
    input sequence without factorizing into individual terms for each
    transition as it is done in the previous case.  That way, the
    probability of the correct labels would run up to:
    \begin{align*}
      P(KON, ADV, NN, VA) =& P(KON)\times
      P(ADV|KON)\times P(NN|ADV)\\
      &\times P(VA|NN)\\ =&\frac{\exp(1 + 0.5
        \times 3 + 0.2 + 0.9 + 1)}{Z} \approx 0.252,
    \end{align*}
    where $Z = \exp(1 + 0.5 \times 3 + 0.2 + 0.9 + 1) + \exp(1 + 0.5
    \times 3 + 0.8 + 0.1 + 1) + \exp(1 + 0.5 \times 3 + 0.7 + 0.9 + 1)
    + \exp(1 + 0.5 \times 3 + 0.3 + 0.1 + 1)$ is the total score of
    all possible label assignments.  The incorrect alternative
    ($KON-ADV-ADJA-VA$), however, would get a probability score of
    $\approx$ 0.207, which is smaller than the likelihood of the
    correct labeling.
  \end{example}
\end{enumerate}
\textbf{Training} CRFs get these useful properties thanks to a neatly
formulated objective function in which they seek to optimize the
global log-likelihood of the gold labels $\mathbf{Y}$ conditioned on
the training data $\mathbf{X}$.  In particular, given a set of
training instances $\mathcal{D} = \{(\mathbf{x}^{(n)},
\mathbf{y}^{(n)})\}_{n=1}^N$, where $\mathbf{x}^{(n)}$ stands for the
input variables of the $n$-th instance and $\mathbf{y}^{(n)}$ denotes
its respective gold labels, CRF's training adds up to finding feature
coefficients $\mathbf{w}$ that maximize the log-probabilities $\ell$
of $\mathbf{y}^{(i)}$ given their covariates $\mathbf{x}^{(i)}$ over
the whole corpus:
\begin{equation}\label{snt:fgsa:eq:crf-ell}
  \mathbf{w} = \argmax_{\mathbf{w}}\sum_{n=1}^N\ell
  \left(\mathbf{y}^{(n)}|\mathbf{x}^{(n)}\right).
\end{equation}
The likelihood term $\ell(\mathbf{y}^{(n)}|\mathbf{x}^{(n)})$ in this
equation is commonly estimated using a globally normalized softmax
function:
\begin{equation}
  \ell\left(\mathbf{y}^{(n)}|\mathbf{x}^{(n)}\right) =
  \ln\left(P(\mathbf{y}^{(n)}|\mathbf{x}^{(n)})\right) =
  \ln\left(\frac{ \exp\left(\sum_{m=1}^{M}\sum_jw_{j} \times f_j(x_{m},
    y_{m-1}, y_{m})\right)}{Z}\right),
\end{equation}
where $M$ stands for the length of the $n$-th training instance,
$f_j(x_{m}, y_{m-1}, y_{m})$ denotes the value of the $j$-th feature
function $f$ at the sequence position $m$, $w_j$ represents the
corresponding weight of this feature, and $Z$ is a normalization
factor calculated over all possible label assignments:
\begin{equation}
  Z =
  \sum_{y'\in\mathcal{Y},y''\in\mathcal{Y}}\exp\left(\sum_{m=1}^{M}\sum_jw_{j}
  \times f_j(x_{m}, y'_{m-1}, y''_{m})\right).
\end{equation}
Since this normalizing term appears in the denominator and couples
together all feature weights that need to be optimized, it becomes
prohibitively expensive to find the best solution to
Equation~\ref{snt:fgsa:eq:crf-ell} analytically with a single shot.  A
possible remedy to this problem is to resort to other optimization
techniques such as the gradient descent method, in which the weights
of the features are successively changed towards the direction of the
gradient until the global minimum of the loss function is reached.

We can easily see that the partial derivative of the log-likelihood
$\ell$ w.r.t. the single feature weight $w_j$ amounts to the following
solution:
\begin{equation}
  \frac{\partial}{\partial w_j}\ell =%
  \sum_{n=1}^N\sum_{m=1}^Mf_j(x_{m}, y_{m-1}, y_{m}) -%
  \sum_{n=1}^N\sum_{m=1}^{M}\sum_{y'\in\mathcal{Y},y''\in\mathcal{Y}}f_j(x_{m},%
  y'_{m-1}, y''_{m})P(y',y''|\mathbf{x}^{(n)}),
\end{equation}
which, after dividing both parts of the equation with the constant
term $N$---the size of the corpus, can, in turn, be transformed into:
\begin{equation}
  \frac{1}{N}\frac{\partial}{\partial w_j}\ell = \E[f_j(\mathbf{x},
    \mathbf{y}] - \E_{\mathbf{w}}[f_j(\mathbf{x}, \mathbf{y})],
\end{equation}
where the first term is the expectation of the feature $f_j$ under the
empirical distribution, and the second term is its expectation under
the model's parameters $\mathbf{w}$.  That way, the optimal solution
to the log-likelihood function in Equation~\ref{snt:fgsa:eq:crf-ell}
would be the one, where the model's expectation of the features
matches their (true) empirical expectation in the corpus.

The marginal probability of the features, which is required for
computing their model's expectation, can be estimated dynamically
using the forward-backward (FB) algorithm \cite{Rabiner:90}---a
particular case of the more general belief propagation method
\cite[cf.][p.~81]{Barber:12}.

\noindent\textbf{Inference} Once the optimal feature weights are
learned, one can unproblematically compute the most likely label
assignment for a new instance by using the Viterbi algorithm
\cite{Viterbi:67}, which effectively corresponds to the forward pass
of the FB method with the summation over the alternative preceding
states replaced by the maximum operator (hence the other name for the
inference algorithm---``max-product'').

\noindent\textbf{Features} A crucial component which accounts for a
huge part of the success (or failure) of a CRF system are feature
attributes which are defined by its developer.  Tranditionally,
feature functions used in CRFs are divided into transition- and
state-based ones.  The former attributes represent real- or
binary-valued functions $f(\mathbf{x}, y'', y')\rightarrow\mathbb{R}$
associated with some data predicate
$\phi(\mathbf{x})\rightarrow\mathbb{R}$ and the labels $y''$ and $y'$.
The value of theses attributes at position $m$ in the sequence
$\mathbf{x}$ is usually defined as follows:
\begin{equation}
  f(\mathbf{x}_m, y'', y') = \begin{cases} \phi(\mathbf{x}_m), &
    \mbox{if } \mathbf{y}_{m-1} = y''\mbox{ and }\mathbf{y}_{m} =
    y'\\ 0, & \mbox{otherwise;}
  \end{cases}
\end{equation}
where the predicate~$\phi$ typically represents a simple unit
function: $\phi(\mathbf{x}_m)\mapsto 1$, $\forall\mathbf{x}_m$.

In contrast to the ternary transition features, state attributes are
associated with binary predicates, whose output depends on the input
data at the given position and the label $y'$ at the respective state:
\begin{equation}
  f(\mathbf{x}_m, y') = \begin{cases} \phi(\mathbf{x}_m), & \mbox{if }
    \mathbf{y}_{m} = y'\\ 0, & \mbox{otherwise;}
  \end{cases}
\end{equation}
this time, the predicate~$\phi$ is usually much more sophisticated
than in the previous case and reflects various properties of the input
sequence at the respective position, such as whether the current token
is capitalized or whether it begins with a specific prefix or ends
with a specific suffix.  This type of features commonly accounts for
the overwhelming majority of all attributes used in a CRF system.

As state attributes in our experiments, we used the following types of
predicates (which, for simplicity, are listed here in groups):
\begin{itemize}
\item\emph{formal}, which included the initial three characters of
  each token (e.g., $\phi_{abc}(\mathbf{x}_m) = 1\mbox{ if
  }\mathbf{x}_m\sim\mbox{ /\textasciicircum abc/ else } 0$), its last
  three characters, and the general spelling class of the token (e.g.,
  alphanumeric, digit, or punctuation);

\item\emph{morphological}, which encompassed the part-of-speech tag of
  the analyzed token as well as case and gender values for inflectable
  PoS types, the degree of comparison for adjectives, and mood, tense,
  and person forms for verbs;

\item\emph{lexical}, which comprised the actual lemma and form of the
  token (using one-hot encoding), its polarity class (positive,
  negative, or neutral), which we obtained from the Zurich Polarity
  Lexicon~\cite{Clematide:10};

\item and, finally, \emph{syntactic}, which included the dependency
  relation via which the token $\mathbf{x}_m$ was connected to its
  parent; two binary features reflecting whether the previos token in
  the sentence was the parent or child of the current word; as well as
  two other features, one of which encoded the dependency relation of
  the previous token in the sentence to its parent + dependency
  relation of the current token to its ancestor, and the other
  reflected the dependency link of the next token + dependency
  relation of the current token to its parent.
\end{itemize}

In addition to the above attributes, we also used a set of complex
lexico-syntactic features, which simultaneously combined several
semantic and syntactic traits.  These included:
\begin{itemize}
\item the lemma of the syntactic parent;
\item the part-of-speech tag and the polarity class of the
  grandparent;
\item the lemma of the child node + dependency relation connecting the
  parent with this child;
\item the PoS tag of the child node + its dependency relation + PoS
  tag of the current token;
\item the lemma of the child node + its dependency relation + lemma of
  the current token;
\item the overall polarity of children, which was computed by summing
  up polarity scores of all immediate syntactic descendants and
  checking whether the resulting value was greater, less than or equal
  to zero.\footnote{We again used the Zurich Polarity Lexicon
    of~\citet{Clematide:10} for computing these scores.}
\end{itemize}

\textbf{Results} The results of our experiments are shown in
Table~\ref{snt-fgsa:tbl:crf-res}.  As we can see from the table, with
the given set of features, the model can perfectly well fit the
training data, achieving a macro-averaged \F-score of~0.904.  The
learned parameters, however, only partially generalize to unseen data,
leading to notably lower \F-results on the test corpus (0.287).  This
disbalance indicates a strong ``overfitting'' of the model to the
training data (i.e., the assignment of unreasonably high weights to
rather sporadic, noisy features, which only accidentally co-occurred
with the target classes in the observed training instances).

Another notable tendency, which can be observed both on the training
and test splits, is that the recall of the CRF system is generally
lower than its precision.  This again can be attributed to the
overfitting effect, due to which, less indicative features become more
important than attributes which actually give rise to subjective
evaluations.  Since the former features might not to appear in the
test data or, even if they do, are unlikely to correlate with the
sentiment entities, the model often fails to recognize sentiments in
new contexts which do have important but underweighted attributes.

\begin{table*}
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{1}{>{\centering\arraybackslash}p{0.136\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Data Set} & \multicolumn{3}{c}{\bfseries Sentiment} & %
      \multicolumn{3}{c}{\bfseries Source} & %
      \multicolumn{3}{c}{\bfseries Target} & %
      \multirow{2}{0.136\columnwidth}{\bfseries\centering Macro\newline \F{}}\\\cline{2-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} &\\\midrule

      Training Set & 0.949 & 0.908 & 0.928 & 0.903 & 0.87 & 0.886 & %
      0.933 & 0.865 & 0.898 & 0.904\\
      Test Set & 0.37 & 0.28 & 0.319 & 0.305 & 0.244 & 0.271 & 0.304 & %
      0.244 & 0.271 & 0.287\\\bottomrule
    \end{tabular}
    \egroup
    \caption{Results of fine-grained sentiment analysis with the
      first-order linear-chain CRFs.}
    \label{snt-fgsa:tbl:crf-res}
  \end{center}
\end{table*}

\section{Fine-grained Sentiment Analysis with Recurrent Neural Networks}

A competive alternative to the conditional random fields are deep
recurrent neural networks (RNNs).  Introduced in the
mid-nineties~\cite{Hochreiter:97}, RNNs have become one of the most
popular trends in the recent surge of deep learning research, showing
superior performance on many important NLP tasks including
part-of-speech tagging~\cite{Wang:15:pos}, dependency
parsing~\cite{Kiperwasser:16a}, machine
translation~\cite{Kalchbrenner:13,Bahdanau:14,Sutskever:14} etc.  The
main factors which contributed to this success are
\begin{enumerate}[1)]
\item \emph{the ability of these systems to learn the optimal feature
  representations automatically}, which favorably sets them apart from
  traditional supervised machine learning frameworks such as SVMs or
  CRFs where all features need to be defined by the user; and
\item \emph{the ability to deal with arbitrary sequence lengths},
  which advantageously distinguishes them from other NN architectures
  such as convolutional or standard feed-forward networks where the
  size of all layers has to be constant.
\end{enumerate}

A key component which underlies any popular RNN system is a fixed-size
hidden vector $\vec{h}$, which gets recurrently updated over the input
sequence $\mathbf{x}$ and is supposed to encode the meaning of the
sequence seen so far.  The general form of this vector at input
position $t$ is usually defined as:
\begin{align}
  \vec{h}^{(t)} = f(\vec{h}^{(t-1)}, \mathbf{x}^{(t)});
\end{align}
where $f$ represents some pointwise non-linear transformation
function, $\vec{h}^{(t-1)}$ denotes the state of the hidden vector at
the previous time step $t-1$, and $\mathbf{x}^{(t)}$ is the input
vector at position $t$.

\textbf{LSTM} A fundamental problem which arises from the above
formula is that the gradients of the trained parameters (the ones
involved in computing the $\vec{h}$ vector), rapidly vanish to zero or
explode to infinity (depending on whether the abosule values of
$\vec{h}$ are less or greater than one) as the length of the input
increases.  In order to solve this issue, \citet{Hochreiter:97}
proposed the long short-term memory mechanism (LSTM), in which they
explicitly incorporated the goal of dropping parts of the input which
appeared to be irrelevant for the final outcome.  In particular, given
an input sequence $\mathbf{x}$, they introduced a special
\emph{activation unit} $\vec{i}^{(t)}$:
\begin{align*}
  \vec{i}^{(t)} &= \sigma\left(W_i\cdot \mathbf{x}^{(t)} + U_i \cdot \vec{h}^{(t-1)} + \vec{b}_i\right);
\end{align*}
where $\sigma$ denotes the sigmoid function; $W_i$, $U_i$, and
$\vec{b_i}$ represent the optimized model's parameters; and
$\mathbf{x}^{(t)}$ and $\vec{h}^{(t-1)}$ stand for the input and
previous hidden states respectively.  In addition to that, the authors
also estimated a dedicated \emph{forget gate}~$\vec{f}^{(t)}$:
\begin{align*}
  \vec{f}^{(t)} &= \sigma\left(W_f\cdot \mathbf{x}^{(t)} + U_f
  \cdot \vec{h}^{(t-1)} + \vec{b}_f\right),
\end{align*}
which was then used to erase parts of the previous input.

After computing an \emph{intermediate update value}
$\widetilde{c}^{(t)}$ for the current time step~$t$:
\begin{align*}
  \widetilde{c}^{(t)} &= tanh\left(W_c\cdot \mathbf{x}^{(t)} + U_c
  \cdot \vec{h}^{(t-1)} + \vec{b}_c\right),
\end{align*}
they estimated the \emph{final update} state~$\vec{c}^{(t)}$ by taking a
weighted sum of the candidate update vector~$\widetilde{c}^{(t)}$ and
the previous update value~$\vec{c}^{(t-1)}$:
\begin{align*}
  \vec{c}^{(t)} &= \vec{i}^{(t)} * \widetilde{c}^{(t)} + \vec{f}^{(t)} * \vec{c}^{(t-1)};
\end{align*}
from which, they finally computed the output vector
$\vec{o}^{(t)}$ and the new value of the hidden state $\vec{h}^{(t)}$:
\begin{align*}
  \vec{o}^{(t)} &= \sigma\left(W_o\cdot \mathbf{x}^{(t)} + U_o \cdot \vec{h}^{(t-1)} + V_o \cdot \vec{c}^{(t)} + \vec{b}_o\right),\\
  \vec{h}^{(t)} &= \vec{o}^{(t)} * tanh(\vec{c}^{(t)}).
\end{align*}

\textbf{GRU} Despite their enormous popularity and many successful
practical applications~\cite[cf.][]{Filippova:15,Ghosh:16,Rao:16},
LSTMs have often been criticized for the high complexity of the
recurrent unit.  In order to overcome this issue while still keeping
the gradients within an appropriate interval, \citet{Cho:14a}
introduced an alternative architecture called Gated Recurrent Units
(GRU).  In this framework, the authors also used activation and forget
gates---$\vec{i}^{(t)}$ and $\vec{f}^{(t)}$---similar to the ones
defined by~\citet{Hochreiter:97}.  They dropped however the
gate-specific bias terms $\vec{b}_i$ and $\vec{b}_f$ while computing
these gates for simplicity:
\begin{align*}
  \vec{i}^{(t)} &= \sigma\left(W_i\cdot \mathbf{x}^{(t)} + U_i \cdot \vec{h}^{(t-1)}\right),\\
  \vec{f}^{(t)} &= \sigma\left(W_f\cdot \mathbf{x}^{(t)} + U_f \cdot \vec{h}^{(t-1)}\right).
\end{align*}
The candidate activation $\widetilde{c}^{(t)}$ was then estimated as:
\begin{align*}
  \widetilde{c}^{(t)} &= tanh\left(W_c\cdot \mathbf{x}^{(t)} + U_c
  \cdot \left(\vec{f}^{(t)} * \vec{h}^{(t-1)}\right)\right),
\end{align*}
and the final hidden state $\vec{h}^{(t)}$ was estimated as follows:
\begin{align*}
  \vec{h}^{(t)} &= \vec{i}^{(t)} * \vec{h}^{(t-1)} + \left(\vec{1} -
  \vec{i}^{(t)}\right) * \widetilde{c}^{(t)}.
\end{align*}
After obtaining the output of the recurrence ($\vec{o}^{(t)}$ in the
case of LSTM and $\vec{h}^{(t)}$ in the GRU case), we estamated the
final probability of the labels $\vec{p}^{(t)}$ by computing the dot
product of the output vector with the matrix $O$, and estimating the
softmax of this product:
\begin{align*}
  \vec{p}^{(t)} &= softmax\left(O\cdot\vec{o}^{(t)}\right).
\end{align*}

\textbf{Training} A neat property of both of these approaches is that
the final equation, which is obtained after unrolling the loop, is
differentiable with respect to all of its parameters and can therefore
be optimized using the standard gradient update techniques.  Since
most of these parameters, however, represent high-dimensional vectors
or matrices, finding an optimal learning rate (i.e., the size of the
update step taken in the direction of the gradient) might pose
considerable difficulties, leading either to prohibitively large
training times (if the steps are too small) or a complete divergence
of the trained model (if the steps are too big).

Several algorithms have been proposed in order to solve this issue,
including the method of momentum~\cite{Rumelhart:88},
AdaGrad~\cite{Duchi:11}, AdaDelta~\cite{Zeiler:12},
RmsProp~\cite{Tieleman:12} etc.  In our RNN experiments, we will rely
on the last option---the RmsProp optimization proposed
by~\citet{Tieleman:12}---as we found this algorithm leading to both a
faster convergence and superior classification results.

Another important factor, which might significantly affect the
training, is the initialization of the learned model's parameters.  As
shown by~\citet{He:15}, an inappropriate choice of the initial
parameter values might result in a complete stalling of the whole
learning process.  Following the recommended practices~\cite{Saxe:13},
we used orthogonal initialization for all linear transformation
matrices, and applied uniform He sampling~\cite{He:15} for setting
their respective bias vectors.

Furthermore, due to a high imbalance of the target classes in the
training set (where most of the instances only contained the
\texttt{NONE} tag without any sentiment entities), we randomly
``over-sampled'' sentiment tweets (i.e., we randomly repeated some of
the training microblogs containing sentiments until we reached an
equal proportion of subjective and objective messages).

In addition to that, in order to make sure, that the correct
prediction of the \texttt{NONE} tags would not dominate the weight
updates for other classes,\footnote{Since most of the tokens in the
  over-sampled training set still had the \texttt{NONE} tag, the
  easiest way for the classifier to minimize the training error would
  be to always predict this tag with a high confidence.} we chose the
hinge-loss as the optimized objective function $L$:
\begin{align}
  L &= \sum_{i}^{N}\sum_{t=0}^{\lvert\mathbf{x}_i\rvert}\max\left(0, %
  c + \max\limits_{y'\neq y}\vec{p}_{t,y'} - \vec{p}_{t,y}\right) + \alpha \norm{O}^2_2,
\end{align}
in which $\vec{p}_{t,y'}$ stands for the probability of the most
likely wrong tag $y'$ at position $t$ in the training instance
$\mathbf{x}_i$, $\vec{p}_{t,y}$ represents the probability of the gold
label, and $\norm{O}^2_2$ stands for the $L2$-norm of the $O$ matrix.
We optimized the scalar hyper-parameters $c$ and $\alpha$ on the dev
set, and trained the final model for 256 epochs, choosing parameter
values that maximized the macro-averaged \F-score on the development
data.

\textbf{Inference} Since each of the above approaches (LSTM and GRU)
explicitly defines an output unit, the inference of the most likely
label assignment for an input instance $\mathbf{x}$ is straightforward
and amounts to finding the $\argmax$ value of the output vector at
each time step of the recurrence:
\begin{equation}
  \mathbf{\hat{y}} =
  \argmax{\vec{h}^{(1)}},\argmax{\vec{h}^{(2)}},\ldots,\argmax{\vec{h}^{(|\mathbf{x}|)}}.
\end{equation}

\textbf{Results} To account for the random factors in the
initialization, we repeated each training experiment three times, and
show the mean and standard deviation of these results in
Table~\ref{snt-fgsa:tbl:rnn-res}.

As can be seen from the table, the LSTM model generally performs
better than the GRU system on both training and test data.  The only
aspect at which it shows a slightly worse performance on the test set
is the precision of sentiments, which, however, can be more than
compensated for by a higher recall.  Furthermore, the overfitting
effect is much less pronounced than in the case of CRFs (where the \F
scores on training and test data differed by a factor of three).
Nonetheless, both RNN systems achieve lower results than the
linear-chain CRFs, which indicates the fact that deeply learned
features still cannot capture the full extent of information which a
human expert can encode with manually defined attributes.

\begin{table*}
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{1}{>{\centering\arraybackslash}p{0.136\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Data Set} & \multicolumn{3}{c}{\bfseries Sentiment} & %
      \multicolumn{3}{c}{\bfseries Source} & %
      \multicolumn{3}{c}{\bfseries Target} & %
      \multirow{2}{0.136\columnwidth}{\bfseries\centering Macro\newline \F{}}\\\cline{2-10}
      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} &\\\midrule

      \multicolumn{11}{c}{\cellcolor{cellcolor}LSTM}\\
      %%  Tag        Precision    Recall F-Measure
      %% O             86.60%    86.63%    86.62%
      %% SENTIMENT     55.50%    74.72%    63.69%
      %% SOURCE        46.27%    68.82%    55.33%
      %% TARGET        43.12%    77.99%    55.53%

      %% Tag        Precision    Recall F-Measure
      %% O             88.76%    67.71%    76.82%
      %% SENTIMENT     30.76%    76.51%    43.88%
      %% SOURCE        38.84%    49.82%    43.65%
      %% TARGET        29.45%    67.18%    40.95%

      %% Tag        Precision    Recall F-Measure
      %% O             86.38%    89.91%    88.11%
      %% SENTIMENT     60.90%    74.87%    67.16%
      %% SOURCE        48.96%    71.18%    58.02%
      %% TARGET        50.61%    74.58%    60.30%

      Training Set & 0.49\stddev{0.16} & 0.75\stddev{0.01} & 0.58\stddev{0.13} & %
      0.45\stddev{0.05} & 0.63\stddev{0.12} & 0.52\stddev{0.08} %
      & 0.41\stddev{0.11} & 0.73\stddev{0.06} & 0.52\stddev{0.11} %
      & 0.54\stddev{0.11}\\

      %% Tag        Precision    Recall F-Measure
      %% O             77.77%    82.91%    80.26%
      %% SENTIMENT     31.69%    28.00%    29.73%
      %% SOURCE        23.05%    31.25%    26.53%
      %% TARGET        21.77%    23.57%    22.63%

      %% Tag        Precision    Recall F-Measure
      %% O             79.83%    71.62%    75.50%
      %% SENTIMENT     26.23%    43.07%    32.60%
      %% SOURCE        26.07%    30.68%    28.19%
      %% TARGET        21.53%    30.16%    25.13%

      %% Tag        Precision    Recall F-Measure
      %% O             77.58%    86.08%    81.61%
      %% SENTIMENT     30.16%    22.60%    25.84%
      %% SOURCE        24.42%    30.60%    27.16%
      %% TARGET        24.99%    21.20%    22.94%

      Test Set & 0.29\stddev{0.03} & \textbf{0.31}\stddev{0.11} & \textbf{0.29}\stddev{0.03} &%
      \textbf{0.25}\stddev{0.02} & \textbf{0.31}\stddev{0.0} & \textbf{0.27}\stddev{0.01} & %
      \textbf{0.23}\stddev{0.02} & \textbf{0.25}\stddev{0.05} & \textbf{0.24}\stddev{0.01} & %
      \textbf{0.27}\stddev{0.02}\\

      \multicolumn{11}{c}{\cellcolor{cellcolor}GRU}\\

      %% Tag        Precision    Recall F-Measure
      %% O             82.40%    89.44%    85.77%
      %% SENTIMENT     60.16%    60.25%    60.21%
      %% SOURCE        44.77%    66.74%    53.59%
      %% TARGET        46.36%    63.22%    53.49%

      %% Tag        Precision    Recall F-Measure
      %% O             87.39%    80.32%    83.71%
      %% SENTIMENT     44.88%    70.75%    54.92%
      %% SOURCE        39.69%    63.31%    48.79%
      %% TARGET        35.37%    73.96%    47.86%

      %% Tag        Precision    Recall F-Measure
      %% O             81.03%    87.80%    84.28%
      %% SENTIMENT     47.95%    66.84%    55.84%
      %% SOURCE        42.49%    56.86%    48.64%
      %% TARGET        58.22%    51.08%    54.42%

      Training Set & 0.51\stddev{0.08} & 0.66\stddev{0.05} & 0.57\stddev{0.03} & %
      0.42\stddev{0.03} & 0.62\stddev{0.05} & 0.5\stddev{0.03} & %
      0.47\stddev{0.11} & 0.63\stddev{0.11} & 0.52\stddev{0.04} & 0.53\stddev{0.03}\\

      %% Tag        Precision    Recall F-Measure
      %% O             76.78%    86.47%    81.34%
      %% SENTIMENT     30.77%    19.68%    24.01%
      %% SOURCE        20.71%    26.44%    23.22%
      %% TARGET        24.20%    20.93%    22.45%

      %% Tag        Precision    Recall F-Measure
      %% O             78.15%    79.14%    78.64%
      %% SENTIMENT     28.61%    30.25%    29.41%
      %% SOURCE        19.71%    29.45%    23.62%
      %% TARGET        21.54%    27.49%    24.15%

      %% Tag        Precision    Recall F-Measure
      %% O             77.52%    86.03%    81.56%
      %% SENTIMENT     30.66%    28.43%    29.50%
      %% SOURCE        24.46%    29.09%    26.58%
      %% TARGET        27.17%    14.15%    18.61%

      Test Set & \textbf{0.3}\stddev{0.01} & 0.26\stddev{0.06} & 0.28\stddev{0.03} & %
      0.22\stddev{0.03} & 0.28\stddev{0.02} & 0.24\stddev{0.02} & %
      0.24\stddev{0.03} & 0.21\stddev{0.07} & 0.22\stddev{0.03} & 0.25\stddev{0.01}\\\bottomrule
    \end{tabular}
    \egroup
    \caption{Results of fine-grained sentiment analysis with recurrent
      neural networks.}
    \label{snt-fgsa:tbl:rnn-res}
  \end{center}
\end{table*}


\section{Evaluation}

After obtaining results for the most popular sentiment analysis
approaches with the (mostly) standard settings, we decided to
investigate the impact of different training factors on the net scores
of these methods.  For this purpose, we reran the evaluation, changing
one aspect of the training at a time, and reestimated the scores on
the development data (in order to keep the test set undisclosed).  The
results of these experiments are presented below.

\subsection{Effect of the Annotation Scheme}

As the first factor which could significantly affect the quality of
the automatic systems, we considered the annotation scheme that we
used to create the corpus.  As described in
Section~\ref{subsec:snt:ascheme}, we initially asked our experts to
assign the \textsc{SENTIMENT} tag to complete syntactic or
discourse-level units which encompassed both the target of an opinion
and its immediate evaluation.  Even though this decision was
linguistically plausible and notably eased the task of determining
opinion boundaries, it also posed considerable difficulties for
sequence labeling approaches, since the same tag got assigned not only
to subjective words, but also to objective terms which reside within
the same syntactic constituent.  Since none of the tested methods
could explicitly encode this logic, we decided to check whether an
alternative interpretation of the annotation scheme could alleviate
their inference.

In particular, instead of unconditionally labeling all words belonging
to a sentiment span in the original annotation with the \textsc{SNT}
tag as we did previously (which we call the \emph{broad}
interpretation of the annotation scheme), we only assigned this label
to emotional expressions found in the corpus (which we dub the
\emph{narrow} interpretation of the scheme).  The difference between
these two approaches is shown in Examples~\ref{snt:fgsa:exmp:wide}
and~\ref{snt:fgsa:exmp:narrow}.
\begin{example}[Broad Sentiment
  Interpretation]\label{snt:fgsa:exmp:wide}
  \noindent\sentiment{\target{Francis} makes a \intensifier{very}
    \emoexpression{good} impression on\\ \source{me}!
    \emoexpression{:)}}

  $\rightarrow$

  \noindent Francis/TRG makes/SNT a/SNT very/SNT good/SNT
  impression/SNT on/SNT\\ me/SRC !/SNT :)/SNT
\end{example}

\begin{example}[Narrow Sentiment Interpretation]\label{snt:fgsa:exmp:narrow}
  \noindent\sentiment{\target{Francis} makes a \intensifier{very}
    \emoexpression{good} impression on\\ \source{me}!
    \emoexpression{:)}}

  $\rightarrow$

  \noindent Francis/TRG makes/NON a/NON very/NON good/SNT
  impression/NON on/NON\\ me/SRC !/NON :)/SNT
\end{example}
\noindent In the former case (with the broad interpretation), we
labeled the whole opinionated sentence with the \texttt{SNT} tag
except for the words that denoted the target and source of the
opinion.  In the latter case (with the narrow interpretation), we only
assigned the \texttt{SNT} tag to the emotional expression ``good'' and
the emoticon ``:)'', which, however, were expressive enough to convey
the main evaluative sense.

The results of the automatic systems for these two approaches to
defining the boundaris of sentiment spans are shown in
Table~\ref{snt-fgsa:tbl:narrow-wide}.
\begin{table*}
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{1}{>{\centering\arraybackslash}p{0.136\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Data Set} & \multicolumn{3}{c}{\bfseries Sentiment} & %
      \multicolumn{3}{c}{\bfseries Source} & %
      \multicolumn{3}{c}{\bfseries Target} & %
      \multirow{2}{0.136\columnwidth}{\bfseries\centering Macro\newline \F{}}\\\cline{2-10}
      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} &\\\midrule

      \multicolumn{11}{c}{\cellcolor{cellcolor}Wide Interpretation}\\

      %% SENTIMENT     37.62%    31.85%    34.49%
      %% SOURCE        29.75%    33.00%    31.29%
      %% TARGET        29.25%    23.06%    25.79%

      CRF & 0.38 & 0.32 & 0.34 & %
      \textbf{0.3} & 0.33 & 0.31 & %
      \textbf{0.29} & 0.23 & \textbf{0.26} & 0.31\\

      % Tag        Precision    Recall F-Measure
      % O             77.89%    70.02%    73.75%
      % SENTIMENT     24.03%    37.51%    29.29%
      % SOURCE        28.52%    37.43%    32.37%
      % TARGET        22.73%    30.54%    26.06%

      % Tag        Precision    Recall F-Measure
      % O             76.44%    86.10%    80.98%
      % SENTIMENT     30.53%    23.22%    26.38%
      % SOURCE        27.74%    36.33%    31.46%
      % TARGET        28.76%    23.59%    25.92%

      % Tag        Precision    Recall F-Measure
      % O             76.87%    83.75%    80.16%
      % SENTIMENT     30.39%    25.69%    27.84%
      % SOURCE        31.59%    37.88%    34.45%
      % TARGET        24.59%    26.94%    25.71%

      % Summary:
      % Tag             Precision    Recall        F1
      % SENTIMENT           28.32     28.81     27.84
      % SOURCE              29.28     37.21     32.76
      % TARGET              25.36     27.02     25.90
      % Macro-F1  28.8311

      LSTM & 0.28 & 0.29 & 0.28 & %
      0.29 & \textbf{0.37} & \textbf{0.33} & %
      0.25 & \textbf{0.27} & \textbf{0.26} & 0.29\\

      % Tag        Precision    Recall F-Measure
      % O             76.64%    86.78%    81.39%
      % SENTIMENT     30.31%    19.73%    23.90%
      % SOURCE        27.54%    39.00%    32.28%
      % TARGET        23.10%    19.12%    20.93%

      % Tag        Precision    Recall F-Measure
      % O             77.05%    78.92%    77.97%
      % SENTIMENT     28.27%    28.35%    28.31%
      % SOURCE        27.30%    43.05%    33.41%
      % TARGET        22.68%    29.29%    25.56%

      % Tag        Precision    Recall F-Measure
      % O             76.82%    85.63%    80.98%
      % SENTIMENT     27.49%    25.78%    26.60%
      % SOURCE        31.34%    39.30%    34.87%
      % TARGET        29.86%    13.15%    18.26%

      % Summary
      % Tag             Precision    Recall        F1
      % SOURCE              28.73     40.45     33.52
      % SENTIMENT           28.69     24.62     26.27
      % TARGET              25.21     20.52     21.58
      % Macro-F1  27.1244

      GRU & 0.29 & 0.25 & 0.26 & %
       0.29 & 0.4 & 0.34 & %
       0.25 & 0.21 & 0.22 & 0.27\\\midrule

      \multicolumn{11}{c}{\cellcolor{cellcolor}Narrow Interpretation}\\

      %% Tag        Precision    Recall F-Measure
      %% O             85.93%    85.69%    85.81%
      %% SENTIMENT     58.84%    64.49%    61.54%
      %% SOURCE        26.13%    23.00%    24.47%
      %% TARGET        22.14%    20.14%    21.09%

      CRF & 0.59 & 0.64 & 0.62 & %
      0.26 & 0.23 & 0.24 & %
      0.22 & 0.20 & 0.21 & 0.36\\

      % Tag        Precision    Recall F-Measure
      % O             85.08%    90.61%    87.76%
      % SENTIMENT     57.18%    66.52%    61.50%
      % SOURCE        27.22%    40.25%    32.48%
      % TARGET        25.54%    12.56%    16.84%

      % Tag        Precision    Recall F-Measure
      % O             84.57%    91.93%    88.10%
      % SENTIMENT     69.83%    60.92%    65.07%
      % SOURCE        32.28%    35.55%    33.84%
      % TARGET        26.04%    16.18%    19.96%

      % Tag        Precision    Recall F-Measure
      % O             84.05%    91.82%    87.77%
      % SENTIMENT     58.79%    67.52%    62.85%
      % SOURCE        30.90%    27.85%    29.30%
      % TARGET        27.07%    13.31%    17.85%

      LSTM & \textbf{0.62} & \textbf{0.65} & \textbf{0.63} & %
      \textbf{0.3} & 0.35 & 0.32 & %
      0.26 & 0.14 & 0.18 & \textbf{0.38}\\

      %% Tag        Precision    Recall F-Measure
      %% O             84.78%    87.61%    86.17%
      %% SENTIMENT     59.02%    61.71%    60.34%
      %% SOURCE        27.35%    37.83%    31.75%
      %% TARGET        22.37%    21.18%    21.76%

      %% Tag        Precision    Recall F-Measure
      %% O             84.38%    90.46%    87.31%
      %% SENTIMENT     60.14%    64.76%    62.36%
      %% SOURCE        27.93%    29.53%    28.71%
      %% TARGET        27.40%    20.07%    23.17%

      % Tag        Precision    Recall F-Measure
      % O             84.98%    85.60%    85.29%
      % SENTIMENT     65.71%    61.64%    63.61%
      % SOURCE        29.91%    31.20%    30.54%
      % TARGET        19.18%    31.63%    23.88%

      GRU & \textbf{0.62} & 0.63 & 0.62 & %
      0.28 & 0.33 & 0.3 & %
      0.23 & 0.24 & 0.23 & \textbf{0.38}\\\bottomrule

    \end{tabular}
    \egroup
    \caption{Results of fine-grained analysis with broad and narrow
      sentiment interpretations.}
    \label{snt-fgsa:tbl:narrow-wide}
  \end{center}
\end{table*}
As we can see from the table, the wide interpretation generally leads
to notably lower scores for the sentiment spans, but yields much
better results for the prediction of sources and targets of the
opinions.  An opposite situation is observed with the narrow scheme:
even though the \F-score for the \texttt{SNT} labels is twice as high
as in the wide case, the accuracy for holders and objects

\subsection{Effect of Features}

Another important factor which could significantly influence the
results of the CRF system were the features which we provided to it as
input.  In order to check the utility of our attributes, we performed
an ablation test, removing one feature group at a time and rechecking
the performance of the model on the development set.

\begin{table}[hbt]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.47\tabcolsep}\scriptsize
    \begin{tabular}{p{0.14\columnwidth} % first columm
        *{6}{>{\centering\arraybackslash}p{0.13\columnwidth}}} % next five columns
      \toprule
          \multirow{2}{0.2\columnwidth}{\bfseries Element} &
          \multirow{2}{0.1\columnwidth}{\bfseries Original\newline \F-Score} &
          \multicolumn{5}{c}{\bfseries \F-Score after Feature Removal}\\\cline{3-7}
          & & Formal & Morphological & Lexical & Syntactic & Complex\\\midrule

          Sentiment & 0.346 & 0.343\negdelta{0.003} & 0.344\negdelta{0.002} & 0.326\negdelta{0.02} & 0.345\negdelta{0.001} & 0.324\negdelta{0.022}\\
          Source & 0.309 & 0.321\posdelta{0.012} & 0.313\posdelta{0.004} & 0.265\negdelta{0.044} & 0.359\posdelta{0.05} & 0.271\negdelta{0.038}\\
          Target & 0.26 & 0.282\posdelta{0.022} & 0.252\negdelta{0.008} & 0.263\posdelta{0.003} & 0.233\negdelta{0.027} & 0.263\posdelta{0.003}\\\bottomrule
    \end{tabular}
    \egroup
    \caption[Results of feature ablation tests for CRF model.]{
      Results of feature ablation tests for CRF model.\\{\small\itshape
        (negative changes w.r.t. the original scores on the dev set
        are shown in \textsuperscript{\textcolor{red3}{red}}; positive
        changes are depicted in
        \textsuperscript{\textcolor{seagreen}{green}}
        superscript)\footnotemark}}
    \label{tbl:ablation}
  \end{center}
\end{table}

The results of this test are shown in
Table~\ref{tbl:ablation}.\footnotetext{Negative changes indicate good
  features in this context, since their removal leads to a degradation
  of results.}  As we can see from the table, all feature types turn
out to be useful for predicting sentiments as their removal
unequivocally leads to a degradation of the scores.  This quality
drop, however, is usually quite small, suggesting that other attribute
types can easily make up for the removed ones.

A different situation is observed for sources and targets though.  In
the first case, removing formal, morphological, and syntactic features
shows a strongly positive effect, improving the \F-results for sources
by up to 5\%.  However, removing lexical and lexico-syntactic
features, on the contrary, worsens these results, tearing the
\F-scores down by up to 4.4\%.

Except for the formal group, all these attributes behave completely
differently when applied to targets, which seem to benefit from
morphological and syntactic features, while suffering a slight
degradation from lexical and complex attributes.

\begin{table}[hbt]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.47\tabcolsep}\scriptsize
    \begin{tabular}{>{\centering\arraybackslash}p{0.05\columnwidth} % first columm
        *{4}{>{\centering\arraybackslash}p{0.2\columnwidth}}} % next four columns
      \toprule
          \multirow{2}{0.2\columnwidth}{Rank} &
          \multicolumn{2}{c}{\bfseries Top-10 State Features} &
          \multicolumn{2}{c}{\bfseries Top-10 Transition Features}\\\cline{2-5}
          & Feature & Score & Feature & Score\\\midrule

          1 & prntLemma=meiste $\rightarrow$ TRG & 18.68 & NON $\rightarrow$ TRG & -7.01\\
          2 & prntLemma=rettungsschirme $\rightarrow$ TRG & 18.3 & NON $\rightarrow$ SRC & -6.85\\
          3 & initChar=sty $\rightarrow$ NON & -16.04  & NON $\rightarrow$ SNT & -5.39\\
          4 & form=meisten $\rightarrow$ NON & 15.99 & TRG $\rightarrow$ SRC & -2.99\\
          5 & prntLemma=urlauberin $\rightarrow$ SNT & 14.74 & NON $\rightarrow$ NON & 2.69\\
          6 & lemma=anfechten  $\rightarrow$ SNT & 14.07 & SRC $\rightarrow$ NON & -2.59\\
          7 & form=thomasoppermann  $\rightarrow$ TRG & 13.44 & SNT $\rightarrow$ SNT & 2.54\\
          8 & form=bezeichnete $\rightarrow$ SNT & 13.25 & TRG $\rightarrow$ TRG & 2.31\\
          9 & deprel[0]|deprel[1]=NK|AMS $\rightarrow$ NON & 12.92 & SRC $\rightarrow$ SRC & 2.19\\
          10 & trailChar=te. $\rightarrow$ NON & 12.77 & SRC $\rightarrow$ TRG & -2.07\\\bottomrule
    \end{tabular}
    \egroup
    \caption{Top ten state and transition features learned by the CRF
      model.\\{\small (sorted by the absolute values of their
        weights)}}
    \label{fgsa:tbl:ablation}
  \end{center}
\end{table}

In order to get a better overview of the learned model's parameters,
we also extracted top ten state and transition features ranked by the
absolute values of their scores (see Table~\ref{fgsa:tbl:ablation}).
As can be seen from the statistics, three of the five highest ranked
state attributes are complex features reflecting the lemma of the
parent token: ``meiste'' (\emph{most}) and ``rettungsschirme''
(\emph{bailout}), which typically indicate a target, and
``urlauberin'' (\emph{holiday}), which frequently correlates with
sentiments.  Another common group of features are the lemma and form
of the current token: here, we again can encounter ``meisten''
(\emph{most}), which, however, indicates the absence of any sentiment
entities at the current position; two other attributes---``anfechten''
(\emph{doubt}) and ``bezeichnete'' (\emph{called})---represent the
so-called \emph{direct speech events} and expectedly correlate with
the sentiment tags; the remaining feature---``thomasopperman''---is a
person name, which frequently appears as a target of an opinion.

An interesting pattern can be observed for transition attributes: As
we can see from the results, the top-3 transition features indicate a
strong belief that an objective token is highly unlikely to be
followed by any sentiment entity (hence, the high negative weights of
the transitions emanating from $NON$).  It is, however, quite common
that a $NON$ tag will preceed another $NON$ instance (cf. line 5 of
the table).  Other transition attributes also mainly reflect plausible
regularities: It is, for instance, uncommon that a target of an
opinion will appear immediately before a source ($TRG\rightarrow SRC =
-2.99$); in the same vein, it is rather uncommon that a source tag
will preceed a target ($SRC\rightarrow TRG = -2.07$); nonetheless, is
is perfectly acceptable that the same tag will continue over multiple
words (e.g., $SNT\rightarrow SNT = 2.54$, $TRG\rightarrow TRG =
2.31$).

\subsection{Effect of Word Embeddings}


% LSTM

%% word2vec
% Tag        Precision    Recall F-Measure
% O             75.08%    81.90%    78.34%
% SENTIMENT     28.76%    35.81%    31.90%
% SOURCE        27.16%    30.53%    28.75%
% TARGET        23.54%    24.07%    23.80%

% Tag        Precision    Recall F-Measure
% O             76.18%    83.40%    79.63%
% SENTIMENT     30.01%    35.54%    32.54%
% SOURCE        26.85%    27.53%    27.19%
% TARGET        24.25%    23.30%    23.76%

\begin{table*}
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{1}{>{\centering\arraybackslash}p{0.136\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Data Set} & \multicolumn{3}{c}{\bfseries Sentiment} & %
      \multicolumn{3}{c}{\bfseries Source} & %
      \multicolumn{3}{c}{\bfseries Target} & %
      \multirow{2}{0.136\columnwidth}{\bfseries\centering Macro\newline \F{}}\\\cline{2-10}
      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} &\\\midrule

      \multicolumn{11}{c}{\cellcolor{cellcolor}Narrow Interpretation}\\

      LSTM &  &  &  & %
       &  &  & %
       &  &  & \\

      GRU &  &  &  & %
       &  &  & %
       &  &  & \\\bottomrule

    \end{tabular}
    \egroup
    \caption{Results of fine-grained sentiment analysis with different
      types of word embedding.}
    \label{snt-fgsa:tbl:embeddings
    }
  \end{center}
\end{table*}

\subsection{Effect of Lexica and Normalization}

\subsection{Effect of Topologies}

Due to an exponentially increasing running time, we constrained the
maximum number of training iterations to 256 while keeping all other
hyper-parameters of the model the same as the original setting.


\begin{table*}
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.12\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.08\columnwidth}}} % next nine columns
      \toprule
      \multirow{2}*{\bfseries Element} & %
      \multicolumn{9}{c}{\bfseries Topology}\\\cline{2-10}
      & lcCRF$^1$ & lcCRF$^2$ & lcCRF$^3$ & lcCRF$^4$ & %
      smCRF$^1$ & smCRF$^2$ & smCRF$^3$ & smCRF$^4$ & trCRF$^1$\\\midrule

      \multicolumn{10}{c}{\cellcolor{cellcolor}Training Set}\\

      Sentiment & 0.928 & 0.919 & 0.922 & 0.925 & 0.931 & 0.931 & 0.933 & 0.931 & \\
      Source & 0.887 & 0.876 & 0.89  & 0.901 & 0.869 & 0.886 & 0.874 & 0.878 & \\
      Target & 0.898 & 0.811 & 0.816 & 0.827 & 0.813 & 0.827 & 0.815 & 0.817 & \\

      \multicolumn{10}{c}{\cellcolor{cellcolor}Development Set}\\

      Sentiment & 0.345 & 0.334 & 0.332 & 0.335 & 0.395 & 0.385 & 0.389 & 0.378 & \\
      Source & 0.313 & 0.32 & 0.272 & 0.304 & 0.298 & 0.282 & 0.287 & 0.291 & \\
      Target & 0.258 & 0.235 & 0.24 & 0.229 & 0.287 & 0.309 & 0.301 & 0.292 & \\\bottomrule
    \end{tabular}
    \egroup
    \caption{Results of fine-grained sentiment analysis for different
      CRF topologies.\\ {\small lcCRF---linear-chain CRFs,
        smCRF---semi-Markov CRFs, trCRF---tree-structured CRFs;\\1, 2,
        and 3 in the superscriptsdenote the order}}
    \label{fgsa:tbl:topologies}
  \end{center}
\end{table*}
% \subsection{Semi-Markov CRFs}
% Instead of making their predictions over single tokens, semi-Markov
% CRFs try to partition the input sequence $\bm{x}$ into contiguous
% segments $\vec{s} = \langle{}s_1,\ldots,s_l\rangle{}$ where each
% segment $s_i$ consists of one or more tokens which all share the same
% common tag $y_i$.

% Features in semi-Markov models are defined at the level of segments
% and, like in the linear-chain case, are binary- or real-valued
% functions associated either with the segment $s_i$ and its label $y_i$
% or a pair of adjacent segment labels $(y_{i-1}, y_i)$.

% During the training and the later prediction step, the model considers
% all potential segmentations for all possible label assignments and all
% possible segment lengths ranging from 1 to $L$ where $L$ is the length
% of the longest sequence of tokens with identical tags that was
% observed in the training data.  The decoding step then jointly finds
% the most probable segmentation $\hat{s}$ and the label sequence
% $\vec{y}$ for that segmentation using either the greedy search or the
% dynamic Viterbi algorithm.

% \subsection{Higher-order CRFs}
% Unlike the first-order models whose transition features can only
% access a pair of adjacent element labels $(y_{i-1}, y_i)$, transition
% features of higher-order linear-chain and semi-Markov CRFs encode
% information about complete label sequences up to length $d$ that might
% precede the predicted label $y_i$ where $d$ is the maximum order of
% the model.

% But since the number of such label sequences grows exponentially in
% the order of the encoded dependencies (up to
% $\sum_{i=1}^d\left\vert\mathcal{Y}\right\vert^i$), the running time of
% the training and inference algorithms rapidly becomes prohibitively
% expensive and amounts to
% $\mathcal{O}(n(\sum_{i=1}^d\left\vert\mathcal{Y}\right\vert^{i})^2)$
% for the linear-chain case and
% $\mathcal{O}(nL(\sum_{i=1}^d\left\vert\mathcal{Y}\right\vert^{i})^2)$
% for the semi-Markov models.

% Because this problem is intractable in general \cite{Istrail:00},
% researchers are usually forced to resort to some sort of heuristics
% like an early state pruning \cite{Mueller:13} or state-transition
% reduction.

% For the purpose of our experiments, we applied the affix-based
% algorithm of \citet{Nguyen:14}.  This algorithm relies on the
% advantage that the actual number of possible sequences of $d$
% consecutive segment labels that are observed in the training set is
% typically much smaller than
% $\sum_{i=1}^d\left\vert\mathcal{Y}\right\vert^i$.  Furthermore, the
% number of possible transitions that need to be considered for the
% given label sequence $(y_{i-d}, y_{i-d+1},\ldots,y_i)$ can further be
% reduced by restricting them only to those patterns for which this
% label sequence forms the longest possible suffix.

% The running time of this algorithm for the semi-Markov case runs up to
% $\mathcal{O}(nL\left\vert\mathcal{P}\right\vert\left\vert\mathcal{Y}\right\vert)$
% where $\left\vert\mathcal{P}\right\vert$ is the total number of the
% label prefixes observed in the training set.  And even though this
% number can still be exponential in the order of the dependencies in
% the worst case, this algorithm usually leads to much faster results in
% practice especially when the label patterns in the addressed domain
% are sparse.

% \subsection{Tree-structured CRFs}
% In contrast to the linearly structured models, in which the predicted
% label $y_i$ only depends on the state attributes of the respective
% item $x_i$ and the label scores of the preceding tokens or segments
% and where the syntax information can only be expressed as feature
% attributes of single items, tree-CRFs allow to incorporate syntactic
% dependencies directly into the model's structure.

% In our experiments, we used the automatically constructed dependency
% trees of the sentences as the underlying graphs for the
% tree-structured CRF model.  Like in the linear CRF case, every
% predicted label $y_i$ in this model is dependent on the state features
% of the token $x_i$, but, instead of hard-coding dependencies on the
% label scores of the previous tokens, we let transition features
% connect the predicted label with every label score $y_c$ where $c$
% ranges over all indices of the child nodes of the token $x_i$ in the
% dependency tree.

% Since the resulting model graphs are acyclic, the inference in this
% type of CRFs can still be exact and its running time remains the same
% as for the first-order linear chain models:
% $\mathcal{O}(n\left\vert\mathcal{Y}\right\vert^{2})$.

\subsection{Error Analysis}

\section{Related Work}
One of the first attempts at automatically analyzing subjective
opinions below the document and sentence level was made by
\citet{Nasukawa:03}.  In their work, the authors proposed a
lexicon-based approach, in which they first determenied the
occurrences of the given target in text; then analyzed the immediate
context of these occurrences, looking for subjective terms from their
manually compiled lexicon; and, finally, determined the overall
orientation of the opinion, judging by the predetermined polarity
score of the lexicon term and a set of heuristic rules which accounted
for contextual changes.

An alternative way of classifying subjective opinions and their
holders was proposed by \citet{Bethard:04}, who also utilized a
lexicon \cite{Wiebe:03} for finding opinionated terms.  In the
following step, the recognized expressions were used as features for a
set of SVM classifiers which predicted which nodes of a syntactic
constituency trees represented opinionated clauses or sources of the
opinions.  A similar method was also applied by \citet{Kobayashi:07}
for identifying subjective expressions pertaining to product aspects.

One of the first attempts at automatically analyzing subjective
opinions with Conditional Random fields was made by \citet{Choi:05}.
In their work, the authors combined a linear-chain CRF system with the
unsupervised pattern extractor AutoSlog-TS \cite{Riloff:96} in order
to recognize holders of opinions in text.  This approach showed a
significant improvement over compared baselines (noun phrases
pre-selected by heuristic rules).

Later on, \citet{Choi:06} further enhanced this system, simultaneously
trying to predict both sources and expressions of opinions, also
establishing links between these entities.  For this purpose, they
again applied two CRF classifiers (one for each entity type) to
determine the textual spans of opinion holders and subjective
elements, getting top-10 results from each of the systems.  In the
next step, they pruned off invalid suggestions with a set of hard and
soft constraints, whose weights were optimized using the integer
linear programming.  This two-stage approach not only allowed the
authors to reach impressive 68.9\% \F-measure on predicting links but
also helped to improve automatic recognition for each of the predicted
element types.

\citet{Breck:07} concentrated on the prediction of \emph{direct speech
  events} (DSEs) and \emph{expressive subjective elements} (ESE) in
the MPQA corpus \cite[cf.][]{Wiebe:05}.  For this task, they also
adopted the CRF approach, getting a substantial imrovement over
dictionary-based baselines.

\citet{Choi:10} attempted to jointly predict the boundaries, polarity,
and intensity of subjective elements (DSEs and ESEs) with a single CRF
classifier.  For this purpose, they adopted the hierarchical parameter
sharing technique \cite{Zhao:08}, letting their system differentiate
between ten different classes (two classes for the presence of an
opinion times three polarities and three intensities), but ensuring
that parameters belonging to the same group (e.g., high intensity)
were shared among these classes.

Instead of relying on just one most likely label assignment when
predicting subjective expressions, \citet{Johansson:10a} explored the
possibility of reranking by first obtaining top-$k$ alternative
predictions (with $k = 8$) and then reweighting these labelings with a
different type of classifier.  In particular, the authors experimented
with linear and tree-based preference kernels as well as structured
perceptron and passive-aggressive algorithms, finding the last option
performing best and yielding 4\% improvement over the standard
linear-chain CRF baseline.  The same idea was also used for the joint
extraction of subjective expressions and opinion
holders~\cite[cf.][]{Johansson:10b}.

\citet{Yang:12} were allegedly the first who used semi-Markov CRFs in
an opinion mining application.  In order to determine the boundaries
of DSEs and ESEs, the authors derived a set of potential segments from
shallow syntastic parses and then let a semi-Markov model determine
the most likely segmentation and label assignment over these
hypotheses.

One of the first steps towards using deep neural networks for the
prposes of fine-grained sentiment analysis was made
by~\citet{Socher:13}.  In their work on predicting sentiment polarity
of syntactic constituents (including the whole sentences), the authors
explored a recursive neural tensor approach, in which they computed
the semantic orientation of a given node $\vec{v}_c \in \mathbb{R}^n$
in a bottom-up fashion by recursively multiplying the embeddings of
its descendant leaves (e.g., $\vec{w}_1 \in \mathbb{R}^n$ and
$\vec{w}_2 \in \mathbb{R}^n$) with a compositional tensor $V \in
\mathbb{R}^{2n\times2n\times2n}$:
\begin{align*}
  \vec{v}_c = softmax\left(%
  \begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}^T \cdot V^{1:n}\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}%
  + W\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}%
  \right),
\end{align*}
where $W\in\mathbb{R}^{n\times n}$ was an additional shared
compositionality matrix, and the tensor productfor a single element
was defined as:
\begin{align*}
  v'_i = \begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}^T \cdot V^{i}\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}.
\end{align*}
The weights of this tensor as well as the vector representations of
the underlying words, and the compositionality matrix $W$ were learned
automatically during training.

Another recursive approach was suggested by~\citet{Irsoy:14a}
\cite[see also][]{Irsoy:14b}, who explored deep bidirectional neural
networks for predicting DSEe and ESEs on the MPQA corpus.  In
particular, they estimated the label $y$ of the $t$-th token as:
\begin{align*}
  y_t = softmax\left(U_{\rightarrow}\cdot \vec{h}_t^{(L)} + U_{\leftarrow}\cdot \cev{h}_t^{(L)} + \vec{c}\right),
\end{align*}
where $\vec{h}_t^{(L)}$ and $\cev{h}_t^{(L)}$ were the outputs of the
top-$L$ levels computed from left to right and from right to left
respectively; and $U_{\rightarrow}$, $U_{\leftarrow}$, and the bias
term $\vec{c}$ were the other learned parameters used to reduce the
dimensions.  This system could outperform the previous state of the
art set by \citet{Yang:12}, reaching a proportional \F-score of 66.01
for DSEs and 56.26 for ESEs.

Other notable works on fine-grained opinion mining with deep neural
networks include thos of \citet{Irsoy:14c}, who used a tensor
multiplication within the recursive loop; \citet{Tang:16}, who
proposed a memory-based multi-level network for classifying opinions
about the given aspects; and, finally, \citet{Wang:16}, who used the
output of a deep recursive network as input for a CRF system, updating
the weights of the network along with the CRF parameters.

\section{Summary and Conclusions}\label{fgsa:subsec:conclusions}

TODO

We also note that all our obtained results are significantly lower
than the corresponding figures achieved for other domains and other
languages.  A closer look into the errors showed us that the main
reason for such severe accuracy drop was an insufficiently good
recognition of opinionated words and phrases.  This can be partially
explained by the creativity of the users in expressing their thoughts
but also by the lack of appropriate sentiment dictionaries which would
not only contain standard language expressions but also slang words
and cusses.

An additional challenge was posed by emoticons which could not only
convey an evaluative meaning but also serve as politeness expressions
and were therefore ambiguous sentiment markers.  Furthermore, we
should also mention that the German language itself is more difficult
for automatic processing due to its relatively free word order,
ambiguous inflections, and a rich morphology.  And these problems
become even more aggravated when dealing with Twitter.

We also saw that the outcomes of our experiments crucially depended on
the definition of the sentiments that we applied and that different
CRF variants had different influence on the classification accuracy
for either interpretation.  Unfortunately, none of these models could
clearly outperform its rivals, but we could detect some general trends
for each of the proposed interpretations.  If these trends would also
hold whe using other feature sets and other corpora is one of the
questions that we want to address in future research.
\newpage
