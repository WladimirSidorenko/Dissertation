{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">\n",
       "<script type=\"text/javascript\" src=\"custom.css\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<script src='js/animation.js' type='text/javascript'>\n",
    "</script>\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">\n",
    "<div class=\"supertitle animate\">\n",
    "Presentation of the Ph.D. Dissertation\n",
    "</div>\n",
    "\n",
    "<div class=\"title\">\n",
    "Sentiment Analysis of German Twitter\n",
    "</div>\n",
    "\n",
    "<div class=\"author\">\n",
    "<p>Uladzimir Sidarenka</p>\n",
    "<p class=\"alternative-name\">(Vladimir Sidorenko)</p>\n",
    "</div>\n",
    "\n",
    "<img src=\"img/uni-potsdam-logo.svg\" alt=\"University of Potsdam\" class=\"logo\"/>\n",
    "\n",
    "<div class=\"institution\">\n",
    "University of Potsdam\n",
    "</div>\n",
    "\n",
    "<div class=\"date\">\n",
    "July 12, 2019\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 class=\"chapter-title animate\">Chapter I: Introduction</h1>\n",
    "<img src=\"img/twitter-5.svg\" alt=\"Twitter Logo\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Definition\n",
    "\n",
    "**Sentiment Analysis**, also called *opinion mining*, is the field of study that analyzes people's opinions, sentiments, evaluations, appraisals, attitudes, and emotions towards entities such as products, services, organizations, individuals, issues, events, topics, and their attributes. <div class=\"quote\">(Liu, 2012)</div>\n",
    "\n",
    "<img src=\"img/emotions.jpg\" alt=\"emotions\" id=\"emotions\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Main Challenges of Sentiment Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* There can be **different opinions**;\n",
    "<div class=\"examples hide-on-next-fragment\" >\n",
    "    <div class=\"left\">\n",
    "        <img src=\"img/trump_happy.jpg\" alt=\"Happy Trump\"/>\n",
    "    </div>\n",
    "    <div class=\"right\">\n",
    "        <img src=\"img/trump_anger.jpg\" alt=\"Angry Trump\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Even the same opinions can be differently expressed by **different people**;\n",
    "<div class=\"examples hide-on-next-fragment\">\n",
    "    <div class=\"left\">\n",
    "        <img src=\"img/laughing_politicians.jpg\" alt=\"Laughing Politicians\"/>\n",
    "    </div>\n",
    "    <div class=\"right\">\n",
    "        <img src=\"img/smiling_queen.jpg\" alt=\"Smiling Queen\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Opinions can come in **different languages**;\n",
    "<div class=\"examples hide-on-next-fragment\">\n",
    "    <div class=\"left\">\n",
    "        <img src=\"img/putin_merkel_smile.jpg\" alt=\"Putin and Merkel Laughing\"/>\n",
    "    </div>\n",
    "    <div class=\"right\">\n",
    "        <img src=\"img/trump_kim_jong_un.jpg\" alt=\"Trump and Kim Jong Un\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* They can be communicated via **different channels**;\n",
    "<div class=\"examples hide-on-next-fragment\">\n",
    "    <div class=\"left\">\n",
    "        <img src=\"img/kennedy_tv.jpg\" alt=\"Kennedy on TV\"/>\n",
    "    </div>\n",
    "    <div class=\"right\">\n",
    "        <img src=\"img/texting_people.jpg\" alt=\"Texting People\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* &hellip; but most importantly, opinions **involve all language levels**\n",
    "<div class=\"examples hide-on-next-fragment\">\n",
    "    <div class=\"left\" id=\"left-bear-div\">\n",
    "        <img src=\"img/bear_discourse.jpg\" alt=\"Bear Meme\" id=\"left-bear-img\"/>\n",
    "    </div>\n",
    "    <div class=\"right\" id=\"right-bear-div\">\n",
    "        <img src=\"img/bear_word.jpg\" alt=\"Bear from Bush Meme\"  id=\"right-bear\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* and are often **difficult to spot**.\n",
    "\n",
    "<div class=\"examples hide-on-next-fragment\">\n",
    "    <div class=\"center\" id=\"chuck-norris-div\">\n",
    "      <img src=\"img/chuck_norris_emotions.jpg\" alt=\"Chuck Norris' Emotions\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this presentation, I am going to analyze the ways and means by how people express their **opinions** on the levels of:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **words**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **syntactic constituents**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **sentences**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **discourse**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "&hellip; and I will do it on **German Twitter**.\n",
    "<div id=\"merkel-twitter-div\"><img src=\"img/merkel-twitter.jpg\" alt=\"Merkel with Twitter Logo\" id=\"merkel-twitter\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Twitter** is an American online news and social networking service on which users post and interact with short messages (up to 140 [280] characters) known as *tweets*. Registered users can post, like, and retweet tweets, whereas unregistered users can only read them. <div class=\"quote\">(Wikipedia)</div>\n",
    "\n",
    "<img src=\"img/justin-bieber-twitter-page.png\" alt=\"Justin Bieber's Twitter Page\" id=\"justin-bieber-twitter\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* How difficult is it to analyze sentiments for humans and computers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Which factors might affect this difficulty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Can we apply opinion mining methods devised for standard English to German Twitter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Which groups of approaches are best suited for which sentiment tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Can we do better than existing methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Does text normalization help analyze sentiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What Do We Need to Get these Answers?\n",
    "\n",
    "<img src=\"img/what_do_we_want.jpg\" alt=\"What do we want?\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 class=\"chapter-title animate\">Chapter II: Data</h1>\n",
    "<img src=\"img/data.png\" alt=\"Data\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Compared to the total number of tweets, sentiments are sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/screencapture-twitter.png\" alt=\"Screenshot of Golm Tweets\" id=\"screencapture-twitter\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Main Goals When Creating a Sentiment Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* On the one hand, we want to get **as many sentiments as possible**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* But on the other hand, we still want to **keep the data bias low**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# How Can We Achieve the First Goal? \n",
    "\n",
    "* We can preselect messages for the corpus based on their **topic**;\n",
    "\n",
    "* We can also use the **form** of the tweets as a clue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As criteria that could help us get more opinions, we considered topic and form of the\n",
    "tweets, assuming that some subjects, especially social or political issues, would be more\n",
    "amenable to subjective statements. Because we started creating the corpus in spring 2013,\n",
    "obvious choices of opinion-rich topics to us were the papal conclave, which took place in\n",
    "March of that year, and the German federal elections, which were held in autumn. Since\n",
    "both of these events implied some form of voting, we decided to counterbalance the election\n",
    "specifics by including general political discussions as the third subject in our dataset. Finally,\n",
    "to obey the second principle, i.e., to keep the corpus bias low, we sampled the rest of the\n",
    "data from casual everyday conversations without any prefiltering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **German Federal Elections** 2013;\n",
    "\n",
    "<div class=\"examples hide-on-next-fragment\">\n",
    "    <div class=\"center\">\n",
    "        <img src=\"img/german_elections_2013.jpeg\" alt=\"German Federal Elections 2013\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Papal Conclave** 2013;\n",
    "\n",
    "<div class=\"examples hide-on-next-fragment\">\n",
    "    <div class=\"center\">\n",
    "        <img src=\"img/papal_conclave.jpg\" alt=\"Papal Conclave 2013\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* General **Political Discussions**;\n",
    "\n",
    "<div class=\"examples hide-on-next-fragment\">\n",
    "    <div class=\"center\">\n",
    "        <img src=\"img/politics.jpg\" alt=\"General Political Discussions\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Everyday Conversations**.\n",
    "\n",
    "<div class=\"examples hide-on-next-fragment\">\n",
    "    <div class=\"center\">\n",
    "        <img src=\"img/casual_conversation.jpg\" alt=\"Casual Conversations\"/>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Formal Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the next step, I divided all tweets of the same topic into three groups based on the following formal criteria:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Polar Terms** (e.g., \"gut\" *good*, \"schlecht\" *bad*, \"weinen\" *to cry*, etc.);\n",
    "\n",
    "<div class=\"examples hide-on-next-fragment\">\n",
    "    <div class=\"example\">\n",
    "        <div class=\"title\">Example 2.1</div>\n",
    "        <div class=\"tweet\"><span class=\"label\">Tweet:</span>RT @ProSieben: Gutes Datum, gutes Omen? Der neue #Papst ist am 13.M&#228;rz 2013 gew&#228;hlt.</div>\n",
    "        <div class=\"translation\">RT @ProSieben: Good date, good omen? The new #Pope was elected on March 13, 2013.</div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " I used the sentiment lexicon by Remus et al. (2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Exclamation Marks or Emoticons** (e.g., \":-)\", \":-(\", \":-D\");\n",
    "\n",
    "<div class=\"examples hide-on-next-fragment\">\n",
    "    <div class=\"example\">\n",
    "        <div class=\"title\">Example 2.2</div>\n",
    "        <div class=\"tweet\"><span class=\"label\">Tweet:</span>RT \"@GrumpyMerkel: :-[ ist das offizielle Smiley der Bundeskanzlerin der Bundesrepublik Deutschland\" :-)</div>\n",
    "        <div class=\"translation\">RT \"@GrumpyMerkel: :-[ is the official smiley of the federal chancellor of the German Federal Republic\" :-)</div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **All remaining microblogs**.\n",
    "\n",
    "<div class=\"examples hide-on-next-fragment\">\n",
    "    <div class=\"example\">\n",
    "        <div class=\"title\">Example 2.3</div>\n",
    "        <div class=\"tweet\"><span class=\"label\">Tweet:</span>Mehr Katz3n in der Politik und im Landtag NRW #opennrw #opennrw13 http://t.co/0nBTtG8EWW</div>\n",
    "        <div class=\"translation\">More cats in the politics and in the state parliament of NRW #opennrw #opennrw13 http://t.co/0nBTtG8EWW</div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <caption>\n",
    "        Distribution of downloaded messages across topics and formal groups\n",
    "     </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Topic</td>\n",
    "<td colspan=\"4\">Formal Criterion</td>\n",
    "<td rowspan=\"2\">Sample Keywords</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> Polar Terms </td>\n",
    "<td> Emoticons </td>\n",
    "<td> Remaining Tweets </td>\n",
    "<td> Total </td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Federal Elections</td>\n",
    "<td>537,083<br/>(22.38%)</td>\n",
    "<td>50,567<br/>(2.1%)</td>\n",
    "<td>1,811,742<br/>(75.5%)</td>\n",
    "<td>2,399,392</td>\n",
    "<td class=\"example\">Abgeordnete (<span class=\"translation\">representative</span>), Regierung (<span class=\"translation\">government</span>)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Papal Conclave</td>\n",
    "<td>7,859<br/>(15.11%)</td>\n",
    "<td>1,260<br/>(2.42%)</td>\n",
    "<td>42,879<br/>(82.46%)</td>\n",
    "<td>51,998</td>\n",
    "<td class=\"example\">Papst (<span class=\"translation\">pope</span>), Pabst (<span class=\"translation\">pobe</span>)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Political Discussions</td>\n",
    "<td>10,552<br/>(25.8%)</td>\n",
    "<td>777<br/>(1.9%)</td>\n",
    "<td>29,555<br/>(72.29%)</td>\n",
    "<td>40,884</td>\n",
    "<td class=\"example\">Politik (<span class=\"translation\">politics</span>), Minister (<span class=\"translation\">minister</span>)</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>General Conversations</td>\n",
    "<td>3,201,847<br/>(18.7%)</td>\n",
    "<td>813,478<br/>(4.7%)</td>\n",
    "<td>13,088,008<br/>(76.5%)</td>\n",
    "<td>17,103,333</td>\n",
    "<td class=\"example\">den (<span class=\"translation\">the</span>), sie (<span class=\"translation\">she</span>)</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 id=\"annotation-scheme-title\"> Annotation Scheme</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **sentiments**, which were defined as *polar subjective evaluative opinions about people, entities, or events*, e.g.:\n",
    "    <div class=\"fade-out-on-next-fragment\">\n",
    "    <div class=\"example\">\n",
    "         <div class=\"title\">Example 2.4</div>\n",
    "         <div class=\"tweet\"><span class=\"label\">Tweet:</span><span class=\"sentiment\">Mir hat die letzte Folge von Games of Thrones gar nicht gefallen.</span><span class=\"tag\">sentiment</span></div>\n",
    "         <div class=\"translation\"><span class=\"sentiment\">I absolutely didn't like the last episode of Game of Thrones.</span><span class=\"tag\">sentiment</span></div>\n",
    "    </div>\n",
    "    <p>\n",
    "      <strong>Attributes</strong>:\n",
    "    </p>\n",
    "    <ul>\n",
    "    <li><strong>polarity</strong>: <em>positive</em>, <em>negative</em>, and <em>comparative</em>;</li>\n",
    "    <li><strong>intensity</strong>: <em>weak</em>, <em>medium</em>, and <em>strong</em>;</li>\n",
    "    <li><strong>sarcasm</strong>: <em>true</em>, <em>false</em>.</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **targets**, which are *entities or events evaluated by opinions*.\n",
    "    <div class=\"fade-out-on-next-fragment\">\n",
    "      <div class=\"example\">\n",
    "          <div class=\"title\">Example 2.5</div>\n",
    "          <div class=\"tweet\"><span class=\"label\">Tweet:</span>Mir hat <span class=\"target\">die letzte Folge von Games of Thrones</span><span class=\"tag\">target</span> gar nicht gefallen.</div>\n",
    "          <div class=\"translation\">I absolutely didn't like <span class=\"target\">the last episode of Game of Thrones</span><span class=\"tag\">target</span>.</div>\n",
    "      </div>\n",
    "    <p>\n",
    "      <strong>Attributes</strong>:\n",
    "    </p>\n",
    "    <ul>\n",
    "        <li><strong>preferred</strong>: <em>true</em>, <em>false</em>;</li>\n",
    "        <li><strong>anaph-ref</strong>: &xrarr;;</li>\n",
    "        <li><strong>sentiment-ref</strong>: &xrarr;.</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **sources**, which mark the immediate *authors of the opinions*.\n",
    "    <div class=\"fade-out-on-next-fragment\">\n",
    "      <div class=\"example\">\n",
    "          <div class=\"title\">Example 2.6</div>\n",
    "          <div class=\"tweet\"><span class=\"label\">Tweet:</span><span class=\"source\">Mir</span><span class=\"tag\">source</span> hat die letzte Folge von Games of Thrones gar nicht gefallen.</div>\n",
    "          <div class=\"translation\"><span class=\"source\">I</span><span class=\"tag\">source</span> absolutely didn't like the last episode of Game of Thrones.</div>\n",
    "      </div>\n",
    "    <p>\n",
    "      <strong>Attributes</strong>:\n",
    "    </p>\n",
    "    <ul>\n",
    "      <li><strong>sentiment-ref</strong>: &xrarr;.</li>\n",
    "    </ul>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In addition to the three core opinion-level elements (sentiments, sources, and targets), I also defined a set of word-level items that had to be labeled by the annotators and were supposed to ease automatic sentiment analysis.  These were:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **polar terms**, which were defined as *words or idioms that have a distinguishable evaluative lexical meaning* (e.g., \"ekelhaft\" *disgusting*, \"lieben\" *to love*, \"Held\" *hero*, \"wie die Pest meiden\" *to avoid like the pest*).\n",
    "   <div class=\"fade-out-on-next-fragment\">\n",
    "   <div class=\"example\">\n",
    "       <div class=\"title\">Example 2.7</div>\n",
    "       <div class=\"tweet\"><span class=\"label\">Tweet:</span>Mir hat die letzte Folge von Games of Thrones gar nicht <span class=\"polar-term\">gefallen</span><span class=\"tag\">polar-term</span>.</div>\n",
    "       <div class=\"translation\">I absolutely didn't <span class=\"polar-term\">like</span><span class=\"tag\">polar-term</span> the last episode of Game of Thrones.</div>\n",
    "   </div>\n",
    "    <p>\n",
    "      <strong>Attributes</strong>:\n",
    "    </p>\n",
    "    <ul>\n",
    "    <li><strong>polarity</strong>: <em>positive</em>, <em>negative</em>;</li>\n",
    "    <li><strong>intensity</strong>: <em>weak</em>, <em>medium</em>, <em>strong</em>;</li>\n",
    "    <li><strong>sarcasm</strong>: <em>true</em>, <em>false</em>;</li>\n",
    "    <li><strong>subjective-fact</strong>: <em>true</em>, <em>false</em>;</li>\n",
    "    <li><strong>uncertain</strong>: <em>true</em>, <em>false</em>;</li>\n",
    "    <li><strong>sentiment-ref</strong>: &xrarr;.</li>\n",
    "    </ul>\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **intensifiers** and **diminishers** are *elements that increase (decrease) the expressivity and subjective sense of a polar term* (e.g., \"sehr\" *very*, \"super\" *super*, \"stark\" *strongly*, \"kaum\" *hardly*);\n",
    "  <div class=\"fade-out-on-next-fragment\">\n",
    "   <div class=\"example\">\n",
    "       <div class=\"title\">Example 2.8</div>\n",
    "       <div class=\"tweet\"><span class=\"label\">Tweet:</span>Mir hat die letzte Folge von Games of Thrones <span class=\"intensifier\">gar</span><span class=\"tag\">intensifier</span> nicht gefallen.</div>\n",
    "       <div class=\"translation\">I <span class=\"intensifier\">absolutely</span><span class=\"tag\">intensifier</span> didn't like the last episode of Game of Thrones.</div>\n",
    "   </div>\n",
    "    <p>\n",
    "      <strong>Attributes</strong>:\n",
    "    </p>\n",
    "    <ul>\n",
    "    <li><strong>degree</strong>: <em>medium</em>, <em>strong</em>;</li>\n",
    "    <li><strong>polar-term-ref</strong>: &xrarr;.</li>\n",
    "    </ul>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* and, finally, **negations**, which are grammatical or lexical means that reverse the semantic orientation of a polar term. (e.g., \"nicht\" *not*).\n",
    "  <div class=\"hide-on-next-fragment\">\n",
    "   <div class=\"example\">\n",
    "   <div class=\"title\">Example 2.9</div>\n",
    "   <div class=\"tweet\"><span class=\"tag\">Tweet:</span>Mir hat die letzte Folge von Games of Thrones gar <span class=\"negation\">nicht</span><span class=\"tag\">negation</span> gefallen.</div>\n",
    "   <div class=\"translation\">I absolutely did<span class=\"negation\">n't</span><span class=\"tag\">negation</span> like the last episode of Game of Thrones.</div>\n",
    "   </div>\n",
    "    <p>\n",
    "      <strong>Attributes</strong>:\n",
    "    </p>\n",
    "    <ul>\n",
    "  <li><strong>polar-term-ref</strong>: &xrarr;.</li>\n",
    "    </ul>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Annotation Scheme\n",
    "\n",
    "<div class=\"example\">\n",
    "  <div class=\"title\">Example 2.10</div>\n",
    "  <div class=\"tweet\"><span class=\"label\">Tweet:</span><span class=\"sentiment\"><span class=\"source\">Mir</span><span class=\"tag\">source</span> hat <span class=\"target\">die letzte Folge von Games of Thrones</span><span class=\"tag\">target</span> <span class=\"intensifier\">gar</span><span class=\"tag\">intensifier</span> <span class=\"negation\">nicht</span><span class=\"tag\">negation</span> <span class=\"polar-term\">gefallen</span><span class=\"tag\">polar-term</span>.</span><span class=\"tag\">sentiment</span></div>\n",
    "  <div class=\"translation\"><span class=\"sentiment\"><span class=\"source\">I</span><span class=\"tag\">source</span> <span class=\"intensifier\">absolutely</span><span class=\"tag\">intensifier</span> did<span class=\"negation\">n't</span><span class=\"tag\">negation</span> <span class=\"polar-term\">like</span><span class=\"tag\">polar-term</span> <span class=\"target\">the last episode of Game of Thrones</span><span class=\"tag\">target</span>.</span><span class=\"tag\">sentiment</span></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Inter-Annotator Agreement\n",
    "\n",
    "For estimating the inter-annotator agreement, I adopted the popular $\\kappa$ metric (Cohen, 1960):\n",
    "\n",
    "$$\\kappa = \\frac{p_o - p_c}{1 - p_c},$$\n",
    "\n",
    "where $p_o$ denotes the **observed agreement** and $p_c$ is the **agreement by chance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **observed agreement** is normally estimated as:\n",
    "$$p_o = \\frac{T âˆ’ A_1 + M_1 âˆ’ A_2 + M_2}{T},$$\n",
    "where $T$ means the total number of tokens; $A_1$ and $A_2$ are the numbers of tokens annotated by the first and second annotator respectively; and $M_1$ and $M_2$ denote the numbers of tokens with matching annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The **agreement by chance** is computed as:\n",
    "$$p_c = c_1\\times c_2 + (1 - c_1)\\times(1 - c_2),$$\n",
    "where c 1 and c 2 are the proportions of tokens annotated with the given class in the first and\n",
    "second annotation respectively, i.e., $c_1 = \\frac{A_1}{T}$ and $c_2 = \\frac{A_2}{T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"example\">\n",
    "    <div class=\"title\">Example 2.10</div>\n",
    "    <div class=\"annotator1\">Annotation 1:</div>\n",
    "    <div class=\"tweet\"><span class=\"label\">Tweet:</span><span class=\"sentiment\">Kim Kardashian hasst <span class=\"sentiment\">ihre schÃ¶ne Schwester</span><span class=\"tag\">sentiment</span> .</span><span class=\"tag\">sentiment</span></div>\n",
    "    <div class=\"translation\"><span class=\"sentiment\">Kim Kardashian hates <span class=\"sentiment\">her nice sister</span><span class=\"tag\">sentiment</span> .</span><span class=\"tag\">sentiment</span></div>\n",
    "    <div class=\"annotator2\">Annotation 2:</div>\n",
    "    <div class=\"tweet\"><span class=\"label\">Tweet:</span>Kim <span class=\"sentiment\">Kardashian hasst <span class=\"sentiment\">her nice sister</span><span class=\"tag\">sentiment</span> .</span><span class=\"tag\">sentiment</span></div>\n",
    "    <div class=\"translation\">Kim <span class=\"sentiment\">Kardashian hates <span class=\"sentiment\">her nice sister</span><span class=\"tag\">sentiment</span> .</span><span class=\"tag\">sentiment</span></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"row\">\n",
    "<div class=\"cell left\">\n",
    "<strong>Binary $\\kappa$</strong>\n",
    "<p>\n",
    "A$_1$ = $10$\n",
    "</p>\n",
    "<p>\n",
    "A$_2$ = $9$\n",
    "</p>\n",
    "<p>\n",
    "M$_1$ = A$_1$\n",
    "</p>\n",
    "<p>\n",
    "M$_2$ = A$_2$\n",
    "</p>\n",
    "<p>\n",
    "$\\kappa = 1$\n",
    "</p>\n",
    "</div>\n",
    "<div class=\"cell right\">\n",
    "<strong>Proportional $\\kappa$</strong>\n",
    "<p>\n",
    "A$_1 = 7$\n",
    "</p>\n",
    "<p>\n",
    "A$_2 = 6$\n",
    "</p>\n",
    "<p>\n",
    "M$_1 = 6$\n",
    "</p>\n",
    "<p>\n",
    "M$_2 = 6$\n",
    "</p>\n",
    "<p>\n",
    "$\\kappa$ = $0$\n",
    "</p>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To address these issues, we introduced two different agreement metrics---binary and proportional kappa.  With the former variant, we counted tokens belonging to overlapping annotation spans of the same class multiple times (\\ie{} $A_1$ and $A_2$ would amount to $10$ and $9$ respectively in the above tweet) and considered all tokens belonging to the given annotated element as matching if this span agreed with the annotation from the other expert on at least one token (\\ie{} $M_1$ and $M_2$ would have the same values as $A_1$ and $A_2$ in this case).  With the latter metric, every labeled token was counted only once (\\ie{} the numbers of labeled words in the first and second annotations would be $7$ and $6$ respectively), and we only calculated the actual number of tokens with matching labels when computing the $M$ scores (\\ie{} both $M_1$ and $M_2$ would be equal to $6$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Stage I: Initial Annotation\n",
    "\n",
    "<table>\n",
    "   <caption>Inter-annotator agreement after the initial annotation stage</caption>\n",
    "   <thead>\n",
    "      <tr>\n",
    "    <td rowspan=2>Element</td>\n",
    "    <td colspan=5>Binary $\\kappa$</td>\n",
    "    <td colspan=5>Proportional $\\kappa$</td>\n",
    "          </tr>\n",
    "<tr>\n",
    "<td>M$_1$ </td>\n",
    "<td>A$_1$ </td>\n",
    "<td>M$_2$ </td>\n",
    "<td>A$_2$ </td>\n",
    "<td>$\\kappa$</td>\n",
    "<td>M$_1$</td>\n",
    "<td>A$_1$</td>\n",
    "<td>M$_2$</td>\n",
    "<td>A$_2$</td>\n",
    "<td>$\\kappa$</td>\n",
    "</tr>\n",
    "  </thead>\n",
    "<tbody>\n",
    "    <tr>\n",
    "<td>Sentiment</td>\n",
    "<td>4,215</td>\n",
    "<td>7,070</td>\n",
    "<td>3,484</td>\n",
    "<td>9,827</td>\n",
    "<td>38.05</td>\n",
    "<td>3,269</td>\n",
    "<td>6,812</td>\n",
    "<td>3,269</td>\n",
    "<td>9,796</td>\n",
    "<td>31.21</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>Target</td>\n",
    "<td>1,103</td>\n",
    "<td>1,943</td>\n",
    "<td>1,217</td>\n",
    "<td>4,162</td>\n",
    "<td>35.48</td>\n",
    "<td>898</td>\n",
    "<td>1,905</td>\n",
    "<td>898</td>\n",
    "<td>4,148</td>\n",
    "<td>26.85</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>Source</td>\n",
    "<td>159</td>\n",
    "<td>445</td>\n",
    "<td>156</td>\n",
    "<td>456</td>\n",
    "<td>34.53</td>\n",
    "<td>153</td>\n",
    "<td>439</td>\n",
    "<td>153</td>\n",
    "<td>456</td>\n",
    "<td>33.75</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>Polar Term</td>\n",
    "<td>1,951</td>\n",
    "<td>2,854</td>\n",
    "<td>2,029</td>\n",
    "<td>3,188</td>\n",
    "<td>64.29</td>\n",
    "<td>1,902</td>\n",
    "<td>2,851</td>\n",
    "<td>1,902</td>\n",
    "<td>3,180</td>\n",
    "<td>61.36</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>Intensifier</td>\n",
    "<td>57</td>\n",
    "<td>101</td>\n",
    "<td>59</td>\n",
    "<td>123</td>\n",
    "<td>51.71</td>\n",
    "<td>57</td>\n",
    "<td>101</td>\n",
    "<td>57</td>\n",
    "<td>123</td>\n",
    "<td>50.81</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>Diminisher</td>\n",
    "<td>3</td>\n",
    "<td>10</td>\n",
    "<td>3</td>\n",
    "<td>8</td>\n",
    "<td>33.32</td>\n",
    "<td>3</td>\n",
    "<td>10</td>\n",
    "<td>3</td>\n",
    "<td>8</td>\n",
    "<td>33.32</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>Negation</td>\n",
    "<td>21</td>\n",
    "<td>63</td>\n",
    "<td>21</td>\n",
    "<td>83</td>\n",
    "<td>28.69</td>\n",
    "<td>21</td>\n",
    "<td>63</td>\n",
    "<td>21</td>\n",
    "<td>83</td>\n",
    "<td>28.69</td>\n",
    "    </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Stage II: Adjudication Step\n",
    "\n",
    "<table>\n",
    "   <caption>Inter-annotator agreement after the adjudication step</caption>\n",
    "  <thead>\n",
    "      <tr>\n",
    "    <td rowspan=2>Element</td>\n",
    "    <td colspan=5>Binary $\\kappa$</td>\n",
    "    <td colspan=5>Proportional $\\kappa$</td>\n",
    "          </tr>\n",
    "<tr>\n",
    "<td>M$_1$ </td>\n",
    "<td>A$_1$ </td>\n",
    "<td>M$_2$ </td>\n",
    "<td>A$_2$ </td>\n",
    "<td>$\\kappa$</td>\n",
    "<td>M$_1$</td>\n",
    "<td>A$_1$</td>\n",
    "<td>M$_2$</td>\n",
    "<td>A$_2$</td>\n",
    "<td>$\\kappa$</td>\n",
    "</tr>\n",
    "  </thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>8,198</td>\n",
    "<td>8,530</td>\n",
    "<td>8,260</td>\n",
    "<td>14,034</td>\n",
    "<td>67.92</td>\n",
    "<td>7,435</td>\n",
    "<td>8,243</td>\n",
    "<td>7,435</td>\n",
    "<td>13,714</td>\n",
    "<td>61.94</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Target</td>\n",
    "<td>3,088</td>\n",
    "<td>3,407</td>\n",
    "<td>2,814</td>\n",
    "<td>5,303</td>\n",
    "<td>65.66</td>\n",
    "<td>2,554</td>\n",
    "<td>3,326</td>\n",
    "<td>2,554</td>\n",
    "<td>5,212</td>\n",
    "<td>57.27</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Source</td>\n",
    "<td>573</td>\n",
    "<td>690</td>\n",
    "<td>545</td>\n",
    "<td>837</td>\n",
    "<td>72.91</td>\n",
    "<td>539</td>\n",
    "<td>676</td>\n",
    "<td>539</td>\n",
    "<td>833</td>\n",
    "<td>71.12</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Polar Term</td>\n",
    "<td>3,164</td>\n",
    "<td>3,298</td>\n",
    "<td>3,261</td>\n",
    "<td>4,134</td>\n",
    "<td>85.68</td>\n",
    "<td>3,097</td>\n",
    "<td>3,290</td>\n",
    "<td>3,097</td>\n",
    "<td>4,121</td>\n",
    "<td>82.64</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Intensifier</td>\n",
    "<td>111</td>\n",
    "<td>219</td>\n",
    "<td>113</td>\n",
    "<td>180</td>\n",
    "<td>56.01</td>\n",
    "<td>111</td>\n",
    "<td>219</td>\n",
    "<td>111</td>\n",
    "<td>180</td>\n",
    "<td>55.51</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Diminisher</td>\n",
    "<td>9</td>\n",
    "<td>16</td>\n",
    "<td>10</td>\n",
    "<td>16</td>\n",
    "<td>59.37</td>\n",
    "<td>9</td>\n",
    "<td>16</td>\n",
    "<td>9</td>\n",
    "<td>15</td>\n",
    "<td>58.05</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Negation</td>\n",
    "<td>68</td>\n",
    "<td>84</td>\n",
    "<td>67</td>\n",
    "<td>140</td>\n",
    "<td>60.21</td>\n",
    "<td>67</td>\n",
    "<td>83</td>\n",
    "<td>67</td>\n",
    "<td>140</td>\n",
    "<td>60.03</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Stage III: Final Annotation\n",
    "\n",
    "<table>\n",
    "   <caption>Inter-annotator agreement of the final corpus</caption>\n",
    "  <thead>\n",
    "      <tr>\n",
    "    <td rowspan=2>Element</td>\n",
    "    <td colspan=5>Binary $\\kappa$</td>\n",
    "    <td colspan=5>Proportional $\\kappa$</td>\n",
    "          </tr>\n",
    "<tr>\n",
    "<td>M$_1$ </td>\n",
    "<td>A$_1$ </td>\n",
    "<td>M$_2$ </td>\n",
    "<td>A$_2$ </td>\n",
    "<td>$\\kappa$</td>\n",
    "<td>M$_1$</td>\n",
    "<td>A$_1$</td>\n",
    "<td>M$_2$</td>\n",
    "<td>A$_2$</td>\n",
    "<td>$\\kappa$</td>\n",
    "</tr>\n",
    "  </thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>14,748</td>\n",
    "<td>15,929</td>\n",
    "<td>14,969</td>\n",
    "<td>26,047</td>\n",
    "<td>65.03</td>\n",
    "<td>13,316</td>\n",
    "<td>15,375</td>\n",
    "<td>13,316</td>\n",
    "<td>25,352</td>\n",
    "<td>58.82</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Target</td>\n",
    "<td>5,765</td>\n",
    "<td>6,629</td>\n",
    "<td>5,292</td>\n",
    "<td>9,852</td>\n",
    "<td>64.76</td>\n",
    "<td>4,789</td>\n",
    "<td>6,462</td>\n",
    "<td>4,789</td>\n",
    "<td>9,659</td>\n",
    "<td>56.61</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Source</td>\n",
    "<td>966</td>\n",
    "<td>1,207</td>\n",
    "<td>910</td>\n",
    "<td>1,619</td>\n",
    "<td>65.99</td>\n",
    "<td>898</td>\n",
    "<td>1,180</td>\n",
    "<td>898</td>\n",
    "<td>1,604</td>\n",
    "<td>64.1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Polar Term</td>\n",
    "<td>5,574</td>\n",
    "<td>5,989</td>\n",
    "<td>5,659</td>\n",
    "<td>7,419</td>\n",
    "<td>82.83</td>\n",
    "<td>5,441</td>\n",
    "<td>5,977</td>\n",
    "<td>5,441</td>\n",
    "<td>7,395</td>\n",
    "<td>80.29</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Intensifier</td>\n",
    "<td>192</td>\n",
    "<td>432</td>\n",
    "<td>194</td>\n",
    "<td>338</td>\n",
    "<td>49.97</td>\n",
    "<td>192</td>\n",
    "<td>432</td>\n",
    "<td>192</td>\n",
    "<td>338</td>\n",
    "<td>49.71</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Diminisher</td>\n",
    "<td>16</td>\n",
    "<td>30</td>\n",
    "<td>17</td>\n",
    "<td>34</td>\n",
    "<td>51.55</td>\n",
    "<td>16</td>\n",
    "<td>30</td>\n",
    "<td>16</td>\n",
    "<td>33</td>\n",
    "<td>50.78</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Negation</td>\n",
    "<td>111</td>\n",
    "<td>132</td>\n",
    "<td>110</td>\n",
    "<td>243</td>\n",
    "<td>58.87</td>\n",
    "<td>110</td>\n",
    "<td>131</td>\n",
    "<td>110</td>\n",
    "<td>242</td>\n",
    "<td>58.92</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Inter-Annotator Agreement on Attributes\n",
    "\n",
    "<table class=\"narrow\">\n",
    "    <caption>Inter-annotator agreement on polarity and intensity of sentiments and polar terms</caption>\n",
    "    <thead>\n",
    "    <tr>\n",
    "      <td>Element</td>\n",
    "      <td>Polarity $\\kappa$</td>\n",
    "      <td>Intensity $\\alpha$</td>\n",
    "    </tr>\n",
    "  </thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>58.8 </td>\n",
    "<td>73.54</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Polar Term</td>\n",
    "<td>87.12</td>\n",
    "<td>78.79</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Qualitative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "<div class=\"example\">\n",
    "  <div class=\"title\">Example 2.11</div>\n",
    "  <div class=\"annotator1\">Annotation 1:</div>\n",
    "  <div class=\"tweet\"><span class=\"label\">Tweet:</span>@TinaPannes immerhin ist die #afd nicht dabei ðŸ™‚</div>\n",
    "  <div class=\"translation\">@TinaPannes anyway the #afd is not there <span style=\"font-style:normal;\">ðŸ™‚</span></div>\n",
    "\n",
    "  <div class=\"annotator2\">Annotation 2:</div>\n",
    "  <div class=\"tweet\"><span class=\"label\">Tweet:</span>@TinaPannes <span class=\"sentiment\"><span class=\"target\">immerhin ist die #afd nicht dabei</span><span class=\"tag\">target</span> ðŸ™‚</span><span class=\"tag\">sentiment</span></div>\n",
    "  <div class=\"translation\">@TinaPannes <span class=\"sentiment\"><span class=\"target\">anyway the #afd is not there</span><span class=\"tag\">target</span> <span style=\"font-style:normal;\">ðŸ™‚</span></span><span class=\"tag\">sentiment</span></div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "<div class=\"example\">\n",
    "<div class=\"title\">Example 2.12</div>\n",
    "<div class=\"annotator1\">Annotation 1:</div>\n",
    "<div class=\"tweet\"><span class=\"label\">Tweet:</span><span class=\"sentiment\">Koalition wirft der SPD <span class=\"target\">Blockadehaltung</span><span class=\"tag\">target</span> vor</span><span class=\"tag\">sentiment</span></div>\n",
    "<div class=\"translation\"><span class=\"sentiment\">Coalition accuses the SPD of <span class=\"target\">blocking politics</span><span class=\"tag\">target</span></span><span class=\"tag\">sentiment</span></div>\n",
    "\n",
    "<div class=\"annotator2\">Annotation 2:</div>\n",
    "<div class=\"tweet\"><span class=\"label\">Tweet:</span><span class=\"sentiment\">Koalition wirft <span class=\"target\">der SPD</span><span class=\"tag\">target</span> target Blockadehaltung vor</span><span class=\"tag\">sentiment</span></div>\n",
    "<div class=\"translation\"><span class=\"sentiment\">Coalition accuses <span class=\"target\">the SPD</span><span class=\"tag\">target</span> of blocking politics</span><span class=\"tag\">sentiment</span></div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"example\">\n",
    "<div class=\"title\">Example 2.13</div>\n",
    "<div class=\"annotator1\">Annotation 1:</div>\n",
    "<div class=\"tweet\"><span class=\"label\">Tweet:</span>Syrien vor dem Angriff&mdash;bringen diese Bomben den Frieden?</div>\n",
    "<div class=\"translation\">Syria facing an attack&mdash;will these bombs bring peace?</div>\n",
    "\n",
    "<div class=\"annotator2\">Annotation 2:</div>\n",
    "<div class=\"tweet\"><span class=\"label\">Tweet:</span>Syrien vor dem <span class=\"polar-term\">Angriff</span><span class=\"tag\">polar-term</span>&mdash;bringen diese <span class=\"polar-term\">Bomben</span><span class=\"tag\">polar-term</span> polar-term den <span class=\"polar-term\">Frieden</span><span class=\"tag\">polar-term</span> ?</div>\n",
    "<div class=\"translation\">Syria facing an <span class=\"polar-term\">attack</span><span class=\"tag\">polar-term</span>&mdash;will these <span class=\"polar-term\">bombs</span><span class=\"tag\">polar-term</span> bring <span class=\"polar-term\">peace</span><span class=\"tag\">polar-term</span> ?</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Effect of the Selection Criteria\n",
    "\n",
    "<figure>\n",
    "    <figure>\n",
    "        <img src=\"img/sentiment_stat.png\" class=\"heatmap\">\n",
    "        <figcaption>\n",
    "           a) Sentiments\n",
    "        </figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"img/emo-expression_stat.png\" class=\"heatmap\"/>\n",
    "        <figcaption>\n",
    "            b) Polar Terms\n",
    "        </figcaption>\n",
    "    </figure>\n",
    "    <figcaption>\n",
    "        Distribution of sentiments and polar terms across topics and formal groups\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Effect of the Selection Criteria\n",
    "\n",
    "<figure>\n",
    "    <figure>\n",
    "        <img src=\"img/sentiment_agreement.png\" class=\"heatmap\"/>\n",
    "        <figcaption>\n",
    "           a) Sentiments\n",
    "        </figcaption>\n",
    "    </figure>\n",
    "    <figure>\n",
    "        <img src=\"img/emo-expression_agreement.png\" class=\"heatmap\"/>\n",
    "        <figcaption>\n",
    "           b) Polar Terms\n",
    "        </figcaption>\n",
    "    </figure>\n",
    "    <figcaption>\n",
    "        Inter-annotator agreement on sentiments and polar terms across topics and formal groups\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Effect of the Selection Criteria\n",
    "\n",
    "<table>\n",
    "<caption>Correlation coefficients of topics and formal selection criteria with the number and\n",
    "agreement scores of sentiments and polar terms</caption>\n",
    "    <thead>\n",
    "<tr>\n",
    "<td rowspan=\"3\">Selection Criteria</td>\n",
    "<td colspan=\"4\">Correlation Coefficients</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"2\">Sentiments</td>\n",
    "<td colspan=\"2\">Polar Terms</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> # of Elements </td>\n",
    "<td>agreement</td>\n",
    "<td> # of elements </td>\n",
    "<td>agreement</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Federal Elections</td>\n",
    "<td>0.312</td>\n",
    "<td>0.169</td>\n",
    "<td>0.356</td>\n",
    "<td>0.289</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Papal Conclave</td>\n",
    "<td>0.149</td>\n",
    "<td>0.124</td>\n",
    "<td>0.182</td>\n",
    "<td>0.264</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Political Discussions</td>\n",
    "<td>0.195</td>\n",
    "<td>0.148</td>\n",
    "<td>0.218</td>\n",
    "<td>0.244</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>General Conversations</td>\n",
    "<td>0.183</td>\n",
    "<td>0.19</td>\n",
    "<td>0.372</td>\n",
    "<td>0.452</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Polar Terms</td>\n",
    "<td>0.445</td>\n",
    "<td>0.352</td>\n",
    "<td>0.38</td>\n",
    "<td>0.301</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Emoticons</td>\n",
    "<td>0.127</td>\n",
    "<td>0.096</td>\n",
    "<td>0.47</td>\n",
    "<td>0.615</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Random</td>\n",
    "<td>0.216</td>\n",
    "<td>0.134</td>\n",
    "<td>0.143</td>\n",
    "<td>0.138</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* I have presented a corpus of **7,992 German tweets**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The microblogs in this collection were sampled from **four different topics**, based on **three formal categories**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Two human experts** annotated these messages with **sentiments**, **sources**, **targets**, **polar terms**, their **inensifiers**, **diminishers**, and **negations**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* At the beginning, the mutual **inter-annotator agreement** of the experts could hardly reach **35%** (low IAA);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* But it could be significantly improved by applying an **adjudication step**, which increased the IAA score to $\\approx$ **59%**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Last but not least, we could see a significant **correlation between the initial selection criteria and the number and reliability of annotated opinions and their elements**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 class=\"chapter-title animate\">Chapter III: Lexicons</h1>\n",
    "<img src=\"img/lexicon.png\" alt=\"Sentiment Lexicon\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<pre>\n",
    "abandoned       negative        -1\n",
    "abandonment     negative        -1\n",
    "abandon negative        -1\n",
    "abase   negative        -2\n",
    "abasement       negative        -2\n",
    "abash   negative        -2\n",
    "abate   negative        -1\n",
    "abdicate        negative        -1\n",
    "aberration      negative        -2\n",
    "abhor   negative        -2\n",
    "abhorred        negative        -2\n",
    "abhorrence      negative        -2\n",
    "abhorrent       negative        -2\n",
    "abhorrently     negative        -2\n",
    "abhors  negative        -2\n",
    "abidance        positive        2\n",
    "abide   positive        2\n",
    "...\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Lexicon Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **manual**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **semi-automatic**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **automatic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **intrinsic**;\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "<div class=\"row\" id=\"intrinsic-lex-evaluation\">\n",
    "<div class=\"cell left\">\n",
    "<pre>\n",
    "abandoned       negative        -1\n",
    "abandonment     negative        -1\n",
    "abandon negative        -1\n",
    "abase   negative        -2\n",
    "abasement       negative        -2\n",
    "abash   negative        -2\n",
    "abate   negative        -1\n",
    "abdicate        negative        -1\n",
    "aberration      negative        -2\n",
    "abhor   negative        -2\n",
    "abhorred        negative        -2\n",
    "abhorrence      negative        -2\n",
    "abhorrent       negative        -2\n",
    "abhorrently     negative        -2\n",
    "abhors  negative        -2\n",
    "abidance        positive        2\n",
    "abidance        positive        2\n",
    "abide   positive        2\n",
    "...\n",
    "</pre></div>\n",
    "<div class=\"cell right\">\n",
    "<pre>\n",
    "#inaperfectworld  negative   -1.753927\n",
    "unfortunate ,     positive    1.729423\n",
    "nothing wrong     positive    1.680036\n",
    "#dontyouhate      negative   -1.638145\n",
    "wont hurt         positive    1.632656\n",
    "not scared        positive    1.602032\n",
    "miss dat          positive    1.600405\n",
    "no shame          positive    1.584831\n",
    "bad either        positive    1.581659\n",
    "disappointed but  positive    1.570227\n",
    "no problem        positive    1.559862\n",
    "not worried       positive    1.554723\n",
    "no hangover       positive    1.508815\n",
    "mess me           positive    1.508504\n",
    "t_t but           positive    1.476175\n",
    "people oh         positive    1.471193\n",
    "doesnt hurt       positive    1.450521\n",
    "abash             negative   -1.384156\n",
    "...\n",
    "</pre>\n",
    "</div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **extrinsic**.\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "<img src=\"img/ml.jpg\" alt=\"ML Classifier\" id=\"extrinsic-lex-evaluation\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Corpus-Based Intrinsic Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For each token in the corpus:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If the token is **present in the lexicon** and is **annotated in the corpus as a polar term with the same polarity**, consider it as **true positive (TP)**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* If the token is **present in the lexicon** but **not annotated in the corpus as a polar term or has a different polarity**, consider it as **false positive (FP)**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Finally, if the token is **absent in the lexicon** but is **annotated as a polar term**, consider it as **false negative (FN)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With these metrics, we can then compute **precision (P)**, **recall (R)**, and **F$_1$-score**:\n",
    "$$P = \\frac{TP}{TP + FP}$$\n",
    "$$R = \\frac{TP}{TP + FN}$$\n",
    "$$F_1 = 2 \\times \\frac{P \\times R}{P + R}$$\n",
    "for each polarity class (*positive*, *negative*, *neutral*).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Corpus-Based Intrinsic Evaluation\n",
    "\n",
    "Once we know precision, recall, and F$_1$ for each of the classes, we can estimate the **macro- and micro-averaged F$_1$-scores** for all three polarities:\n",
    "\n",
    "$$\\mathrm{macro-}F_1 = \\frac{F^+_1 + F^-_1 + F^0_1}{3};$$\n",
    "\n",
    "$$\\mathrm{micro-}F_1 = 2 \\times \\frac{P^* \\times R^*}{P^* + R^*};$$\n",
    "\n",
    "where $P^*$ is the **micro-averaged precision**:\n",
    "\n",
    "$$P^* = \\frac{TP^+ + TP^- + TP^0}{TP^+ + FP^+ + TP^- + FP^- + TP^0 + FP^0},$$\n",
    "\n",
    "and $R^*$ is the **micro-averaged recall**:\n",
    "\n",
    "$$R^* = \\frac{TP^+ + TP^- + TP^0}{TP^+ + FN^+ + TP^- + FN^- + TP^0 + FN^0}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Data\n",
    "\n",
    "I used **400 randomly selected tweets** annotated by the first expert as **development set** for tweaking the parameters and determining the optimal size of automatic lexicons, and utilized the **whole corpus** annotated by the other linguist for **testing** (3,459 positive and 2,755 negative labeled term; 1,738 and 1,943 unique expressions respectively)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Semi-Automatic Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **German Polarity Clues** (Waltinger, 2010);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**10,141** polar terms\n",
    "\n",
    "These terms were automatically obtained from the English sentiment lexicons Subjectivity Clues (Wilson et al., 2005) and SentiSpin (Takamura et al., 2005) that were automatically translated into German and then manually revised by the author. Apart from that, Waltinger also manually en riched these translations with their frequent synonyms and 290 negated phrases; 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **SentiWS** (Remus et al., 2010);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " **1,818** positive and **1,650** negative entries (**32,734** word forms);\n",
    "\n",
    "As in the previous case, the authors obtained the initial entries for their resource by translating an English polarity list (the General Inquirer lexicon) and then manually correcting these translations. In addition to this, they expanded the translated set with words and phrases that frequently co-occurred with positive and negative seed terms in a corpus of 10,200 customer reviews or in the\n",
    "German Collocation Dictionary (Quasthoff, 2010);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* the **Zurich Polarity List** (Clematide and Klenner, 2010)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**8,000** subjective terms\n",
    "\n",
    "These terms were extracted from **GermaNet** synsets (Hamp and Feldweg, 1997) and manually annotated by human experts with their prior polarities. Since the authors, however, found the number of polar adjectives obtained this way to be insufficient for their classification experiments, they automatically enriched this lexicon with more attributive terms, using the collocation method of Hatzivassiloglou and McKeown (1997)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Results of Semi-Automatic Lexicons\n",
    "\n",
    "<table>\n",
    "    <caption>\n",
    "        Evaluation of semi-automatic German sentiment lexicons<br/>\n",
    "        <em>(GPC â€” German Polarity Clues, SWS â€” SentiWS, ZPL â€” Zurich Polarity List)</em>\n",
    "    </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\" class=\"lexicon\">Lexicon</td>\n",
    "<td colspan=\"3\">Positive Expressions</td>\n",
    "<td colspan=\"3\">Negative Expressions</td>\n",
    "<td colspan=\"3\">Neutral Terms</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1$</td>\n",
    "<td rowspan=\"2\">Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tr>\n",
    "<td>GPC</td>\n",
    "<td>0.209</td>\n",
    "<td>0.535</td>\n",
    "<td>0.301</td>\n",
    "<td>0.195</td>\n",
    "<td>0.466</td>\n",
    "<td>0.275</td>\n",
    "<td>0.983</td>\n",
    "<td>0.923</td>\n",
    "<td>0.952</td>\n",
    "<td>0.509</td>\n",
    "<td>0.906 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SWS</td>\n",
    "<td>0.335</td>\n",
    "<td>0.435</td>\n",
    "<td>0.379</td>\n",
    "<td>0.484</td>\n",
    "<td>0.344</td>\n",
    "<td class=\"best\">0.402</td>\n",
    "<td>0.977</td>\n",
    "<td>0.975</td>\n",
    "<td>0.976</td>\n",
    "<td>0.586</td>\n",
    "<td>0.952</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ZPL</td>\n",
    "<td>0.411</td>\n",
    "<td>0.424</td>\n",
    "<td>0.417</td>\n",
    "<td>0.38</td>\n",
    "<td>0.352</td>\n",
    "<td>0.366</td>\n",
    "<td>0.977</td>\n",
    "<td>0.979</td>\n",
    "<td>0.978</td>\n",
    "<td>0.587</td>\n",
    "<td>0.955 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GPC $\\cap$ SWS $\\cap$ ZPL</td>\n",
    "<td class=\"best\">0.527</td>\n",
    "<td>0.372</td>\n",
    "<td class=\"best\">0.436</td>\n",
    "<td class=\"best\">0.618</td>\n",
    "<td>0.244</td>\n",
    "<td>0.35</td>\n",
    "<td>0.973</td>\n",
    "<td class=\"best\">0.99</td>\n",
    "<td class=\"best\">0.982</td>\n",
    "<td class=\"best\">0.589</td>\n",
    "<td class=\"best\">0.964</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GPC $\\cup$ SWS $\\cup$ ZPL</td>\n",
    "<td>0.202</td>\n",
    "<td class=\"best\">0.562</td>\n",
    "<td>0.297</td>\n",
    "<td>0.195</td>\n",
    "<td class=\"best\">0.532</td>\n",
    "<td>0.286</td>\n",
    "<td class=\"best\">0.985</td>\n",
    "<td>0.917</td>\n",
    "<td>0.95</td>\n",
    "<td>0.51</td>\n",
    "<td>0.901 </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Automatic Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **dictionary-based**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **corpus-based**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **word-embedding&ndash;based**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Seed Set \n",
    "\n",
    "<div class=\"seed-set\"><strong>Positive Terms</strong>: <span class=\"translation positive\">good</span>, <span class=\"translation positive\">nice</span>, <span class=\"translation positive\">excellent</span>, <span class=\"translation positive\">positive</span>, <span class=\"translation positive\">fortunate</span>, <span class=\"translation positive\">correct</span>, <span class=\"translation positive\">superior</span>;</div>\n",
    "\n",
    "<div class=\"seed-set\"><strong>Negative Terms</strong>: <span class=\"translation negative\">bad</span>, <span class=\"translation negative\">nasty</span>, <span class=\"translation negative\">poor</span>, <span class=\"translation negative\">negative</span>, <span class=\"translation negative\">unfortunate</span>, <span class=\"translation negative\">wrong</span>, <span class=\"translation negative\">inferior</span>;</div>\n",
    "\n",
    "<div class=\"seed-set\"><strong>Neutral Terms</strong>: <span class=\"translation neutral\">neutral</span>, <span class=\"translation neutral\">objective</span>, <span class=\"translation neutral\">technical</span>, <span class=\"translation neutral\">chemical</span>, <span class=\"translation neutral\">physical</span>, <span class=\"translation neutral\">financial</span>, <span class=\"translation neutral\">theoretical</span>, <span class=\"translation neutral\">practical</span>.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "Turney and Littman (2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Dictionary-Based Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "<img src=\"img/wordnet-strong.png\" alt=\"WordNet Entry for 'strong'\" id=\"wordnet\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "<img src=\"img/wordnet-good.png\" alt=\"WordNet Graph Representation\" id=\"wordnet-graph\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Hu and Liu** (2004);\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "    <em>propagate polarity scores of seed terms to their WordNet synonyms</em>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The authors automatically compiled a list of polar adjectives (which were supposed to be the most relevant part of speech for mining people's opinions)\n",
    "by taking a set of seed terms with known semantic orientations and propagating the polarity scores of these seeds to their WordNet synonyms.  A similar procedure was also applied to\n",
    "antonyms, but the polarity values were reversed in this case.  This expansion continued until no more adjectives could be reached via synonymy-antonymy links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Blair-Goldensohn** et al. (2008);\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "    <img src=\"img/blair-goldensohn.png\" alt=\"SVM Classifier\" id=\"blair-goldensohn\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Blair-Goldensohn refined this approach by considering polarity scores of all \\textsc{WordNet} terms as a single vector $\\vec{v}$; the values of all negative seeds in this vector were set to\n",
    "$-1$, and the scores of all positive seed terms were fixed to $+1$. To derive their polarity list, the authors multiplied $\\vec{v}$ with an adjacency matrix $A$.  Each cell $a_{ij}$ in this matrix was set to $\\lambda=0.2$, if there was a synonymy link between synsets $i$ and $j$, and to $-\\lambda$, if these synsets were antonymous to each other.  By performing this multiplication multiple times and storing the results of the previous iterations in the $\\vec{v}$ vector, the authors ensured that all polarity scores were propagated transitively through the network, decaying by a constant factor ($\\lambda$) as the length of the paths starting from the original seeds increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Kim and Hovy** (2004, 2006);\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "$$P(c|w) = P(c)P(w|c) = P(c)\\frac{\\sum\\limits_{i=1}^{n}count(syn_i, c)}{count(c)},$$\n",
    "where $P(c)$ is the prior probability of the class (estimated as the number of words belonging to class $c$ divided by the total number of words); $count(syn_i, c)$ denotes the   number of times a seed term with polarity $c$ appeared in a synset of $w$; and $count(c)$ means the total number of synsets that contain seeds with this polarity.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Where $P(c)$ is the prior probability of the class (estimated as the number of words belonging to class $c$ divided by the total number of words); $count(syn_i, c)$ denotes the   number of times a seed term with polarity $c$ appeared in a synset of $w$; and $count(c)$ means the total number of synsets that contain seeds with this polarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Esuli and Sebastiani** (2006a);\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "    <img src=\"img/ensemble.jpg\" alt=\"Esuli and Sebastian Ensemble\" id=\"esuli-sebastiani\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Another popular dictionary-based resource, \\textsc{SentiWordNet}, was created by \\citet{Esuli:06c}, who enriched a small set of positive and negative seed adjectives with their \\textsc{WordNet} synonyms and antonyms in $k \\in \\{0, 2, 4, 6\\}$ iterations, considering the rest of the terms as neutral if they did not have a subjective tag in the General Inquirer lexicon.  In each of these $k$ steps, the authors optimized two ternary classifiers (Rocchio and SVM) that used tf-idf--vectors of synset glosses as features.  Afterwards, they predicted polarity scores for all \\textsc{WordNet} synsets using an ensemble of all trained classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Rao and Ravichandran** (2009):\n",
    "  * **randomized min-cut**;\n",
    "  * **label propagation**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In deterministic min-cut, the authors propagated the polarity values of seeds to their WordNet synonyms and hypernyms and then determined a minimum cut between the polarity\n",
    "  clusters using the algorithm of Blum (2001);\n",
    "\n",
    "The label propagation algorithm can be considered as a probabilistic variant of \\citeauthor{Blair-Goldensohn:08}'s approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Awdallah and Radev** (2010).\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "    <img src=\"img/walker.png\" alt=\"Random Walker\" id=\"awdallah\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Random walkers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Results of Dictionary-Based Methods\n",
    "\n",
    "<table>\n",
    "    <caption>\n",
    "        Results of dictionary-based methods<br/>\n",
    "        <em>HL â€” Hu and Liu (2004), BG â€” Blair-Goldensohn et al (2008), KH â€” Kim and Hovy (2004), ES â€”  Esuli and Sebastian (2006), RR â€” Rao and Ravichandran (2009), AR â€” Awdallah and Radev (2010)</em>\n",
    "   </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Lexicon</td>\n",
    "<td rowspan=\"2\"># of Terms</td>\n",
    "<td colspan=\"3\">Positive Expressions</td>\n",
    "<td colspan=\"3\">Negative Expressions</td>\n",
    "<td colspan=\"3\">Neutral Terms</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1$</td>\n",
    "<td rowspan=\"2\">Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tr>\n",
    "<td>Seeds</td>\n",
    "<td>20</td>\n",
    "<td class=\"best\">0.771</td>\n",
    "<td>0.102</td>\n",
    "<td>0.18</td>\n",
    "<td>0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td class=\"best\">0.999</td>\n",
    "<td class=\"best\">0.981</td>\n",
    "<td>0.398</td>\n",
    "<td class=\"best\">0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HL</td>\n",
    "<td>5,745</td>\n",
    "<td>0.161</td>\n",
    "<td>0.266</td>\n",
    "<td>0.2</td>\n",
    "<td>0.2</td>\n",
    "<td>0.133</td>\n",
    "<td>0.16</td>\n",
    "<td>0.969</td>\n",
    "<td>0.96</td>\n",
    "<td>0.965</td>\n",
    "<td>0.442</td>\n",
    "<td>0.93</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BG</td>\n",
    "<td>1,895</td>\n",
    "<td>0.503</td>\n",
    "<td>0.232</td>\n",
    "<td class=\"best\">0.318</td>\n",
    "<td>0.285</td>\n",
    "<td>0.093</td>\n",
    "<td>0.14</td>\n",
    "<td>0.968</td>\n",
    "<td>0.991</td>\n",
    "<td>0.979</td>\n",
    "<td class=\"best\">0.479</td>\n",
    "<td>0.959</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>KH</td>\n",
    "<td>356</td>\n",
    "<td>0.716</td>\n",
    "<td>0.159</td>\n",
    "<td>0.261</td>\n",
    "<td>0.269</td>\n",
    "<td>0.044</td>\n",
    "<td>0.076</td>\n",
    "<td>0.965</td>\n",
    "<td>0.997</td>\n",
    "<td class=\"best\">0.981</td>\n",
    "<td>0.439</td>\n",
    "<td class=\"best\">0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ES</td>\n",
    "<td>39,181</td>\n",
    "<td>0.042</td>\n",
    "<td class=\"best\">0.564</td>\n",
    "<td>0.078</td>\n",
    "<td>0.033</td>\n",
    "<td class=\"best\">0.255</td>\n",
    "<td>0.059</td>\n",
    "<td class=\"best\">0.981</td>\n",
    "<td>0.689</td>\n",
    "<td>0.81</td>\n",
    "<td>0.315</td>\n",
    "<td>0.644</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RR$_{mc}$</td>\n",
    "<td>8,060</td>\n",
    "<td>0.07</td>\n",
    "<td>0.422</td>\n",
    "<td>0.12</td>\n",
    "<td>0.216</td>\n",
    "<td>0.073</td>\n",
    "<td>0.109</td>\n",
    "<td>0.972</td>\n",
    "<td>0.873</td>\n",
    "<td>0.92</td>\n",
    "<td>0.383</td>\n",
    "<td>0.849</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RR$_{lp}$</td>\n",
    "<td>1,105</td>\n",
    "<td>0.567</td>\n",
    "<td>0.176</td>\n",
    "<td>0.269</td>\n",
    "<td class=\"best\">0.571</td>\n",
    "<td>0.046</td>\n",
    "<td>0.085</td>\n",
    "<td>0.965</td>\n",
    "<td>0.997</td>\n",
    "<td class=\"best\">0.981</td>\n",
    "<td>0.445</td>\n",
    "<td class=\"best\">0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>AR</td>\n",
    "<td>23</td>\n",
    "<td>0.768</td>\n",
    "<td>0.1</td>\n",
    "<td>0.176</td>\n",
    "<td>0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td class=\"best\">0.999</td>\n",
    "<td class=\"best\">0.981</td>\n",
    "<td>0.397</td>\n",
    "<td class=\"best\">0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HL $\\cap$ BG $\\cap$ RR$_{lp}$</td>\n",
    "<td>752</td>\n",
    "<td>0.601</td>\n",
    "<td>0.165</td>\n",
    "<td>0.259</td>\n",
    "<td>0.567</td>\n",
    "<td>0.045</td>\n",
    "<td>0.084</td>\n",
    "<td>0.965</td>\n",
    "<td>0.997</td>\n",
    "<td class=\"best\">0.981</td>\n",
    "<td>0.441</td>\n",
    "<td class=\"best\">0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HL $\\cup$ BG $\\cup$ RR$_{lp}$</td>\n",
    "<td>6,258</td>\n",
    "<td>0.166</td>\n",
    "<td>0.288</td>\n",
    "<td>0.21</td>\n",
    "<td>0.191</td>\n",
    "<td>0.146</td>\n",
    "<td class=\"best\">0.165</td>\n",
    "<td>0.97</td>\n",
    "<td>0.958</td>\n",
    "<td>0.964</td>\n",
    "<td>0.446</td>\n",
    "<td>0.929</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Corpus-Based Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "<div class=\"citation\">On Wednesday night it had rained very heavily, the lane was <span class=\"negative\">wet</span> and <span class=\"negative\">muddy</span>, but the Thursday morning sun was <span class=\"positive\">bright</span> and <span class=\"positive\">clear</span> as it shone on Arthur Dent's house for what was to be the last time.</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Takamura et al.** (2005);\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "  <img src=\"img/ising.png\" alt=\"Ising-Spin Model\" id=\"ising\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Velikovich et al.** (2010);\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "  <img src=\"img/blair-goldensohn.png\" alt=\"SVM Classifier\" id=\"blair-goldensohn\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Kiritchenko et al.** (2014);\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "  $$  \\textrm{SO-A}(w) = \\sum_{w_p\\in\\mathcal{P}}PMI(w, w_p) - \\sum_{w_n\\in\\mathcal{N}}PMI(w, w_n);$$\n",
    "  where $\\mathcal{P}$ represents the set of all positive seeds; $\\mathcal{N}$ denotes the collection of known negative words; and\n",
    "  $PMI$ is computed as a log-ratio $PMI(w, w_x) = \\log_2\\frac{p(w, w_x)}{p(w)p(w_x)}$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Severyn and Moschitti** (2015).\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "  <img src=\"img/svm.png\" alt=\"SVM Classifier\" id=\"severyn-moschitti\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Results of Corpus-Based Methods\n",
    "\n",
    "<table>\n",
    "    <caption>\n",
    "    <caption>\n",
    "        Results of corpus-based methods<br/>\n",
    "        <em>TKM â€” Takamura et al. (2005), VEL â€” Velikovich et al (2010), KIR â€” Kiritchenko et al. (2014), SEV â€”  Severyn and Moschitti (2015)</em>\n",
    "   </caption>\n",
    "    </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\"  class=\"lexicon\">Lexicon</td>\n",
    "<td rowspan=\"2\"># of Terms</td>\n",
    "<td colspan=\"3\">Positive Expressions</td>\n",
    "<td colspan=\"3\">Negative Expressions</td>\n",
    "<td colspan=\"3\">Neutral Terms</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1$</td>\n",
    "<td rowspan=\"2\">Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tr>\n",
    "<td>Seeds</td>\n",
    "<td>20</td>\n",
    "<td class=\"best\">0.771</td>\n",
    "<td>0.102</td>\n",
    "<td>0.18</td>\n",
    "<td class=\"best\">0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td class=\"best\">0.999</td>\n",
    "<td class=\"best\">0.981</td>\n",
    "<td>0.398</td>\n",
    "<td class=\"best\">0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TKM</td>\n",
    "<td>920</td>\n",
    "<td>0.646</td>\n",
    "<td class=\"best\">0.134</td>\n",
    "<td class=\"best\">0.221</td>\n",
    "<td>0.565</td>\n",
    "<td class=\"best\">0.029</td>\n",
    "<td class=\"best\">0.055</td>\n",
    "<td class=\"best\">0.964</td>\n",
    "<td>0.998</td>\n",
    "<td class=\"best\">0.981</td>\n",
    "<td class=\"best\">0.419</td>\n",
    "<td class=\"best\">0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>VEL</td>\n",
    "<td>60</td>\n",
    "<td>0.764</td>\n",
    "<td>0.102</td>\n",
    "<td>0.18</td>\n",
    "<td class=\"best\">0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.999</td>\n",
    "<td>0.98</td>\n",
    "<td>0.398</td>\n",
    "<td class=\"best\">0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>KIR</td>\n",
    "<td>320</td>\n",
    "<td>0.386</td>\n",
    "<td>0.106</td>\n",
    "<td>0.166</td>\n",
    "<td class=\"best\">0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.996</td>\n",
    "<td>0.979</td>\n",
    "<td>0.393</td>\n",
    "<td>0.959</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>60</td>\n",
    "<td>0.68</td>\n",
    "<td>0.102</td>\n",
    "<td>0.177</td>\n",
    "<td class=\"best\">0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td class=\"best\">0.999</td>\n",
    "<td class=\"best\">0.981</td>\n",
    "<td>0.397</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TKM $\\cap$ VEL $\\cap$ SEV</td>\n",
    "<td>20</td>\n",
    "<td class=\"best\">0.771</td>\n",
    "<td>0.102</td>\n",
    "<td>0.18</td>\n",
    "<td>0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td class=\"best\">0.999</td>\n",
    "<td class=\"best\">0.981</td>\n",
    "<td>0.398</td>\n",
    "<td class=\"best\">0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TKM $\\cup$ VEL $\\cup$ SEV</td>\n",
    "<td>1,020</td>\n",
    "<td>0.593</td>\n",
    "<td class=\"best\">0.134</td>\n",
    "<td>0.218</td>\n",
    "<td>0.565</td>\n",
    "<td class=\"best\">0.029</td>\n",
    "<td class=\"best\">0.055</td>\n",
    "<td class=\"best\">0.964</td>\n",
    "<td>0.998</td>\n",
    "<td>0.98</td>\n",
    "<td>0.418</td>\n",
    "<td class=\"best\">0.962</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# NWE-Based Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "    <img src=\"img/word2vec.png\" alt=\"word2vec vectors\" id=\"word2vec\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "I trained all vectors on the German Twitter snapshot, performed their mean scaling, and normalized their length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Tang** et al. (2014);\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "    <img src=\"img/tang.png\" alt=\"Tang's Method\" id=\"tang\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The authors used a large collection of distantly labeled tweets in order to learn hybrid word embeddings. In contrast to standard word2vec vectors (Mikolov et al., 2013) and purely task-specific representations (Collobert et al., 2011), such embeddings were optimized with respect to both objectives - predicting the occurrence of nearby words and classifying the overall polarity of a message. Using these hybrid vectors, Tang et al. trained a one-layer feed-forward neural network that predicted the polarity of a microblog, and subsequently applied this classifier separately to each word embedding, considering the predicted value as polarity score for the respective term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Vo and Zhang** (2016);\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "    <img src=\"img/vo_zhang.png\" alt=\"Vo and Zhang's Method\" id=\"vo-zhang\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In contrast to this approach, Vo and Zhang immediately optimized\n",
    "two-dimensional task-specific embeddings, and regarded the two dimensions of these learned\n",
    "vectors as positive and negative scores of corresponding words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Nearest Centroids**;\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "    <img src=\"img/nearest-centroids.png\" alt=\"Nearest Centroids\" id=\"nearest-centroids\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $k$-**Nearest Neighbors**;\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "    <img src=\"img/knn.png\" alt=\"k Nearest neighbors\" id=\"knn\"/>\n",
    "    \n",
    "    $$score_{w_{c}} = \\frac{\\sum_{n \\in \\mathcal{N}_c}\\lVert\\vec{w} - \\vec{n}\\rVert}{|\\mathcal{N}_c|^2}$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Principal Component Analysis**;\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "    <img src=\"img/pca.png\" alt=\"Principal Component Analysis\" id=\"pca\"/>\n",
    "    \n",
    "    $$E = U \\Sigma V^T$$,\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Linear Projection**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Linear Projection\n",
    "\n",
    "Given two sets of vectors with opposite semantic orientations ($\\mathcal{P} =\\{\\vec{p}_{+_1},\\ldots,\\vec{p}_{+_m}\\}$ and $\\mathcal{N} = \\{\\vec{p}_{-_1},\\ldots,\\vec{p}_{-_n}\\}$),\n",
    "we are looking for a line $\\vec{b}$ that maximizes the distance between the projections of embeddings from these sets on that line, i.e.:\n",
    "\n",
    "$$\\vec{b} = \\underset{\\vec{b}}{\\operatorname{argmax}}\\frac{1}{2}\\sum_{\\vec{p}_+}\\sum_{\\vec{p}_-}\\left\\lVert\\frac{\\vec{b}\\cdot\\vec{p}_+}{\\vec{b}^2}\\vec{b} - \\frac{\\vec{b}\\cdot\\vec{p}_-}{\\vec{b}^2}\\vec{b}\\right\\rVert^2=\\underset{\\vec{b}}{\\operatorname{argmax}}\\frac{1}{2}\\sum_{\\vec{p}_+}\\sum_{\\vec{p}_-}\\left\\lVert\\frac{\\vec{b}\\cdot\\left(\\vec{p}_{+}-\\vec{p}_{-}\\right)}{\\vec{b}^2}\\vec{b}\\right\\rVert^2,$$\n",
    "\n",
    "where $\\frac{\\vec{b}\\cdot\\vec{p}_+}{\\vec{b}^2}\\vec{b}$ is the projection of a word embedding with the positive polarity on line $\\vec{b}$, and $\\frac{\\vec{b}\\cdot\\vec{p}_-}{\\vec{b}^2}\\vec{b}$ is the respective projection of a negative seed term.\n",
    "\n",
    "Considering the $\\operatorname{argmax}$ argument in the above expression as our objective function $f$, we can compute the gradient of $f$ with respect\n",
    "to $\\vec{b}$ as follows:\n",
    "$$\\nabla_{\\vec{b}} f = \\sum_{\\vec{p}_+}\\sum_{\\vec{p}_-}\\gamma\\left(\\Delta - \\gamma\\vec{b}\\right),$$\n",
    "\n",
    "where $\\Delta$ stands for the difference between the positive and negative vectors $\\vec{p}_{+}$ and $\\vec{p}_{-}$: $\\Delta =\n",
    "\\vec{p}_{+}-\\vec{p}_{-}$; and $\\gamma$ denotes the dot product of this difference with vector $\\vec{b}$: $\\gamma = \\Delta \\cdot\n",
    "\\vec{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Linear Projection\n",
    "\n",
    "<div class=\"row\" id=\"linproj\">\n",
    "<div class=\"cell left\">\n",
    "<figure>\n",
    "<img src=\"img/linproj_polarity.png\" alt=\"Polarity Line Determined by the LinProj Algorithm\" id=\"linproj-right\"/>\n",
    "<figcaption>\n",
    "Subjectivity Line\n",
    "</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "\n",
    "<div class=\"cell right\">\n",
    "<figure>\n",
    "<img src=\"img/linproj_subjectivity.png\" alt=\"Subjectivity Line Determined by the LinProj Algorithm\" id=\"linproj-right\"/>\n",
    "<figcaption>\n",
    "Subjectivity Line\n",
    "</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Results of NWE-Based Methods\n",
    "\n",
    "<table>\n",
    "    <caption>\n",
    "        Results of NWE-based methods<br/>\n",
    "        <em>TNG â€” Tang et al. (2014a), VO â€” Vo and Zhang (2016), NC â€” nearest centroids, k-NN â€” k-nearest neighbors, PCA â€” principal component analysis, LP â€” linear projection</em>\n",
    "    </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Lexicon</td>\n",
    "<td rowspan=\"2\"># of Terms</td>\n",
    "<td colspan=\"3\">Positive Expressions</td>\n",
    "<td colspan=\"3\">Negative Expressions</td>\n",
    "<td colspan=\"3\">Neutral Terms</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1$</td>\n",
    "<td rowspan=\"2\">Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tr>\n",
    "<td>Seeds</td>\n",
    "<td>20</td>\n",
    "<td class=\"best\">0.771</td>\n",
    "<td>0.102</td>\n",
    "<td>0.18</td>\n",
    "<td>0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td class=\"best\">0.999</td>\n",
    "<td>0.981</td>\n",
    "<td>0.398</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TNG</td>\n",
    "<td>1,600</td>\n",
    "<td>0.088</td>\n",
    "<td>0.153</td>\n",
    "<td>0.112</td>\n",
    "<td>0.193</td>\n",
    "<td class=\"best\">0.155</td>\n",
    "<td class=\"best\">0.172</td>\n",
    "<td class=\"best\">0.966</td>\n",
    "<td>0.953</td>\n",
    "<td>0.959</td>\n",
    "<td>0.414</td>\n",
    "<td>0.921</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>VO</td>\n",
    "<td>40</td>\n",
    "<td>0.117</td>\n",
    "<td>0.115</td>\n",
    "<td>0.116</td>\n",
    "<td>0.541</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.98</td>\n",
    "<td>0.971</td>\n",
    "<td>0.374</td>\n",
    "<td>0.944</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>NC</td>\n",
    "<td>5,200</td>\n",
    "<td class=\"best\">0.771</td>\n",
    "<td>0.102</td>\n",
    "<td>0.18</td>\n",
    "<td>0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.999</td>\n",
    "<td>0.981</td>\n",
    "<td>0.398</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$k$-NN</td>\n",
    "<td>420</td>\n",
    "<td class=\"best\">0.486</td>\n",
    "<td class=\"best\">0.182</td>\n",
    "<td class=\"best\">0.265</td>\n",
    "<td>0.65</td>\n",
    "<td>0.091</td>\n",
    "<td>0.16</td>\n",
    "<td>0.966</td>\n",
    "<td>0.995</td>\n",
    "<td>0.98</td>\n",
    "<td class=\"best\">0.468</td>\n",
    "<td>0.961</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PCA</td>\n",
    "<td>40</td>\n",
    "<td>0.771</td>\n",
    "<td>0.102</td>\n",
    "<td>0.18</td>\n",
    "<td>0.529</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td class=\"best\">0.999</td>\n",
    "<td>0.981</td>\n",
    "<td>0.398</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LP</td>\n",
    "<td>6,340</td>\n",
    "<td>0.741</td>\n",
    "<td>0.156</td>\n",
    "<td>0.257</td>\n",
    "<td>0.436</td>\n",
    "<td>0.088</td>\n",
    "<td>0.147</td>\n",
    "<td class=\"best\">0.966</td>\n",
    "<td>0.998</td>\n",
    "<td>0.982</td>\n",
    "<td>0.462</td>\n",
    "<td class=\"best\">0.963</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Evaluation: Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **word2vec**;\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "  <img src=\"img/word2vec.png\" alt=\"word2vec vectors\" id=\"word2vec2\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **task-specific**;\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "  <img src=\"img/task-specific.png\" alt=\"task-specific vectors\" id=\"task-specific\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **task-specific** + **word2vec**;\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "  <div class=\"row\" id=\"task-specific-word2vec\">\n",
    "  <div class=\"cell left\">\n",
    "    <img src=\"img/word2vec.png\" alt=\"word2vec vectors\" id=\"word2vec3\"/>\n",
    "  </div>\n",
    "\n",
    "  <div class=\"cell right\">\n",
    "    <img src=\"img/task-specific.png\" alt=\"task-specific vectors\" id=\"task-specific2\"/>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **task-specific with least-squares mapping**.\n",
    "\n",
    "$$W = \\mathrm{argmin}_{W}\\lVert E_{TS} - W^T\\cdot E_{W2V}\\rVert_F$$\n",
    "\n",
    "where $E_{TS} \\in \\mathbb{R}^{n\\times m}$ is the matrix of task-specific embeddings and $E^*_{W2V} \\in \\mathbb{R}^{n\\times m}$ is the corresponding matrix of word2vec vectors.  Then we can approximate task-specific representations of terms missing in the task-specific corpus as:\n",
    "\n",
    "$$E^*_{TS} = W^T\\cdot E^*_{W2V}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/potts_embeddings.png\" alt=\"t-SNE visualization of different word-embedding types\" id=\"embedding-types\"/>\n",
    "<figcaption>\n",
    "    t-SNE visualization of PotTS corpus tokens and Turney and Littmann's seed set with different embedding types\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Macro-averaged F$_1$-scores of NWE-based methods with different embedding types    \n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\"  class=\"lexicon\">Lexicon</td>\n",
    "<td colspan=\"4\">Embedding Type</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>word2vec</td>\n",
    "<td>task-specific + word2vec</td>\n",
    "<td>task-specific + least squares</td>\n",
    "<td>task-specific</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>NC</td>\n",
    "<td>0.398</td>\n",
    "<td>0.398</td>\n",
    "<td class=\"best\">0.401</td>\n",
    "<td>0.399</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$k$-NN</td>\n",
    "<td class=\"best\">0.468</td>\n",
    "<td>0.43</td>\n",
    "<td>0.398</td>\n",
    "<td>0.392</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PCA</td>\n",
    "<td>0.398</td>\n",
    "<td>0.398</td>\n",
    "<td>0.404</td>\n",
    "<td class=\"best\">0.409</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LP</td>\n",
    "<td class=\"best\">0.462</td>\n",
    "<td>0.441</td>\n",
    "<td>0.398</td>\n",
    "<td>0.399</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Evaluation: Vector Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **no normalization**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **length normalization**:\n",
    "  <div class=\"hide-on-next-fragment\">\n",
    "    $\\vec{v}^* = \\frac{\\vec{v}}{\\left\\lVert\\vec{v}\\right\\rVert}$;\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **mean normalization**:\n",
    "  <div class=\"hide-on-next-fragment\">\n",
    "    $\\vec{v}^* = \\frac{\\vec{v} - \\vec{\\mu}}{\\vec{\\sigma}}$;\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **mean and length normalization**:\n",
    "  <div class=\"hide-on-next-fragment\">\n",
    "    $\\vec{v}^* = \\frac{\\frac{\\vec{v}}{\\left\\lVert\\vec{v}\\right\\rVert} - \\vec{\\mu}^*}{\\vec{\\sigma}^*}$;\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vector Normalization\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Macro-averaged F-scores of NWE-based methods with different vector normalizations\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">SLG Method</td>\n",
    "<td colspan=\"4\">Vector Normalization</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>no normalization</td>\n",
    "<td>mean normalization</td>\n",
    "<td>length normalization</td>\n",
    "<td>mean normalization + length normalization</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>NC</td>\n",
    "<td class=\"best\">0.398</td>\n",
    "<td class=\"best\">0.398</td>\n",
    "<td class=\"best\">0.398</td>\n",
    "<td class=\"best\">0.398</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$k$-NN</td>\n",
    "<td>0.417</td>\n",
    "<td>0.418</td>\n",
    "<td>0.467</td>\n",
    "<td class=\"best\">0.468</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PCA</td>\n",
    "<td>0.396</td>\n",
    "<td>0.396</td>\n",
    "<td class=\"best\">0.398</td>\n",
    "<td class=\"best\">0.398</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LP</td>\n",
    "<td>0.442</td>\n",
    "<td>0.416</td>\n",
    "<td>0.461</td>\n",
    "<td class=\"best\">0.462</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Evaluation: Seed Sets\n",
    "\n",
    "<table>\n",
    "    <caption>\n",
    "        Overview of alternative seed sets\n",
    "    </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td>Seed Set</td>\n",
    "<td>Cardinality</td>\n",
    "<td>Part of Speech</td>\n",
    "<td>Examples</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Hu and Liu (2004)</td>\n",
    "<td>14 positive, 15 negative, and 10 neutral terms</td>\n",
    "<td>adjectives</td>\n",
    "<td class=\"example\">fantastisch, lieb, sympathisch, bÃ¶se, dumm, schwierig</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Kim and Hovy (2004)</td>\n",
    "<td>60 positive, 60 negative, and 60 neutral terms</td>\n",
    "<td>any</td>\n",
    "<td class=\"example\">fabelhaft, Hoffnung, lieben, hÃ¤sslich, Missbrauch, tÃ¶ten</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Esuli and Sebastiani (2006)</td>\n",
    "<td>16 positive, 35 negative, and 4,122 neutral terms</td>\n",
    "<td>any</td>\n",
    "<td class=\"example\">angenehm, ausgezeichnet, freundlich, arm, bedauernswert, dÃ¼rftig</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Remus (2010)</td>\n",
    "<td>12 positive, 12 negative, and 10 neutral terms</td>\n",
    "<td>adjectives</td>\n",
    "<td class=\"example\">gut, schÃ¶n, richtig, schlecht, unschÃ¶n, falsch</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Seed Sets: Dictionary-Based Methods\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/sentilex-dict-alt-seed-sets.png\" als=\"Effect of Seed Sets on Dictionary-Based Methods\"/>\n",
    "<figcaption>\n",
    "    Effect of the seed sets on dictionary-based methods\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Seed Sets (Corpus-Based Methods)\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/sentilex-crp-alt-seed-sets.png\" als=\"Effect of Seed Sets on Corpus-Based Methods\"/><figcaption>\n",
    "    Effect of the seed sets on corpus-based methods\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Seed Sets: NWE-Based Methods\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/sentilex-nwe-alt-seed-sets.png\" als=\"Effect of Seed Sets on Dictionary-Based Methods\"/>\n",
    "<figcaption>\n",
    "    Effect of the seed sets on NWE-based methods\n",
    "</figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In this chapter, I have **compared existing German translation of English sentiment lexicons with automatically generated resources**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Semi-automatic translations of English polarity lists notably outperform automatic lexicons**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Despite their allegedly worse ability to accommodate new domains, **dictionary-based approaches are better than corpus-based systems**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **New NWE-based solutions represent a viable alternative to dictionary-based lexicons**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **All NWE-based methods benefit from mean-scaling and length normalization** of the input vectors;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **The results of all tested lexicon-generaion algorithms crucially depend on the quality of their initial seeds sets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 class=\"chapter-title animate\">Chapter IV: Aspect-Based Sentiment Analysis</h1>\n",
    "<img src=\"img/magnifier.jpeg\" alt=\"Magnifier\" id=\"magnifier\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given an input sentence $x_1, x_2, \\ldots, x_n$, we need to automatically find textual spans of **sentiments**, **sources**, and **targets**, i.e., to assign a label $y_i\\in{SNT, SRC, TRG, NON}$ to each token $x_i$ in the sentence.\n",
    "\n",
    "<div class=\"example\">\n",
    "  <div class=\"title\">Example 4.1</div>\n",
    "  <div class=\"tweet\"><span class=\"label\">Tweet:</span><span class=\"sentiment\"><span class=\"source\">Mir</span><span class=\"tag\">source</span> hat <span class=\"target\">die letzte Folge von Games of Thrones</span><span class=\"tag\">target</span> gar nicht gefallen.</span><span class=\"tag\">sentiment</span></div>\n",
    "  <div class=\"translation\"><span class=\"sentiment\"><span class=\"source\">I</span><span class=\"tag\">source</span> absolutely didn't like <span class=\"target\">the last episode of Game of Thrones</span><span class=\"tag\">target</span>.</span><span class=\"tag\">sentiment</span></div>\n",
    "  <div class=\"arrow\">$\\rightarrow$</div>\n",
    "  <div class=\"tweet\"><span class=\"pre\">Mir/SRC hat/SNT die/TRG letzte/TRG Folge/TRG von/TRG Games/TRG of/TRG Thrones/TRG gar/SNT nicht/SNT gefallen/SNT ./SNT</span></div>\n",
    "  <div class=\"translation\"><span class=\"pre\">I/SRC absolutely/SNT did/SNT n't/SNT like/SNT the/TRG last/TRG episode/TRG of/TRG Game/TRG of/TRG Thrones/TRG ./SNT</span></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "Since this problem involves simultaneous prediction of tags for multiple inter-connected random variables ($y_1, y_2, \\ldots, y_n$), it is commonly considered as a structured-prediction task, viz. as a sequence-labeling problem (SLP), and addressed with two common SLP methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **probabilistic graphical models** (e.g., conditional random fields);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **recurrent neural networks** (e.g., long-short term memory or gated recurrent units)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "<div class=\"example\">\n",
    "  <div class=\"title\">Example 4.2</div>\n",
    "  <div class=\"annotator1\">Gold Annotation:</div>\n",
    "  <div class=\"tweet\"><span class=\"pre\">Mir/SRC hat/SNT <strong style=\"color:darkred;\">die/TRG</strong> letzte/TRG Folge/TRG von/TRG Games/TRG of/TRG Thrones/TRG gar/SNT nicht/SNT gefallen/SNT ./SNT</span></div>\n",
    "  <div class=\"translation\"><span class=\"pre\">I/SRC absolutely/SNT did/SNT n't/SNT like/SNT <strong style=\"color:darkred;\">the/TRG</strong> last/TRG episode/TRG of/TRG Game/TRG of/TRG Thrones/TRG ./SNT</span></div>\n",
    "\n",
    "  <div class=\"annotator2\">Predicted Annotation:</div>\n",
    "  <div class=\"tweet\"><span class=\"pre\">Mir/SRC hat/SNT <strong style=\"color:darkred;\">die/NON</strong> letzte/TRG Folge/TRG von/TRG Games/TRG of/TRG Thrones/TRG gar/SNT nicht/SNT gefallen/SNT ./SNT</span></div>\n",
    "  <div class=\"translation\"><span class=\"pre\">I/SRC absolutely/SNT did/SNT n't/SNT like/SNT <strong style=\"color:darkred;\">the/NON</strong> last/TRG episode/TRG of/TRG Game/TRG of/TRG Thrones/TRG ./SNT</span></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **exact match**: **0**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **binary overlap**: **1**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **proportional overlap**: **0.857**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Proportional Overlap\n",
    "\n",
    "<div class=\"example\">\n",
    "  <div class=\"title\">Example 4.2</div>\n",
    "  <div class=\"annotator1\">Gold Annotation:</div>\n",
    "  <div class=\"tweet\"><span class=\"pre\">Mir/SRC hat/SNT <strong style=\"color:darkred;\">die/TRG</strong> letzte/TRG Folge/TRG von/TRG Games/TRG of/TRG Thrones/TRG gar/SNT nicht/SNT gefallen/SNT ./SNT</span></div>\n",
    "\n",
    "  <div class=\"annotator2\">Predicted Annotation:</div>\n",
    "  <div class=\"tweet\"><span class=\"pre\">Mir/SRC hat/SNT <strong style=\"color:darkred;\">die/NON</strong> letzte/TRG Folge/TRG von/TRG Games/TRG of/TRG Thrones/TRG gar/SNT nicht/SNT gefallen/SNT ./SNT</span></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "Given two sets of manually and automatically tagged spans ($\\mathcal{S}$ and $\\widehat{\\mathcal{S}}$, respectively), we estimate the precision of automatic assignment as:\n",
    "$$P(\\mathcal{S}, \\widehat{\\mathcal{S}}) = \\frac{C(\\mathcal{S},\n",
    "    \\widehat{\\mathcal{S}})}{|\\widehat{\\mathcal{S}}|},$$\n",
    "where $C(\\mathcal{S},\\widehat{\\mathcal{S}})$ stands for the proportion of overlapping tokens across all pairs of manually ($s_i$) and automatically ($s_j$) annotated spans:\n",
    "\n",
    "$$C(\\mathcal{S}, \\widehat{\\mathcal{S}}) = \\sum_{s_i \\in \\mathcal{S}}\\sum_{s_j \\in \\widehat{\\mathcal{S}}}c(s_i, s_j).$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "Similarly, the recall of this assignment is estimated as:\n",
    "\n",
    "$$R(\\mathcal{S}, \\widehat{\\mathcal{S}}) = \\frac{C(\\mathcal{S}, \\widehat{\\mathcal{S}})}{|\\mathcal{S}|}.$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using these two values, one then normally computes the $F_1$-measure as:\n",
    "\n",
    "$$F_1 = 2\\times\\frac{P \\times R}{P + R}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **preprocessed** all tweets with a **rule-based text normalization system** (Sidarenka et al., 2013);\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "<ul>\n",
    "<li><strong>unified Twitter-specific phenomena</strong> (e.g., \"@GrumpyMerkel\" $\\rightarrow$ \"%User\", \"ðŸ˜Š\" $\\rightarrow$ \"%PosSmiley\", \"#glÃ¼cklich\" $\\rightarrow$ \"glÃ¼cklich\");</li>\n",
    "<li><strong>restored frequent misspellings</strong> (e.g., \"zuguckn\" $\\rightarrow$ \"zugucken\", \"Tach\" $\\rightarrow$ \"Tag\");</li>\n",
    "<li><strong>replaced frequent slang terms and abbrebiations with their standard-language\n",
    "equivalents</strong> (e.g., \"n bissl\" $\\rightarrow$ \"ein bisschen\", \"iwie\" $\\rightarrow$ \"irgendwie\").</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **tokenized** all messages with an adjusted version of **Christopher Potts' social-media tokenizer**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **lemmatized** and **part-of-speech tagged** these tokens with **TreeTagger** (Schmid, 1995);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Afterwards, I obtained **morphological and syntactic analyses** with the help of **Mate dependency parser** (Bohnet et al., 2013);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Finally, I used **70%** of these data for **training**, **10% as development set**, and the remaining **20%** for **testing**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Conditional Random Fields\n",
    "\n",
    "**Conditional random fields** are an undirected discriminative probabilistic graphical model, which estimates the conditional probability of of unobserved variables $\\mathbf{y}$ given an input $\\mathbf{x}$ as:\n",
    "$$P\\left(\\mathbf{y}|\\mathbf{x}\\right) = \\frac{\\exp{\\left(\\sum_{m=1}^{M}\\sum_{j}w_{j} \\cdot f_j(x_{m}, y_{m-1}, y_{m})\\right)}}{Z},$$\n",
    "where $Z$ is a normalization factor computed over all possible label assignments :\n",
    "$$Z =\\sum_{y'\\in\\mathcal{Y},y''\\in\\mathcal{Y}}\\exp\\left(\\sum_{m=1}^{M}\\sum_{j}w_{j} \\cdot f_j(x_{m}, y'_{m-1}, y''_{m})\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/folc-crf.png\" alt=\"First-order Linear Chain CRF\" id=\"folc-crf-definition\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **formal**:\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "<ul>\n",
    "  <li>the initial three characters of each token,</li>\n",
    "  <li>its last three characters,</li>\n",
    "  <li>general spelling class of the word (e.g., alphanumeric, digit, or punctuation);</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **morphological**:\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "<ul>\n",
    "  <li>part-of-speech tag of the current token,</li>\n",
    "  <li>grammatical case and gender of inflectable PoS types,</li>\n",
    "  <li>degree of comparison for adjectives,</li>\n",
    "  <li>mood, tense, and person forms of verbs;</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **lexical**:\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "<ul>\n",
    "  <li>lemma and form of the current token (using one-hot encoding),</li>\n",
    "  <li>its polarity class (positive, negative, or neutral), obtained from the Zurich Polarity Lexicon (Clematide and Klenner, 2010);</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **syntactic**:\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "<ul>\n",
    "  <li>dependency relation via which token $x_i$ was connected to its parent,</li>\n",
    "  <li>two binary attributes that showed whether the previous token in the sentence was the parent or the child of the current word,</li>\n",
    "  <li>dependency relation of the previous token in the sentence to its parent + the dependency relation of the current token to its ancestor,</li>\n",
    "  <li>dependency link of the next token + the dependency relation of the current token to its parent;</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **lexico-syntactic**:\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "<ul>\n",
    "  <li>lemma of the syntactic parent;</li>\n",
    "  <li>part-of-speech tag and polarity class of the grandparent in the syntactic tree;</li>\n",
    "  <li>lemma of the child node + dependency relation between the current token and its child;</li>\n",
    "  <li>PoS tag of the child node + its dependency relation + the PoS tag of the current token;</li>\n",
    "  <li>lemma of the child node + its dependency relation + the lemma of the current token;</li>\n",
    "  <li>overall polarity of syntactic children, which was computed by summing up the polarity scores of all immediate dependents, and checking whether the resulting value was greater, less than, or equal to zero.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# CRF Results\n",
    "\n",
    "<table>\n",
    "    <caption>\n",
    "        Results of aspect-based sentiment analysis with linear-chain CRFs\n",
    "    </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Data Set</td>\n",
    "<td colspan=\"3\">Sentiment</td>\n",
    "<td colspan=\"3\">Source</td>\n",
    "<td colspan=\"3\">Target</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision </td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Training Set</td>\n",
    "<td>0.949</td>\n",
    "<td>0.908</td>\n",
    "<td>0.928</td>\n",
    "<td>0.903</td>\n",
    "<td>0.87</td>\n",
    "<td>0.886</td>\n",
    "<td>0.933</td>\n",
    "<td>0.865</td>\n",
    "<td>0.898</td>\n",
    "<td>0.904</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Test Set</td>\n",
    "<td>0.37</td>\n",
    "<td>0.28</td>\n",
    "<td>0.319</td>\n",
    "<td>0.305</td>\n",
    "<td>0.244</td>\n",
    "<td>0.271</td>\n",
    "<td>0.304</td>\n",
    "<td>0.244</td>\n",
    "<td>0.271</td>\n",
    "<td>0.287</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Feature Analysis\n",
    "\n",
    "<table id=\"top10-crf-features\">\n",
    "<caption>\n",
    "Top-10 state and transition features learned by the CRF model\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Rank</td>\n",
    "<td colspan=\"2\">State Features</td>\n",
    "<td colspan=\"2\">Transition Features</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Feature </td>\n",
    "<td>Score </td>\n",
    "<td>Feature </td>\n",
    "<td>Score</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>prntLemma=meiste $\\rightarrow$ TRG</td>\n",
    "<td>18.68</td>\n",
    "<td>NON $\\rightarrow$ TRG</td>\n",
    "<td>-7.01</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>prntLemma=rettungsschirme $\\rightarrow$ TRG</td>\n",
    "<td>18.3</td>\n",
    "<td>NON $\\rightarrow$ SRC</td>\n",
    "<td>-6.85</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3</td>\n",
    "<td>initChar=sty $\\rightarrow$ NON</td>\n",
    "<td>-16.04</td>\n",
    "<td>NON $\\rightarrow$ SNT</td>\n",
    "<td>-5.39</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td>\n",
    "<td>form=meisten $\\rightarrow$ NON</td>\n",
    "<td>15.99</td>\n",
    "<td>TRG $\\rightarrow$ SRC</td>\n",
    "<td>-2.99</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>5</td>\n",
    "<td>prntLemma=urlauberin $\\rightarrow$ SNT</td>\n",
    "<td>14.74</td>\n",
    "<td>NON $\\rightarrow$ NON</td>\n",
    "<td>2.69</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>6</td>\n",
    "<td>lemma=anfechten  $\\rightarrow$ SNT</td>\n",
    "<td>14.07</td>\n",
    "<td>SRC $\\rightarrow$ NON</td>\n",
    "<td>-2.59</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>7</td>\n",
    "<td>form=thomasoppermann  $\\rightarrow$ TRG</td>\n",
    "<td>13.44</td>\n",
    "<td>SNT $\\rightarrow$ SNT</td>\n",
    "<td>2.54</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>8</td>\n",
    "<td>form=bezeichnete $\\rightarrow$ SNT</td>\n",
    "<td>13.25</td>\n",
    "<td>TRG $\\rightarrow$ TRG</td>\n",
    "<td>2.31</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>9</td>\n",
    "<td>deprel[0]|deprel[1]=NK|AMS $\\rightarrow$ NON</td>\n",
    "<td>12.92</td>\n",
    "<td>SRC $\\rightarrow$ SRC</td>\n",
    "<td>2.19</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>10</td>\n",
    "<td>trailChar=te. $\\rightarrow$ NON</td>\n",
    "<td>12.77</td>\n",
    "<td>SRC $\\rightarrow$ TRG</td>\n",
    "<td>-2.07</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Feature Analysis\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Results of the feature ablation tests for the CRF model\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Element</td>\n",
    "<td rowspan=\"2\">Original<br/>F$_1$-Score </td>\n",
    "<td colspan=\"5\">F$_1$-Score after Feature Removal</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Formal</td>\n",
    "<td>Morphological</td>\n",
    "<td>Lexical</td>\n",
    "<td>Syntactic</td>\n",
    "<td>Lexico-Syntactic</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>0.346</td>\n",
    "    <td>0.343<span class=\"negdelta\">0.003</span></td>\n",
    "<td>0.344<span class=\"negdelta\">0.002</span></td>\n",
    "<td>0.326<span class=\"negdelta\">0.02</span></td>\n",
    "<td>0.345<span class=\"negdelta\">0.001</span></td>\n",
    "<td>0.324<span class=\"negdelta\">0.022</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Source</td>\n",
    "<td>0.309</td>\n",
    "<td>0.321<span class=\"posdelta\">0.012</span></td>\n",
    "<td>0.313<span class=\"posdelta\">0.004</span></td>\n",
    "<td>0.265<span class=\"negdelta\">0.044</span></td>\n",
    "<td>0.359<span class=\"posdelta\">0.05</span></td>\n",
    "<td>0.271<span class=\"negdelta\">0.038</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Target</td>\n",
    "<td>0.26</td>\n",
    "<td>0.282<span class=\"posdelta\">0.022</span></td>\n",
    "<td>0.252<span class=\"negdelta\">0.008</span></td>\n",
    "<td>0.263<span class=\"posdelta\">0.003</span></td>\n",
    "<td>0.233<span class=\"negdelta\">0.027</span></td>\n",
    "<td>0.263<span class=\"posdelta\">0.003</span></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "**Recurrent neural networks** (RNN) are a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence and which can use their internal state (memory) to process sequences of arbitrary lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **LSTM**\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "<img src=\"img/lstm.png\" alt=\"LSTM Cell Architecture\" id=\"lstm\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **GRU**\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "<img src=\"img/gru.png\" alt=\"GRU Cell Architecture\" id=\"gru\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training\n",
    "\n",
    "* I **upsampled** subjective tweets by randomly repeating microblogs that contained seniments until I got an equal proportion of subjective and objective messages in the training set;\n",
    "\n",
    "* I **initialized** all matrix parameters to random **orthogonal** matrices and used **uniform He sampling** (He et al., 2015) for setting the initial values of bias vectores;\n",
    "\n",
    "* I used **categorical hinge-loss** as **objective function**;\n",
    "\n",
    "* &hellip; and trained the models for **256 epochs** using the **RMSProp algorithm** (Tielemann and Hinton, 2012), **selecting the parameters that achieved the highest macro-averaged F$_1$-score on the dev data** during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RNN Results\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Results of aspect-based sentiment analysis with recurrent neural networks\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Data Set</td>\n",
    "<td colspan=\"3\">Sentiment</td>\n",
    "<td colspan=\"3\">Source</td>\n",
    "<td colspan=\"3\">Target</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"11\" class=\"separator\">LSTM</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Training Set</td>\n",
    "<td>0.49<span class=\"stddev\">0.16</span></td>\n",
    "<td>0.75<span class=\"stddev\">0.01</span></td>\n",
    "<td>0.58<span class=\"stddev\">0.13</span></td>\n",
    "<td>0.45<span class=\"stddev\">0.05</span></td>\n",
    "<td>0.63<span class=\"stddev\">0.12</span></td>\n",
    "<td>0.52<span class=\"stddev\">0.08</span></td>\n",
    "<td>0.41<span class=\"stddev\">0.11</span></td>\n",
    "<td>0.73<span class=\"stddev\">0.06</span></td>\n",
    "<td>0.52<span class=\"stddev\">0.11</span></td>\n",
    "<td>0.54<span class=\"stddev\">0.11</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Test Set</td>\n",
    "<td>0.29<span class=\"stddev\">0.03</span></td>\n",
    "<td class=\"best\">0.31<span class=\"stddev\">0.11</span></td>\n",
    "<td class=\"best\">0.29<span class=\"stddev\">0.03</span></td>\n",
    "<td class=\"best\">0.25<span class=\"stddev\">0.02</span></td>\n",
    "<td class=\"best\">0.31<span class=\"stddev\">0.0</span></td>\n",
    "<td class=\"best\">0.27<span class=\"stddev\">0.01</span></td>\n",
    "<td>0.23<span class=\"stddev\">0.02</span></td>\n",
    "<td class=\"best\">0.25<span class=\"stddev\">0.05</span></td>\n",
    "<td class=\"best\">0.24<span class=\"stddev\">0.01</span></td>\n",
    "<td class=\"best\">0.27<span class=\"stddev\">0.02</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<tr>\n",
    "<td colspan=\"11\" class=\"separator\">GRU</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Training Set</td>\n",
    "<td>0.51<span class=\"stddev\">0.08</span></td>\n",
    "<td>0.66<span class=\"stddev\">0.05</span></td>\n",
    "<td>0.57<span class=\"stddev\">0.03</span></td>\n",
    "<td>0.42<span class=\"stddev\">0.03</span></td>\n",
    "<td>0.62<span class=\"stddev\">0.05</span></td>\n",
    "<td>0.5<span class=\"stddev\">0.03</span></td>\n",
    "<td>0.47<span class=\"stddev\">0.11</span></td>\n",
    "<td>0.63<span class=\"stddev\">0.11</span></td>\n",
    "<td>0.52<span class=\"stddev\">0.04</span></td>\n",
    "<td>0.53<span class=\"stddev\">0.03</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Test Set</td>\n",
    "<td class=\"best\">0.3<span class=\"stddev\">0.01</span></td>\n",
    "<td>0.26<span class=\"stddev\">0.06</span></td>\n",
    "<td>0.28<span class=\"stddev\">0.03</span></td>\n",
    "<td>0.22<span class=\"stddev\">0.03</span></td>\n",
    "<td>0.28<span class=\"stddev\">0.02</span></td>\n",
    "<td>0.24<span class=\"stddev\">0.02</span></td>\n",
    "<td class=\"best\">0.24<span class=\"stddev\">0.03</span></td>\n",
    "<td>0.21<span class=\"stddev\">0.07</span></td>\n",
    "<td>0.22<span class=\"stddev\">0.03</span></td>\n",
    "<td>0.25<span class=\"stddev\">0.01</span></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Evaluation: Word Embeddings\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Results of aspect-based sentiment analysis with different word embedding types\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">RNN</td>\n",
    "<td colspan=\"3\">Sentiment</td>\n",
    "<td colspan=\"3\">Source</td>\n",
    "<td colspan=\"3\">Target</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1$</td>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"11\" class=\"separator\">Task-Specific Embeddings</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LSTM</td>\n",
    "<td>0.283</td>\n",
    "<td>0.288</td>\n",
    "<td>0.278</td>\n",
    "<td class=\"best\">0.293</td>\n",
    "<td>0.372</td>\n",
    "<td>0.328</td>\n",
    "<td class=\"best\">0.254</td>\n",
    "<td>0.27</td>\n",
    "<td class=\"best\">0.259</td>\n",
    "<td>0.288</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GRU</td>\n",
    "<td>0.287</td>\n",
    "<td>0.246</td>\n",
    "<td>0.263</td>\n",
    "<td>0.287</td>\n",
    "<td>0.405</td>\n",
    "<td class=\"best\">0.335</td>\n",
    "<td>0.252</td>\n",
    "<td>0.205</td>\n",
    "<td>0.216</td>\n",
    "<td>0.271</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"11\" class=\"separator\">Least-Squares Embeddings</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LSTM</td>\n",
    "<td>0.268</td>\n",
    "<td class=\"best\">0.37</td>\n",
    "<td>0.307</td>\n",
    "<td>0.261</td>\n",
    "<td class=\"best\">0.414</td>\n",
    "<td>0.314</td>\n",
    "<td>0.223</td>\n",
    "<td class=\"best\">0.275</td>\n",
    "<td>0.245</td>\n",
    "<td class=\"best\">0.289</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GRU</td>\n",
    "<td>0.256</td>\n",
    "<td>0.341</td>\n",
    "<td>0.291</td>\n",
    "<td>0.267</td>\n",
    "<td>0.395</td>\n",
    "<td>0.318</td>\n",
    "<td>0.229</td>\n",
    "<td>0.262</td>\n",
    "<td>0.245</td>\n",
    "<td>0.285</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"11\" class=\"separator\">Word2Vec Embeddings</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LSTM</td>\n",
    "<td class=\"best\">0.291</td>\n",
    "<td>0.329</td>\n",
    "<td class=\"best\">0.309</td>\n",
    "<td>0.2</td>\n",
    "<td>0.311</td>\n",
    "<td>0.244</td>\n",
    "<td>0.221</td>\n",
    "<td>0.219</td>\n",
    "<td>0.22</td>\n",
    "<td>0.257</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GRU</td>\n",
    "<td>0.273</td>\n",
    "<td>0.355</td>\n",
    "<td>0.301</td>\n",
    "<td>0.207</td>\n",
    "<td>0.353</td>\n",
    "<td>0.257</td>\n",
    "<td>0.213</td>\n",
    "<td>0.26</td>\n",
    "<td>0.233</td>\n",
    "<td>0.264</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Evaluation: Annotation Scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "<h2>Broad Interpretation</h2>\n",
    "\n",
    "   <div class=\"example\">\n",
    "     <div class=\"title\">Example 4.3</div>\n",
    "     <div class=\"tweet\"><span class=\"label\">Tweet:</span><span class=\"sentiment\"><span class=\"target\">Francis</span><span class=\"tag\">target</span> makes a very <span class=\"polar-term\">good</span><span class=\"tag\">polar-term</span> impression on <span class=\"source\">me</span><span class=\"tag\">source</span>! <span class=\"polar-term\">:)</span><span class=\"tag\">polar-term</span></span><span class=\"tag\">sentiment</span></div>\n",
    "     <div class=\"arrow\">$\\rightarrow$</div>\n",
    "     <div class=\"tweet\"><span class=\"pre\">Francis/TRG makes/SNT a/SNT very/SNT good/SNT impression/SNT on/SNT me/SRC !/SNT :)/SNT</span></div>\n",
    "    </div>        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Narrow Interpretation\n",
    "\n",
    "   <div class=\"example\">\n",
    "     <div class=\"title\">Example 4.3</div>\n",
    "     <div class=\"tweet\"><span class=\"label\">Tweet:</span><span class=\"sentiment\"><span class=\"target\">Francis</span><span class=\"tag\">target</span> makes a very <span class=\"polar-term\">good</span><span class=\"tag\">polar-term</span> impression on <span class=\"source\">me</span><span class=\"tag\">source</span>! <span class=\"polar-term\">:)</span><span class=\"tag\">polar-term</span></span><span class=\"tag\">sentiment</span></div>\n",
    "     <div class=\"arrow\">$\\rightarrow$</div>\n",
    "     <div class=\"tweet\"><span class=\"pre\">Francis/TRG makes/NON a/NON very/NON good/SNT impression/NON on/NON me/SRC !/NON :)/SNT</span></div>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Annotation Scheme: Results\n",
    "\n",
    "<table>\n",
    "    <caption>\n",
    "        Results of aspect-based analysis with broad and narrow sentiment interpretations\n",
    "    </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Sentiment</td>\n",
    "<td colspan=\"3\">Source</td>\n",
    "<td colspan=\"3\">Target</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"11\" class=\"separator\">Broad Interpretation</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>CRF</td>\n",
    "<td>0.38</td>\n",
    "<td>0.32</td>\n",
    "<td>0.34</td>\n",
    "<td class=\"best\">0.3</td>\n",
    "<td>0.33</td>\n",
    "<td>0.31</td>\n",
    "<td class=\"best\">0.29</td>\n",
    "<td>0.23</td>\n",
    "<td class=\"best\">0.26</td>\n",
    "<td>0.31</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LSTM</td>\n",
    "<td>0.28</td>\n",
    "<td>0.29</td>\n",
    "<td>0.28</td>\n",
    "<td>0.29</td>\n",
    "<td class=\"best\">0.37</td>\n",
    "<td class=\"best\">0.33</td>\n",
    "<td>0.25</td>\n",
    "<td class=\"best\">0.27</td>\n",
    "<td class=\"best\">0.26</td>\n",
    "<td>0.29</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GRU</td>\n",
    "<td>0.29</td>\n",
    "<td>0.25</td>\n",
    "<td>0.26</td>\n",
    "<td>0.29</td>\n",
    "<td>0.4</td>\n",
    "<td>0.34</td>\n",
    "<td>0.25</td>\n",
    "<td>0.21</td>\n",
    "<td>0.22</td>\n",
    "<td>0.27</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"11\" class=\"separator\">Narrow Interpretation</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>CRF</td>\n",
    "<td>0.59</td>\n",
    "<td>0.64</td>\n",
    "<td>0.62</td>\n",
    "<td>0.26</td>\n",
    "<td>0.23</td>\n",
    "<td>0.24</td>\n",
    "<td>0.22</td>\n",
    "<td>0.20</td>\n",
    "<td>0.21</td>\n",
    "<td>0.36</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LSTM</td>\n",
    "<td class=\"best\">0.62</td>\n",
    "<td class=\"best\">0.65</td>\n",
    "<td class=\"best\">0.63</td>\n",
    "<td class=\"best\">0.3</td>\n",
    "<td>0.35</td>\n",
    "<td>0.32</td>\n",
    "<td>0.26</td>\n",
    "<td>0.14</td>\n",
    "<td>0.18</td>\n",
    "<td class=\"best\">0.38</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GRU</td>\n",
    "<td class=\"best\">0.62</td>\n",
    "<td>0.63</td>\n",
    "<td>0.62</td>\n",
    "<td>0.28</td>\n",
    "<td>0.33</td>\n",
    "<td>0.3</td>\n",
    "<td>0.23</td>\n",
    "<td>0.24</td>\n",
    "<td>0.23</td>\n",
    "<td class=\"best\">0.38</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Evaluation: Graph Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* first-order linear chain CRFs;\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "  <img src=\"img/folc-crf.png\" alt=\"First-Order Linear-Chain CRFs\" id=\"folc-crf\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* higher-order linear chain CRFs;\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "  <img src=\"img/holc-crf.png\" alt=\"Higher-Order Linear-Chain CRFs\" id=\"holc-crf\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* first- and higher-order semi-Markov CRF models;\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "  <img src=\"img/semim-crf.png\" alt=\"Semi-Markov CRFs\" id=\"semim-crf\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* tree-structured CRFs.\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "  <img src=\"img/tree-crf.png\" alt=\"Tree-Structured CRFs\" id=\"tree-crf\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Graph Structure: CRF\n",
    "\n",
    "<table>\n",
    "    <caption>\n",
    "        Results of aspect-based sentiment analysis with different CRF topologies<br/>\n",
    "        <em>lcCRF â€” linear-chain CRFs, smCRF â€” semi-Markov CRFs, trCRF â€” tree-structured CRFs;<br/>\n",
    "            1, 2, 3, and 4 in the superscripts denote the order</em>\n",
    "    </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Element</td>\n",
    "<td colspan=\"9\">Structure</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>lcCRF$^1$</td>\n",
    "<td>lcCRF$^2$</td>\n",
    "<td>lcCRF$^3$</td>\n",
    "<td>lcCRF$^4$</td>\n",
    "<td>smCRF$^1$</td>\n",
    "<td>smCRF$^2$</td>\n",
    "<td>smCRF$^3$</td>\n",
    "<td>smCRF$^4$</td>\n",
    "<td>trCRF$^1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"10\" class=\"separator\">Training Set</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>0.928</td>\n",
    "<td>0.919</td>\n",
    "<td>0.922</td>\n",
    "<td>0.925</td>\n",
    "<td>0.931</td>\n",
    "<td>0.931</td>\n",
    "<td>0.933</td>\n",
    "<td>0.931</td>\n",
    "<td>0.906</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Source</td>\n",
    "<td>0.887</td>\n",
    "<td>0.876</td>\n",
    "<td>0.89</td>\n",
    "<td>0.901</td>\n",
    "<td>0.869</td>\n",
    "<td>0.886</td>\n",
    "<td>0.874</td>\n",
    "<td>0.878</td>\n",
    "<td>0.881</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Target</td>\n",
    "<td>0.898</td>\n",
    "<td>0.811</td>\n",
    "<td>0.816</td>\n",
    "<td>0.827</td>\n",
    "<td>0.813</td>\n",
    "<td>0.827</td>\n",
    "<td>0.815</td>\n",
    "<td>0.817</td>\n",
    "<td>0.876</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"10\" class=\"separator\">Development Set</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>0.345</td>\n",
    "<td>0.334</td>\n",
    "<td>0.332</td>\n",
    "<td>0.335</td>\n",
    "<td class=\"best\">0.395</td>\n",
    "<td>0.385</td>\n",
    "<td>0.389</td>\n",
    "<td>0.378</td>\n",
    "<td>0.331</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Source</td>\n",
    "<td>0.313</td>\n",
    "<td class=\"best\">0.32</td>\n",
    "<td>0.272</td>\n",
    "<td>0.304</td>\n",
    "<td>0.298</td>\n",
    "<td>0.282</td>\n",
    "<td>0.287</td>\n",
    "<td>0.291</td>\n",
    "<td>0.223</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Target</td>\n",
    "<td>0.258</td>\n",
    "<td>0.235</td>\n",
    "<td>0.24</td>\n",
    "<td>0.229</td>\n",
    "<td>0.287</td>\n",
    "<td class=\"best\">0.309</td>\n",
    "<td>0.301</td>\n",
    "<td>0.292</td>\n",
    "<td>0.243</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Graph Structure: RNN\n",
    "\n",
    "<table>\n",
    "    <caption>\n",
    "        Results of aspect-based sentiment analysis with different neural network topologies<br/>\n",
    "<em>lcLSTM â€” linear-chain LSTM, lcGRU â€” linear-chain GRU, trLSTM â€” tree-structured LSTM, trGRU â€” tree-structured GRU;<br/>\n",
    "1, 2, and 3 in the superscripts denote the order</em>\n",
    "    </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Element</td>\n",
    "<td colspan=\"8\">Structure</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>lcLSTM$^1$</td>\n",
    "<td>lcLSTM$^2$</td>\n",
    "<td>lcLSTM$^3$</td>\n",
    "<td>lcGRU$^1$</td>\n",
    "<td>lcGRU$^2$</td>\n",
    "<td>lcGRU$^3$</td>\n",
    "<td>trLSTM$^1$</td>\n",
    "<td>trGRU$^1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"9\" class=\"separator\">Training Set</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>0.584</td>\n",
    "<td>0.559</td>\n",
    "<td>0.54</td>\n",
    "<td>0.57</td>\n",
    "<td>0.587</td>\n",
    "<td>0.606</td>\n",
    "<td>0.43</td>\n",
    "<td>0.518</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Source</td>\n",
    "<td>0.525</td>\n",
    "<td>0.458</td>\n",
    "<td>0.424</td>\n",
    "<td>0.503</td>\n",
    "<td>0.546</td>\n",
    "<td>0.548</td>\n",
    "<td>0.317</td>\n",
    "<td>0.372 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Target</td>\n",
    "<td>0.521</td>\n",
    "<td>0.513</td>\n",
    "<td>0.501</td>\n",
    "<td>0.519</td>\n",
    "<td>0.544</td>\n",
    "<td>0.605</td>\n",
    "<td>0.305</td>\n",
    "<td>0.425</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<tr>\n",
    "<td colspan=\"9\" class=\"separator\">Development Set</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>0.278</td>\n",
    "<td>0.285</td>\n",
    "<td>0.281</td>\n",
    "<td class=\"best\">0.335</td>\n",
    "<td>0.252</td>\n",
    "<td>0.253</td>\n",
    "<td>0.314</td>\n",
    "<td>0.292</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Source</td>\n",
    "<td class=\"best\">0.328</td>\n",
    "<td>0.314</td>\n",
    "<td>0.303</td>\n",
    "<td>0.263</td>\n",
    "<td>0.298</td>\n",
    "<td>0.306</td>\n",
    "<td>0.256</td>\n",
    "<td>0.262</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Target</td>\n",
    "<td class=\"best\">0.259</td>\n",
    "<td>0.218</td>\n",
    "<td>0.222</td>\n",
    "<td>0.216</td>\n",
    "<td>0.219</td>\n",
    "<td>0.188</td>\n",
    "<td>0.205</td>\n",
    "<td>0.193</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Evaluation: Text Normalization\n",
    "\n",
    "<table>\n",
    "  <caption>\n",
    "    Results of aspect-based sentiment analysis with (w) and without (w/o) text normalization\n",
    "  </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Data Set</td>\n",
    "<td colspan=\"3\">Sentiment</td>\n",
    "<td colspan=\"3\">Source</td>\n",
    "<td colspan=\"3\">Target</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"11\" class=\"separator\">w Normalization</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>CRF</td>\n",
    "<td class=\"best\">0.376</td>\n",
    "<td class=\"best\">0.319</td>\n",
    "<td class=\"best\">0.345</td>\n",
    "<td class=\"best\">0.298</td>\n",
    "<td>0.33</td>\n",
    "<td>0.313</td>\n",
    "<td class=\"best\">0.293</td>\n",
    "<td>0.231</td>\n",
    "<td>0.258</td>\n",
    "<td class=\"best\">0.305</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LSTM</td>\n",
    "<td>0.283</td>\n",
    "<td>0.288</td>\n",
    "<td>0.278</td>\n",
    "<td>0.293</td>\n",
    "<td>0.372</td>\n",
    "<td>0.328</td>\n",
    "<td>0.254</td>\n",
    "<td class=\"best\">0.27</td>\n",
    "<td class=\"best\">0.259</td>\n",
    "<td>0.288</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GRU</td>\n",
    "<td>0.287</td>\n",
    "<td>0.246</td>\n",
    "<td>0.263</td>\n",
    "<td>0.287</td>\n",
    "<td class=\"best\">0.405</td>\n",
    "<td class=\"best\">0.335</td>\n",
    "<td>0.252</td>\n",
    "<td>0.205</td>\n",
    "<td>0.216</td>\n",
    "<td>0.271</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"11\" class=\"separator\">w/o Normalization</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>CRF</td>\n",
    "<td>0.301</td>\n",
    "<td>0.278</td>\n",
    "<td>0.289</td>\n",
    "<td>0.276</td>\n",
    "<td>0.3</td>\n",
    "<td>0.287</td>\n",
    "<td>0.255</td>\n",
    "<td>0.23</td>\n",
    "<td>0.242</td>\n",
    "<td>0.273</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LSTM</td>\n",
    "<td>0.274</td>\n",
    "<td>0.252</td>\n",
    "<td>0.261</td>\n",
    "<td>0.284</td>\n",
    "<td>0.367</td>\n",
    "<td>0.32</td>\n",
    "<td>0.237</td>\n",
    "<td>0.241</td>\n",
    "<td>0.237</td>\n",
    "<td>0.273</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GRU</td>\n",
    "<td>0.266</td>\n",
    "<td>0.245</td>\n",
    "<td>0.252</td>\n",
    "<td>0.296</td>\n",
    "<td>0.369</td>\n",
    "<td>0.328</td>\n",
    "<td>0.232</td>\n",
    "<td>0.268</td>\n",
    "<td>0.245</td>\n",
    "<td>0.275</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In this chapter, I have compared two popular approaches to aspect-based sentiment analysis: **conditional random fields** and **recurrent neural networks**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **CRFs outperform RNNs on aspect-based sentiment analysis** (at least in limited-data settings);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **RNN approaches achieve their highest macro F$_1$-scores with the least-squares embeddings**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Even better prediction results can be attained by narrowing the sentiment spans only to polar terms.**  This, however, might negatively\n",
    "  affect the classification of targets and source;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Further improvements for CRFs can be obtained by redefining model's graph structure** (increasing the order of dependencies or performing inference over trees instead of linear sequences);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Text normalization improves the results of ABSA by up to 3%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 class=\"chapter-title animate\">Chapter V: Message-Level Sentiment Analysis</h1>\n",
    "<img src=\"img/funny-smiley.jpg\" alt=\"Funny Smiley\" id=\"funny-smiley\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/oezil.png\" alt=\"Oezil and Erdogan\" id=\"oezil\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"smileys\">\n",
    "<span class=\"emoticon\">ðŸ¥º</span> <span class=\"emoticon\">ðŸ˜</span> <span class=\"emoticon\">ðŸ˜Š</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **lexicon-based**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **machine-learning&ensp;based**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **deep-learning&ensp;based**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **macro-averaged F$_1$-score** over two main polarity classes (*positive* and *negative*);\n",
    "$$F_1 = \\frac{F_{pos} + F_{neg}}{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* and **micro-averaged F$_1$-score** over all three semantic orientations (*positive*, *negative*, and *neutral*), which essentially corresponds to the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "I pre-processed, tokenized, pos-tagged, and parsed all messages in the same way as I did in the previous chapter.  I again used the 70-10-20 split into training, development, and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Inference of Gold Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* I assigned the **positive (negative) label** to the microblogs that had **exclusively positive (negative) sentiments**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Messages that **did not have any sentiments, but had exclusively positive (negative) polar terms** were also assigned the respective label;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Tweets that featured **sentiments of both polarities** or had both **positive and negative polar terms** were considered as **mixed** and skipped from the experiments;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Finally, all **remaining microblogs** were regarded as **neutral**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Inference of Gold Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"example\">\n",
    "  <div class=\"title\">Example 5.1</div>\n",
    "  <div class=\"tweet\"><span class=\"label\">Tweet:</span><span class=\"sentiment\">Ich finde den Papst <span class=\"polar-term\">putzig</span><span class=\"tag\">polar-term</span> <span class=\"polar-term positive\">ðŸ™‚</span><span class=\"tag\">polar-term</span></span><span class=\"tag\">sentiment</span></div>\n",
    "  <div class=\"translation\"><span class=\"sentiment\">I find the pope <span class=\"polar-term\">cute</span><span class=\"tag\">polar-term</span> <span class=\"polar-term positive\" style=\"font-style: normal;\">ðŸ™‚</span><span class=\"tag\">polar-term</span></span><span class=\"tag\">sentiment</span></div>\n",
    "\n",
    "  <div class=\"label\"><span class=\"label\">Correct Label:</span><span class=\"gold-label positive\" style=\"padding-left: 0.4em;\">positive</span></div>\n",
    "  <div class=\"label\"><span class=\"label\">Inferred Label:</span><span class=\"predicted-label positive\">positive</span></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"example\">\n",
    "  <div class=\"title\">Example 5.2</div>\n",
    "  <div class=\"tweet\"><span class=\"label\">Tweet:</span><span class=\"sentiment\"><span class=\"polar-term\">typisch</span><span class=\"tag\">polar-term</span> Bayern kaum ist der neue Papst da und schon haben sie ihn <span class=\"polar-term\">in der Tasche</span><span class=\"tag\">polar-term</span>&#8230;</span><span class=\"tag\">sentiment</span></div>\n",
    "  <div class=\"translation\"><span class=\"sentiment\"><span class=\"polar-term\">typical</span><span class=\"tag\">polar-term</span> Bavaria The new Pope is hardly there, as they already have him <span class=\"polar-term\">in their pocket</span><span class=\"tag\">polar-term</span>&#8230;</span><span class=\"tag\">sentiment</span></div>\n",
    "\n",
    "  <div class=\"label\"><span class=\"label\">Correct Label:</span><span class=\"gold-label negative\" style=\"padding-left: 0.4em;\">negative</span></div>\n",
    "  <div class=\"label\"><span class=\"label\">Inferred Label:</span><span class=\"predicted-label negative\">negative</span></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Inference of Gold Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"example\">\n",
    "  <div class=\"title\">Example 5.3</div>\n",
    "<div class=\"tweet\"><span class=\"label\">Tweet:</span>Unser Park, unser Geld, unsere Stadt! -NICHT unser Finanzminister! <span class=\"polar-term\">ðŸ™‚</span><span class=\"tag\">polar-term</span> #schmid #spd #s21 #btw13</div>\n",
    "<div class=\"translation\">Our park, our money, our city! -NOT our Finance Minister! <span class=\"polar-term positive\" style=\"font-style: normal;\">ðŸ™‚</span><span class=\"tag\">polar-term</span> #schmid #spd #s21 #btw13</div>\n",
    "  <div class=\"label\"><span class=\"label\">Correct Label:</span><span class=\"gold-label negative\" style=\"padding-left: 0.4em;\">negative</span></div>\n",
    "  <div class=\"label\"><span class=\"label\">Inferred Label:</span><span class=\"predicted-label positive\">positive*</span></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"example\">\n",
    "  <div class=\"title\">Example 5.4</div>\n",
    "<div class=\"tweet\"><span class=\"label\">Tweet:</span>Auf die Lobby-FDP von heute kann Deutschland verzichten&#8230;</div>\n",
    "<div class=\"translation\">Germany can go without today's lobby FDP</div>\n",
    "  <div class=\"label\"><span class=\"label\">Correct Label:</span><span class=\"gold-label negative\" style=\"padding-left: 0.4em;\">negative</span></div>\n",
    "  <div class=\"label\"><span class=\"label\">Inferred Label:</span><span class=\"predicted-label neutral\">neutral*</span></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Data Preparation: SB10k\n",
    "\n",
    "**SB10k** is a corpus a total of **9,738 German microblogs**, which were **sampled from a snapshot of 5M tweets** gathered between August and November 2013. To\n",
    "ensure lexical diversity and proportional polarity distribution in this corpus, the authors first grouped all posts of this snapshot into **2,500 clusters** using $k$-means with unigram features, and then selected **tweets that contained at least one positive or one negative term from the German Polarity Clues** lexicon (Waltinger, 2010), letting three human experts annotate these microblogs with their message-level polarity (positive, negative, neutral, or mixed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "Unfortunately, due to the restrictions of Twitterâ€™s terms of use, I could only retrieve 7,476 tweets of this collection, which, however, still represents\n",
    "a substantial part of the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Data Statistics\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    " <caption>Polarity class distribution in PotTS and SB10k<br/>\n",
    "<em>&#42; â€“ the mixed polarity was excluded from our experiments</em></caption>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Dataset</td>\n",
    "<td colspan=\"4\">Polarity Class</td>\n",
    "<td colspan=\"2\">Label Agreement</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Positive</td>\n",
    "<td>Negative</td>\n",
    "<td>Neutral</td>\n",
    "<td>Mixed*</td>\n",
    "<td>$\\alpha$ </td>\n",
    "<td>$\\kappa$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>PotTS</td>\n",
    "<td>3,380</td>\n",
    "<td>1,541</td>\n",
    "<td>2,558</td>\n",
    "<td>513</td>\n",
    "<td>0.66</td>\n",
    "<td>0.4</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SB10k</td>\n",
    "<td>1,717</td>\n",
    "<td>1,130</td>\n",
    "<td>4,629</td>\n",
    "<td>0</td>\n",
    "<td>0.39</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<!--\n",
    "<tr>\n",
    "<td>GTS</td>\n",
    "<td>3,326,829</td>\n",
    "<td>350,775</td>\n",
    "<td>19,453,669</td>\n",
    "<td>73,776</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "-->\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Lexicon-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The most well-known lexicon-based systems are the methods of:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Hu and Liu** (2004);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Hu and Liu (2004) computed the overall polarity of a sentence by comparing the numbers of its positive and negative terms, reversing their orientation if they appeared in a negated context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Taboada et al.** (2011);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Finally, a seminal work on lexicon-based techniques was presented by Taboada et al. (2011), who introduced a manually compiled polarity list 2 and used this resource to estimate the overall semantic orientation of texts. Drawing on the ideas of Polanyi and Zaenen (2006), the authors incorporated a set of additional heuristic rules into their computation by changing the prior SO values of negated, intensified, and downtoned terms, ignoring irrealis and interrogative sentences, and adjusting the weights of specific document sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Musto et al.** (2014);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The authors first split the input message into a list of micro-phrases based on the occurrence of punctuation marks and conjunctions. Afterwards, they calculated the polarity score for each of these segments and finally estimated the overall polarity of the whole tweet by uniting the scores of its micro-phrases. Musto et al. obtained their best results (58.99% accuracy on the SemEval- 2013 dataset) with the normalized-emphasized approach, in which they averaged the polarity\n",
    "scores of segmentsâ€™ tokens, boosting these values by 50% for adjectives, adverbs, nouns, and verbs; and computed the final overall polarity of the microblog by taking the sum of all micro-phrase scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Jurek et al.** (2015);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Jurek et al computed the\n",
    "negative and positive polarity of a message (F p and F n respectively) as:\n",
    "\\begin{align}\n",
    "  \\small\n",
    "  \\begin{split}\n",
    "  F_P &= \\min\\left(\\frac{A_P}{2 - \\log(3.5\\times W_P + I_P)}, 100\\right),\\\\\n",
    "  F_N &= \\max\\left(\\frac{A_N}{2 - \\log(3.5\\times W_N + I_N)}, -100\\right);\\label{cgsa:eq:jurek}\n",
    "  \\end{split}\n",
    "\\end{align}%\n",
    "\n",
    "where A P and A N represent the average scores of positive and negative lexicon terms found\n",
    "in the tweet; W P and W N stand for the raw counts of polar tokens; and I P and I N denote\n",
    "the number of intensifiers preceding these words. In addition to that, before estimating the\n",
    "average values, the authors modified the polarity scores s w of all negated words w using the\n",
    "following rule:\n",
    "\\begin{align*}\n",
    "  \\small%\n",
    "neg(s_w) =\n",
    "    \\begin{cases}\n",
    "        \\min\\left(\\frac{s_w - 100}{2}, -10\\right) & \\text{if } s_w > 0,\\\\\n",
    "        \\max\\left(\\frac{s_w + 100}{2}, 10\\right), & \\text{if } s_w < 0.\n",
    "    \\end{cases}\n",
    "\\end{align*}%\n",
    "Furthermore, besides computing the polarity scores F p and F n , Jurek et al. also determined\n",
    "the subjectivity degree of the message by replacing the A P and A N terms in Equation 5.1 with\n",
    "the average of conditional probabilities of the tweet being subjective given the occurrences\n",
    "of the respective polar terms. 3 The authors considered a microblog as neutral if its absolute\n",
    "polarity was less than 25, and the subjectivity value was not greater than 0.5. Otherwise,\n",
    "they assigned a positive or negative label to this message depending on the sign of the\n",
    "polarity score. With this approach, Jurek et al. achieved an accuracy of 77.3% on the\n",
    "manually annotated subset of the Go et al.â€™s corpus and reached 74.2% on the IMDB review\n",
    "dataset (Maas et al., 2011)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Kolchyna et al.** (2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Kolchyna et al. (2015) explored two different ways of computing the overall polarity of a microblog: (i) by simply averaging the scores of all lexicon terms found in the message and (ii) by taking a signed logarithm of this average: The authors determined the final polarity of a tweet by using k-means clustering, which\n",
    "utilized both of the above polarity values as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Lexicon-Based Methods: Results\n",
    "\n",
    "<table>\n",
    "    <caption>Results of lexicon-based MLSA methods<br/>\n",
    "<em>HL &mdash; Hu and Liu (2004), TBD &mdash; Taboada et al. (2011), MST &mdash; Musto et al. (2014), JRK &mdash; Jurek\n",
    "et al. (2015), KLCH &mdash; Kolchyna et al. (2015)</em></caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1^{+/-}$</td>\n",
    "<td rowspan=\"2\">Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision </td>\n",
    "<td>Recall </td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision </td>\n",
    "<td>Recall </td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision </td>\n",
    "<td>Recall </td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">PotTS</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HL</td>\n",
    "<td>0.75</td>\n",
    "<td class=\"best\">0.76</td>\n",
    "<td class=\"best\">0.76</td>\n",
    "<td>0.53</td>\n",
    "<td>0.43</td>\n",
    "<td>0.47</td>\n",
    "<td>0.67</td>\n",
    "<td>0.73</td>\n",
    "<td>0.69</td>\n",
    "<td class=\"best\">0.615</td>\n",
    "<td class=\"best\">0.685</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TBD</td>\n",
    "<td class=\"best\">0.77</td>\n",
    "<td>0.71</td>\n",
    "<td>0.74</td>\n",
    "<td class=\"best\">0.54</td>\n",
    "<td>0.39</td>\n",
    "<td>0.45</td>\n",
    "<td>0.63</td>\n",
    "<td>0.77</td>\n",
    "<td>0.69</td>\n",
    "<td>0.597</td>\n",
    "<td>0.674</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MST</td>\n",
    "<td>0.75</td>\n",
    "<td>0.72</td>\n",
    "<td>0.74</td>\n",
    "<td>0.48</td>\n",
    "<td class=\"best\">0.47</td>\n",
    "<td class=\"best\">0.48</td>\n",
    "<td class=\"best\">0.68</td>\n",
    "<td>0.72</td>\n",
    "<td>0.7</td>\n",
    "<td>0.606</td>\n",
    "<td>0.675</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>JRK</td>\n",
    "<td>0.6</td>\n",
    "<td>0.31</td>\n",
    "<td>0.41</td>\n",
    "<td>0.42</td>\n",
    "<td>0.2</td>\n",
    "<td>0.27</td>\n",
    "<td>0.43</td>\n",
    "<td>0.8</td>\n",
    "<td>0.56</td>\n",
    "<td>0.339</td>\n",
    "<td>0.467</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>KLCH</td>\n",
    "<td>0.71</td>\n",
    "<td>0.72</td>\n",
    "<td>0.71</td>\n",
    "<td>0.34</td>\n",
    "<td>0.17</td>\n",
    "<td>0.22</td>\n",
    "<td>0.66</td>\n",
    "<td>0.82</td>\n",
    "<td>0.73</td>\n",
    "<td>0.468</td>\n",
    "<td>0.651</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">SB10k</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HL</td>\n",
    "<td class=\"best\">0.49</td>\n",
    "<td class=\"best\">0.62</td>\n",
    "<td class=\"best\">0.55</td>\n",
    "<td>0.27</td>\n",
    "<td>0.33</td>\n",
    "<td>0.3</td>\n",
    "<td class=\"best\">0.73</td>\n",
    "<td>0.62</td>\n",
    "<td>0.67</td>\n",
    "<td class=\"best\">0.421</td>\n",
    "<td>0.577</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TBD</td>\n",
    "<td>0.48</td>\n",
    "<td>0.6</td>\n",
    "<td>0.53</td>\n",
    "<td>0.24</td>\n",
    "<td>0.27</td>\n",
    "<td>0.25</td>\n",
    "<td>0.72</td>\n",
    "<td>0.63</td>\n",
    "<td>0.67</td>\n",
    "<td>0.393</td>\n",
    "<td>0.57</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MST</td>\n",
    "<td>0.45</td>\n",
    "<td>0.49</td>\n",
    "<td>0.47</td>\n",
    "<td>0.29</td>\n",
    "<td class=\"best\">0.35</td>\n",
    "<td class=\"best\">0.32</td>\n",
    "<td>0.7</td>\n",
    "<td>0.64</td>\n",
    "<td>0.67</td>\n",
    "<td>0.395</td>\n",
    "<td>0.568</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>JRK</td>\n",
    "<td>0.41</td>\n",
    "<td>0.39</td>\n",
    "<td>0.4</td>\n",
    "<td class=\"best\">0.36</td>\n",
    "<td>0.26</td>\n",
    "<td>0.3</td>\n",
    "<td>0.69</td>\n",
    "<td>0.75</td>\n",
    "<td>0.72</td>\n",
    "<td>0.351</td>\n",
    "<td>0.592</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>KLCH</td>\n",
    "<td>0.39</td>\n",
    "<td>0.22</td>\n",
    "<td>0.28</td>\n",
    "<td>0.34</td>\n",
    "<td>0.13</td>\n",
    "<td>0.19</td>\n",
    "<td>0.66</td>\n",
    "<td class=\"best\">0.86</td>\n",
    "<td class=\"best\">0.75</td>\n",
    "<td>0.235</td>\n",
    "<td class=\"best\">0.606</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Lexicon-Based Methods (An Error Made by the System of Taboada et al.)\n",
    "\n",
    "<div class=\"example\">\n",
    "Der beste Microsoft Knowledgebase-Artikel, den ich je gelesen habe.\n",
    "<div class=\"translation\">The best Microsoft-Knowledgebase article I've ever read.</div>\n",
    "Gold Label:<div class=\"label positive\">positive</div>\n",
    "Predicted Label:<div class=\"label neutral\">neutral*</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Lexicon-Based Methods (An Error Made by the System of Musto et al.)\n",
    "\n",
    "<div class=\"example\">\n",
    "Mensch Meier, Mensch Meier! Das sieht gut aus f&uuml;r die %User:\n",
    "<div class=\"translation\">Gosh Meier, Gosh Meier! It looks good for the %User:</div>\n",
    "Gold Label:<div class=\"label positive\">positive</div>\n",
    "Predicted Label:<div class=\"label neutral\">neutral*</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Lexicon-Based Methods (An Error Made by the System of Jurek et al.)\n",
    "\n",
    "<div class=\"example\">\n",
    "Normal bin ich ja nicht der mensch dwer sich beschwert wegen dem essen aber diese Pizza von Joeys&8230; boah wie ekelhaft\n",
    "<div class=\"translation\">Normally I'm not a person who complains about food but  this pizza from Joeys&8230; Boah it's so disgusting</div>\n",
    "Gold Label:<div class=\"label negative\">negative</div>\n",
    "Predicted Label:<div class=\"label positive\">positive*</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Polarity Changing Factors\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Effect of polarity-changing factors on lexicon-based MLSA methods\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"3\">Polarity-Changing Factors</td>\n",
    "<td colspan=\"10\">System Scores</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"2\">HL</td>\n",
    "<td colspan=\"2\">TBD</td>\n",
    "<td colspan=\"2\">MST</td>\n",
    "<td colspan=\"2\">JRK</td>\n",
    "<td colspan=\"2\">KLCH</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> Macro<br/>F$_1$</td>\n",
    "<td> Micro<br/>F$_1$</td>\n",
    "<td> Macro<br/>F$_1$</td>\n",
    "<td> Micro<br/>F$_1$</td>\n",
    "<td> Macro<br/>F$_1$</td>\n",
    "<td> Micro<br/>F$_1$</td>\n",
    "<td> Macro<br/>F$_1$</td>\n",
    "<td> Micro<br/>F$_1$</td>\n",
    "<td> Macro<br/>F$_1$</td>\n",
    "<td> Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"11\" class=\"separator\">\n",
    "PotTS\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>All</td>\n",
    "<td>0.615</td>\n",
    "<td>0.685</td>\n",
    "<td>0.593</td>\n",
    "<td>0.671</td>\n",
    "<td>0.606</td>\n",
    "<td>0.675</td>\n",
    "<td>0.339</td>\n",
    "<td>0.467</td>\n",
    "<td>0.468</td>\n",
    "<td>0.651</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Negation</td>\n",
    "<td>0.622</td>\n",
    "<td class=\"best\">0.691</td>\n",
    "<td>0.596</td>\n",
    "<td>0.672</td>\n",
    "<td class=\"best\">0.641</td>\n",
    "<td>0.7</td>\n",
    "<td>0.357</td>\n",
    "<td>0.473</td>\n",
    "<td>0.298</td>\n",
    "<td>0.463</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Intensification</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.595</td>\n",
    "<td>0.672</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.339</td>\n",
    "<td>0.467</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Other Modifiers</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.613</td>\n",
    "<td>0.684</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"11\" class=\"separator\">\n",
    "SB10k\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>All</td>\n",
    "<td class=\"best\">0.421</td>\n",
    "<td>0.577</td>\n",
    "<td>0.392</td>\n",
    "<td>0.569</td>\n",
    "<td>0.395</td>\n",
    "<td>0.568</td>\n",
    "<td>0.351</td>\n",
    "<td>0.592</td>\n",
    "<td>0.235</td>\n",
    "<td>0.606</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Negation</td>\n",
    "<td>0.415</td>\n",
    "<td>0.576</td>\n",
    "<td>0.395</td>\n",
    "<td>0.572</td>\n",
    "<td>0.381</td>\n",
    "<td>0.559</td>\n",
    "<td>0.316</td>\n",
    "<td>0.586</td>\n",
    "<td>0.218</td>\n",
    "<td class=\"best\">0.609</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Intensification</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.4</td>\n",
    "<td>0.576</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.352</td>\n",
    "<td>0.59</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Other Modifiers</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.406</td>\n",
    "<td>0.566</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ML-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Gamon (2004)**\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "   <ul>\n",
    "  <li>part-of-speech trigrams,</li>\n",
    "  <li>context-free phrase-structure patterns,</li>\n",
    "  <li>and part-of-speech information coupled with syntactic relations;</li>\n",
    "   </ul>\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "who used an SVM classifier with linguistic and surface-level features, such as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Mohammad et al. (2013)**:\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "  <ul>\n",
    "    <li>character and token $n$-grams;</li>\n",
    "    <li>Brown clusters (Brown et al., 1992);</li>\n",
    "    <li>various statistics on part-of-speech tags, punctuation marks, and elongated words;</li>\n",
    "    <li>numerous sentiment-lexicon features, which were extracted from both manual and automatic lexicons;</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **G&uuml;nther and Furrer (2013)**:\n",
    "\n",
    " <ul>\n",
    "    <li>original and lemmatized unigrams;</li>\n",
    "    <li>word clusters;</li>\n",
    "    <li>various statistics on part-of-speech tags, punctuation marks, and elongated words;</li>\n",
    "    <li>numerous sentiment-lexicon features, which were extracted from both manual and automatic lexicons;</li>\n",
    "  </ul>\n",
    "  * original and lemmatized unigrams;\n",
    "  * word clusters;\n",
    "  * and lexicon features (only SentiWordNet [Esuli and Sebastiani, 2005])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ML-Based Methods: Results\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Results of machine-learning&ndash;based MLSA methods<br/>\n",
    "<em>GMN &mdash; Gamon (2004), MHM &mdash; Mohammad et al. (2013), GNT &mdash; GÃ¼nther et al. (2014)</em>\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1$</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">\n",
    "PotTS\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GMN</td>\n",
    "<td>0.67</td>\n",
    "<td>0.73</td>\n",
    "<td>0.7</td>\n",
    "<td>0.35</td>\n",
    "<td>0.15</td>\n",
    "<td>0.21</td>\n",
    "<td>0.6</td>\n",
    "<td>0.72</td>\n",
    "<td>0.66</td>\n",
    "<td>0.453</td>\n",
    "<td>0.617</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MHM</td>\n",
    "<td class=\"best\">0.79</td>\n",
    "<td>0.77</td>\n",
    "<td class=\"best\">0.78</td>\n",
    "<td class=\"best\">0.58</td>\n",
    "<td class=\"best\">0.56</td>\n",
    "<td class=\"best\">0.57</td>\n",
    "<td class=\"best\">0.73</td>\n",
    "<td class=\"best\">0.76</td>\n",
    "<td class=\"best\">0.74</td>\n",
    "<td class=\"best\">0.674</td>\n",
    "<td class=\"best\">0.727</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GNT</td>\n",
    "<td>0.71</td>\n",
    "<td class=\"best\">0.8</td>\n",
    "<td>0.75</td>\n",
    "<td>0.55</td>\n",
    "<td>0.45</td>\n",
    "<td>0.5</td>\n",
    "<td>0.68</td>\n",
    "<td>0.63</td>\n",
    "<td>0.65</td>\n",
    "<td>0.624</td>\n",
    "<td>0.673</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"12\"  class=\"separator\">\n",
    "SB10k\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GMN</td>\n",
    "<td>0.65</td>\n",
    "<td>0.45</td>\n",
    "<td>0.53</td>\n",
    "<td>0.38</td>\n",
    "<td>0.08</td>\n",
    "<td>0.13</td>\n",
    "<td>0.72</td>\n",
    "<td class=\"best\">0.93</td>\n",
    "<td>0.81</td>\n",
    "<td>0.329</td>\n",
    "<td>0.699</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MHM</td>\n",
    "<td class=\"best\">0.71</td>\n",
    "<td class=\"best\">0.65</td>\n",
    "<td class=\"best\">0.68</td>\n",
    "<td class=\"best\">0.51</td>\n",
    "<td class=\"best\">0.4</td>\n",
    "<td class=\"best\">0.45</td>\n",
    "<td class=\"best\">0.8</td>\n",
    "<td>0.87</td>\n",
    "<td class=\"best\">0.84</td>\n",
    "<td class=\"best\">0.564</td>\n",
    "<td class=\"best\">0.752</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GNT</td>\n",
    "<td>0.67</td>\n",
    "<td>0.62</td>\n",
    "<td>0.64</td>\n",
    "<td>0.44</td>\n",
    "<td>0.28</td>\n",
    "<td>0.34</td>\n",
    "<td>0.78</td>\n",
    "<td>0.87</td>\n",
    "<td>0.82</td>\n",
    "<td>0.491</td>\n",
    "<td>0.724</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## ML-Based Methods (An Error Made by the System of Mohammad et al.)\n",
    "\n",
    "<div class=\"example\">\n",
    "das klingt richtig gut! Was f&uuml;r eine hast du denn? (uvu) %PosSmiley3\n",
    "<div class=\"translation\">It sounds really great. Which one do you have? (uvu) %PosSmiley3</div>\n",
    "Gold Label:<div class=\"label positive\">positive</div>\n",
    "Predicted Label:<div class=\"label neutral\">neutral*</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Top-ranking features of this example:\n",
    "\n",
    "1. &#42; (neutral): 0.131225868029;\n",
    "2. &#42; (negative): -0.0840804221845;\n",
    "3. %PoS-CARD (neutral): 0.0833658576233;\n",
    "4. %PoS-ADJD (neutral): -0.069745190018;\n",
    "5. t-â£-n (positive): 0.0556721202587."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## ML-Based Methods (An Error Made by the System of G&uuml;nther et al.)\n",
    "\n",
    "<div class=\"example\">\n",
    "Den CDU-WÃ¤hlern traue ich durchaus zu der FDP 8 bis 9% zu bescheren! Die sind so borniert, nicht nur in Niedersachsen!\n",
    "<div class=\"translation\">I don't put giving 8 to 9% to the FDP past the CDU-voters! They are so narrow-minded, not only in Lower Saxony!</div>\n",
    "Gold Label:<div class=\"label negative\">negative</div>\n",
    "Predicted Label:<div class=\"label positive\">positive*</div>\n",
    "</div>\n",
    "\n",
    "The resason for this misclassification is the prevalence of many general features (e.g., 8, nicht-nur_NEG, nur_NEG, etc.) and their strong bias towards the majority class in the PotTS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ML-Based Methods: Top-10 Features\n",
    "\n",
    "<table>\n",
    "    <caption>\n",
    "       Top-10 features learned by ML-based MLSA methods<br />\n",
    "<em>sorted by the absolute values of their weights</em>\n",
    "    </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Rank</td>\n",
    "<td colspan=\"3\">GMN</td>\n",
    "<td colspan=\"3\">MHM</td>\n",
    "<td colspan=\"3\">GNT</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Feature</td>\n",
    "<td>Label</td>\n",
    "<td>Weight</td>\n",
    "<td>Feature</td>\n",
    "<td>Label</td>\n",
    "<td>Weight</td>\n",
    "<td>Feature</td>\n",
    "<td>Label</td>\n",
    "<td>Weight</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>NK-ITJ|</td>\n",
    "<td>POS</td>\n",
    "<td>0.457</td>\n",
    "<td>*</td>\n",
    "<td>NEUT</td>\n",
    "<td>0.131</td>\n",
    "<td>hate</td>\n",
    "<td>NEG</td>\n",
    "<td>1.86 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>DM-ITJ|</td>\n",
    "<td>POS</td>\n",
    "<td>0.334</td>\n",
    "<td>Last-%QMark-Cnt</td>\n",
    "<td>NEUT</td>\n",
    "<td>0.088</td>\n",
    "<td>sick</td>\n",
    "<td>NEG</td>\n",
    "<td>1.7</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3</td>\n",
    "<td>V-DM-I</td>\n",
    "<td>POS</td>\n",
    "<td>0.244</td>\n",
    "<td>s-c</td>\n",
    "<td>NEG</td>\n",
    "<td>0.079</td>\n",
    "<td>kahretsinn</td>\n",
    "<td>NEG</td>\n",
    "<td>1.69</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td>\n",
    "<td>N-NK-I</td>\n",
    "<td>POS</td>\n",
    "<td>0.24</td>\n",
    "<td>*-%possmiley</td>\n",
    "<td>POS</td>\n",
    "<td>0.067</td>\n",
    "<td>dasisaberschade</td>\n",
    "<td>NEG</td>\n",
    "<td>1.69</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>5</td>\n",
    "<td>MO-ITJ|</td>\n",
    "<td>POS</td>\n",
    "<td>0.211</td>\n",
    "<td>c-h-e-i-s</td>\n",
    "<td>NEG</td>\n",
    "<td>0.064</td>\n",
    "<td>Anziehen</td>\n",
    "<td>POS</td>\n",
    "<td>1.67</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>6</td>\n",
    "<td>A-DM-I</td>\n",
    "<td>POS</td>\n",
    "<td>0.196</td>\n",
    "<td>h-a-h</td>\n",
    "<td>POS</td>\n",
    "<td>0.064</td>\n",
    "<td>&#7452;</td>\n",
    "<td>POS</td>\n",
    "<td>1.65</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>7</td>\n",
    "<td>A-MO-I</td>\n",
    "<td>POS</td>\n",
    "<td>0.191</td>\n",
    "<td>t-&blank;-.</td>\n",
    "<td>NEG</td>\n",
    "<td>0.064</td>\n",
    "<td>p&auml;rchenabend</td>\n",
    "<td>POS</td>\n",
    "<td>1.65</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>8</td>\n",
    "<td>NK-ITJ</td>\n",
    "<td>POS</td>\n",
    "<td>0.165</td>\n",
    "<td>geil</td>\n",
    "<td>POS</td>\n",
    "<td>0.062</td>\n",
    "<td>derienâ¤ï¸â¤ï¸</td>\n",
    "<td>POS</td>\n",
    "<td>1.65</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>9</td>\n",
    "<td>NK-$.</td>\n",
    "<td>NEUT</td>\n",
    "<td>0.16</td>\n",
    "<td>*-?</td>\n",
    "<td>NEUT</td>\n",
    "<td>0.062</td>\n",
    "<td>sch&ouml;n-nicht</td>\n",
    "<td>POS</td>\n",
    "<td>1.56</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>10</td>\n",
    "<td>DM-ITJ</td>\n",
    "<td>POS</td>\n",
    "<td>0.157</td>\n",
    "<td>?</td>\n",
    "<td>NEUT</td>\n",
    "<td>0.061</td>\n",
    "<td>applause</td>\n",
    "<td>POS</td>\n",
    "<td>1.5</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ML-Based Methods: Feature Ablation Test\n",
    "\n",
    "<table id=\"cgsa-ml-feature-ablation\">\n",
    "<caption>\n",
    "Results of the feature-ablation test for ML-based MLSA methods\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"3\">Features</td>\n",
    "<td colspan=\"6\">System Scores</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"2\">GMN</td>\n",
    "<td colspan=\"2\">MHM</td>\n",
    "<td colspan=\"2\">GNT</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Macro<br/>F$_1^{+/-}$</td>\n",
    "<td>Micro<br/>F$_1$</td>\n",
    "<td>Macro<br/>F$_1^{+/-}$</td>\n",
    "<td>Micro<br/>F$_1$</td>\n",
    "<td>Macro<br/>F$_1^{+/-}$</td>\n",
    "<td>Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"7\" class=\"separator\">\n",
    "PotTS\n",
    "</td>\n",
    "</tr>   \n",
    "<tr>\n",
    "<td>All</td>\n",
    "<td class=\"best\">0.453</td>\n",
    "<td class=\"best\">0.617</td>\n",
    "<td class=\"best\">0.674</td>\n",
    "<td>0.727</td>\n",
    "<td class=\"best\">0.624</td>\n",
    "<td>0.673</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Constituents</td>\n",
    "<td>0.388</td>\n",
    "<td>0.545</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;PoS Tags</td>\n",
    "<td>0.417</td>\n",
    "<td>0.607</td>\n",
    "<td>0.669</td>\n",
    "<td>0.721</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Character Features</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.671</td>\n",
    "<td class=\"best\">0.734</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Token Features</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.659</td>\n",
    "<td>0.704</td>\n",
    "<td>0.0</td>\n",
    "<td>0.366</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Automatic Lexicons</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.667</td>\n",
    "<td>0.717</td>\n",
    "<td>0.613</td>\n",
    "<td>0.666</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Manual Lexicons</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.665</td>\n",
    "<td>0.715</td>\n",
    "<td>0.617</td>\n",
    "<td class=\"best\">0.675</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"7\" class=\"separator\">\n",
    "SB10k\n",
    "</td>\n",
    "<tr>\n",
    "<tr>\n",
    "<td>All</td>\n",
    "<td class=\"best\">0.329</td>\n",
    "<td>0.699</td>\n",
    "<td>0.564</td>\n",
    "<td>0.752</td>\n",
    "<td>0.491</td>\n",
    "<td>0.724</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Constituents</td>\n",
    "<td>0.127</td>\n",
    "<td>0.646</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;PoS Tags</td>\n",
    "<td>0.301</td>\n",
    "<td class=\"NA\">0.7</td>\n",
    "<td class=\"NA\">0.57</td>\n",
    "<td class=\"NA\">0.757</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Character Features</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.546</td>\n",
    "<td>0.753</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Token Features</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.559</td>\n",
    "<td>0.741</td>\n",
    "<td>0.046</td>\n",
    "<td>0.62</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Automatic Lexicons</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.54</td>\n",
    "<td>0.753</td>\n",
    "<td class=\"NA\">0.517</td>\n",
    "<td>0.735</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>&ndash;Manual Lexicons</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.553</td>\n",
    "<td>0.751</td>\n",
    "<td>0.51</td>\n",
    "<td class=\"NA\">0.739</td>\n",
    "    </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ML-Based Methods: Classifiers\n",
    "\n",
    "<table>\n",
    "    <caption>\n",
    "       Results of ML-based MLSA methods with different classifiers\n",
    "    </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"3\">Classifier</td>\n",
    "<td colspan=\"6\">System Scores</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"2\">GMN</td>\n",
    "<td colspan=\"2\">MHM</td>\n",
    "<td colspan=\"2\">GNT</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Macro<br/>F$_1^{+/-}$</td>\n",
    "<td>Micro<br/>F$_1$</td>\n",
    "<td>Macro<br/>F$_1^{+/-}$</td>\n",
    "<td>Micro<br/>F$_1$</td>\n",
    "<td>Macro<br/>F$_1^{+/-}$</td>\n",
    "<td>Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"7\" class=\"separator\">\n",
    "PotTS\n",
    "</td>\n",
    "</tr>   \n",
    "<tr>\n",
    "<td>SVM</td>\n",
    "<td class=\"best\">0.453</td>\n",
    "<td class=\"best\">0.617</td>\n",
    "<td>0.674</td>\n",
    "<td>0.727</td>\n",
    "<td class=\"best\">0.624</td>\n",
    "<td>0.673</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Naive Bayes</td>\n",
    "<td>0.432</td>\n",
    "<td>0.577</td>\n",
    "<td>0.635</td>\n",
    "<td>0.675</td>\n",
    "<td>0.567</td>\n",
    "<td>0.59</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Logistic Regression</td>\n",
    "<td>0.431</td>\n",
    "<td>0.612</td>\n",
    "<td class=\"best\">0.677</td>\n",
    "<td class=\"best\">0.741</td>\n",
    "<td class=\"best\">0.624</td>\n",
    "<td class=\"best\">0.688</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"7\" class=\"separator\">\n",
    "SB10k\n",
    "</td>\n",
    "</tr>   \n",
    "<tr>\n",
    "<td>SVM</td>\n",
    "<td>0.329</td>\n",
    "<td class=\"best\">0.699</td>\n",
    "<td class=\"best\">0.564</td>\n",
    "<td>0.752</td>\n",
    "<td>0.491</td>\n",
    "<td>0.724</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Naive Bayes</td>\n",
    "<td class=\"best\">0.351</td>\n",
    "<td>0.637</td>\n",
    "<td>0.516</td>\n",
    "<td>0.755</td>\n",
    "<td>0.453</td>\n",
    "<td>0.675</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Logistic Regression</td>\n",
    "<td>0.309</td>\n",
    "<td>0.693</td>\n",
    "<td>0.553</td>\n",
    "<td class=\"best\">0.772</td>\n",
    "<td class=\"best\">0.512</td>\n",
    "<td class=\"best\">0.75</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DL-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* the **matrix-space** approach (Yessenalina and Cardie, 2011);\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "$$\\xi = \\vec{u}^\\top\\left(\\prod_{j=1}^{|x|}W_{w_j}\\right)\\vec{v},$$\n",
    "where $W_{w_j}\\in\\mathbb{R}^{m\\times{}m}$ is a matrix representation of word $w_j$;\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **deep recursive autoencoder** (**RAE**) (Socher et al., 2011):\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "\n",
    "    $$\\vec{w}_p = softmax\\left(W\\begin{bmatrix}\n",
    "      \\vec{w}_l\\\\\n",
    "      \\vec{w}_r\n",
    "  \\end{bmatrix}\\right),$$\n",
    "  where $\\vec{w}_p\\in\\mathbb{R}^n$ stands for the embedding of the parent node; $\\vec{w}_l$ and $\\vec{w}_r$ represent the embeddings of its left and right dependents; and $W\\in\\mathbb{R}^{n\\times{}2n}$ is a model parameter.\n",
    "    \n",
    "  <img src=\"img/rae.png\" alt=\"Recursive Auto-Encoder\" id=\"rae\"/>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **matrix-vector recursive neural network** (**MVRNN**) (Socher et al., 2012):\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "    \n",
    " $$\\vec{w}_p = tanh\\left(W_v \\begin{bmatrix}W_r\\vec{w}_l\\\\\n",
    "    W_l \\vec{w}_r\\end{bmatrix} \\right),\\\\\n",
    "    W_p = W_m \\begin{bmatrix}W_l;\\\\\n",
    "    W_r\\end{bmatrix};$$\n",
    "    \n",
    "  where, in addition to the previously defined word vectors $\\vec{w}_p$, $\\vec{w}_l$, and $\\vec{w}_r$, the authors introduced additional matrix parameters for each   word ($W_p$, $W_l$, $W_r$);\n",
    "\n",
    "  <img src=\"img/mvrnn.png\" alt=\"Matrix-Vector Recursive Neural Network\" id=\"mvrnn\"/>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **recursive neural tensor network** (**RNTN**) (Socher et al., 2013):\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "  $$\\vec{w}_p = softmax\\left(\\begin{bmatrix}\n",
    "  \\vec{w}_l\\\\\n",
    "  \\vec{w}_r\n",
    "  \\end{bmatrix}^{\\top}V^{[1:d]}\\begin{bmatrix}\n",
    "  \\vec{w}_l\\\\\n",
    "  \\vec{w}_r\n",
    "  \\end{bmatrix}\n",
    "            + W\\begin{bmatrix}\n",
    "  \\vec{w}_l\\\\\n",
    "  \\vec{w}_r\n",
    "  \\end{bmatrix}\\right),\n",
    "  $$\n",
    "  where $\\vec{w}_p$, $\\vec{w}_l$, $\\vec{w}_r$, and $W$ are defined as before, and $V$ is a $2n\\times{}2n\\times{}n$-dimensional tensor;\n",
    "\n",
    "  <img src=\"img/rntn.png\" alt=\"Recursive Neural Tensor Network\" id=\"rntn\"/>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* a **convolutional system** by **Severyn and Moschitti** (2015);\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "  \n",
    "    <img src=\"img/severyn-moschitti.png\" alt=\"Severyn Moschitti CNN\" id=\"severyn-moschitti\"/>   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "which was the winner of the SemEval task in 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* an **attention-based system** by **Baziotis et al.** (2017):\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "\n",
    "    <img src=\"img/baziotis.png\" alt=\"Architecture of the neural network proposed by Baziotis et al. (2017)\" id=\"baziotis\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* my own **lexicon-based attention system**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Lexicon-Based Attention\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/baziotis.png\" alt=\"Architecture of the neural network proposed by Baziotis et al. (2017)\" id=\"baziotis2\"/>\n",
    "<figcaption>\n",
    "Architecture of the neural network proposed by Baziotis et al. (2017)\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "The attention coefficients in this model are computed as follows:\n",
    "$$\\vec{a} = \\sum_{i=1}^{|\\mathbf{x}|}a_i\\vec{h}_i,$$\n",
    "where\n",
    "$$a_i =\\frac{\\exp(e_i)}{\\sum_{j=1}^{|\\mathbf{x}|}\\exp(e_j)},$$\n",
    "such that\n",
    "$$e_i = tanh\\left(\\vec{\\alpha}\\vec{h}^{(2)}_i + \\beta_i\\right)$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **positional attention** $\\vec{a}$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **lexicon-based attention** $\\vec{b}$;\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "which is computed as:\n",
    "$$\\vec{b} = \\sum_{i=1}^{|\\mathbf{x}|}b_i\\vec{h}_i,$$\n",
    "where $b_i = \\frac{\\exp(f_i)}{\\sum_{j=1}^{|\\mathbf{x}|}\\exp(f_j)}$, such that $f_i = \\left\\{\n",
    "  \\begin{array}{ll}\n",
    "    \\tanh(abs(V[{w_i}]) + \\epsilon) & \\textrm{ if } w_i\\in V\\\\\n",
    "    \\tanh(\\epsilon) & \\, \\textrm{otherwise.} \\\\\n",
    "  \\end{array}\n",
    "  \\right.$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **context-based attention** $\\vec{c}$.\n",
    "\n",
    "$$ \\vec{c} = \\sum_{i=1}^{|\\mathbf{x}|}c_i\\vec{h}_i,$$\n",
    "where $c_i =\\frac{\\exp(g_i)}{\\sum_{j=1}^{|\\mathbf{x}|}\\exp(g_j)},$ such that $g_i =\\tanh\\left(C [\\vec{w}_i, \\vec{b}_p]^\\top\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The $C$ term in this expression represents a model parameter (context matrix), and the $\\vec{b}_p$ value denotes the output of the lexicon-based attention produced for word $p$ (the syntactic parent of the $i$-th word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Lexicon-Based Attention\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/lba.png\" alt=\"Architecture of the neural network with lexicon- and context-based attention\">\n",
    "<figcaption>\n",
    "Architecture of the neural network with lexicon- and context-based attention\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DL-Based Methods: Results\n",
    "\n",
    "<table class=\"cgsa-dl-results\">\n",
    "<caption>\n",
    "Results of deep-learning&mdash;based MLSA methods<br/>\n",
    "<em>Y&amp;C &mdash; Yessenalina and Cardie (2011), RAE &mdash; Recursive Auto-Encoder (Socher et al., 2011),\n",
    "MVRNN &mdash; Matrix-Vector RNN (Socher et al., 2012), RNTN &mdash; Recursive Neural-Tensor Network\n",
    "(Socher et al., 2013), SEV &mdash; Severyn and Moschitti (2015b), BAZ &mdash; Baziotis et al. (2017),\n",
    "LBA (1) &mdash; lexicon-based attention with one Bi-LSTM layer, LBA (2) &mdash; lexicon-based attention with\n",
    "two Bi-LSTM layers</em>\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1^{+/-}$</td>\n",
    "<td rowspan=\"2\">Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">\n",
    "PotTS\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Y&amp;C</td>\n",
    "<td>0.45</td>\n",
    "<td class=\"best\">1.0</td>\n",
    "<td>0.62</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.308</td>\n",
    "<td>0.446</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.64</td>\n",
    "<td>0.78</td>\n",
    "<td>0.7</td>\n",
    "<td>0.38</td>\n",
    "<td>0.04</td>\n",
    "<td>0.08</td>\n",
    "<td>0.57</td>\n",
    "<td>0.68</td>\n",
    "<td>0.62</td>\n",
    "<td>0.389</td>\n",
    "<td>0.605</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MVRNN</td>\n",
    "<td>0.45</td>\n",
    "<td class=\"best\">1.0</td>\n",
    "<td>0.62</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.308</td>\n",
    "<td>0.446</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RNTN</td>\n",
    "<td>0.45</td>\n",
    "<td>0.87</td>\n",
    "<td>0.59</td>\n",
    "<td>0.19</td>\n",
    "<td>0.02</td>\n",
    "<td>0.03</td>\n",
    "<td>0.32</td>\n",
    "<td>0.1</td>\n",
    "<td>0.15</td>\n",
    "<td>0.312</td>\n",
    "<td>0.428</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.73</td>\n",
    "<td>0.79</td>\n",
    "<td>0.76</td>\n",
    "<td class=\"best\">0.41</td>\n",
    "<td class=\"best\">0.52</td>\n",
    "<td class=\"best\">0.46</td>\n",
    "<td class=\"best\">0.72</td>\n",
    "<td>0.55</td>\n",
    "<td>0.62</td>\n",
    "<td class=\"best\">0.608</td>\n",
    "<td>0.651</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.45</td>\n",
    "<td class=\"best\">1.0</td>\n",
    "<td>0.62</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.308</td>\n",
    "<td>0.446</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td class=\"best\">0.82</td>\n",
    "<td>0.73</td>\n",
    "<td class=\"best\">0.77</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.56</td>\n",
    "<td class=\"best\">0.92</td>\n",
    "<td class=\"best\">0.69</td>\n",
    "<td>0.387</td>\n",
    "<td class=\"best\">0.662</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.45</td>\n",
    "<td class=\"best\">1.0</td>\n",
    "<td>0.62</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.308</td>\n",
    "<td>0.446</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">\n",
    "SB10k\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Y&amp;C</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.62</td>\n",
    "<td class=\"best\">1.0</td>\n",
    "<td>0.77</td>\n",
    "<td>0.0</td>\n",
    "<td>0.622</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.63</td>\n",
    "<td>0.57</td>\n",
    "<td>0.6</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td class=\"best\">0.75</td>\n",
    "<td>0.94</td>\n",
    "<td>0.83</td>\n",
    "<td>0.299</td>\n",
    "<td>0.721</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MVRNN</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.62</td>\n",
    "<td class=\"best\">1.0</td>\n",
    "<td>0.77</td>\n",
    "<td>0.0</td>\n",
    "<td>0.622</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RNTN</td>\n",
    "<td>0.2</td>\n",
    "<td>0.03</td>\n",
    "<td>0.05</td>\n",
    "<td>0.07</td>\n",
    "<td>0.01</td>\n",
    "<td>0.02</td>\n",
    "<td>0.62</td>\n",
    "<td>0.94</td>\n",
    "<td>0.75</td>\n",
    "<td>0.033</td>\n",
    "<td>0.594</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.62</td>\n",
    "<td class=\"best\">1.0</td>\n",
    "<td>0.77</td>\n",
    "<td>0.0</td>\n",
    "<td>0.622</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.75</td>\n",
    "<td>0.47</td>\n",
    "<td>0.58</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.71</td>\n",
    "<td>0.98</td>\n",
    "<td>0.83</td>\n",
    "<td>0.291</td>\n",
    "<td>0.72</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.72</td>\n",
    "<td class=\"best\">0.58</td>\n",
    "<td class=\"best\">0.64</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.74</td>\n",
    "<td>0.97</td>\n",
    "<td class=\"best\">0.84</td>\n",
    "<td class=\"best\">0.321</td>\n",
    "<td class=\"best\">0.737</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td class=\"best\">0.76</td>\n",
    "<td>0.49</td>\n",
    "<td>0.6</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.72</td>\n",
    "<td>0.98</td>\n",
    "<td>0.83</td>\n",
    "<td>0.298</td>\n",
    "<td>0.723</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## DL-Based Methods (An Error Made by the System of Baziotis et al.)\n",
    "\n",
    "<div class=\"example\">\n",
    "Wollte meinen Kleiderschrank aufr&auml;umen ... sitze nun darin und singe Liebeslieder ...\n",
    "<div class=\"translation\">Wanted to clean up my wardrobe... Now sitting in it and singing love songs...</div>\n",
    "Gold Label:<div class=\"label neutral\">neutral</div>\n",
    "Predicted Label:<div class=\"label positive\">positive*</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## DL-Based Methods (An Error Made by the LBA System)\n",
    "\n",
    "<div class=\"example\">\n",
    "Gerade super Lust, mit Carls Haaren was zu machen aber ca 300 km Distanz halten mich davon ab.\n",
    "<div class=\"translation\">Wanted to clean up my wardrobe... Now sitting in it and singing love songs...</div>\n",
    "Gold Label:<div class=\"label neutral\">neutral</div>\n",
    "Predicted Label:<div class=\"label positive\">positive*</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DL-Based Methods: Word2Vec\n",
    "\n",
    "<table class=\"alternative-embeddings-cgsa\">\n",
    "<caption>\n",
    "Results of deep-learning&ndash;based MLSA methods with pretrained word2vec vectors\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1^{+/-}$</td>\n",
    "<td rowspan=\"2\">Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">\n",
    "PotTS\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.58<span class=\"negdelta\">0.06</span></td>\n",
    "<td>0.74<span class=\"negdelta\">0.04</span></td>\n",
    "<td>0.65<span class=\"negdelta\">0.05</span></td>\n",
    "<td>0.34<span class=\"negdelta\">0.04</span></td>\n",
    "<td>0.26<span class=\"posdelta\">0.22</span></td>\n",
    "<td>0.29<span class=\"posdelta\">0.21</span></td>\n",
    "<td>0.59<span class=\"posdelta\">0.02</span></td>\n",
    "<td>0.46<span class=\"negdelta\">0.22</span></td>\n",
    "<td>0.52<span class=\"negdelta\">0.1</span></td>\n",
    "<td>0.47<span class=\"posdelta\">0.08</span></td>\n",
    "<td>0.55<span class=\"negdelta\">0.06</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RNTN</td>\n",
    "<td>0.48<span class=\"posdelta\">0.03</span></td>\n",
    "<td>0.77<span class=\"negdelta\">0.1</span></td>\n",
    "<td>0.59</td>\n",
    "<td>0.33<span class=\"posdelta\">0.14</span></td>\n",
    "<td>0.03<span class=\"posdelta\">0.01</span></td>\n",
    "<td>0.06<span class=\"posdelta\">0.03</span></td>\n",
    "<td>0.46<span class=\"posdelta\">0.14</span></td>\n",
    "<td>0.33<span class=\"posdelta\">0.23</span></td>\n",
    "<td>0.38<span class=\"posdelta\">0.01</span></td>\n",
    "<td>0.33<span class=\"posdelta\">0.02</span></td>\n",
    "<td>0.47<span class=\"posdelta\">0.04</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.69<span class=\"negdelta\">0.04</span></td>\n",
    "<td>0.74<span class=\"negdelta\">0.05</span></td>\n",
    "<td>0.72<span class=\"negdelta\">0.04</span></td>\n",
    "<td>0.0<span class=\"negdelta\">0.41</span></td>\n",
    "<td>0.0<span class=\"negdelta\">0.52</span></td>\n",
    "<td>0.0<span class=\"negdelta\">0.46</span></td>\n",
    "<td>0.58<span class=\"negdelta\">0.14</span></td>\n",
    "<td>0.84<span class=\"posdelta\">0.29</span></td>\n",
    "<td>0.69<span class=\"posdelta\">0.07</span></td>\n",
    "<td>0.36<span class=\"negdelta\">0.25</span></td>\n",
    "<td>0.64<span class=\"negdelta\">0.01</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.85<span class=\"posdelta\">0.4</span></td>\n",
    "<td>0.61<span class=\"negdelta\">0.39</span></td>\n",
    "<td>0.71<span class=\"posdelta\">0.09</span></td>\n",
    "<td>0.57<span class=\"posdelta\">0.57</span></td>\n",
    "<td>0.32<span class=\"posdelta\">0.32</span></td>\n",
    "<td>0.41<span class=\"posdelta\">0.41</span></td>\n",
    "<td>0.55<span class=\"posdelta\">0.55</span></td>\n",
    "<td>0.87<span class=\"posdelta\">0.87</span></td>\n",
    "<td>0.68<span class=\"posdelta\">0.68</span></td>\n",
    "<td>0.56<span class=\"posdelta\">0.25</span></td>\n",
    "<td>0.65<span class=\"posdelta\">0.2</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.86<span class=\"posdelta\">0.04</span></td>\n",
    "<td>0.6<span class=\"negdelta\">0.13</span></td>\n",
    "<td>0.71<span class=\"negdelta\">0.06</span></td>\n",
    "<td>0.61<span class=\"posdelta\">0.61</span></td>\n",
    "<td>0.46<span class=\"posdelta\">0.46</span></td>\n",
    "<td>0.53<span class=\"posdelta\">0.53</span></td>\n",
    "<td>0.6<span class=\"posdelta\">0.04</span></td>\n",
    "<td>0.89<span class=\"negdelta\">0.03</span></td>\n",
    "<td>0.72<span class=\"posdelta\">0.03</span></td>\n",
    "<td>0.62<span class=\"posdelta\">0.23</span></td>\n",
    "<td>0.68<span class=\"posdelta\">0.02</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.84<span class=\"posdelta\">0.39</span></td>\n",
    "<td>0.65<span class=\"negdelta\">0.35</span></td>\n",
    "<td>0.73<span class=\"posdelta\">0.11</span></td>\n",
    "<td>0.57<span class=\"posdelta\">0.57</span></td>\n",
    "<td>0.54<span class=\"posdelta\">0.54</span></td>\n",
    "<td>0.55<span class=\"posdelta\">0.55</span></td>\n",
    "<td>0.63<span class=\"posdelta\">0.63</span></td>\n",
    "<td>0.82<span class=\"posdelta\">0.82</span></td>\n",
    "<td>0.72<span class=\"posdelta\">0.72</span></td>\n",
    "<td>0.64<span class=\"posdelta\">0.33</span></td>\n",
    "<td>0.69<span class=\"posdelta\">0.24</span></td>\n",
    "</tr> \n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">\n",
    "SB10k\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.61<span class=\"negdelta\">0.02</span></td>\n",
    "<td>0.56<span class=\"negdelta\">0.01</span></td>\n",
    "<td>0.58<span class=\"negdelta\">0.02</span></td>\n",
    "<td>0.29<span class=\"posdelta\">0.29</span></td>\n",
    "<td>0.01<span class=\"posdelta\">0.01</span></td>\n",
    "<td>0.02<span class=\"posdelta\">0.02</span></td>\n",
    "<td>0.74<span class=\"negdelta\">0.01</span></td>\n",
    "<td>0.92<span class=\"negdelta\">0.02</span></td>\n",
    "<td>0.82<span class=\"negdelta\">0.01</span></td>\n",
    "<td>0.3</td>\n",
    "<td>0.71<span class=\"negdelta\">0.01</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RNTN</td>\n",
    "<td>0.54<span class=\"posdelta\">0.34</span></td>\n",
    "<td>0.02<span class=\"negdelta\">0.01</span></td>\n",
    "<td>0.04<span class=\"negdelta\">0.01</span></td>\n",
    "<td>0.0<span class=\"negdelta\">0.07</span></td>\n",
    "<td>0.0<span class=\"negdelta\">0.01</span></td>\n",
    "<td>0.0<span class=\"negdelta\">0.02</span></td>\n",
    "<td>0.63<span class=\"posdelta\">0.01</span></td>\n",
    "<td>1.0<span class=\"posdelta\">0.06</span></td>\n",
    "<td>0.77<span class=\"posdelta\">0.02</span></td>\n",
    "<td>0.02<span class=\"negdelta\">0.01</span></td>\n",
    "<td>0.62<span class=\"posdelta\">0.03</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.72<span class=\"posdelta\">0.72</span></td>\n",
    "<td>0.5<span class=\"posdelta\">0.5</span></td>\n",
    "<td>0.59<span class=\"posdelta\">0.59</span></td>\n",
    "<td>0.49<span class=\"posdelta\">0.49</span></td>\n",
    "<td>0.27<span class=\"posdelta\">0.27</span></td>\n",
    "<td>0.35<span class=\"posdelta\">0.35</span></td>\n",
    "<td>0.75<span class=\"negdelta\">0.13</span></td>\n",
    "<td>0.92<span class=\"negdelta\">0.08</span></td>\n",
    "<td>0.82<span class=\"posdelta\">0.05</span></td>\n",
    "<td>0.47<span class=\"posdelta\">0.47</span></td>\n",
    "<td>0.73<span class=\"posdelta\">0.11</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.78<span class=\"posdelta\">0.03</span></td>\n",
    "<td>0.51<span class=\"posdelta\">0.04</span></td>\n",
    "<td>0.61<span class=\"posdelta\">0.03</span></td>\n",
    "<td>0.49<span class=\"posdelta\">0.49</span></td>\n",
    "<td>0.42<span class=\"posdelta\">0.42</span></td>\n",
    "<td>0.45<span class=\"posdelta\">0.45</span></td>\n",
    "<td>0.78<span class=\"posdelta\">0.07</span></td>\n",
    "<td>0.91<span class=\"negdelta\">0.07</span></td>\n",
    "<td>0.84<span class=\"posdelta\">0.01</span></td>\n",
    "<td>0.53<span class=\"posdelta\">0.24</span></td>\n",
    "<td>0.75<span class=\"posdelta\">0.03</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.84<span class=\"posdelta\">0.12</span></td>\n",
    "<td>0.42<span class=\"negdelta\">0.16</span></td>\n",
    "<td>0.56<span class=\"negdelta\">0.08</span></td>\n",
    "<td>0.5<span class=\"posdelta\">0.5</span></td>\n",
    "<td>0.28<span class=\"posdelta\">0.28</span></td>\n",
    "<td>0.36<span class=\"posdelta\">0.36</span></td>\n",
    "<td>0.74</td>\n",
    "<td>0.96<span class=\"negdelta\">0.01</span></td>\n",
    "<td>0.84</td>\n",
    "<td>0.46<span class=\"posdelta\">0.14</span></td>\n",
    "<td>0.73<span class=\"posdelta\">0.01</span></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.79<span class=\"posdelta\">0.03</span></td>\n",
    "<td>0.45<span class=\"negdelta\">0.04</span></td>\n",
    "<td>0.57<span class=\"negdelta\">0.03</span></td>\n",
    "<td>0.57<span class=\"posdelta\">0.57</span></td>\n",
    "<td>0.23<span class=\"posdelta\">0.23</span></td>\n",
    "<td>0.33<span class=\"posdelta\">0.33</span></td>\n",
    "<td>0.74<span class=\"posdelta\">0.02</span></td>\n",
    "<td>0.96<span class=\"negdelta\">0.02</span></td>\n",
    "<td>0.84<span class=\"posdelta\">0.01</span></td>\n",
    "<td>0.45<span class=\"posdelta\">0.15</span></td>\n",
    "<td>0.74<span class=\"posdelta\">0.02</span></td>\n",
    "</tr>   \n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# DL-Based Methods: Least-Squares Embeddings\n",
    "\n",
    "<table class=\"alternative-embeddings-cgsa\">\n",
    "<caption>\n",
    "Results of deep-learning&ndash;based MLSA methods with least-squares embeddings\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1^{+/-}$</td>\n",
    "<td rowspan=\"2\">Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">\n",
    "PotTS\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.61<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.61<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.61<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.22<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.01<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.03<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.48<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.72<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.32<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.07</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RNTN</td>\n",
    "<td>0.45</td>\n",
    "<td>0.82<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.59</td>\n",
    "<td>0.24<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.06<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.1<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.43<div class=\"posdelta\">0.09</div></td>\n",
    "<td>0.17<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.24<div class=\"posdelta\">0.09</div></td>\n",
    "<td>0.34<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.44<div class=\"negdelta\">0.01</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.73</td>\n",
    "<td>0.74<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.74<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.41</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.52</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.46</div></td>\n",
    "<td>0.56<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.29</div></td>\n",
    "<td>0.68<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.37<div class=\"negdelta\">0.24</div></td>\n",
    "<td>0.64<div class=\"negdelta\">0.01</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.82<div class=\"posdelta\">0.37</div></td>\n",
    "<td>0.72<div class=\"negdelta\">0.28</div></td>\n",
    "<td>0.77<div class=\"posdelta\">0.15</div></td>\n",
    "<td>0.62<div class=\"posdelta\">0.62</div></td>\n",
    "<td>0.49<div class=\"posdelta\">0.49</div></td>\n",
    "<td>0.55<div class=\"posdelta\">0.55</div></td>\n",
    "<td>0.68<div class=\"posdelta\">0.68</div></td>\n",
    "<td>0.85<div class=\"posdelta\">0.85</div></td>\n",
    "<td>0.76<div class=\"posdelta\">0.76</div></td>\n",
    "<td>0.66<div class=\"posdelta\">0.35</div></td>\n",
    "<td>0.73<div class=\"posdelta\">0.28</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.76<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.11</div></td>\n",
    "<td>0.79<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.6<div class=\"posdelta\">0.6</div></td>\n",
    "<td>0.56<div class=\"posdelta\">0.56</div></td>\n",
    "<td>0.58<div class=\"posdelta\">0.58</div></td>\n",
    "<td>0.75<div class=\"posdelta\">0.19</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.24</div></td>\n",
    "<td>0.72<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.69<div class=\"posdelta\">0.3</div></td>\n",
    "<td>0.73<div class=\"posdelta\">0.07</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.84<div class=\"posdelta\">0.39</div></td>\n",
    "<td>0.73<div class=\"negdelta\">0.27</div></td>\n",
    "<td>0.78<div class=\"posdelta\">0.16</div></td>\n",
    "<td>0.57<div class=\"posdelta\">0.57</div></td>\n",
    "<td>0.48<div class=\"posdelta\">0.48</div></td>\n",
    "<td>0.53<div class=\"posdelta\">0.53</div></td>\n",
    "<td>0.66<div class=\"posdelta\">0.66</div></td>\n",
    "<td>0.82<div class=\"posdelta\">0.82</div></td>\n",
    "<td>0.73<div class=\"posdelta\">0.73</div></td>\n",
    "<td>0.65<div class=\"posdelta\">0.34</div></td>\n",
    "<td>0.72<div class=\"posdelta\">0.27</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">\n",
    "SB10k\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.5<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.73<div class=\"posdelta\">0.16</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.35<div class=\"posdelta\">0.35</div></td>\n",
    "<td>0.06<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.1<div class=\"posdelta\">0.1</div></td>\n",
    "<td>0.8<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.35<div class=\"posdelta\">0.15</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.04</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RNTN</td>\n",
    "<td>0.0<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.62</td>\n",
    "<td>1.0<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.77<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.62<div class=\"posdelta\">0.03</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.64<div class=\"posdelta\">0.64</div></td>\n",
    "<td>0.58<div class=\"posdelta\">0.58</div></td>\n",
    "<td>0.61<div class=\"posdelta\">0.61</div></td>\n",
    "<td>0.51<div class=\"posdelta\">0.51</div></td>\n",
    "<td>0.21<div class=\"posdelta\">0.21</div></td>\n",
    "<td>0.3<div class=\"posdelta\">0.3</div></td>\n",
    "<td>0.76<div class=\"posdelta\">0.14</div></td>\n",
    "<td>0.89<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.82<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.45<div class=\"posdelta\">0.45</div></td>\n",
    "<td>0.72<div class=\"posdelta\">0.1</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.72<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.59<div class=\"posdelta\">0.12</div></td>\n",
    "<td>0.65<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.53<div class=\"posdelta\">0.53</div></td>\n",
    "<td>0.33<div class=\"posdelta\">0.33</div></td>\n",
    "<td>0.41<div class=\"posdelta\">0.41</div></td>\n",
    "<td>0.79<div class=\"posdelta\">0.08</div></td>\n",
    "<td>0.91<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.53<div class=\"posdelta\">0.24</div></td>\n",
    "<td>0.75<div class=\"posdelta\">0.03</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.6<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.72<div class=\"posdelta\">0.14</div></td>\n",
    "<td>0.66<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.47<div class=\"posdelta\">0.47</div></td>\n",
    "<td>0.42<div class=\"posdelta\">0.42</div></td>\n",
    "<td>0.44<div class=\"posdelta\">0.44</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.1</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.82<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.55<div class=\"posdelta\">0.23</div></td>\n",
    "<td>0.73<div class=\"negdelta\">0.01</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.72<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.57<div class=\"posdelta\">0.08</div></td>\n",
    "<td>0.64<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.55<div class=\"posdelta\">0.55</div></td>\n",
    "<td>0.39<div class=\"posdelta\">0.39</div></td>\n",
    "<td>0.46<div class=\"posdelta\">0.46</div></td>\n",
    "<td>0.79<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.9<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.55<div class=\"posdelta\">0.25</div></td>\n",
    "<td>0.75<div class=\"posdelta\">0.03</div></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Distant Supervision: Data\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    " <caption>Polarity class distribution in PotTS, SB10k, and the German Twitter Snapshot (GTS)<br/>\n",
    "<em>&#42; â€“ the mixed polarity was excluded from our experiments</em></caption>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Dataset</td>\n",
    "<td colspan=\"4\">Polarity Class</td>\n",
    "<td colspan=\"2\">Label Agreement</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Positive</td>\n",
    "<td>Negative</td>\n",
    "<td>Neutral</td>\n",
    "<td>Mixed*</td>\n",
    "<td>$\\alpha$ </td>\n",
    "<td>$\\kappa$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>PotTS</td>\n",
    "<td>3,380</td>\n",
    "<td>1,541</td>\n",
    "<td>2,558</td>\n",
    "<td>513</td>\n",
    "<td>0.66</td>\n",
    "<td>0.4</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SB10k</td>\n",
    "<td>1,717</td>\n",
    "<td>1,130</td>\n",
    "<td>4,629</td>\n",
    "<td>0</td>\n",
    "<td>0.39</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GTS</td>\n",
    "<td>3,326,829</td>\n",
    "<td>350,775</td>\n",
    "<td>19,453,669</td>\n",
    "<td>73,776</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Distant Supervision: PotTS\n",
    "\n",
    "<table class=\"distant-supervision\">\n",
    "<caption>Results of MLSA methods on the PotTS corpus with distantly supervised data</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro<br />F$_1$</td>\n",
    "<td rowspan=\"2\">Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">\n",
    "    PotTS\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GMN</td>\n",
    "<td>0.8<div class=\"posdelta\">0.13</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.39</div></td>\n",
    "<td>0.48<div class=\"negdelta\">0.22</div></td>\n",
    "<td>0.2<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.29<div class=\"posdelta\">0.14</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.79<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.63<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.36<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.49<div class=\"negdelta\">0.12</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MHM</td>\n",
    "<td>0.86<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.7<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.31<div class=\"negdelta\">0.27</div></td>\n",
    "<td>0.39<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.35<div class=\"negdelta\">0.22</div></td>\n",
    "<td>0.55<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.61<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.52<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.14</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GNT</td>\n",
    "<td>0.86<div class=\"posdelta\">0.15</div></td>\n",
    "<td>0.6<div class=\"negdelta\">0.2</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.26<div class=\"negdelta\">0.29</div></td>\n",
    "<td>0.31<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.28<div class=\"negdelta\">0.22</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.5<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.1</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.68<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.31<div class=\"negdelta\">0.3</div></td>\n",
    "<td>0.43<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.25<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.46<div class=\"posdelta\">0.45</div></td>\n",
    "<td>0.32<div class=\"posdelta\">0.29</div></td>\n",
    "<td>0.49<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.61<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.38<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.45<div class=\"negdelta\">0.09</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.87<div class=\"posdelta\">0.14</div></td>\n",
    "<td>0.51<div class=\"negdelta\">0.23</div></td>\n",
    "<td>0.64<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.27<div class=\"posdelta\">0.27</div></td>\n",
    "<td>0.49<div class=\"posdelta\">0.49</div></td>\n",
    "<td>0.35<div class=\"posdelta\">0.35</div></td>\n",
    "<td>0.55<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.58<div class=\"negdelta\">0.26</div></td>\n",
    "<td>0.56<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.49<div class=\"posdelta\">0.12</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.11</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.0<div class=\"negdelta\">0.82</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.72</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.77</div></td>\n",
    "<td>0.19<div class=\"negdelta\">0.43</div></td>\n",
    "<td>1.0<div class=\"posdelta\">0.51</div></td>\n",
    "<td>0.32<div class=\"negdelta\">0.23</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.68</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.85</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.76</div></td>\n",
    "<td>0.16<div class=\"negdelta\">0.5</div></td>\n",
    "<td>0.19<div class=\"negdelta\">0.43</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.48<div class=\"negdelta\">0.28</div></td>\n",
    "<td>0.88<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.62<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.25<div class=\"negdelta\">0.35</div></td>\n",
    "<td>0.23<div class=\"negdelta\">0.33</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.34</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.75</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.68</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.72</div></td>\n",
    "<td>0.43<div class=\"negdelta\">0.26</div></td>\n",
    "<td>0.44<div class=\"negdelta\">0.29</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.91<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.08<div class=\"negdelta\">0.65</div></td>\n",
    "<td>0.14<div class=\"negdelta\">0.64</div></td>\n",
    "<td>0.19<div class=\"negdelta\">0.38</div></td>\n",
    "<td>0.99<div class=\"posdelta\">0.51</div></td>\n",
    "<td>0.32<div class=\"negdelta\">0.21</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.66</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.82</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.73</div></td>\n",
    "<td>0.23<div class=\"negdelta\">0.42</div></td>\n",
    "<td>0.22<div class=\"negdelta\">0.5</div></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Distant Supervision: SB10k\n",
    "\n",
    "<table class=\"distant-supervision\">\n",
    "<caption>Results of MLSA methods on the SB10k corpus with distantly supervised data</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro<br />F$_1$</td>\n",
    "<td rowspan=\"2\">Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">\n",
    "    SB10k\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GMN</td>\n",
    "<td>0.71<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.27<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.4<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.11<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.15<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.96<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.82<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.27<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.02</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MHM</td>\n",
    "<td>0.77<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.4<div class=\"negdelta\">0.25</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.61<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.1<div class=\"negdelta\">0.3</div></td>\n",
    "<td>0.18<div class=\"negdelta\">0.27</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.97<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.82<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.35<div class=\"negdelta\">0.21</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.04</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GNT</td>\n",
    "<td>0.77<div class=\"posdelta\">0.1</div></td>\n",
    "<td>0.39<div class=\"negdelta\">0.23</div></td>\n",
    "<td>0.52<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.25<div class=\"negdelta\">0.19</div></td>\n",
    "<td>0.13<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.17<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.92<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.04</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.44<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.27<div class=\"negdelta\">0.51</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.25</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.59<div class=\"posdelta\">0.53</div></td>\n",
    "<td>0.34<div class=\"posdelta\">0.24</div></td>\n",
    "<td>0.78<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.62<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.69<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.14</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.64</td>\n",
    "<td>0.39<div class=\"negdelta\">0.19</div></td>\n",
    "<td>0.49<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.12<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.18<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.7<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.9<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.78<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.33<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.69<div class=\"negdelta\">0.03</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.24<div class=\"negdelta\">0.48</div></td>\n",
    "<td>1.0<div class=\"posdelta\">0.41</div></td>\n",
    "<td>0.38<div class=\"negdelta\">0.27</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.53</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.33</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.41</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.79</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.91</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.84</div></td>\n",
    "<td>0.19<div class=\"negdelta\">0.34</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.51</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.64<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.43<div class=\"negdelta\">0.29</div></td>\n",
    "<td>0.52<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.59<div class=\"posdelta\">0.12</div></td>\n",
    "<td>0.09<div class=\"negdelta\">0.33</div></td>\n",
    "<td>0.16<div class=\"negdelta\">0.28</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.93<div class=\"posdelta\">0.13</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.21</div></td>\n",
    "<td>0.69<div class=\"negdelta\">0.04</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.0<div class=\"negdelta\">0.72</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.57</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.64</div></td>\n",
    "<td>0.14<div class=\"negdelta\">0.41</div></td>\n",
    "<td>1.0<div class=\"posdelta\">0.61</div></td>\n",
    "<td>0.25<div class=\"negdelta\">0.21</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.79</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.9</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.84</div></td>\n",
    "<td>0.12<div class=\"negdelta\">0.43</div></td>\n",
    "<td>0.14<div class=\"negdelta\">0.61</div></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Lexicons: PotTS\n",
    "\n",
    "<figure>\n",
    "<figure>\n",
    "<img src=\"img/cgsa_potts_macro_lexicons.png\" alt=\"Macro F-Scores of MLSA Classifiers with Different Lexicons on the PotTS corus\">\n",
    "<figcaption>Macro-$F_1$</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "<img src=\"img/cgsa_potts_micro_lexicons.png\" alt=\"Micro F-Scores of MLSA Classifiers with Different Lexicons on the PotTS corus\">\n",
    "<figcaption>Micro-$F_1$</figcaption>\n",
    "</figure>\n",
    "<figcaption>\n",
    "Results of MLSA methods with different lexicons on the PotTS corpus\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Lexicons: SB10k\n",
    "\n",
    "<figure>\n",
    "<figure>\n",
    "<img src=\"img/cgsa_sb10k_macro_lexicons.png\" alt=\"Macro F-Scores of MLSA Classifiers with Different Lexicons on the SB10k corus\"/>\n",
    "<figcaption>Macro-$F_1$</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "<img src=\"img/cgsa_sb10k_micro_lexicons.png\" alt=\"Micro F-Scores of MLSA Classifiers with Different Lexicons on the SB10k corus\"/>\n",
    "<figcaption>Micro-$F_1$</figcaption>\n",
    "</figure>\n",
    "<figcaption>\n",
    "    Results of MLSA methods with different lexicons on the SB10k corpus\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Text Normalization: PotTS\n",
    "\n",
    "<table class=\"text-normalization-cgsa\">\n",
    "<caption>\n",
    "Results of MLSA methods w/o text normalization\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1^{+/-}$</td>\n",
    "<td rowspan=\"2\">Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">PotTS</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HL</td>\n",
    "<td>0.63<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.3<div class=\"negdelta\">0.46</div></td>\n",
    "<td>0.4<div class=\"negdelta\">0.36</div></td>\n",
    "<td>0.46<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.29<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.36<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.41<div class=\"negdelta\">0.26</div></td>\n",
    "<td>0.77<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.38<div class=\"negdelta\">0.24</div></td>\n",
    "<td>0.464<div class=\"negdelta\">0.22</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TBD</td>\n",
    "<td>0.65<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.47</div></td>\n",
    "<td>0.36<div class=\"negdelta\">0.38</div></td>\n",
    "<td>0.46<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.27<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.41<div class=\"negdelta\">0.22</div></td>\n",
    "<td>0.83<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.55<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.348<div class=\"negdelta\">0.25</div></td>\n",
    "<td>0.457<div class=\"negdelta\">0.22</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MST</td>\n",
    "<td>0.63<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.29<div class=\"negdelta\">0.43</div></td>\n",
    "<td>0.4<div class=\"negdelta\">0.34</div></td>\n",
    "<td>0.47<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.39<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.42<div class=\"negdelta\">0.26</div></td>\n",
    "<td>0.77<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.4<div class=\"negdelta\">0.21</div></td>\n",
    "<td>0.47<div class=\"negdelta\">0.21</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>JRK</td>\n",
    "<td>0.44<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.22<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.29<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.14<div class=\"negdelta\">0.28</div></td>\n",
    "<td>0.06<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.08<div class=\"negdelta\">0.19</div></td>\n",
    "<td>0.36<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.7<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.47<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.19<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.36<div class=\"negdelta\">0.11</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>KLCH</td>\n",
    "<td>0.61<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.23<div class=\"negdelta\">0.49</div></td>\n",
    "<td>0.33<div class=\"negdelta\">0.38</div></td>\n",
    "<td>0.33<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.21<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.26<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.41<div class=\"negdelta\">0.25</div></td>\n",
    "<td>0.82</td>\n",
    "<td>0.55<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.3<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.44<div class=\"negdelta\">0.21</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GMN</td>\n",
    "<td>0.59<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.77<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.66<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.37<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.14<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.2<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.55<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.56<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.43<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.05</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MHM</td>\n",
    "<td>0.78<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.76<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.77<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.59<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.56<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.7<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.74<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.72<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.67<div class=\"negdelta\">0.006</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.007</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GNT</td>\n",
    "<td>0.68<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.8</td>\n",
    "<td>0.73<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.55</td>\n",
    "<td>0.43<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.48<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.67<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.62<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.61<div class=\"negdelta\">0.017</div></td>\n",
    "<td>0.65<div class=\"negdelta\">0.02</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.46<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.98<div class=\"posdelta\">0.37</div></td>\n",
    "<td>0.62<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.22</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.63<div class=\"posdelta\">0.15</div></td>\n",
    "<td>0.05<div class=\"negdelta\">0.67</div></td>\n",
    "<td>0.09<div class=\"negdelta\">0.48</div></td>\n",
    "<td>0.31<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.46<div class=\"negdelta\">0.08</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.56<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.79<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.66<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.57<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.27</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.33<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.56<div class=\"negdelta\">0.08</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.65<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.62<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.62</td>\n",
    "<td>0.22<div class=\"negdelta\">0.27</div></td>\n",
    "<td>0.32<div class=\"negdelta\">0.23</div></td>\n",
    "<td>0.5<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.74<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.6<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.47<div class=\"negdelta\">0.19</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.16</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.58<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.77<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.66<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.63<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.37<div class=\"negdelta\">0.31</div></td>\n",
    "<td>0.46<div class=\"negdelta\">0.26</div></td>\n",
    "<td>0.6<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.58<div class=\"negdelta\">0.15</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.67<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.52<div class=\"negdelta\">0.21</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.19</div></td>\n",
    "<td>0.51<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.44<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.47<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.52<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.7<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.6<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.15</div></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Text Normalization: SB10k\n",
    "\n",
    "<table class=\"text-normalization-cgsa\">\n",
    "<caption>\n",
    "Results of MLSA methods w/o text normalization\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1^{+/-}$</td>\n",
    "<td rowspan=\"2\">Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">SB10k</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HL</td>\n",
    "<td>0.41<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.42<div class=\"negdelta\">0.2</div></td>\n",
    "<td>0.42<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.28<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.26<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.66<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.63<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.65<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.05</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TBD</td>\n",
    "<td>0.41<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.37<div class=\"negdelta\">0.23</div></td>\n",
    "<td>0.39<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.21<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.22<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.65<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.66<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.66<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.31<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.04</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MST</td>\n",
    "<td>0.4<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.32<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.35<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.26<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.3<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.28<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.65<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.67</td>\n",
    "<td>0.32<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.03</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>JRK</td>\n",
    "<td>0.4<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.42<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.41<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.36</td>\n",
    "<td>0.26</td>\n",
    "<td>0.3</td>\n",
    "<td>0.69</td>\n",
    "<td>0.72<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.36<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.006</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>KLCH</td>\n",
    "<td>0.42<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.21<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.28</td>\n",
    "<td>0.25<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.13</td>\n",
    "<td>0.17<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.66</td>\n",
    "<td>0.86</td>\n",
    "<td>0.75</td>\n",
    "<td>0.23<div class=\"negdelta\">0.005</div></td>\n",
    "<td>0.6<div class=\"negdelta\">0.002</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GMN</td>\n",
    "<td>0.48<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.31<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.37<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.27<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.07<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.11<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.69<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.9<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.78<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.64<div class=\"negdelta\">0.06</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MHM</td>\n",
    "<td>0.67<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.62<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.65<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.42<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.49<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.8</td>\n",
    "<td>0.88<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.84</td>\n",
    "<td>0.56<div class=\"negdelta\">0.002</div></td>\n",
    "<td>0.75<div class=\"negdelta\">0.001</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GNT</td>\n",
    "<td>0.42<div class=\"negdelta\">0.25</div></td>\n",
    "<td>0.21<div class=\"negdelta\">0.41</div></td>\n",
    "<td>0.28<div class=\"negdelta\">0.36</div></td>\n",
    "<td>0.25<div class=\"negdelta\">0.19</div></td>\n",
    "<td>0.13<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.17<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.66<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.86<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.75<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.22<div class=\"negdelta\">0.2</div></td>\n",
    "<td>0.604<div class=\"negdelta\">0.12</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.46<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.62<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.18<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.02<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.03<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.77<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.82<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.79<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.28<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.66<div class=\"negdelta\">0.02</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.58<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.39<div class=\"negdelta\">0.19</div></td>\n",
    "<td>0.47<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.23<div class=\"negdelta\">0.28</div></td>\n",
    "<td>0.05<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.08<div class=\"negdelta\">0.22</div></td>\n",
    "<td>0.7<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.92<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.27<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.67<div class=\"negdelta\">0.05</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.69<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.6<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.36<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.49<div class=\"posdelta\">0.16</div></td>\n",
    "<td>0.41</td>\n",
    "<td>0.79</td>\n",
    "<td>0.79<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.79<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.51<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.69<div class=\"negdelta\">0.06</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.24<div class=\"negdelta\">0.36</div></td>\n",
    "<td>0.86<div class=\"posdelta\">0.14</div></td>\n",
    "<td>0.38<div class=\"negdelta\">0.28</div></td>\n",
    "<td>0.45<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.45<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.45<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.69<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.01<div class=\"negdelta\">0.79</div></td>\n",
    "<td>0.02<div class=\"negdelta\">0.8</div></td>\n",
    "<td>0.41<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.27<div class=\"negdelta\">0.46</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.74<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.42<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.62<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.25<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.35<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.73<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.95<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.82<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.45<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.72<div class=\"negdelta\">0.03</div></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In this chapter, I have **compared lexicon-, machine-learning&ndash; and deep-learning&ndash;based MLSA methods**, finding that **ML- and DL-based approaches significantly outperform systems of the first group**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Among all compared lexicon methods, the most simple one (Hu and Liu, 2004) achieved the best macro- and micro-F$_1$-scores on both corpora**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The **SVM-based classifier by Mohammad et al. (2013) outperformed all other ML-based competitors and gained further improvements after removal of character and part-of-speech features**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Deep-learningâ€“based systems frequently simply fall into always predicting the majority class for all tweets, but sometimes yield extraordinarily good results**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **All DL-based solutions show fairly low scores when used with randomly initialized, task-specific word vectors, but they notably improve their results after switching to pre-trained word2vec and least-squares embeddings**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Adding more distantly supervised training data does not help overcome the majority-class pitfall of DL-based approaches**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Using better sentiment lexicons improves the scores of the systems that rely on this resource**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Last but not least, I **proved the utility of the text normalization step**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 class=\"chapter-title animate\">Chapter VI: Discourse-Aware Sentiment Analysis</h1>\n",
    "<img src=\"img/discourse.jpg\" alt=\"Discourse\" id=\"discourse\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Motivation\n",
    "\n",
    "<div class=\"example\">\n",
    "  <div class=\"title\">Example 6.1</div>\n",
    "  <div class=\"tweet\"><span class=\"label\">Tweet:</span>Syrien ist Freund von Iran , das ist das Problem ! annewill</div>\n",
    "  <div class=\"translation\">Syria is a friend of Iran . That's the problem ! annewill</div>\n",
    "  <div class=\"label\"><span class=\"label\">Gold Label:</span><span class=\"gold-label negative\">negative</span></div>\n",
    "  <div class=\"label\"><span class=\"label\">Predicted Label:</span><span class=\"predicted-label neutral\">neutral*</span></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Discourse Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Rhetorical Structure Theory** (Mann and Thompson, 1988);\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "    <img src=\"img/rst.png\" alt=\"RST Tree\" id=\"rst-tree-example\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "which divides the text into elementary discourse units (EDUs) and infers a hierarchical structure (typically a tree) between these units;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **SDRT** (Lascarides and Asher, 2001).\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "    <img src=\"img/sdrt.png\" alt=\"SDRT Anaysis\" id=\"sdrt-example\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "which is conceptually similar to RST in that it also assumes a hierarchical structure if the text, with the leaves of this structure representing Elementary Discourse Units.  But in contrast to the first theory, SDRT allows this structure to be a graph, not just a tree (i.e., a node can have multiple parents and there can be multiple links between the same pair of nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **PDTB** (Prasad et al., 2004);\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "    <img src=\"img/pdtb.png\" alt=\"PDTB Anaysis\" id=\"pdtb-example\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "which analyzes the occurrences of (either explicitly mentioned or implicitly assumed) connectives (i.e., lexico-grammatic elements that connect two sentences) in the text and considers text sentences as arguments of these connectives;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* I **split all microblogs from the PotTS and SB10k corpora into elementary discourse units** with the ML-based discourse segmenter (Sidarenka et al., 2015);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Afterwards, I **filtered out all tweets that had only one EDU**, getting 4,771 messages (12,137 segments) for PotTS and 3,763 microblogs (9,625 segments) for SB10k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "<figure>\n",
    "<figure>\n",
    "<img src=\"img/dasa_potts_edu_distribution.png\" alt=\"EDU distribution in PotTS\" id=\"edu-distribution-potts\"/>\n",
    "<figcaption>PotTS</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "<img src=\"img/dasa_sb10k_edu_distribution.png\" alt=\"EDU distribution in SB10k\" id=\"edu-distribution-sb10k\"/>\n",
    "<figcaption>SB10k</figcaption>\n",
    "</figure>\n",
    "<figcaption>\n",
    "Distribution of elementary discourse units and polarity classes in the training and development sets of PotTS and SB10k\n",
    "</figcaption>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "<div class=\"example\">\n",
    "<div class=\"title\">Example 6.1</div>\n",
    "<div class=\"tweet\"><span class=\"label\">Tweet:</span>[Guinness on Wheelchairs :]$_1$ [Das .]$_2$ [Ist .]$_3$ [Verdammt .]$_4$ [Noch .]$_5$ [Mal .]$_6$ [Einer .]$_7$ [Der .]$_8$ [Besten .]$_9$ [Werbespots .]$_{10}$ [Des .]$_{11}$ [Jahrzehnts .]$_{12}$ [( Auch ...]$_{13}$</div>\n",
    "<div class=\"translation\">\n",
    "[Guinness on Wheelchairs :]$_1$ [This .]$_2$ [Is .]$_3$ [Gosh .]$_4$ [Darn .]$_5$ [It .]$_6$ [One .]$_7$ [Of .]$_8$ [The best .]$_9$ [Commercials .]$_{10}$ [Of .]$_{11}$ [The Decade .]$_{12}$ [( Also ...]$_{13}$</div>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In the next step, I **assigned polarity scores to the microblogs' discourse segments** with the help of the lexicon-based attention classifier, analyzing each elementary unit in isolation;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Finally, I **derived RST trees for the segmented tweets with the DPLP parser** (Yi and Eisenstein, 2014) that I had previously retrained on the Potsdam Commentary Corpus (PCC 2.0; Stede and Neumann, 2014).  Following Bhatia et al. (2015), I used a binary discourse relation scheme, only distinguishing between **contrastive** and **non-contrastive** relations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "I again used the same 70--10--20 split into training, development, and test sets as I did in the previous chapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example of an Automatic RST Tree\n",
    "\n",
    "<figure>\n",
    "<div class=\"example\">\n",
    "<div class=\"title\">Example 6.2</div>\n",
    "<img src=\"img/rst-tweet.png\" alt=\"Example of an automatically costructed RST Tree\" id=\"rst-tweet-example\"/>\n",
    "<div class=\"tweet\"><span class=\"label\">Tweet:</span>[Mooooiiinn.]$_{2A}$ [Gegen solche NÃ¤chte hilft die beste Kur nicht.]$_{2B}$ [Aber Kaffee!]$_{2C}$</div>\n",
    "<div class=\"translation\">[Hellloooo!]$_{2A}$ [Even the best cure won't help against such nights.]$_{2B}$ [But coffee!]$_{2C}$</div>\n",
    "</div>\n",
    "<figcaption>\n",
    "    Example of an automatically costructed RST Tree for a tweet\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Baselines\n",
    "\n",
    "<div class=\"example\">\n",
    "<div class=\"tweet\"><span class=\"label\">Tweet:</span>[Mooooiiinn.]$_{2A}$ [Gegen solche NÃ¤chte hilft die beste Kur nicht.]$_{2B}$ [Aber Kaffee!]$_{2C}$</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **No-Discourse**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "baseline, in which I simply re-use the scores assigned by the LBA classifier to the whole message;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Last**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "in which I determine the overall polarity of a tweet by taking the LBA scores assigned to its last EDU;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Root**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "which is conceptually similar to **Last** with the only difference that it infers the polarity of the microblog from the root EDU in the discourse tree instead of the last segment;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Wang and Wu** (2013);\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "$$\\psi = \\sum_{i}p_i\\times d_i + b,$$\n",
    "where $\\psi$ is the final sentiment score of the whole document, $p_i$ is the sentiment score of the $i$-th EDU, and $d_i$ and $b$ are automatically learned model parameters, with the former term denoting the strength of the discourse relation via which $i$-th EDU is connected to its parent;\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "who determine the semantic orientation of a document by taking a linear combination of the polarity scores of its EDUs and multiplying these scores with automatically learned coefficients:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Discourse-Depth Reweighting** (DDR; Bhatia et al., 2015);\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "$$\\psi = \\sum_i\\lambda_i\\vec{\\theta}^{\\top}\\vec{w}_i = \\vec{\\theta}^{\\top}\\sum_i\\lambda_i\\vec{w}_i,$$\n",
    "where $\\vec{w}$ is a vector of features, $\\vec{\\theta}$ stands for the corresponding parameters, and $\\lambda_i$ represents the relevance of discourse unit $i$, estimated as:\n",
    "$$\\lambda_i = \\max(0.5, 1. - \\frac{d_i}{6}),$$\n",
    "where $d_i$ stands for the depth of the $i$-th EDU in the document's discourse tree.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Recurrent Rhetorical Neural Network** (R2N2; Bhatia et al., 2015).\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "$$  \\psi_i = \\tanh\\left(K_n^{(r_i)} \\psi_{n(i)} + K_s^{(r_i)}\\psi_{s(i)} \\right),$$\n",
    "where $K_n^{(r_i)}$ and $K_s^{(r_i)}$ stand for the nucleus and satellite coefficients associated with the rhetorical relation $r_i$, and $\\psi_{n(i)}$ and $\\psi_{s(i)}$ represent sentiment scores of the nucleus and satellite of the $i$-th RST node.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Latent CRFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given a dataset $\\mathcal{D}=\\left\\{\\left(\\mathbf{x}^{(i)}, \\mathbf{y}^{(i)}\\right)\\right\\}_{i=1}^{N}$, we want to find parameters $\\boldsymbol{\\theta}^*$, s.t.:\n",
    "\n",
    "$$\\boldsymbol{\\theta}^* = \\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}}\\ell(\\boldsymbol{\\theta}) = \\sum_{i=1}^{N}\\log\\left(p\\left(\\mathbf{y}^{(i)}\\vert\\mathbf{x}^{(i)}; \\boldsymbol{\\theta}\\right)\\right);$$\n",
    "\n",
    "<div class=\"hide-on-next-fragment\">\n",
    "where the conditional likelihood is normally defined as:\n",
    "\n",
    "$$p\\left(\\mathbf{y}^{(i)}\\vert\\mathbf{x}^{(i)}; \\boldsymbol{\\theta}\\right) =   \\frac{\\exp\\left(\\sum_{t=1}^{T_i}\\sum_k\\boldsymbol{\\theta}_k\\mathbf{f}_k\\left(\\mathbf{x}^{(i)}_t,\\mathbf{y}^{(i)}_{t-t},\\mathbf{y}^{(i)}_{t}\\right)\\right)}{Z}.$$\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "<figure>\n",
    "    <img src=\"img/latent-crf.png\" alt=\"Latent Conditional Random Fields\" id=\"latent-crf\"/>\n",
    "    <figcaption>\n",
    "        Example of an automatically constructed RST-based latent-CRF tree\n",
    "    </figcaption>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "<strong>Correct assignment</strong> is any assignment of latent variables that leads to the maximum probability of the correct observed label:\n",
    "$$\\mathbf{y}^{(i)} =[\\mathbf{y}_o^{(i)}, \\mathbf{y}_h^{*(i)}],$$\n",
    "where\n",
    "$$\\mathbf{y}_h^{*(i)} =\\underset{\\boldsymbol{y}_h^{(i)}}{\\operatorname{argmax}}p\\left(\\mathbf{y}_o^{(i)}\\vert\\mathbf{x}^{(i)}\\right).$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "<strong>Wrong assignment</strong> is any assignment of latent variables that leads to the maximum probability of a wrong observed label:\n",
    "$$\\mathbf{y}^{'(i)} = \\underset{\\boldsymbol{y}_h^{'(i)}\\neq\\mathbf{y}_o^{(i)}}{\\operatorname{argmax}}p\\left([\\mathbf{y}_o^{'(i)},\n",
    "    \\mathbf{y}_h^{*(i)}]\\vert\\mathbf{x}^{(i)}\\right)$$\n",
    "where\n",
    "$$\\mathbf{y}_h^{*(i)} =\\underset{\\boldsymbol{y}_h^{(i)}}{\\operatorname{argmax}}p\\left(\\mathbf{y}_o^{'(i)}\\vert\\mathbf{x}^{(i)}\\right).$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "    <figure>\n",
    "<figure>\n",
    "<img src=\"img/latent-crf-correct.png\" alt=\"Computational path of the probability of the correct label in latent CRF\" id=\"lcrf-left\"/>\n",
    "<figcaption>\n",
    "Correct label\n",
    "</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "<img src=\"img/latent-crf-wrong.png\" alt=\"Computational path of the probability of the wrong label in latent CRF\"/>\n",
    "<figcaption>\n",
    "Wrong label\n",
    "</figcaption>\n",
    "</figure>\n",
    "<figcaption>\n",
    "Confronted computational paths in latent and latent Conditional Random Fields\n",
    "</figcaption>\n",
    "</figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\boldsymbol{\\theta}^* = \\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}} \\sum_{i=1}^{N}\\log\\left(p\\left(\\mathbf{y}^{(i)}\\right)\\right) - \\log\\left(p\\left(\\mathbf{y}^{'(i)}\\right)\\right)= \\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}}\\sum_{i=1}^{N}\\log\\left(\\exp\\left(\\boldsymbol{\\theta}^{\\top}\\mathbf{f}(\\mathbf{x}^{(i)},\\mathbf{y}^{(i)})\\right)\\right) - \\log\\left(\\exp\\left(\\boldsymbol{\\theta}^{\\top}\\mathbf{f}(\\mathbf{x}^{(i)},\\mathbf{y}^{'(i)})\\right)\\right)\n",
    "= \\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}}\\sum_{i=1}^{N}\\boldsymbol{\\theta}^{\\top}\\left(\\mathbf{f}(\\mathbf{x}^{(i)},\\mathbf{y}^{(i)}) - \\mathbf{f}(\\mathbf{x}^{(i)},\\mathbf{y}^{'(i)})\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Latent Marginalized CRFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$  p\\left(\\mathbf{Y}_o{=}\\mathbf{y}_o\\right) = \\sum_{\\mathbf{y}_h} p\\left(\\mathbf{Y}_o{=}\\mathbf{y}_o,\\mathbf{Y}_h{=}\\mathbf{y}_h\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<figure>\n",
    "<figure>\n",
    "<img src=\"img/latent-mcrf-correct.png\" alt=\"Computational path of the probability of the correct label in latent marginalized CRF\">\n",
    "<figcaption>\n",
    "Correct label\n",
    "</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "<img src=\"img/latent-mcrf-wrong.png\" alt=\"Computational path of the probability of the wrong label in latent marginalized CRF\">\n",
    "<figcaption>\n",
    "Wrong label\n",
    "</figcaption>\n",
    "</figure>\n",
    "<figcaption>\n",
    "Confronted computational paths in latent and Latent-Marginalized Conditional Random Fields\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\boldsymbol{\\theta}^* = \\underset{\\boldsymbol{\\theta}}{\\operatorname{argmax}}\\sum_{i=1}^{N}\\frac{p(\\mathbf{y}^{(i)})}{p(\\mathbf{y}^{'(i)})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recursive Dirichlet Process (RDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let $p_{\\theta}(y)$ be the probability of observed label $y \\in \\{ðŸ¥º, ðŸ˜, ðŸ˜Š\\}$, parameterized by parameter $\\theta$, and let $\\mathbf{z}\\in\\mathbb{R}^{n\\times 3}$ denote latent random variables.\n",
    "\n",
    "Our task is to find $\\theta_{max}$, s.t.:\n",
    "\n",
    "$$\\theta_{max} = \\underset{\\theta}{\\operatorname{argmax}}\\log{p_{\\theta}(y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\log{p_{\\theta}(y)} = \\log{\\int p_{\\theta}(y, \\mathbf{z})\\mathrm{d}\\mathbf{z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$q_{\\phi}(\\mathbf{z})\\approx p_{\\theta_{max}}(\\mathbf{z}|y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\mathrm{ELBO}\\overset{\\Delta}{=}\\mathbb{E}_{q_{\\phi}(\\boldsymbol{z})}\\left[\\log p_{\\theta}(y,\\boldsymbol{z}) - \\log q_{\\phi}(\\boldsymbol{z})\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recursive Dirichlet Process: $p(y,\\boldsymbol{z})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "I associate a random variable $z_{j_k} \\in \\mathbb{R}^{3}_{+}$ with every RST node $i$, which represents the probabilities of polarity classes (ðŸ¥º, ðŸ˜,ðŸ˜Š) for that node after seeing its $k$-th child, e.g. $z_{1_2} = [0.2, 0.4, 0.3]$:\n",
    "\n",
    "$$z_{j_k} \\sim Dir(\\boldsymbol{\\alpha}),$$\n",
    "\n",
    "setting the initial values of these variables (i.e., $z_{j_0}$) to the scores predicted by the LBA classifier for EDUs and the root node, and setting them to zeroes for abstract nodes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "Then when analyzing the $k+1$-th child of node $j$, I compute the updated score $z_{j_{k+1}}$ as:\n",
    "$$\\boldsymbol{z}_{k+1}^{*} = \\operatorname{sparsemax}(M_r\\boldsymbol{z}_{k+1}^{\\top}),$$\n",
    "where the $M_r \\in \\mathbb{R}^{3\\times3}$ is a parameter matrix the reflects contextual changes introduced by discourse relation $r$:\n",
    "$$  M_r \\sim \\mathcal{N}_{3\\times3}\\left(\\begin{bmatrix}\n",
    "  1 & 0 & 0\\\\\n",
    "  0 & 0.3 & 0\\\\\n",
    "  0 & 0 & 1\n",
    "  \\end{bmatrix}, \\begin{bmatrix}\n",
    "  1 & 1 & 1\\\\\n",
    "  1 & 1 & 1\\\\\n",
    "  1 & 1 & 1\n",
    "  \\end{bmatrix}\\right)$$\n",
    "\n",
    "Then I compute the $\\boldsymbol{\\alpha}$ parameters as:\n",
    "$$  \\boldsymbol{\\alpha}_{j_{k+1}} = \\boldsymbol{\\beta}\\odot\\boldsymbol{z}^*_{k+1} +\n",
    "  (\\boldsymbol{1} -\n",
    "  \\boldsymbol{\\beta})\\odot\\boldsymbol{z}_{j_{k}},\n",
    "$$\n",
    "where $\\boldsymbol{\\beta}\\in\\mathbb{R}^3$ is another multivariate random variable sampled from the Beta distribution $B(5., 5.)$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"hide-on-next-fragment\">\n",
    "The only thing that I need to do to the above $\\boldsymbol{\\alpha}_{j_k}$ term before drawing the actual probability $\\boldsymbol{z}_{j_{k+1}}$ is to scale this vector by a certain amount in order to reduce the variance of the resulting Dirichlet:\n",
    "$$\\textrm{scale} = \\frac{\\xi \\times \\left(0.1 + \\cos\\left(\\boldsymbol{z}^*_k, \\boldsymbol{z}_{j_{k-1}}\\right)\\right)}{H\\left(\\boldsymbol{\\alpha}_{j_k}\\right)};$$\n",
    "where $\\xi$ is a model parameter sampled from $\\xi\\sim\\chi^2(34)$; 0.1 is a constant used to prevent zero scales in the cases when $\\cos\\left(\\mathbf{z}^*_k, \\mathbf{z}_{j_{k-1}}\\right)$ is zero; and $H\\left(\\boldsymbol{\\alpha}_{j_k}\\right)$ stands for the entropy of the $\\boldsymbol{\\alpha}_{j_k}$ vector.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RDP: Plate Diagram\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/rdp.png\" alt=\"A plate diagram of Recursive Dirichlet Process\" id=\"rdp-plate\">\n",
    "<figcaption>\n",
    "A plate diagram of the Recursive Dirichlet Process\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# RDP: Probability Computation\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/dirichlet-process.png\" alt=\"Recursive Dirichlet Process\" id=\"rdp-computation\">\n",
    "<figcaption>\n",
    "Probability distributions of polar classes computed by the Recursive Dirichlet Process\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Probability distributions of polar classes computed by the Recursive Dirichlet Process\n",
    "\n",
    "(higher probability regions are highlighted in red; $\\boldsymbol{p}_{prnt}$ means the probability of the parent\n",
    "node [the values in the vector represent the scores for the negative, neutral, and positive polarities respectively]; $\\boldsymbol{p}_{chld}$ denotes the probability of the child; and $\\boldsymbol{\\alpha}$, $\\boldsymbol{\\mu}$, and $\\boldsymbol{\\sigma}^2$ represent the parameters of the resulting joint distribution shown in the simplices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Results (PotTS)\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Results of discourse-aware sentiment analysis methods on the PotTS corpus<br/>\n",
    "<em>LCRF &mdash; latent conditional random fields, LMCRF &mdash; latent-marginalized conditional random fields, RDP &mdash; recursive Dirichlet process, DDR &mdash; discourse-depth reweighting (Bhatia et al., 2015), R2N2 &mdash; rhetorical recursive neural network (Bhatia et al., 2015), WNG &mdash; (Wang et al, 2013), Last &mdash; polarity determined by the last EDU, Root &mdash; polarity determined by the root EDU(s), No-Discourse &mdash; discourse-unaware classifier</em>\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1$</td>\n",
    "<td rowspan=\"2\">Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">PotTS</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LCRF</td>\n",
    "<td>0.76</td>\n",
    "<td>0.79</td>\n",
    "<td class=\"best\">0.77</td>\n",
    "<td class=\"best\">0.61</td>\n",
    "<td>0.53</td>\n",
    "<td>0.56</td>\n",
    "<td>0.7</td>\n",
    "<td>0.71</td>\n",
    "<td>0.71</td>\n",
    "<td>0.67</td>\n",
    "<td>0.709</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LMCRF</td>\n",
    "<td class=\"best\">0.77</td>\n",
    "<td>0.77</td>\n",
    "<td class=\"best\">0.77</td>\n",
    "<td class=\"best\">0.61</td>\n",
    "<td>0.54</td>\n",
    "<td>0.57</td>\n",
    "<td>0.69</td>\n",
    "<td class=\"best\">0.74</td>\n",
    "<td class=\"best\">0.72</td>\n",
    "<td>0.671</td>\n",
    "<td class=\"best\">0.712</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RDP</td>\n",
    "<td>0.73</td>\n",
    "<td>0.82</td>\n",
    "<td class=\"best\">0.77</td>\n",
    "<td class=\"best\">0.61</td>\n",
    "<td>0.56</td>\n",
    "<td class=\"best\">0.58</td>\n",
    "<td class=\"best\">0.73</td>\n",
    "<td>0.65</td>\n",
    "<td>0.69</td>\n",
    "<td class=\"best\">0.678</td>\n",
    "<td>0.706</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DDR</td>\n",
    "<td>0.73</td>\n",
    "<td>0.77</td>\n",
    "<td>0.75</td>\n",
    "<td>0.54</td>\n",
    "<td class=\"best\">0.59</td>\n",
    "<td>0.56</td>\n",
    "<td>0.69</td>\n",
    "<td>0.61</td>\n",
    "<td>0.65</td>\n",
    "<td>0.655</td>\n",
    "<td>0.674</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>R2N2</td>\n",
    "<td>0.74</td>\n",
    "<td>0.78</td>\n",
    "<td>0.76</td>\n",
    "<td>0.59</td>\n",
    "<td>0.53</td>\n",
    "<td>0.56</td>\n",
    "<td>0.68</td>\n",
    "<td>0.68</td>\n",
    "<td>0.68</td>\n",
    "<td>0.657</td>\n",
    "<td>0.692</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>WNG</td>\n",
    "<td>0.58</td>\n",
    "<td>0.79</td>\n",
    "<td>0.67</td>\n",
    "<td class=\"best\">0.61</td>\n",
    "<td>0.21</td>\n",
    "<td>0.31</td>\n",
    "<td>0.61</td>\n",
    "<td>0.57</td>\n",
    "<td>0.59</td>\n",
    "<td>0.487</td>\n",
    "<td>0.59</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Last</td>\n",
    "<td>0.52</td>\n",
    "<td class=\"best\">0.83</td>\n",
    "<td>0.64</td>\n",
    "<td>0.57</td>\n",
    "<td>0.17</td>\n",
    "<td>0.26</td>\n",
    "<td>0.61</td>\n",
    "<td>0.43</td>\n",
    "<td>0.5</td>\n",
    "<td>0.453</td>\n",
    "<td>0.549</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Root</td>\n",
    "<td>0.56</td>\n",
    "<td>0.73</td>\n",
    "<td>0.64</td>\n",
    "<td>0.58</td>\n",
    "<td>0.22</td>\n",
    "<td>0.32</td>\n",
    "<td>0.55</td>\n",
    "<td>0.54</td>\n",
    "<td>0.54</td>\n",
    "<td>0.481</td>\n",
    "<td>0.56</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>No-Discourse</td>\n",
    "<td>0.73</td>\n",
    "<td>0.82</td>\n",
    "<td class=\"best\">0.77</td>\n",
    "<td class=\"best\">0.61</td>\n",
    "<td>0.56</td>\n",
    "<td>0.58</td>\n",
    "<td>0.72</td>\n",
    "<td>0.66</td>\n",
    "<td>0.69</td>\n",
    "<td>0.677</td>\n",
    "<td>0.706</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Results (SB10k)\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Results of discourse-aware sentiment analysis methods on the SB10k corpus<br/>\n",
    "<em>LCRF &mdash; latent conditional random fields, LMCRF &mdash; latent-marginalized conditional random fields, RDP &mdash; recursive Dirichlet process, DDR &mdash; discourse-depth reweighting (Bhatia et al., 2015), R2N2 &mdash; rhetorical recursive neural network (Bhatia et al., 2015), WNG &mdash; (Wang et al, 2013), Last &mdash; polarity determined by the last EDU, Root &mdash; polarity determined by the root EDU(s), No-Discourse &mdash; discourse-unaware classifier</em>\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro<br/>F$_1$</td>\n",
    "<td rowspan=\"2\">Micro<br/>F$_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\" class=\"separator\">\n",
    "    SB10k\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LCRF</td>\n",
    "<td class=\"best\">0.64</td>\n",
    "<td class=\"best\">0.69</td>\n",
    "<td>0.66</td>\n",
    "<td class=\"best\">0.45</td>\n",
    "<td>0.45</td>\n",
    "<td>0.45</td>\n",
    "<td class=\"best\">0.82</td>\n",
    "<td>0.79</td>\n",
    "<td class=\"best\">0.8</td>\n",
    "<td>0.557</td>\n",
    "<td>0.713</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LMCRF</td>\n",
    "<td class=\"best\">0.64</td>\n",
    "<td class=\"best\">0.69</td>\n",
    "<td class=\"best\">0.67</td>\n",
    "<td>0.45</td>\n",
    "<td class=\"best\">0.45</td>\n",
    "<td>0.45</td>\n",
    "<td class=\"best\">0.82</td>\n",
    "<td>0.79</td>\n",
    "<td class=\"best\">0.8</td>\n",
    "<td class=\"best\">0.56</td>\n",
    "<td class=\"best\">0.715</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RDP</td>\n",
    "<td class=\"best\">0.64</td>\n",
    "<td class=\"best\">0.69</td>\n",
    "<td>0.66</td>\n",
    "<td>0.45</td>\n",
    "<td>0.45</td>\n",
    "<td>0.45</td>\n",
    "<td>0.82</td>\n",
    "<td>0.79</td>\n",
    "<td>0.8</td>\n",
    "<td>0.557</td>\n",
    "<td>0.713</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DDR</td>\n",
    "<td>0.59</td>\n",
    "<td>0.63</td>\n",
    "<td>0.61</td>\n",
    "<td class=\"best\">0.48</td>\n",
    "<td>0.44</td>\n",
    "<td class=\"best\">0.46</td>\n",
    "<td>0.77</td>\n",
    "<td>0.76</td>\n",
    "<td>0.77</td>\n",
    "<td>0.534</td>\n",
    "<td>0.681</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>R2N2</td>\n",
    "<td class=\"best\">0.64</td>\n",
    "<td class=\"best\">0.69</td>\n",
    "<td>0.66</td>\n",
    "<td>0.46</td>\n",
    "<td class=\"best\">0.45</td>\n",
    "<td>0.45</td>\n",
    "<td>0.81</td>\n",
    "<td>0.79</td>\n",
    "<td class=\"best\">0.8</td>\n",
    "<td>0.559</td>\n",
    "<td>0.713</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>WNG</td>\n",
    "<td>0.61</td>\n",
    "<td>0.63</td>\n",
    "<td>0.62</td>\n",
    "<td>0.46</td>\n",
    "<td>0.29</td>\n",
    "<td>0.36</td>\n",
    "<td>0.76</td>\n",
    "<td class=\"best\">0.82</td>\n",
    "<td>0.79</td>\n",
    "<td>0.488</td>\n",
    "<td>0.693</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Last</td>\n",
    "<td>0.56</td>\n",
    "<td>0.55</td>\n",
    "<td>0.56</td>\n",
    "<td>0.46</td>\n",
    "<td>0.29</td>\n",
    "<td>0.36</td>\n",
    "<td>0.73</td>\n",
    "<td>0.8</td>\n",
    "<td>0.76</td>\n",
    "<td>0.459</td>\n",
    "<td>0.661</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Root</td>\n",
    "<td>0.51</td>\n",
    "<td>0.55</td>\n",
    "<td>0.53</td>\n",
    "<td>0.4</td>\n",
    "<td>0.3</td>\n",
    "<td>0.35</td>\n",
    "<td>0.74</td>\n",
    "<td>0.76</td>\n",
    "<td>0.75</td>\n",
    "<td>0.438</td>\n",
    "<td>0.64</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>No-Discourse</td>\n",
    "<td class=\"best\">0.64</td>\n",
    "<td class=\"best\">0.69</td>\n",
    "<td>0.66</td>\n",
    "<td>0.45</td>\n",
    "<td class=\"best\">0.45</td>\n",
    "<td>0.45</td>\n",
    "<td class=\"best\">0.82</td>\n",
    "<td>0.79</td>\n",
    "<td class=\"best\">0.8</td>\n",
    "<td>0.557</td>\n",
    "<td>0.713</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Base Classifier\n",
    "\n",
    "Since the scores of the presented discourse-aware sentiment systems crucially depended on the accuracy of the base classifier (the one we use to assign sentiment scores to single EDUs), I decided to rerun the experiments using the best systems from the two other message-level sentiment anaysis groups (lexicon- and machine-learning&ndash;), namely:\n",
    "* the system of Hu and Liu (2004);\n",
    "* and the SVM classifier of Mohammad et al. (2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Evaluation: Base Classifier (PotTS)\n",
    "\n",
    "<figure>\n",
    "<div class=\"img-top\">\n",
    "<figure>\n",
    "<img src=\"img/dasa-potts-bc-macro-F1.png\" alt=\"Macro-$F_1$ Results on the PotTS corpus with Different Base Classifiers\">\n",
    "<figcaption>Macro-$F_1$</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "<div class=\"img-bottom\">\n",
    "<figure>\n",
    "<img src=\"img/dasa-potts-bc-micro-F1.png\" alt=\"Micro-$F_1$ Results on the PotTS corpus with Different Base Classifiers\">\n",
    "<figcaption>Micro-$F_1$</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "<figcaption>\n",
    "Results of discourse-aware sentiment analysis methods with different base classifiers on the PotTS corpus\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Evaluation: Base Classifier (SB10k)\n",
    "\n",
    "<figure>\n",
    "<div class=\"img-top\">\n",
    "<figure>\n",
    "<img src=\"img/dasa-sb10k-bc-macro-F1.png\" alt=\"Macro-$F_1$ Results on the SB10k corpus with Different Base Classifiers\">\n",
    "<figcaption>Macro-$F_1$</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "<div class=\"img-bottom\">\n",
    "<figure>\n",
    "<img src=\"img/dasa-sb10k-bc-micro-F1.png\" alt=\"Micro-$F_1$ Results on the SB10k corpus with Different Base Classifiers\">\n",
    "<figcaption>Micro-$F_1$</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "<figcaption>\n",
    "Results of discourse-aware sentiment analysis methods with different base classifiers on the SB10k corpus\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Evaluation: Relation Scheme\n",
    "\n",
    "<table id=\"discourse-relation-schemes\">\n",
    "<caption>\n",
    "RST relations used in the original Potsdam Commentary Corpus and different discourse-aware sentiment methods<br/>\n",
    "(default relation, which subsumes the rest of the links, is shown in <strong>bold</strong>)    \n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td>Scheme</td>\n",
    "<td>Relation Set</td>\n",
    "<td>Equivalence Classes</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Bhatia et al.</td>\n",
    "<td>{Contrastive, <span class=\"default-rel\">Non-Contrastive</span>}</td>\n",
    "<td>Contrastive &#x225D; {Antithesis, Antithesis-E, Comparison, Concession, Consequence-S, Contrast, Problem-Solution}.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Chenlo et al.</td>\n",
    "<td>{Attribution, Background, Cause, Comparison, Condition, Consequence, Contrast, Elaboration, Enablement, Evaluation, Explanation, Joint, Otherwise, Temporal, <span class=\"default-rel\">Other</span>}</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Heerschop et al.</td>\n",
    "<td>{Attribution, Background, Cause, Condition, Contrast, Elaboration, Enablement, Explanation, <span class=\"default-rel\">Other</span>}</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PCC</td>\n",
    "<td>{Antithesis, Background, Cause, Circumstance, Concession, Condition, Conjunction, Contrast, Disjunction, E-Elaboration, Elaboration, Enablement, Evaluation-N, Evaluation-S, Evidence, Interpretation, Joint, Justify, List, Means, Motivation, Otherwise, Preparation, Purpose, Reason, Restatement, Restatement-MN, Result, Sequence, Solutionhood, Summary, Unconditional, Unless, Unstated-Relation}</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Zhou et al.</td>\n",
    "<td>{Contrast, Condition, Continuation, Cause, Purpose, <span class=\"default-rel\">Other</span>}</td>\n",
    "<td>Contrast &#x225D; {Antithesis, Concession, Contrast, Otherwise}<br/>\n",
    "    Continuation &#x225D; {Continuation, Parallel}<br/>\n",
    "    Cause &#x225D; {Evidence, Nonvolitional-Cause, Nonvolitional-Result, Volitional Cause, Volitional-Result}</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Another factor that could significantly affect the results of discourse-aware sentiment methods was the set of discourse relations distinguished by the parsing system. On the one hand, this set considerably affected the quality of discourse parsing (with richer sets typically leading to lower accuracy); on the other hand, it was also important to the sentiment systems (with richer sets allowing them to distinguish more facets).  To check which of these factors had a greater impact on the net results of discourse-aware sentiment methods, I reran the experiments with the following alternative sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "<caption>\n",
    "Results of the DPLP parser on PCC 2.0 with different relation schemes\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td>Relation Scheme</td>\n",
    "<td>Span F$_1$</td>\n",
    "<td>Nuclearity F$_1$</td>\n",
    "<td>Relation F$_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Bhatia et al.</td>\n",
    "<td class=\"best\">0.777</td>\n",
    "<td>0.512</td>\n",
    "<td class=\"best\">0.396</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Chenlo et al.</td>\n",
    "<td>0.769</td>\n",
    "<td>0.505</td>\n",
    "<td>0.362</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Heerschop et al.</td>\n",
    "<td>0.774</td>\n",
    "<td>0.51</td>\n",
    "<td>0.361</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PCC</td>\n",
    "<td>0.776</td>\n",
    "<td class=\"best\">0.534</td>\n",
    "<td>0.326</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Zhou et al.</td>\n",
    "<td>0.776</td>\n",
    "<td>0.501</td>\n",
    "<td>0.388</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Relation Scheme (PotTS)\n",
    "\n",
    "<figure>\n",
    "<div class=\"img-top\">\n",
    "<figure>\n",
    "<img src=\"img/dasa-potts-macro-F1.png\" alt=\"Macro-$F_1$ Results on the PotTS corpus with Different Relation Schemes\">\n",
    "<figcaption>Macro-$F_1$</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "<div class=\"img-bottom\">\n",
    "<figure>\n",
    "<img src=\"img/dasa-potts-micro-F1.png\" alt=\"Micro-$F_1$ Results on the PotTS corpus with Different Relation Schemes\">\n",
    "<figcaption>Micro-$F_1$</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "<figcaption>\n",
    "Results of discourse-aware sentiment classifiers with different relation schemes on the PotTS corpus\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Relation Scheme (SB10k)\n",
    "\n",
    "<figure>\n",
    "<div class=\"img-top\">\n",
    "<figure>\n",
    "<img src=\"img/dasa-sb10k-macro-F1.png\" alt=\"Macro-$F_1$ Results on the SB10k corpus with Different Relation Schemes\">\n",
    "<figcaption>Macro-$F_1$</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "<div class=\"img-bottom\">\n",
    "<figure>\n",
    "<img src=\"img/dasa-sb10k-micro-F1.png\" alt=\"Micro-$F_1$ Results on the SB10k corpus with Different Relation Schemes\">\n",
    "<figcaption>Micro-$F_1$</figcaption>\n",
    "</figure>\n",
    "</div>\n",
    "<figcaption>\n",
    "Results of discourse-aware sentiment classifiers with different relation schemes on the sb10k corpus\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* I have presented an **overview of the most popular approaches to automatic discourse analysis** (RST, PDTB, and SDRT);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* I **segmented all microblogs from the PotTS and SB10k corpora into elementary discourse units** and **parsed these messages with the RST parser** by Ji and Eisenstein (2014);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Afterwards, I **estimated the results of existing discourse-aware sentiment methods and simpler baselines**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* I **improved on these scores and also outperform the discourse-unaware system with thee proposed LCRF, LMCRF, and RDP solutions**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* A subsequent evaluation of these approaches with different settings showed that the **results of all DASA methods largely correlate with the scores of the base sentiment classifier**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* &hellip; and that **almost all discourse-aware solutions benefit richer discourse relation sets**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 class=\"chapter-title animate\">Final</h1>\n",
    "<img src=\"img/final.png\" alt=\"Final Falg\" id=\"final-flag\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In this presentation, I have presented a **corpus of $\\approx8,000$ German tweets, which had been annotated with sentiments, sources, targets, polar terms, intensifiers, diminishers, and negations)** and I also conducted an **inter-annotator agreement study on that corpus**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* On the level of **words**, I have **compared existing German sentiment lexicons with automatically generated polarity lists** and also **proposed several new NWE-bsed methods**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* On the level of **syntactic constituents**, I have **evaluated common approaches to aspect-based sentiment analysis (CRFs and RNNs)** and als came up with several **improvements of these models (alternative graph structures and word embedding types)**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* On the level of **statements**, I have **compared three groups of message-level sentiment methods (dictionary-, ML-, and DL-based ones)** and also **proposed my own lexicon-based attention system**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Finally, on the level of **discourse**, I **evaluated existing discourse-aware sentiment methods and proposed three additional algorithms for this task (LCRF, LMCRFs, and RDP)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Answers to Research Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* How difficult is it to analyze sentiments for humans and computers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"answer hide-on-next-fragment\">\n",
    "  <ul>\n",
    "    <li>Initial IAA on sentiments 31% proportional $\\kappa$ (but could be improved to 61% via adjudication);</li>\n",
    "    <li>Sentiment Lexicons: 0.479 macro-F$_1$ and 0.963 micro-F$_1$;</li>\n",
    "    <li>Aspect-Based Sentiment Analysis: 0.305 macro-F$_1$;</li>\n",
    "    <li>Message-Level Sentiment Analysis: 0.674 macro- and 0.727 micro-F$_1$ on PotTS, and 0.564 macro- and 0.752 micro-F$_1$ on the SB10k test set;</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Which factors might affect this difficulty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  <div class=\"answer hide-on-next-fragment\">\n",
    "  &mdash; Topic and form of the tweets;\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Can we apply opinion mining methods devised for standard English to German Twitter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  <div class=\"answer hide-on-next-fragment\">\n",
    "  &mdash; Yes, we can, but the success of these approaches might significantly vary depending on the task, the size, and the reliability of the training data, as well as the evaluation metric that we use;\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Which groups of approaches are best suited for which sentiment tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  <div class=\"answer hide-on-next-fragment\">\n",
    "    <ul>\n",
    "    <li><strong>generation of sentiment lexicons</strong> is most amenable to <strong>dictionary- and NWE-based methods</strong>;</li>\n",
    "        <li><strong>aspect-based sentiment analysis</strong> is better addressed with <strong>probabilistic graphical models</strong> such as <em>Conditional Random Fields</em>;</li>\n",
    "        <li><strong>message-level sentiment analysis</strong> shows best scores in conjunction with <strong>ML- and DL-based methods</strong>;</li>\n",
    "        <li>finally, <strong>probabilistic graphical models</strong> can still beat neural networks on the task of <strong>discourse-aware sentiment analysis</strong>;</li>\n",
    "</ul>\n",
    "        </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Can we do better than existing methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  <div class=\"answer hide-on-next-fragment\">\n",
    "  &mdash; Yes, we can, with the proposed <strong>linear projection</strong>, <strong>lexicon-based attention</strong>, <strong>latent-marginalized CRFs</strong>, and <strong>RDP algorithms</strong> as well as alternative CRF structures and <strong>least-squares word embeddings</strong>;\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Does text normalization help analyze sentiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"answer\">\n",
    "  &mdash; Yes, it definitely does.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Contributions\n",
    "\n",
    "* The **Potsdam Twitter Sentiment Corpus**: https://github.com/WladimirSidorenko/PotTS;\n",
    "\n",
    "* **Lexicon Generation Methods**: https://github.com/WladimirSidorenko/SentiLex;\n",
    "\n",
    "* **Text-Normalization Pipeline** and **Aspect-Based Sentiment Methods**: https://github.com/WladimirSidorenko/TextNormalization;\n",
    "\n",
    "* **MLSA Approaches**: https://github.com/WladimirSidorenko/CGSA;\n",
    "\n",
    "* **Discourse-Aware Sentiment Systems**: https://github.com/WladimirSidorenko/DASA;\n",
    "\n",
    "* **Discourse Segmenter**: https://github.com/WladimirSidorenko/DiscourseSegmenter;\n",
    "\n",
    "* **Retrained Discourse Parser**: https://github.com/WladimirSidorenko/RSTParser;\n",
    "\n",
    "* **Extended Version of RST Markup Tool**: https://github.com/WladimirSidorenko/RSTTool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/questions.jpeg\" alt=\"questions\" id=\"questions\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
