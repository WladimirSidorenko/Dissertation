\chapter{Coarse-Grained Sentiment Analysis}\label{sec:snt:cgsa}

\section{Coarse-Grained Sentiment Analysis with Lexicon-Based Methods}

\section{Coarse-Grained Sentiment Analysis with Machine Learning
  Frameworks}

\section{Coarse-Grained Sentiment Analysis with Deep Neural Networks}

\section{Coarse-Grained Sentiment Analysis using Language and Domain
  Adaptation}

\section{Evaluation}
\subsection{Effect of Distant Supervision}
\subsection{Effect of Normalization}

\section{Related Work}

\textbf{Lexicon-Based Methods}

% Das and Chen, 2001; Turney, 2002;  Kim and Hovy, 2004
The usage of lexicon-based approaches for estimating semantic
orientation of complex text chunks dates back to the very origins of
the sentiment analysis research in general: For example,
\citet{Das:01} already used two purely lexicon-based and three
machine-learning methods, which heavily relied on lexicon features, in
an ensemble of five classifiers to predict the polarity of stock
messages (buy, sell, or neutral), attaining an accuracy of 62\% on a
corpus of several hundreds stock board messages.  \citet{Turney:02},
while proposing his PMI method for automatically generating sentiment
lexicons, also suggested computing the orientation of the whole review
by averaging the sentiment scores of its terms found in the lexicon.
With this approach, the author reached an accuracy of 74\% on a corpus
of 410 manually labeled Epinions comments.  Finally, \citet{Kim:04}
compared three different methods of determining the overall polarity
of a sentence:
\begin{inparaenum}[(i)]
  \item by multiplying the signs of polar terms found in sentence,
  \item taking their sum, and
  \item taking the geometric mean of polarity scores;
\end{inparaenum}
finding that the first and last options worked best for the Document
Undestanding Corpus.\footnote{\url{http://duc.nist.gov/}}

% Hatzivassiloglou and Wiebe, 2000
\citet{Hatzivassi:00} proved, for instance, via statistical
significance tests that the mere presence of a subjective or even just
gradable adjective from a lexicon was a highly reliable indicator that
the sentence it appeared in also was subjective.

% Hu and Liu, 2004
Similarly, \citet{Hu:04} determined the semantic orientation of
sentences in customer reviews by simply comparing the number of
positive and negative terms found in these passages. Since the
authors, however, were primarily interested in estimating the polarity
towards particular product features mentioned in the clauses, they
additionally applied a fallback strategy in case of a tie by checking
which of the polar lexicon terms appeared closer to the features, and
assuming the polarity of the preceding sentence if these numbers were
also equal.

% Taboada et al., 2004
Largely inspired by the Appraisal theory of~\citet{Martin:00},
\citet{Taboada:04} enhanced the original method of~\citet{Turney:02}
by increasing the weights of polar adjectives which occurred in the
middle and at the end of a document, and also augmenting these values
with the affect, judgement, and appreciation scores.  Similarly to
polarity, the appraisal scores were calculated automatically by
computing the PMI of their cooccurrence with different pronouns using
a web search engine.

% Polanyi and Zaenen, 2006
An early analysis of lexicon-based approaches to sentiment analysis
was made by~\citet{Polanyi:06}, who argued that, besides considering
the lexical valence (i.e., semantic orientation) of polar terms, it
was also necessary to incorporate syntactic, discourse-level, and
extra-linguistic factors such as negations, intensifiers, modal
operators (e.g., \emph{could} or \emph{might}), presuppositional items
(e.g., \emph{barely} or \emph{failure}), irony, reported speech,
discourse connectors and structure, genre and attitude assessment,
reported speech, multi-entity evaluation, and so on.

% Kennedy and Inkpen, 2006
This theoretical hypothesis was later proven empirically by
\citet{Kennedy:06}, who compared two lexicon-based approaches:
\begin{inparaenum}[(i)]
\item simply counting positive and negative expressions, and assigning
  the input text (a review) to the polarity class with the greater
  number of found terms (considering a review as neutral if it had an
  equal number of expressions with either polarities); and
\item the same procedure as above, but additionally enhanced with the
  information about contextual valence shifters~(intensifiers,
  downtoners, and negations).
\end{inparaenum}
In the latter method, the authors swapped the polarity of a polar
expression to the opposite if it followed a negation, and decreased or
increased the score of this term by a constant factor if it was
preceded by a diminisher or intensifier.  This enhancement was shown
to bring a statistically significant improvement, boosting the
accuracy of the two-class prediction on a corpus of product and movie
reviewa from 67.9 to 69.3\%.

% Taboada et al., 2006
Another important contribution to the development of lexicon-based
approaches was made by~\citet{Taboada:06}, who compared three popular
polarity lists---a PMI lexicon computed with the original method
of~\citet{Turney:02} using the AltaVista's NEAR operator; a similar
polarity list obtained with the help of Google's AND queries; and,
finally, the manually compiled General Inquirer lexicon
of~\citet{Stone:66}.  The authors evaluated these resources both
intrinsically (by comparing them with GI entries) and extrinsically
(by computing the polarity of 400 manually annotated Epinions
reviews).  To estimate the overall polarity of a review for the second
task, \citeauthor{Taboada:06} calculated the average SO value of all
polar terms found in the review, obtaining these scores from the
mean-normalized lexicons, and flipping the polarity sign to the
opposite in case of the negation.

% Taboada et al., 2011
Finally, a veritably seminal work on lexicon-based approaches was
introduced by~\citet{Taboada:11} who presented a manually compiled
sentiment lexicon,\footnote{The authors hand-annotated all occurrences
  of adjectives, nouns, and verbs found in a corpus of 400 Epinions
  reviews with ordinal categories ranging from -5 to 5 which reflected
  the semantic orientation of a term (positive vs. negative) and its
  polar strength (weak vs. strong).} and used this resource to compute
the overall \emph{semantic orientation} (SO) of text.  Inspired by the
ideas of~\citet{Polanyi:06}, to make this calculation more precise,
the authors also incorporated a set of additional heuristic rules by
changing the prior SO values of negated, itensified, and downtoned
terms, ignoring irrealis sentences, and adjusting weights of specific
sections of a document.  An extensive evaluation of this method showed
a superior performance of the manual lexicon in comparison with other
polarity lists including the Subjectivity Dictionary~\cite{Wilson:05},
Maryland Polarity Set~\cite{Mohammad:09}, and
\textsc{SentiWordNet}~\cite{Esuli:06c}.  Moreover, the authors also
proved the robustness of their SO computation procedure for other
topics and text genres, hypothesizing that lexion-based approaches
were more domain-independent than traditional supervised learning
techniques.

% Musto et al., 2014
A lexicon-based system created specifically for Twitter data was
presented by~\citet{Musto:14}, who examined four different strategies
for computing the overall polarity scores of tweets---\emph{basic},
\emph{normalized}, \emph{emphasized}, and
\emph{normalized-emphasized}; checking these approaches with four
distinct lexicons---\textsc{SentiWordNet}~\cite{Esuli:06c},
\textsc{WordNet-Affect}~\cite{Strapparava:04},
\textsc{MPQA}~\cite{Wiebe:05}, and
\textsc{SenticNet}~\cite{Cambria:14}.  In all of these strategies, the
authors first split the input messages into \emph{micro-phrases} based
on the occurrence of punctuation marks and conjunctions; then
calculated the polarity score of each of these segments by summing
(\emph{basic} and \emph{emphasized}) or averaging (\emph{normalized}
and \emph{normalized-emphasized}) the lexicon scores of their tokens;
and, finally, estimated the overall polarity of the whole tweet by
summing or averaging the scores of the micro-phrases.
\citeauthor{Musto:14} obtained their best results (58.99\% accuracy on
the SemEval-2013 dataset) with the \textsc{SentiWordNet} lexicon
of~\citet{Esuli:06c}, using the normalized-emphasized approach, in
which they averaged the polarity scores of segment tokens, boosting
these values by 50\% for informative parts of speech; and regarded the
sum of the segements' scores as the final overall polarity of the
message.

\todo[inline]{\citet{Jurek:15}}

\citet{Jurek:15}

The sentiment lexicon constructed contains about 6300 words. It was
generated manually with application of Sen- tiWordNet [28] as a
baseline. Each word in the lexicon has assigned a value representing
sentiment in the range of -100 (most negative) to 100 (most
positive). From an empirical knowledge it is known that some of the
positive and negative words sometime occur with neutral meaning in a
sentence context.

For example, sentence ``Enjoying my lazy Sunday!!'' represents a
positive message that contains one positive (enjoying) and one
negative (lazy) word. It may be difficult in such a case to decide
between positive and negative. In an effort to alleviate this issue,
besides the sen- timent value, for each word from the lexicon we
estimated a conditional probability (denoted by P) as presented in
Eq. 1.  P(positive | w) for positive w P(negative | w) for negative w

Based on a set of labelled data, for each positive word we estimated
the probability that a random message containing this word is
positive. For the purpose of calculating the probabili- ties we
applied a training data set provided by Stanford [29] that contains
1.6 million (including 800,000 posi- tive and 800,000 negative)
labelled tweets. The training dataset was created automatically based
on the absence of emoticons within a message. It was assumed that any
tweets with positive emoticons were positive and tweets with negative
emoticons were negative. List of emoti- cons was applied as query for
Twitter API and the col- lected messages were automatically labelled
as positive or negative, depending on the type of emoticon they con-
tained. The process of calculating the probabilities has been
performed as follow. A sample of 100,000 positive and 100,000 negative
tweets has been selected randomly.  Following this, for each word form
the lexicon, denoted as w, its frequency among the selected positive
and nega- tive messages was calculated. Depending if the word was
positive or negative, the conditional probability was cal- culated as
presented in Eq. 2.  In order to obtain more precise result, the
process was repeated 100 times and the aver- age probability obtained
for each word has been stored in the lexicon. The probabilities are
referred to as pieces of evidence later in the paper.


The most common approach to handling negation with a lexicon-based
approach is by reversing the polar- ity of the lexicon item that
stands next to the negator in a sentence [30] (e.g. good: 100 and not
good: -100).  In our work we proposed to take a different approach.
Rather than reversing the sentiment value we proposed to formulate a
negating function that calculates the sentiment value of a negated
word. First, we manually created a lexicon composed of 38 negating
words. Fol- lowing this, applying the Twitter corpus, we selected most
commonly used phrases containing negation of verbs and adjectives. In
the next step, a group of 20 peo- ple was asked to rank the
expressions from both of the list from most positive to most
negative. Taking under consideration all the results, the final two
rankings were estimated. Based on the two rankings we determined the
most corresponding negating function represented as follow:

$\max(\frac{S+100}{2}, 10)$ if $S < 0$

$\min(\frac{S-100}{2}, -10)$ if $S > 0$

Once a negation is recognised in a sentence, the first non-neutral
word that occurs within the following three positions after the
negator is searched. If a positive or negative word is identified, its
new sentiment value is calculated by using Eq. (3) (e.g. enjoy: 20, do
not enjoy: -40).

In our approach 25 most frequently applied intensifiers were selected
and then, depending on their polarity, they were divided into 3 cat-
egories, namely downtoners, weak amplifiers and strong
amplifiers. Empirically downtoners represent intensifiers that
decrease value of the sentiment by 50 \%. Weak and strong amplifiers
increase sentiment by 50 and 100 \%, respectively.

None of the negators and intensifiers is included in the sentiment
lexicon. Consequently, if they appear in a sentence surrounded by only
neutral text, they are con- sidered as neutral words. However, if they
appear in a neighbourhood of positive or negative words they are
considered as non-neutral given that they influence the final
sentiment of a sentence.

Once all positive and negative words are identified in a sentence and
their local context is verified, a combining process is performed in
order to obtain the final sentiment value. In most of the existing
approaches to sentiment analysis, the output of the process is repre-
sented as a positive or negative class label. In our work we attempted
to design a sentiment combining function that, based on the sentiment
of single words, provides the absolute sentiment of the message as a
normalised value from the range of -100 to 100. The motivation for
such an approach was the possibility to analyse the sentiment in the
degree of intensity as opposed to positive and negative only. Apart
from the polarity, we wanted to be able to determine how strongly
positive/ negative a sentence is and which of any two sentences is
more positive/negative than the other.

The combining function should be able to model the relation between
sentences depending on the number of non- neutral words and the value
of the sentiment they contain. In the first attempt an average was
considered as a combining function for the sentiment within a message.
This solution, however, did not provide an accurate differentiation
between sentences. For example, for the two sentences presented below,
based on the average we are not able to recognise correctly which
sentence express more positive opinion. In this study, we proposed a
new normalisation formula that combines the average sentiment of a
sentence and the number of words to calculate the average. The idea
was that, for a given average sentiment of a message, the dif- ference
between the overall positive and overall negative sentiments should
also depend on the number of positive and negative words in the
message. Therefore, the over- all positive/negative sentiment should
be represented as a product of the average sentiment and a coefficient
that's value depends on the number of positive/negative words.
Following this rationale, we developed the normalisation formulas,
denoted by F P and F N that calculate the overall positive and
negative sentiment in a sentence as follow:

$F_p = \min\{\frac{A_p}{2 - \log(p\times W_p)}, 100\}$

$F_n = \max\{\frac{A_n}{2 - \log(p\times W_n)}, 100\}$

where $A_p$ P , A N stand for an average of positive and nega- tive
sentiment respectively, and W P , W N represent the number of positive
and negative words applied while cal- culating A P and A N ,
respectively. The idea was to apply the logarithmic function in order
to model the relation between the number of positive/negative words
and F P / F N for a given value of the average positive/negative sen-
timent in a sentence. The parameter p determines shape of the
logarithmic function. The greater the value of p the faster the value
of F P /F N increases as the number of non- neutral words changes. In
order to determine the optimal value of p we performed a simple
statistical analysis of 13,500 tweets and analyse non-neutral words'
distribution across messages. Figure 1 demonstrated the results we
obtained.

Following the aforementioned evaluation, the formulas for calculating
the overall positive and negative sentiment of a sentence were written
as Eq. 6.

$F_p = \min\{\frac{A_p}{2 - \log(p\times W_p + I_p)}, 100\}$

$F_n = \max\{\frac{A_n}{2 - \log(p\times W_n + I_n)}, 100\}$

where I P and I N stand for the number of intensifiers that refer
respectively to positive and negative words in a sen- tence. Instead
of decreasing or increasing values of word's sentiment by 50 or 100 \%
, we simply decrease or increase the number of words by appropriate
values of 0.5 or 1, respectively.

As opposed to the train dataset, the Stanford test dataset was
manually collected and labelled hence it is more appropriate for
evaluation of the classification models' performance. It contains 177
negative, 182 posi- tive and 139 neutral manually labelled tweets. The
clas- sification accuracy of all the algorithms described in the
previous section is presented in Table 1.

\todo[inline]{Kolchyna et al., 2015}

\textbf{Traditional Supervised Machine Learning Frameworks}

\todo[inline]{\citet{Wiebe:99}}

Wiebe et al. (1999) developed an automatic system to perform
subjectivity tagging. In 10-fold cross valida- tion experiments
applied to the corpus described above, a probabilistic classifier
obtained an average accuracy on subjectivity tagging of 72.17\%, more
than 20 percentage points higher than the baseline accuracy obtained
by al- ways choosing the more frequent class.

\todo[inline]{\citet{Pang:02}}

Pang et al.  (2002) adopted a more direct approach, using super- vised
machine learning with words and n-grams as features to predict
orientation at the document level with up to 83\% precision.

\todo[inline]{}

One of the first attempts to analyze message-level sentiments on
Twitter was made by \citet{Go:09}.  For their experiments, the authors
collected a set of 1,600,000 tweets containing smileys.  Based on
these emoticons, they automatically derived polarity classes for these
messages (positive or negative) and used them to train a Na\"{\i}ve
Bayes, MaxEnt, and SVM classifier.  The best $F$-score for this
two-class classification problem could be achieved by the last system
and run up to 82.2\%.

Similar work was also done by \citet{Pak:10} who used the Na\"{\i}ve
Bayes approach to differentiate between neutral, positive, and
negative microblogs; and \citet{Barbosa:10} who gathered a collection
of 200,000 tweets, subsequently analyzing them with three publicly
available sentiment web-services and training an SVM classifier on the
results of these predictors.  In a similar way, \citet{Agarwal:11}
compared a simple unigram-based SVM approach with two other
full-fledged systems, one which relied on a rich set of manually
defined features, and another used partial tree
kernels~\cite{Moschitti:06}.  The authors evaluated these methods on a
commercially acquired corpus of 8,753 foreign-language tweets, which
were automatically translated into English, finding that a combination
of these methods worked best for both two- and three-way prediction
tasks.

The state-of-the-art results for message level polarity prediction on
tweets were established by~\citet{Mohammad:13}, whose system (a
supervised SVM classifier) used a rich set of various features
including word and character n-grams, PoS statistics, Brown
clusters~\cite{Brown:92}, etc., and also strongly benefitted from
automatic corpus-based polarity lists---Sentiment~140 and NRC
Hashtag~\cite{Mohammad:12,Kiritchenko:14}.  This approach ranked first
at the SemEval competition~2013~\cite{Nakov:13} and anchieved the
fourth place on the rerun of this task one year
later~\cite{Rosenthal:14}, being outperformed by the supervised
logistic regression approach of~\citet{Miura:14}, who used a heavy
preprocessing of the data and a special balancing scheme for
underrepresented classes.  Later on, these results were further
improved by the apporaches of~\citet{Hagen:15} and \citet{Deriu:16},
which both relied on ensembles of multiple independent classifiers.

\cite{Nakagawa:10}

Wiebe 2002, Riloff 2003

Wiebe, Bruce, \& O'Hara 1999
Hatzivassiloglou \& Wiebe 2000
Wiebe 2000;
Wiebe et al. 2002
Yu \& Hatzivassiloglou 2003

Bruce and Wiebe (1999) annotated 1,001 sentences as sub- jective or
objective, and Wiebe et al. (1999) de- scribed a sentence-level Naive
Bayes classifier using as features the presence or absence of
particular syn- tactic classes (pronouns, adjectives, cardinal num-
bers, modal verbs, adverbs), punctuation, and sen- tence position.

More recently, Wiebe et al.  (2002) report on document-level
subjectivity classi- fication, using a k-nearest neighbor algorithm
based on the total count of subjective words and phrases within each
document.

A semi-supervised classification approach was proposed
by~\citet{Yu:03}, who presented a three-stage method, in which they
first distinguished between subjective and objective documents, then
differentiated between polar and neutral sentences, and, finally,
classified the polarity of opinionated clauses.  The authors used a
Na{\"i}ve Bayes classifer for the document-level task, reaching a
remarkable \F-score of~0.96 on this objective; and applied an ensemble
of NB systems to predict the subjectivity of the sentences.  In the
final step, they determined the semantic orientation of subjective
clauses by averaging the polarity scores of their tokens, getting
these scores from an automatically constructed sentiment
lexicon~\cite{Hatzivassi:97}.  With this approach, \citeauthor{Yu:03}
attained an accuracy of~91\% on a set of 38 sentences which had a
perfect inter-annotator agreement in their data.

\textbf{Deep Neural Networks}

\citet{Yessenalina:11}

A real breakthrough in the use of deep neural networks for the
sentence-level sentiment analysis happened with the pioneering work
of~\citet{Socher:11}, who first introduced a recursive autoencoder
(RAE).  In this system, the authors obtained a fixed-width vector
representation for complex phrases $\vec{v}$ by recursively merging
the vectors of adjacent tokens (say $\vec{w}_1$ and $\vec{w}_2$),
first multiplying these vectors with a compositional matrix $W$ and
then applying a non-linear function ($softmax$) to the resulting
product:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}\right)
\end{align*}
Using a max-margin classifier on top of the resulting phrase
representation, \citet{Socher:11} could improve the state-of-the-art
results on predicting the sentence-level polarity of user's blog
posts~\cite{Potts:10} and also outperformed the system
of~\citet{Nasukawa:03} on the MPQA data set~\cite{Wiebe:05}.

Later on, \citet{Socher:12} further improved these scores with the
help of a recursive matrix-vectors space model (RMVSM), in which each
word was associated with a 2-tuple of a vector and matrix---e.g.,
$(\vec{w}_1, W_1)$ and $(\vec{w}_2, W_2)$---and the compositionality
function was redefined as follows:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  W_2\cdot\vec{w}_1\\
  W_1\cdot\vec{w}_2
  \end{bmatrix}\right)
\end{align*}

\citet{Wang:15}

\textbf{Language and Domain Adaptation}

One of the first works which pointed out the importance of domain
adaptation for sentiment analysis was introduced by~\citet{Aue:05}.
In their experiments, the authors trained separate SVM classifiers on
four different document sets: movie reviews, book reviews, customer
feedback from a product support service, and a feedback survey from a
customer knowledge base; finding that each classifier performed best
when applied to the same domain as it was trained on.  In order to
find an optimal way of overcoming this domain specificity,
\citet{Aue:05} tried out four different options:
\begin{inparaenum}[(i)]
\item\label{sent-cgsa:lst:rel-wrk1} training one classifier on all but
  the target domain and applying it to the latter;
\item using the same procedure as above, but limiting the features to
  only those which also appeared in the target texts;
\item taking an ensemble of individual classifiers each of which was
  trained on a different data collection; and, finally,
\item using a minimal subset of labeled in-domain data to train a
  Na{\"i}ve Bayes system with the expectation-maximization algorithm
  \cite[EM;][]{Dempster:77}.
\end{inparaenum}
The authors found that the ensemble and EM options worked best for
their cross-domain task, achieving an accuracy of up to 82.39\% for
the two-class prediction (positive vs negative) on new unseen text
genres.

Another notable milestone in the domain adaptation research was set
by~\citet{Blitzer:07}.  Relying on their previous work on structural
correspondence learning~\cite{Blitzer:07}, in which they used a set of
\emph{pivot features} (features which frequently appeared in both
target and source domains) to find an optimal correspondence of the
remaining attributes,\footnote{In particular, the authors trained $m$
  binary predictors for each of their $m$ pivot features in order to
  find other attributes which frequently co-occurred with the pivots.
  Afterwards, they composed these $m$ resulting weight vectors into a
  single matrix $W := [\vec{w}_{1},\ldots,\vec{w}_{m}]$, took an SVD
  decomposition of this matrix, and used the top $h$ left singular
  vectors to translate source features to the new domain.} the authors
refined their method by pre-selecting the pivots using their PMI
scores and improving misaligned feature projections using a small set
of labeled target examples.  With these modifications,
\citeauthor{Blitzer:07} were able to reduce the average adaptation
loss (the accuracy drop when transferring a classifier to a different
domain) from 9.1 to 4.9~percent when testing a sentiment predictor on
the domains of book, dvd, electical appliances, and kitchen reviews.

Other important works on domain adaptation for opinion mining include
those of~\citet{Read:05}, who pointed out that sentiment
classification might not only depend on the domain but also on topic,
time, and language style in which the text was written;
\citet{Tan:07}, who proposed using the classifier trained on the
source domain to classify unlabeled instances from the target genre,
and then iteratively retrain the system on the enriched data set.
Finally, \citet{Andreevskaia:08} proposed a combination of a lexicon-
and ML-based systems, claiming that this ensemble would be more
resistible to the domain shift than each of these classifiers on their
own.

Another line of research was introduced by~\citet{Glorot:11} who
proposed stacked denoising autoencoders (SDA)---a neural network
architecture in which an input vector $\vec{x}$ was first mapped to a
smaller representation $\vec{x}'$ via some function
$h: \vec{x}\mapsto\vec{x}'$, and then restored to its approximate
original state via an inverse transformation
$g: \vec{x}'\mapsto\vec{x}''\approx\vec{x}$.  In their experiments,
the authors optimized the parameters of the functions $h$ and $g$ on
both target and source data, getting approximate representations of
instances from both data sets; and then trained a linear SVM
classifier on the restored representations of the source instances,
subsequently applying this classifier to the target domain.  This
approach was further refined by~\citet{Chen:12} who analytically
computed the reconstruction function~$g$, and used both original and
restored features to predict the polarity labels of the target
data.\footnote{Both approaches were trained tested on the Amazon
  Review Corpus of~\citet{Blitzer:07}.}


Further notable contributions to domain adaptation in general were
made by~\citet{Daume:07} who proposed to replicate each extracted
feature three times and train the first replication on both domains,
the second repetion only on source, and the third copy only on target
domain, for which he assumed a small subset of labeled examples was
available; \citet{Yang:15} who trained neural embeddings of features,
trying to predict which instance attributes frequently co-occured with
each other;

\section{Summary and Conclusions}\label{slsa:subsec:conclusions}
