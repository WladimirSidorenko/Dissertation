% FILE: sentiment_fgsa.tex  Version 0.0.1
% AUTHOR: Uladzimir Sidarenka

% This is a modified version of the file main.tex developed by the
% University Duisburg-Essen, Duisburg, AG Prof. Dr. Günter Törner
% Verena Gondek, Andy Braune, Henning Kerstan Fachbereich Mathematik
% Lotharstr. 65., 47057 Duisburg entstanden im Rahmen des
% DFG-Projektes DissOnlineTutor in Zusammenarbeit mit der
% Humboldt-Universitaet zu Berlin AG Elektronisches Publizieren Joanna
% Rycko und der DNB - Deutsche Nationalbibliothek

\section{Fine-Grained Sentiment Analysis}\label{sec:snt:fgsa}

The main goal of fine-grained sentiment analysis (FGSA) is
traditionally defined as the identification of subjective evaluative
opinions (\emph{sentiments}), the holders of these opinions
(\emph{sources}), and their respectively evaluated objects
(\emph{targets}) in text.  Since an accurate automatic prediction of
these elements would enable us to track public attitude to literally
any object (e.g., a product, a service, or a political decision), FGSA
is commonly considered to be one of the most attractive, necessary,
but, unfortunately, also challenging goals in computational
linguistics.

Researchers usually consider this objective as a sequence labeling
(SL) task and address it with either of the two popular SL techniques:
conditional random fields or recurrent neural networks.  The former
approach represents a discriminative probabilistic graphical model
which relies on hand-crafted features; the latter framework utilizes a
recursive computational loop which can learn feature representations
completely automatically.  In this section, we are going to evaluate
each of these methods in detail in order to find out which of these
algorithms is better suited for the domain of German Twitter.
However, before we proceed with our evaluation, we should first make a
short linguistic digression and briefly discuss the definition of
textual spans, to which these approaches should assign their labels,
and the evaluation metrics, with which we should estimate the quality
of this assignment.

\subsection{Definition of Sentiment, Target, and Source Spans}
Despite some notable advances and an ongoing active research on
fine-grained opinion extraction, the crucial task of defining the
exact boundaries of sentiment spans and the spans of their respective
targets and sources has not been addressed in the literature with the
due attention yet.  Researchers typically overlook this problem,
leaving its solution to the discretion of their annotators
\cite[cf.][]{Wiebe:05,Klinger:13}.

In contrast to this, instead of relying on intuitive decisions of our
coders, we explicitly defined a rule for determining opinions'
boundaries by telling the experts to assign the \texttt{sentiment} tag
to ``\emph{minimal complete syntactic or discourse-level units that
  included both the target of an opinion and its actual evaluation}''.

In other words, during the annotation, our coders first had to
identify evaluated objects (targets) in text, then find the respective
evaluative expressions of these objects (usually but not necessarily
polar terms), and, finally, determine the smallest syntactic
components (noun or verb phrases) or discourse units (clauses or
sentences) where both of these entities appeared together.  An
annotation example labeled in compliance with this rule is provided
below:
\begin{example}\label{snt:fgsa:exmp:sent-anno1}
  \upshape\sentiment{Der neue Papst gilt als
    bescheidener, zur\"uckgenommener Typ.}\\[0.8em]
  \noindent\sentiment{The new Pope is believed to be a sober, modest
    man.}
\end{example}
\noindent In this sentence, our experts had to label the complete
clause as a sentiment, since this unit was the minimal syntactic
constituent which included both the object of an evaluation---``der
neue Papst'' (\textit{the new pope})---and the evaluation
itself---``bescheidener, zur\"uckgenommener Typ'' (\textit{a sober,
  modest man}).

We applied the same principles of minimality and completeness to the
annotation of targets and sources, requiring the main components of
these elements (typically nouns or verbs) to be labeled along with all
their syntactic dependants.  Accordingly, the correct annotation of
the target in the previous example had to look as follows:
\begin{example}\label{snt:fgsa:exmp:sent-anno2}
  \upshape\sentiment{\target{Der neue Papst} gilt als
    bescheidener, zur\"uckgenommener Typ.}\\[0.8em]
  \noindent\sentiment{\target{The new Pope} is believed to be a sober,
    modest man.}
\end{example}
\noindent with the \texttt{target} tag encompassing the whole noun
phrase---``der neue Papst'' (\textit{the new pope})---not only its
main noun.

Similarly, source elements had to cover complete syntactic structures
as shown in Example~\ref{snt:fgsa:exmp:src-anno1}:
\begin{example}\label{snt:fgsa:exmp:src-anno1}
  \upshape\sentiment{Die Homosexuellenehe war f\"ur \source{den Kardinal, der jetzt Papst ist,} eine Zerst\"orung von Gottes Plan}\\[0.8em]
  \noindent\sentiment{For \source{the cardinal, who is the Pope now,}
    the same-sex marriage was a destruction of God's plan.}
\end{example}
\noindent This time, again, the whole noun phrase including its
dependent attributive clause---``den Kardinal, der jetzt Papst ist,''
(\textit{the cardinal, who is the Pope now,})---had to be labeled with
the \texttt{source} tag because this constituent was the only
\emph{minimal complete} syntactic node which encompassed both the
immediate holder of the opinion---``Kardinal'' \textit{cardinal}---and
its grammatical dependants without including any of its parental
elements.

\subsection{Data}\label{snt:fgsa:subsec:data}

For our experiments, we split our final corpus into three parts using
70\% of it as a training set, 15\% as cross-validation data, and the
remaining 15\% for testing purposes.  The data were tokenized with an
adjusted version of Christopher Potts'
Twitter-tokenizer\footnote{\url{http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py}}
and preprocessed using the rule-based normalization approach of
\citet{Sidarenka:13}.

%% During the normalization, Twitter-specific phenomena like @-mentions,
%% retweets, and URIs that were not syntactically integrated in any
%% sentence of the message were removed from the tweets and those
%% elements which played an integral syntactic role were replaced with
%% the special artificial tokens \%User, \%Link etc.  Emoticons like :-),
%% \smiley{}, \frownie{} etc. were also replaced with the placeholders
%% \%PosSmiley, \%NegSmiley, or simply \%Smiley depending on their prior
%% polarity.  Furthemore, out-of-vocabulary words which could be
%% converted to in-vocabulary terms with a pre-defined set of
%% transformations were also normalized.

We then labeled the preprocessed data with their part-of-speech tags
using \texttt{TreeTagger}\footnote{We used \texttt{TreeTagger} Version
  3.2 with the German parameter file UTF-8.}  \cite{Schmid:95} and
parsed obtained sentences with the \texttt{Mate} dependency
parser\footnote{We used \texttt{Mate} Version \texttt{3.61} with the
  German parameter model 3.6.}  \cite{Bohnet:13}.  These automatically
derived trees were then used as the source of syntactic features for
all our models and the underlying graph structures for tree-CRFs.

Since \texttt{MMAX2} did not provide a straightforward support for
storing character offsets of the tokens and the automatically
tokenized preprocessed data did not necessarily agree with the corpus
tokenization, we next applied the Needleman-Wunsch algorithm
\cite{Needleman:70} to align the manual annotation with the
automatically segmented data.

\subsection{Evaluation Metrics}
A natural question which arises after defining the span boundaries for
human coders is that of the best way to compare these spans with
automatically assigned labels.  One possibility to estimate the
quality of such automatic assignment is to compute the precision,
recall, and \F{}-scores using either the binary overlap or exact match
metric \cite{Choi:06,Breck:07} .  The former method considers an
automatically labeled span as correct if it has at least one token in
common with a labeled entity from the gold annotation.  The latter
metric only regards as true positives automatic spans which have
absolutely identical boundaries with the gold labels.  Unfortunately,
both of these approaches are somewhat problematic: While the binary
overlap might be overly optimistic, always assigning perfect scores to
automatic spans which cover the whole sentence; the method of exact
match might, vice versa, be too drastic, considering the whole
assignment as false if only one (possibly irrelevant) token is
missing.

Instead of relying on these measures, we opted for the ``golden mean''
solution to this problem which was proposed by \citet{Johansson:10a}.
In their work, the authors introduced another metric for estimating
the quality of an automatic assignment, in which they penalized the
predicted spans proportionally to the number of tokens whose labels
were different from the gold annotation.  More precisely, given a set
of manually annotated gold entities $\mathcal{S}$ and automatically
tagged spans $\widehat{\mathcal{S}}$, they estimated the precision of
the automatic assignment as:
\begin{equation*}
  P(\mathcal{S}, \widehat{\mathcal{S}}) = \frac{C(\mathcal{S}, \widehat{\mathcal{S}})}{|\widehat{\mathcal{S}}|},
\end{equation*}
where the \emph{span coverage} metric
$C(\mathcal{S},\widehat{\mathcal{S}})$ was computed as the number of
overlapping tokens across all pairs of manually ($s_i$) and
automatically ($s_j$) annotated entities: $C(\mathcal{S},
\widehat{\mathcal{S}}) = \sum_{s_i \in \mathcal{S}}\sum_{s_j \in
  \widehat{\mathcal{S}}}c(s_i, s_j)$; and $|\widehat{\mathcal{S}}|$
meant the total number of tokens automatically labeled with the given
tag.  Similarly, the recall of such assignment was estimated as:
\begin{equation*}
  R(\mathcal{S}, \widehat{\mathcal{S}}) = \frac{C(\mathcal{S}, \widehat{\mathcal{S}})}{|\mathcal{S}|},
\end{equation*}
and the \F{}-measure was normally computed as the harmonic mean of the
precision and recall scores:
\begin{equation*}
  F_1 = 2\times\frac{P \times R}{P + R}.
\end{equation*}
Since this proportional estimation adequately accommodates both
extrema of an automatic assignment---too long and too short
spans---and also penalizes for erroneous and spurious labels, we will
rely on it throughout our subsequent experiments.

\subsection{Fine-Grained Sentiment Analysis with Conditional Random
  Fields} The first automatic approach that we are going to evaluate
in this section is that of the conditional random fields (CRFs).
First introduced by \citet{Lafferty:01}, CRFs rapidly grew in
popularity, turning into one of the most commonly used probabilistic
frameworks, which dominated the NLP field for more than a decade.  The
main resons for such huge success were:
\begin{enumerate}[1)]
\item the \emph{structural nature} of CRFs, which, in contrast to
  single-entity classifiers such as logistic regression or SVM, make
  their predictions over a sequence of covariates, trying to find the
  most likely label assignment for the whole sequence not only its
  individual elements;
\item the \emph{discriminative power} of this framework, which, in
  contrast to generative probabilistic models such as HMM
  \cite{Rabiner:86}, optimizes the conditional probability
  $P(\boldsymbol{Y}|\boldsymbol{X})$ instead of maximizing the joint
  distribution $P(\boldsymbol{X},\boldsymbol{Y})$ and consequently can
  efficiently deal with overlapping and correlated features;
\begin{example}[Overlapping and Correlated Features]
  In order to demonstrate the different effects of correlated and
  overlapping features on generative and discriminative models, let us
  go through an example where we need to predict whether a tweet
  mentioning ``Merkel'' and ``Steinmeier'' is about the Christian
  Democratic Union (\texttt{CDU}) or Social Democratic Party of
  Germany (\texttt{SPD}).  As features for this task, we will use
  lexical unigrams appearing in the training data.  Assuming that our
  training set consists of three messages mentioning ``Merkel'' and
  one microblog mentioning ``Steinmeier'' which are labeled as
  \texttt{CDU} plus one tweets mentioning ``Merkel'' and three posts
  mentioning ``Steinmeier'' which are annotated as \texttt{SPD}, the
  na\"{i}ve Bayes model will estimate the probability of the two
  competing classes as:
  \begin{align*}
    P(\mathbf{x}, CDU) =& P(\textrm{Merkel},\textrm{Steinmeier}|CDU)\times P(CDU)\\
    =& P(\textrm{Merkel}|CDU)\times P(\textrm{Steinmeier}|CDU) \times P(CDU)\\
    =&\frac{3}{4}\times\frac{1}{4}\times\frac{4}{8}\approx 0.0938\\
    P(\mathbf{x}, SPD) =& P(\textrm{Merkel},\textrm{Steinmeier}|SPD)\times P(SPD)\\
    =& P(\textrm{Merkel}|SPD)\times P(\textrm{Steinmeier}|SPD) \times P(SPD)\\
    =&\frac{1}{4}\times\frac{3}{4}\times\frac{4}{8}\approx 0.0938.\\
  \end{align*}
  After normalizing these probabilities, we will get equal 50\%
  chances for each of the parties, which is fair regarding the token
  distribution in our corpus.  However, if we replace ``Merkel'' with
  ``von der Leyen'' in the training data and test example and rerun
  this experiment, the probability will get significantly skewed:
  \begin{align*}
    P(\mathbf{x}, CDU) =& P(\textrm{von},\textrm{der},\textrm{Leyen},\textrm{Steinmeier}|CDU)\times P(CDU)\\
    =& P(\textrm{von}|CDU)\times P(\textrm{der}|CDU)\times P(\textrm{Leyen}|CDU)\\
    &\times P(\textrm{Steinmeier}|CDU) \times P(CDU)\\
    =&\frac{3}{4}\times\frac{3}{4}\times\frac{3}{4}\times\frac{1}{4}\times\frac{4}{8}\approx 0.0527\\
    P(\mathbf{x}, SPD) =& P(\textrm{von},\textrm{der},\textrm{Leyen},\textrm{Steinmeier}|SPD)\times P(SPD)\\
    =& P(\textrm{von}|SPD)\times P(\textrm{der}|SPD)\times P(\textrm{Leyen}|SPD)\\
    &\times P(\textrm{Steinmeier}|SPD) \times P(SPD)\\
    =&\frac{1}{4}\times\frac{1}{4}\times\frac{1}{4}\times\frac{3}{4}\times\frac{4}{8}\approx 0.0059,\\
  \end{align*}
  which, after normalization, would result in 90\% chances for
  \texttt{CDU} and a 10\% score for \texttt{SPD}, even though we only
  changed the name of the politician.

  A different situation is observed for discriminative models such as
  maximum entropy: Instead of optimizing the joint distribution
  $P(\mathbf{x}, y)$ as it is done by generative frameworks,
  discriminative classifiers seek to optimize the conditional
  likelihood $P(y|\mathbf{x})$ by maximizing the probability of the
  training set $\sum_{i=1}^N\log P(y_i|\mathbf{x}_i, \mathbf{w})$,
  where $N$ is the number of training instances, $y_i$ is the gold
  label for the $i$-th instance (e.g., 1 for \texttt{CDU} and 0 for
  \texttt{SPD}), $\mathbf{x}_i$ stands for the feature vector of this
  instance, and $\mathbf{w}$ represent the learned coefficients of
  these feature.  The probability $P$ is typically normally computed
  using the sigmoid function $\frac{1}{1 + e^{-(\mathbf{x}_i,
      \mathbf{w})}}$.  Working through the partial derivatives of this
  probability, we will arrive at the solution $w_1 \approx 0.5$ for
  the feature ``Merkel'' and $w_2 \approx -0.5$ for the feature
  ``Steinmeier'' for the first example, which would again result in
  equal 50\% chances for both classes.  In the second example,
  however, all three features ``von'', ``der'', and ``Leyen'' will get
  an equal weight of $\approx 0.3$, and the ``Steinmeier'' feature
  will receive a coefficient of $\approx -0.4$, which would result in
  60\% probability for the test message being about the CDU and 40\%
  that the tweet is about the SPD.  Even though this still means a
  slight skewness towards \texttt{CDU}; this time, the effect of
  correlated features is much less dramatic than in the generative
  case.
\end{example}
\item and, finally, the \emph{avoidance of the label bias problem}, to
  which other discriminative classifiers such as MEMM
  \cite{McCallum:00} are known to be susceptible.
  \begin{example}[Label Bias Problem]
    The label bias problem arises in the cases where a locally optimal
    decision outweights globally superior solutions.  Consider, for
    example, the sentence ``Aber gerade Erwachsene haben damit
    Schwierigkeiten.'' (\textit{But especially adults have
      difficulties with it.}), for which we need to compute the most
    probable sequence of part-of-speech tags.

    \begin{center}
      \begin{tikzpicture}[node distance=5cm]
        \tikzstyle{tag}=[circle split,draw=gray!50,%
          minimum size=2.5em,inner ysep=2,inner xsep=0,%
          circle split part fill={yellow!20,blue!30}]
      \tikzstyle{word}=[draw=none,inner sep=10pt]

      \node[word] (A) at (1, 1) {Aber};
      \node[tag] (B) at (1, 3) {\footnotesize KON \nodepart{lower} 1.};
      \node[word] (D) at (3, 1) {gerade};
      \node[tag] (E) at (3, 2) {\footnotesize ADJA \nodepart{lower} .5};
      \node[tag] (F) at (3, 4) {\footnotesize ADV \nodepart{lower} .5} ;
      \node[word] (G) at (7, 1) {Erwachsene};
      \node[tag] (I) at (7,2) {\footnotesize ADJA \nodepart{lower} .5} ;
      \node[tag] (H) at (7,4) {\footnotesize NN \nodepart{lower} .5};
      \node[word] (J) at (9,1) {haben};
      \node[tag] (K) at (9,3) {\footnotesize VA \nodepart{lower}\small 1.};
      \node[word] (J) at (11,1) {\ldots};

      \path [-] (B) edge node[below] {$.5$} (E);
      \path [-] (B) edge node[above] {$.5$} (F);

      \path [-] (E) edge node[below] {$.3$} (I);
      \path [-] (E) edge node[below left=0.4] {$.7$} (H);
      \path [-] (F) edge node[above left=0.4] {$.8$} (I);
      \path [-] (F) edge node[above] {$.2$} (H);

      \path [-] (I) edge node[below] {$.1$} (K);
      \path [-] (H) edge node[above] {$.9$} (K);
    \end{tikzpicture}
    \captionof{figure}{\emph{Feature weights for states and
        transitions of the part-of-speech example.}\label{fig:snt:memm-crf}}
    \end{center}
    Assuming that feature weights are distributed as shown in
    Figure~\ref{fig:snt:memm-crf}, we will first estimate the
    probability of the correct label sequence for the initial part of
    this sentence using the Maximum Entropy Markov Model (MEMM)---the
    predecessor of Conditional Random Fields.  According to the MEMM's
    definition, the probability of the correct labeling
    ($KON-ADV-NN-VA$) is equal to:
    \begin{align*}
      P(KON, ADV, NN, VA) =& P(KON)\times P(ADV|KON)\times P(NN|ADV)\times P(VA|NN)\\
      =&\frac{\exp(1)}{\exp(1)}\times\frac{\exp(0.5 + 0.5)}{\exp(0.5 + 0.5) + \exp(0.5 + 0.5)}\\%
      \times&\frac{\exp(0.2 + 0.5)}{\exp(0.2 + 0.5) + \exp(0.8 + 0.5)}\times\frac{\exp(0.9 + 1.)}{\exp(0.9 + 1.)} \approx 0.177
    \end{align*}
    At the same time, the probability of the incorrect variant
    ($KON-ADV-ADJA-VA$) would amount to $\approx$ 0.323 and will
    therefore be preferred during the automatic tagging.

    A different situation is observed with the CRFs, where the
    normalizing factor in the denominator is computed over the whole
    input sequence without factorizing into individual terms for each
    transition as it was done in the previous case.  That way, the
    probability of the correct labels would amount to:
    \begin{align*}
      P(KON, ADV, NN, VA) =& P(KON)\times
      P(ADV|KON)\times P(NN|ADV)\times P(VA|NN)\\ =&\frac{\exp(1 + 0.5
        \times 3 + 0.2 + 0.9 + 1)}{Z} \approx 0.252,
    \end{align*}
    where $Z = \exp(1 + 0.5 \times 3 + 0.2 + 0.9 + 1) + \exp(1 + 0.5
    \times 3 + 0.8 + 0.1 + 1) + \exp(1 + 0.5 \times 3 + 0.7 + 0.9 + 1)
    + \exp(1 + 0.5 \times 3 + 0.3 + 0.1 + 1)$ is the total score of
    all possible label assignments.  The incorrect alternative
    ($KON-ADV-ADJA-VA$), however, will get a probability score of
    $\approx$ 0.207, which is smaller than the likelihood of the
    correct labeling.
  \end{example}
\end{enumerate}
\textbf{Training} CRFs attain these useful properties thanks to a
neatly formulated objective function in which they seek to optimize
the global log-likelihood of the gold labels $\mathbf{Y}$ conditioned
on the training data $\mathbf{X}$.  In particular, given a set of
training instances $\mathcal{D} = \{(\mathbf{x}^{(n)},
\mathbf{y}^{(n)})\}_{n=1}^N$, CRF's training adds up to finding
feature coefficients $\mathbf{w}$ that maximize the log-probabilities
$\ell$ of the dependent variables $\mathbf{y}^{(i)}$ given their
respective covariates $\mathbf{x}^{(i)}$ over the whole corpus:
\begin{equation}\label{snt:fgsa:eq:crf-ell}
  \mathbf{w} = \argmax_{\mathbf{w}}\sum_{n=1}^N\ell
  \left(\mathbf{y}^{(n)}|\mathbf{x}^{(n)}\right).
\end{equation}
The likelihood term $\ell(\mathbf{y}^{(n)}|\mathbf{x}^{(n)})$ in this
equation is commonly estimated using a globally normalized softmax
function:
\begin{equation}
  \ell\left(\mathbf{y}^{(n)}|\mathbf{x}^{(n)}\right) =
  \ln\left(P(\mathbf{y}^{(n)}|\mathbf{x}^{(n)})\right) =
  \ln\left(\frac{ \exp\left(\sum_{m=1}^{M}\sum_jw_{j} \times f_j(x_{m},
    y_{m-1}, y_{m})\right)}{Z}\right),
\end{equation}
where $M$ stands for the length of the $n$-th training instance
$|\mathbf{x}^{(n)}|$, $f_j(x_{m}, y_{m-1}, y_{m})$ denotes the value
of the $j$-th feature function $f$ at the sequence position $m$, $w_j$
represents the corresponding weight of this feature, and $Z$ is a
normalization factor calculated over all possible label assignments:
\begin{equation}
  Z =
  \sum_{y'\in\mathcal{Y},y''\in\mathcal{Y}}\exp\left(\sum_{m=1}^{M}\sum_jw_{j}
  \times f_j(x_{m}, y'_{m-1}, y''_{m})\right).
\end{equation}
Taking the partial derivative of the log-likelihood function from
Equation~\ref{snt:fgsa:eq:crf-ell} w.r.t. the feature weight $w_j$, we
arrive at the solution:
\begin{equation}
  \frac{\partial}{\partial w_j}\ell =%
  \sum_{n=1}^N\sum_{m=1}^Mf_j(x_{m}, y_{m-1}, y_{m}) -%
  \sum_{n=1}^N\sum_{m=1}^{M}\sum_{y'\in\mathcal{Y},y''\in\mathcal{Y}}f_j(x_{m},%
  y'_{m-1}, y''_{m})P(y',y''|\mathbf{x}^{(n)}).
\end{equation}
After dividing both parts of this equation with the constant term $N$,
we eventually obtain the final form for this derivative:
\begin{equation}
  \frac{1}{N}\frac{\partial}{\partial w_j}\ell = \E[f_j(\mathbf{x},
    \mathbf{y}] - \E_{\mathbf{w}}[f_j(\mathbf{x}, \mathbf{y})],
\end{equation}
where the first term is the expectation of the feature $f_j$ under the
empirical distribution, and the second term is its expectation under
the model's parameters $\mathbf{w}$.  The marginal probability of the
feature required for computing the latter term can be easily estimated
dynamically using the forward-backward (FB) algorithm
\cite{Rabiner:90}---a particular case of the belief propagation method
\cite[cf.][p.~81]{Barber:12}.

\noindent\textbf{Inference} Once the optimal feature weights are
learned, one can unproblematically compute the most likely label
assignment for a new instance by using the Viterbi algorithm
\cite{Viterbi:67}, which effectively corresponds to the forward pass
of the FB method with the summation over alternative preceding states
replaced by the maximum operator (hence the other name for this
algorithm---``max-product'').

\noindent\textbf{Features} A crucial component which accounts for a
huge part of the success (or failure) of a CRF system are feature
attributes defined by the developer.  Tranditionally, feature
functions used in Conditional Random Fields are divided into
transition- and state-based ones.  The former attributes represent
real- or binary-valued functions $f(\mathbf{x}, y'',
y')\rightarrow\mathbb{R}$ associated with some data predicate
$\phi(\mathbf{x})\rightarrow\mathbb{R}$ and labels $y''$ and $y'$.
The value of this attribute at position $m$ in the sequence
$\mathbf{x}$ is then normally defined as:
\begin{equation}
  f(\mathbf{x}_m, y'', y') = \begin{cases} \phi(\mathbf{x}_m), &
    \mbox{if } \mathbf{y}_{m-1} = y''\mbox{ and }\mathbf{y}_{m} =
    y'\\ 0, & \mbox{otherwise.}
  \end{cases}
\end{equation}
The most common predicate used for this type of features is a simple
unit function~$\phi$, which always yields one regardless of the data
input.  However other, more complex conditions and return values are
possible and fairly straightforward to implement.

In contrast to the ternary transition functions, state attributes
typically represent binary predicates, whose output depends solely on
the input data at the given position and the label $y'$ at the
respective state:
\begin{equation}
  f(\mathbf{x}_m, y') = \begin{cases} \phi(\mathbf{x}_m), & \mbox{if }
    \mathbf{y}_{m} = y'\\ 0, & \mbox{otherwise.}
  \end{cases}
\end{equation}
This type of attributes commonly accounts for the overwhelming
majority of all features used in a CRF system.

As state attributes in our experiments, we used the following types of
predicates (which, for simplicity, are listed here in groups):
\begin{itemize}
\item\emph{formal}, which included the initial three characters of
  each token (e.g., $\phi_{abc}(\mathbf{x}_m) = 1\mbox{ if
  }\mathbf{x}_m\sim\mbox{ /\textasciicircum abc/ else } 0$), its last
  three characters, and the general spelling class of the token (e.g.,
  alphanumeric, digit, or punctuation);

\item\emph{morphological}, which encompassed the part-of-speech tag of
  the analyzed token, case and gender values for inflectable PoS
  types, the degree of comparison for adjectives, and mood, tense, and
  person forms for verbs;

\item\emph{lexical}, which comprised the actual lemma and form of the
  token (using one-hot encoding), its polarity class (positive,
  negative, or neutral), which we obtained from the Zurich Polarity
  Lexicon~\cite{Clematide:10};

\item and, finally, \emph{syntactic}, which included the dependency
  relation via which the token $\mathbf{x}_m$ was connected to its
  parent, two binary features reflecting whether the previos token in
  the sentence was the parent or child of the current word, as well as
  two other features, one of which encoded the dependency relation of
  the previous token in the sentence to its parent + dependency
  relation of the current token to its ancestor and the other
  reflected the dependency link of the next token + dependency
  relation of the current token to its parent.
\end{itemize}

In addition to the above attributes, we also used a set of complex
lexico-syntactic features, which simultaneously combined several
semantic and syntactic traits.  These included:
\begin{itemize}
\item the lemma of the syntactic parent;
\item the part-of-speech tag and the polarity class of the
  grandparent;
\item the lemma of the child node + dependency relation emanating from
  this child;
\item the PoS tag of the child node + its dependency relation + PoS
  tag of the current token;
\item the lemma of the child node + its dependency relation + lemma of
  the current token;
\item the overall polarity of children, which was computed by summing
  up polarity scores of all immediate syntactic descendants and
  checking whether the resulting value was greater, less than or equal
  to zero.\footnote{We again used the Zurich Polarity Lexicon
    of~\citet{Clematide:10} for computing these scores.}
\end{itemize}

\textbf{Results} The results of our experiments are shown in
Table~\ref{snt-fgsa:tbl:crf-res}.  As can be seen from the table, with
the given features, the model can perfectly well fit the training set,
achieving a macro-averaged \F-score of~0.904.  The learned parameters,
however, only partially generalize to unseen data, leading to notably
lower \F-results on the test corpus (0.287).  This disbalance
indicates a strong ``overfitting'' of the model to the training data
(i.e., the model assigns heigher weights to rather sporadic, noisy
features, which only accidentally co-occurred with the target classes
in the observed training instances).

Another notable tendency, which can be observed both on the training
and test splits, is that the recall of the CRF system is generally
lower than its precision.  This again can be attributed to the
overfitting effect, due to which, less indicative features become more
important than attributes which actually give rise to subjective
evaluations.  Since the former features, might not to appear in the
test data or are unlikely to correlate with the sentiment entities
even if they occur, the model often fails to recognize sentiments in
contexts which do have important but underweighted features.

\begin{table*}
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{1}{>{\centering\arraybackslash}p{0.136\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Data Set} & \multicolumn{3}{c}{\bfseries Sentiment} & %
      \multicolumn{3}{c}{\bfseries Source} & %
      \multicolumn{3}{c}{\bfseries Target} & %
      \multirow{2}{0.136\columnwidth}{\bfseries\centering Macro\newline \F{}}\\
      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} &\\\midrule

      Training Set & 0.949 & 0.908 & 0.928 & 0.903 & 0.87 & 0.886 & %
      0.933 & 0.865 & 0.898 & 0.904\\
      Test Set & 0.37 & 0.28 & 0.319 & 0.305 & 0.244 & 0.271 & 0.304 & %
      0.244 & 0.271 & 0.287\\\bottomrule
    \end{tabular}
    \egroup
    \caption{Results of fine-grained sentiment analysis with the
      first-order linear-chain CRFs.}
    \label{snt-fgsa:tbl:crf-res}
  \end{center}
\end{table*}

\subsection{Fine-grained Sentiment Analysis with Recurrent Neural Networks}

A competive modern alternative to the conditional random fields are
deep recurrent neural networks (RNNs).  Introduced in the
mid-nineties~\cite{Hochreiter:97}, RNNs became one of the most widely
used trends in the recent surge of deep learning.  The main factors
for this success are
\begin{inparaenum}[(i)]
\item the ability of these systems to learn the optimal feature
  representations automatically, which favorably sets them apart from
  traditional supervised machine learning frameworks such as SVMs or
  CRFs, where all features need to be defined by user; and
\item the ability to deal with arbitrary sequence lengths, which
  advantageously distinguishes them from other NN architectures such
  as CNN or feed-forward networks, where the size of all perceptron
  layers needs to be fixed.
\end{inparaenum}

The key component which underlies any popular RNN system is a
fixed-size hidden vector $\vec{h}$, which is recurrently updated over
the input sequence $\mathbf{x}$ and is supposed to encode the meaning
of the input seen so far.  The general form of this vector at the time
step $t$ is typically defined as:
\begin{align*}
  \vec{h}^{(t)} = f(\vec{h}^{(t-1)}, \mathbf{x}^{(t)});
\end{align*}
where $f$ usually represents as a pointwise non-linear transformation
such as a logistic function or the hyperbolic tangent,
$\vec{h}^{(t-1)}$ denotes the state of the hidden vector at the
previous time step, and $\mathbf{x}^{(t)}$ is the input vector at the
step $t$.

\textbf{LSTM} A fundamental problem which arises from the above
definition is that the gradients of the trained parameters, which are
used to estimate the $\vec{h}$ vector, rapidly vanish to zero or
explode to infinity (depending on whether the abosule values of
$\vec{h}$ are less or greater than one) as the length of the input
increases.  In order to solve this issue, \citet{Hochreiter:97}
proposed the long short-term memory mechanism (LSTM), in which they
explicitly incorporated the goal to drop parts of the input which
appeared to be irrelevant for the final outcome.  In particular, given
an input sequence $\mathbf{x}$, they introduced a special
\emph{activation} unit $\vec{i}^{(t)}$:
\begin{align*}
  \vec{i}^{(t)} &= \sigma\left(W_i\cdot \mathbf{x}^{(t)} + U_i \cdot \vec{h}^{(t-1)} + \vec{b}_i\right),
\end{align*}
where $\sigma$ denotes the sigmoid function; $W_i$, $U_i$, and
$\vec{b_i}$ represent the optimized model's parameters; and
$\mathbf{x}^{(t)}$ and $\vec{h}^{(t-1)}$ represent the input and the
previous hidden state respectively.  In addition to that, they also
estimated a dedicated \emph{forget} gate~$\vec{f}^{(t)}$:
\begin{align*}
  \vec{f}^{(t)} &= \sigma\left(W_f\cdot \mathbf{x}^{(t)} + U_f
  \cdot \vec{h}^{(t-1)} + \vec{b}_f\right).
\end{align*}
Then, after computing an \emph{intermediate update} value for the
current time step~$\widetilde{C}^{(t)}$:
\begin{align*}
  \widetilde{C}^{(t)} &= tanh\left(W_c\cdot \mathbf{x}^{(t)} + U_c
  \cdot \vec{h}^{(t-1)} + \vec{b}_c\right),
\end{align*}
they estimated the \emph{final update} state~$C^{(t)}$ by taking a
weighted sum of the candidate update vector~$\widetilde{C}^{(t)}$ and
the previous update value~$C^{(t-1)}$:
\begin{align*}
  C^{(t)} &= \vec{i}^{(t)} * \widetilde{C}^{(t)} + \vec{f}^{(t)} * C^{(t-1)}.
\end{align*}
From this term, they finally computed the output vector
$\vec{o}^{(t)}$ and the new value of the hidden state $\vec{h}^{(t)}$:
\begin{align*}
  \vec{o}^{(t)} &= \sigma\left(W_o\cdot \mathbf{x}^{(t)} + U_o \cdot \vec{h}^{(t-1)} + \vec{b}_o\right),\\
  \vec{h}^{(t)} &= \vec{o}^{(t)} * tanh(C^{(t)}).
\end{align*}

\textbf{GRU} Despite their enormous popularity and many successful
practical applications~\cite[cf.][]{Filippova:15,Ghosh:16,Rao:16},
LSTMs are often criticized for the high complexity of the recurrent
unit.  In order to overcome this issue while still keeping the
gradients within an appropriate interval, \citet{Cho:14a} introduced
an alternative architecture called Gated Recurrent Units (GRU).  In
this framework, the authors also used activation and forget gates
($\vec{i}^{(t)}$ and $\vec{f}^{(t)}$ respecitvely) similar to the ones
defined by~\citet{Hochreiter:97}.  They dropped however the
gate-specific bias terms $\vec{b}_i$ and $\vec{b}_f$ for simplicity:
\begin{align*}
  \vec{i}^{(t)} &= \sigma\left(W_i\cdot \mathbf{x}^{(t)} + U_i \cdot \vec{h}^{(t-1)}\right),\\
  \vec{f}^{(t)} &= \sigma\left(W_f\cdot \mathbf{x}^{(t)} + U_f \cdot \vec{h}^{(t-1)}\right).
\end{align*}
The candidate activation $\widetilde{C}^{(t)}$ was then computed as:
\begin{align*}
  \widetilde{C}^{(t)} &= \phi\left(W_c\cdot \mathbf{x}^{(t)} + U_c
  \cdot \left(\vec{f}^{(t)} * \vec{h}^{(t-1)}\right)\right),
\end{align*}
and the final hidden state $\vec{h}^{(t)}$ was estimated as follows:
\begin{align*}
  \vec{h}^{(t)} &= \vec{i}^{(t)} * \vec{h}^{(t-1)} + \left(\vec{1} -
  \vec{i}^{(t)}\right) * \widetilde{C}^{(t)}.
\end{align*}

\textbf{Training} A neat property of both of these approaches is that
the final equation, which is obtained after unrolling the loop, is
differentiable with respect to all of its parameters and can therefore
be optimized using the standard gradient update methods.  Since most
of these parameters, however, represent high-dimensional vectors or
matrices, finding an optimal learning rate (i.e., the size of the
update step taken in the direction of the gradient) might pose
considerable difficulties, leading either to a prohibitively large
training times (if the steps are too small) or divergence of the
trained model (if the steps are too big).

Several algorithms have been proposed in order to solve this issue,
including the method of momentum~\cite{Rumelhart:88},
AdaGrad~\cite{Duchi:11}, AdaDelta~\cite{Zeiler:12},
RmsProp~\cite{Tieleman:12} etc.  In our RNN experiments, we will rely
on the last option---the RmsProp optimization proposed
by~\citet{Tieleman:12}---as we found this algorithm leading to both a
faster convergence and superior classification quality.

Another important factor, which might significantly affect the
training, is the initialization of the learned model's parameters.  As
shown by~\citet{He:15}, an inappropriate choice of the initial values
might result in a complete stalling of the learning process.
Following the recommended practices~\cite{Saxe:13}, we initialized all
parameter matrices to orthogonal values in order to ensure their full
rank and used He initialization~\cite{He:15} for setting the values of
the bias vectors.

\textbf{Inference}

\textbf{Results}
\begin{table*}
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{1}{>{\centering\arraybackslash}p{0.136\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Data Set} & \multicolumn{3}{c}{\bfseries Sentiment} & %
      \multicolumn{3}{c}{\bfseries Source} & %
      \multicolumn{3}{c}{\bfseries Target} & %
      \multirow{2}{0.136\columnwidth}{\bfseries\centering Macro\newline \F{}}\\
      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} &\\\midrule

      \multicolumn{11}{c}{\cellcolor{cellcolor}GRU}\\
          Training Set\\
          Test Set\\

          \multicolumn{11}{c}{\cellcolor{cellcolor}LSTM}\\
          Training Set\\
          Test Set\\\bottomrule
    \end{tabular}
    \egroup
    \caption{Results of fine-grained sentiment analysis with recurrent
      neural networks.}
    \label{snt-fgsa:tbl:nn-res}
  \end{center}
\end{table*}


\subsection{Evaluation}

\subsubsection{Topologies}
% \subsubsection{Semi-Markov CRFs}
% Instead of making their predictions over single tokens, semi-Markov
% CRFs try to partition the input sequence $\bm{x}$ into contiguous
% segments $\vec{s} = \langle{}s_1,\ldots,s_l\rangle{}$ where each
% segment $s_i$ consists of one or more tokens which all share the same
% common tag $y_i$.

% Features in semi-Markov models are defined at the level of segments
% and, like in the linear-chain case, are binary- or real-valued
% functions associated either with the segment $s_i$ and its label $y_i$
% or a pair of adjacent segment labels $(y_{i-1}, y_i)$.

% During the training and the later prediction step, the model considers
% all potential segmentations for all possible label assignments and all
% possible segment lengths ranging from 1 to $L$ where $L$ is the length
% of the longest sequence of tokens with identical tags that was
% observed in the training data.  The decoding step then jointly finds
% the most probable segmentation $\hat{s}$ and the label sequence
% $\vec{y}$ for that segmentation using either the greedy search or the
% dynamic Viterbi algorithm.

% \subsubsection{Higher-order CRFs}
% Unlike the first-order models whose transition features can only
% access a pair of adjacent element labels $(y_{i-1}, y_i)$, transition
% features of higher-order linear-chain and semi-Markov CRFs encode
% information about complete label sequences up to length $d$ that might
% precede the predicted label $y_i$ where $d$ is the maximum order of
% the model.

% But since the number of such label sequences grows exponentially in
% the order of the encoded dependencies (up to
% $\sum_{i=1}^d\left\vert\mathcal{Y}\right\vert^i$), the running time of
% the training and inference algorithms rapidly becomes prohibitively
% expensive and amounts to
% $\mathcal{O}(n(\sum_{i=1}^d\left\vert\mathcal{Y}\right\vert^{i})^2)$
% for the linear-chain case and
% $\mathcal{O}(nL(\sum_{i=1}^d\left\vert\mathcal{Y}\right\vert^{i})^2)$
% for the semi-Markov models.

% Because this problem is intractable in general \cite{Istrail:00},
% researchers are usually forced to resort to some sort of heuristics
% like an early state pruning \cite{Mueller:13} or state-transition
% reduction.

% For the purpose of our experiments, we applied the affix-based
% algorithm of \citet{Nguyen:14}.  This algorithm relies on the
% advantage that the actual number of possible sequences of $d$
% consecutive segment labels that are observed in the training set is
% typically much smaller than
% $\sum_{i=1}^d\left\vert\mathcal{Y}\right\vert^i$.  Furthermore, the
% number of possible transitions that need to be considered for the
% given label sequence $(y_{i-d}, y_{i-d+1},\ldots,y_i)$ can further be
% reduced by restricting them only to those patterns for which this
% label sequence forms the longest possible suffix.

% The running time of this algorithm for the semi-Markov case runs up to
% $\mathcal{O}(nL\left\vert\mathcal{P}\right\vert\left\vert\mathcal{Y}\right\vert)$
% where $\left\vert\mathcal{P}\right\vert$ is the total number of the
% label prefixes observed in the training set.  And even though this
% number can still be exponential in the order of the dependencies in
% the worst case, this algorithm usually leads to much faster results in
% practice especially when the label patterns in the addressed domain
% are sparse.

% \subsubsection{Tree-structured CRFs}
% In contrast to the linearly structured models, in which the predicted
% label $y_i$ only depends on the state attributes of the respective
% item $x_i$ and the label scores of the preceding tokens or segments
% and where the syntax information can only be expressed as feature
% attributes of single items, tree-CRFs allow to incorporate syntactic
% dependencies directly into the model's structure.

% In our experiments, we used the automatically constructed dependency
% trees of the sentences as the underlying graphs for the
% tree-structured CRF model.  Like in the linear CRF case, every
% predicted label $y_i$ in this model is dependent on the state features
% of the token $x_i$, but, instead of hard-coding dependencies on the
% label scores of the previous tokens, we let transition features
% connect the predicted label with every label score $y_c$ where $c$
% ranges over all indices of the child nodes of the token $x_i$ in the
% dependency tree.

% Since the resulting model graphs are acyclic, the inference in this
% type of CRFs can still be exact and its running time remains the same
% as for the first-order linear chain models:
% $\mathcal{O}(n\left\vert\mathcal{Y}\right\vert^{2})$.

\subsubsection{Effect of the Annotation Scheme}
Experiment I (broad sentiment)

In our first set of experiments, we applied the \emph{broad}
interpretation of sentiments.  With this interpretation, we assigned
the \textsc{SNT} labels to all tokens that belonged to the sentiment
spans in our corpus annotation except for the tokens that also were
labeled as targets or sources of the opinions.  In the latter case, we
used the \textsc{TRG} and \textsc{SRC} tags respectively.  A
translated example of such annotation mapping is provided below:
\begin{example}\label{exmp:3}
  \noindent\sentiment{\target{Francis} makes a \intensifier{very}
    \emoexpression{good} impression on \source{me}!
    \emoexpression{:)}}

  $\rightarrow$

  Francis/TRG makes/SNT a/SNT very/SNT good/SNT impression/SNT on/SNT
  me/SRC !/SNT :)/SNT
\end{example}
After mapping and aligning the annotation from the corpus with the
automatically pre-processed data, we trained various modifications of
CRFs on the training set using the l-BFGS algorithm \cite{Liu:89} and
adjusted the $L1$ and $L2$ regularization parameters using the
cross-validation data.  Our final classification results for the test
set are shown in Table \ref{tbl:res-broad}.  For all reported
evaluations, we used the proportional overlap metric of
\citet{Johansson:10}.

Experiment II (narrow sentiment)
In the next attempt, we had a different take of the annotation scheme
and only assigned the label \textsc{SNT} to the tokens that were
labeled as \textsc{Emo-Expression}s in our corpus.  A translated
example of this annotation mapping is provided below:
\begin{example}\label{exmp:4}
  \noindent\sentiment{\target{Francis} makes a \intensifier{very}
    \emoexpression{good} impression on \source{me}!
    \emoexpression{:)}}

  $\rightarrow$

  Francis/TRG makes/NON a/NON very/NON good/SNT impression/NON on/NON
  me/SRC !/NON :)/SNT
\end{example}
\noindent{} The results obtained for this type of interpretation are
presented in Table \ref{tbl:res-narrow}.

\begin{table*}
  \begin{center}
    \bgroup \setlength\tabcolsep{0.47\tabcolsep}\scriptsize
    \small
    \begin{tabular}{|p{0.12\columnwidth}| % first columm
        *{9}{>{\centering\arraybackslash}p{0.08\columnwidth}|}} % next nine columns
      \hline
          {\bfseries Element} & 1 lcCRF & 2 lcCRF & 3 lcCRF & 4 lcCRF & 1 smCRF & 2 smCRF & 3 smCRF & 4 smCRF & 1 trCRF\\\hline

          \multicolumn{10}{|c|}{\cellcolor{cellcolor}Training Set}\\\hline

          Sentiment & 85.91 & 83.82 & 84.33 & 85.45 & 82.92 & 88.13 & 83.15
          & 80.67 & 87.07\\
          Source & 78.14 & 76.74 & 77.84 & 80.59 & 73.15 & 79.88 & 73.95 &
          71.55 & 80.47\\
          Target & 79.31 & 70.52 & 71.57 & 73.47 & 67.95 & 76.18 & 70.04 &
          67.51 & 82.14\\\hline
          %% Sentiment & 87.77 & 85.25 & 86.18 & 81.91 & 86.11 & 90.06 & 86.91 & 81.96 & 80.53\\
          %% Source & 77.58 & 73.77 & 77.94 & 74.09 & 75.83 & 80.83 & 68.3 & 70.85 & 72.65\\
          %% Target & 79.07 & 78.57 & 79.83 & 75.19 & 70.95 & 77.04 & 72.3 & 75.56 & 72.45\\\hline

          \multicolumn{10}{|c|}{\cellcolor{cellcolor}Test Set}\\\hline

          Sentiment & 32.39 & 31.32 & 33.07 & 32.99 & 30.22 & \textbf{33.09}* & 30.69
          & 30.41 & 31.66\\
          Source & 25.31 & \textbf{29.64}** & 28.05 & 27.43 & 23.49 &
          26.38 & 24.24 & 23.85 & 25.15\\
          Target & \textbf{29.54} & 25.38 & 25.59 & 20.41 & 25.6 &
          26.26 & 26.49 & 26.16 & 25.52\\\hline
          %% Sentiment & 32.91 & 31.85 & 31.71 & 32.7 & 31.16 & 31.52 & 32.01 & \textbf{34} & 32.66\\
          %% Source & 26.67 & 27.09 & \textbf{27.4} & 23.16 & 26.02 & 26.17 & 21.15 & 23.77 & 27.16\\
          %% Target & 24.73 & 23 & 22.87 & \textbf{25.22} & 21.56 & 21.56 & 21.18 & 24.94 & 22.72\\\hline
    \end{tabular}
    \egroup
    \caption{Automatic sentiment analysis (broad sentiment
      interpretation).\\ {\small lcCRF -- linear-chain CRFs, smCRF --
        semi-Markov CRFs, trCRF -- tree-structured CRFs; 1, 2, and 3
        denote the order} \\ {\small token-level classification
        accuracy differs statistically significantly from the 1 lcCRF
        model at $p < 0.01$ (*) or $p < 0.05$ (**)}}
    \label{tbl:res-broad}
  \end{center}
\end{table*}

\begin{table*}
  \begin{center}
    \bgroup \setlength\tabcolsep{0.47\tabcolsep}\scriptsize
    \small
    \begin{tabular}{|p{0.12\columnwidth}| % first columm
        *{9}{>{\centering\arraybackslash}p{0.08\columnwidth}|}} % next nine columns
      \hline {\bfseries Element} & 1 lcCRF & 2 lcCRF & 3
      lcCRF & 4 lcCRF & 1 smCRF & 2 smCRF & 3 smCRF & 4 smCRF & 1
      trCRF\\\hline

          \multicolumn{10}{|c|}{\cellcolor{cellcolor}Training Set}\\\hline

          Sentiment & 87.36 & 83.43 & 83.63 & 81.17 & 82.85 & 81.59 &
          80.03 & 80.37 & 84.68\\
          Source & 67.12 & 69.38 & 71.34 & 66.58 & 61.46 & 65.06 &
          63 & 65.67 & 61.52\\
          Target & 72.84 & 62.72 & 64.77 & 60.52 & 60.02 & 61.47 &
          59.14 & 60.17 & 66.41\\\hline

          %% Sentiment & 87.01 & 83.42 & 83.4 & 81.53 & 82.36 & 81.5 & 81.8 & 80.38 & 84.32\\
          %% Source & 66.63 & 68.41 & 70.79 & 68.67 & 60.88 & 66.09 & 67.8 & 66.82 & 61.56\\
          %% Target & 73.65 & 64.85 & 66.67 & 64.76 & 61.61 & 63.61 & 65.16 & 62.23 & 67.18\\\hline

          \multicolumn{10}{|c|}{\cellcolor{cellcolor}Test Set}\\\hline

          Sentiment & 61.13 & 59.9 & 59.36 & 60.08 & 60.11 & 60.03 &
          59.31 & 60.05 & \textbf{62.92}\\
          Source & 20.18 & 22.53 & 24.51 & 23.26 & 18.13 & 23.55 &
          \textbf{27.04} & 26.06 & 22.05\\
          Target & 22.94 & 21.89 & 23.59 & 22.94 & 19.43 & 23.01 &
          22.8 & \textbf{23.36} & 21.87\\\hline

          %% Sentiment & 60.26 & 59.34 & 59.22 & 60.3 & 59.58 & 59.86 & 58.43 & 59.22 & \textbf{62.86}\\
          %% Source & 19.3 & 21.13 & 23.38 & 25.75 & 19.2 & 24.49 & 24.18 & \textbf{26.77} & 21.83\\
          %% Target & 20.43 & 20.5 & 20.96 & \textbf{22.76} & 17.86 & 21.01 & 21.78 & 20.83 & 19.99\\\hline
    \end{tabular}
    \egroup
    \caption{Automatic sentiment analysis (narrow sentiment
      interpretation).}
    \label{tbl:res-narrow}
  \end{center}
\end{table*}

As can be seen from the tables, classification performance for the
narrow sentiment spans is almost twice as high as the results obtained
with the broad interpretation (62.92\% versus 33.09\%).  The broad
approach, however, leads to better prediction scores for targets and
sources of the opinions.

For the narrow scheme, classification results for these opinion
arguments can be improved by increasing the dependency order of the
linearly structured CRF-models.  This improvement, however, comes at
the cost of a decreased accuracy for the sentiment spans themselves.

We can also see that, depending on the utilized interpretation scheme,
sentiment classification notably depends on the graph structure of the
underlying CRF model: broad sentiments, for example, are best
processed with the second-order semi-Markov CRFs, while narrow
opinions are more amenable to the tree-structured model.

\subsubsection{Effect of Features}

Another important factor which significantly influenced the results of
the CRF system were the features which we provided to this model as
input.  In order to check the utility of the defined attributes, we
performed an ablation test, removing one group of features at a time
and rechecking model's performance on the dev set

We checked the utility of these features by conducting an ablation
test using the first-order linear-chain and the tree-structured models
with the broad interpretation scheme.
%%  All of these attributes proved to increase the classification
%% accuracy with the lexical and syntactic features being the most useful
%% ones.
The relative changes of the classification results on the test set
that were caused by the removal of each of these feature groups are
shown in Table \ref{tbl:ablation}.\footnote{Since we did not observe
  the test data during the feature development stage, some of the
  features that had lead to classification improvements on the
  training corpus showed a negative influence on the test data.}

\begin{table}[hbt]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.47\tabcolsep} \small
    \begin{tabular}{|p{0.2\columnwidth}| % first columm
        *{5}{>{\centering\arraybackslash}p{0.12\columnwidth}|}} % next five columns
      \hline
          \multirow{2}{0.2\columnwidth}{\bfseries Element} &
          \multicolumn{5}{c|}{\bfseries Removed feature type}\\\cline{2-6}
          & Form & Morph & Lex & Synt & Cmplx\\\hline

          \multicolumn{6}{|c|}{\cellcolor{cellcolor}1-st order linear-chain CRFs}\\\hline

          Sentiment & -2.12 & -1.41 & -2.7 & -1.32 & -5.25\\
          Target & -2.62 & -0.7 & -5.24 & -0.47 & -5.49\\
          Source & -1.94 & -1.91 & -2.49 & -2.92 & -7.83\\\hline

          \multicolumn{6}{|c|}{\cellcolor{cellcolor}Tree-structured CRFs}\\\hline

          Sentiment & -0.05 & -0.09 & +0.02 & -0.65 & -5.59\\
          Target & -1.4 & +1.11 & +2.71 & +3.87 & -4.28\\
          Source & -0.63 & +0.12 & +0.15 & +0.65 & -5.72\\\hline
    \end{tabular}
    \egroup
    \caption{Feature ablation tests.}
    \label{tbl:ablation}
  \end{center}
\end{table}


\subsubsection{Error Analysis}

\subsection{Related Work}
One of the first attempts at automatically analyzing subjective
opinions below the document and sentence level was made by
\citet{Nasukawa:03}.  In their work, the authors proposed a
lexicon-based approach, in which they first determenied the
occurrences of the given target in text; then analyzed the immediate
context of these occurrences, looking for subjective terms from their
manually compiled lexicon; and, finally, determined the overall
orientation of the opinion, judging by the predetermined polarity
score of the lexicon term and a set of heuristic rules which accounted
for contextual changes.

An alternative way of classifying subjective opinions and their
holders was proposed by \citet{Bethard:04}, who also utilized a
lexicon \cite{Wiebe:03} for finding opinionated terms.  In the
following step, the recognized expressions were used as features for a
set of SVM classifiers which predicted which nodes of a syntactic
constituency trees represented opinionated clauses or sources of the
opinions.  A similar method was also applied by \citet{Kobayashi:07}
for identifying subjective expressions pertaining to product aspects.

One of the first attempts at automatically analyzing subjective
opinions with Conditional Random fields was made by \citet{Choi:05}.
In their work, the authors combined a linear-chain CRF system with the
unsupervised pattern extractor \texttt{AutoSlog-TS} \cite{Riloff:96}
in order to recognize holders of opinions in text.  This approach
showed a significant improvement over compared baselines (noun phrases
pre-selected by heuristic rules).

Later on, \citet{Choi:06} further enhanced this system, simultaneously
trying to predict both sources and expressions of opinions, also
establishing links between these entities.  For this purpose, they
again applied two CRF classifiers (one for each entity type) to
determine the textual spans of opinion holders and subjective
elements, getting top-10 results from each of the systems.  In the
next step, they pruned off invalid suggestions with a set of hard and
soft constraints, whose weights were optimized using the integer
linear programming.  This two-stage approach not only allowed the
authors to reach impressive 68.9\% \F-measure on predicting links but
also helped to improve automatic recognition for each of the predicted
element types.

\citet{Breck:07} concentrated on the prediction of \emph{direct speech
  events} (DSEs) and \emph{expressive subjective elements} (ESE) in
the MPQA corpus \cite[cf.][]{Wiebe:05}.  For this task, they also
adopted the CRF approach, getting a substantial imrovement over
dictionary-based baselines.

\citet{Choi:10} attempted to jointly predict the boundaries, polarity,
and intensity of subjective elements (DSEs and ESEs) with a single CRF
classifier.  For this purpose, they adopted the hierarchical parameter
sharing technique \cite{Zhao:08}, letting their system differentiate
between ten different classes (two classes for the presence of an
opinion times three polarities and three intensities), but ensuring
that parameters belonging to the same group (e.g., high intensity)
were shared among these classes.

Instead of relying on just one most likely label assignment when
predicting subjective expressions, \citet{Johansson:10a} explored the
possibility of reranking by first obtaining top-$k$ alternative
predictions (with $k = 8$) and then reweighting these labelings with a
different type of classifier.  In particular, the authors experimented
with linear and tree-based preference kernels as well as structured
perceptron and passive-aggressive algorithms, finding the last option
performing best and yielding 4\% improvement over the standard
linear-chain CRF baseline.  The same idea was also used for the joint
extraction of subjective expressions and opinion
holders~\cite[cf.][]{Johansson:10b}.

\citet{Yang:12} were allegedly the first who used semi-Markov CRFs in
an opinion mining application.  In order to determine the boundaries
of DSEs and ESEs, the authors derived a set of potential segments from
shallow syntastic parses and then let a semi-Markov model determine
the most likely segmentation and label assignment over these
hypotheses.

One of the first steps towards using deep neural networks for the
prposes of fine-grained sentiment analysis was made
by~\citet{Socher:13}.  In their work on predicting sentiment polarity
of syntactic constituents (including the whole sentences), the authors
explored a recursive neural tensor approach, in which they computed
the semantic orientation of a given node $\vec{v}_c \in \mathbb{R}^n$
in a bottom-up fashion by recursively multiplying the embeddings of
its descendant leaves (e.g., $\vec{w}_1 \in \mathbb{R}^n$ and
$\vec{w}_2 \in \mathbb{R}^n$) with a compositional tensor $V \in
\mathbb{R}^{2n\times2n\times2n}$:
\begin{align*}
  \vec{v}_c = softmax\left(%
  \begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}^T \cdot V^{1:n}\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}%
  + W\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}%
  \right),
\end{align*}
where $W\in\mathbb{R}^{n\times n}$ was an additional shared
compositionality matrix, and the tensor productfor a single element
was defined as:
\begin{align*}
  v'_i = \begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}^T \cdot V^{i}\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}.
\end{align*}
The weights of this tensor as well as the vector representations of
the underlying words, and the compositionality matrix $W$ were learned
automatically during training.

Another recursive approach was suggested by~\citet{Irsoy:14a}
\cite[see also][]{Irsoy:14b}, who explored deep bidirectional neural
networks for predicting DSEe and ESEs on the MPQA corpus.  In
particular, they estimated the label $y$ of the $t$-th token as:
\begin{align*}
  y_t = softmax\left(U_{\rightarrow}\cdot \vec{h}_t^{(L)} + U_{\leftarrow}\cdot \cev{h}_t^{(L)} + \vec{c}\right),
\end{align*}
where $\vec{h}_t^{(L)}$ and $\cev{h}_t^{(L)}$ were the outputs of the
top-$L$ levels computed from left to right and from right to left
respectively; and $U_{\rightarrow}$, $U_{\leftarrow}$, and the bias
term $\vec{c}$ were the other learned parameters used to reduce the
dimensions.  This system could outperform the previous state of the
art set by \citet{Yang:12}, reaching a proportional \F-score of 66.01
for DSEs and 56.26 for ESEs.

Other notable works on fine-grained opinion mining with deep neural
networks include thos of \citet{Irsoy:14c}, who used a tensor
multiplication within the recursive loop; \citet{Tang:16}, who
proposed a memory-based multi-level network for classifying opinions
about the given aspects; and, finally, \citet{Wang:16}, who used the
output of a deep recursive network as input for a CRF system, updating
the weights of the network along with the CRF parameters.

\subsection{Summary and Conclusions}\label{fgsa:subsec:conclusions}

TODO

We also note that all our obtained results are significantly lower
than the corresponding figures achieved for other domains and other
languages.  A closer look into the errors showed us that the main
reason for such severe accuracy drop was an insufficiently good
recognition of opinionated words and phrases.  This can be partially
explained by the creativity of the users in expressing their thoughts
but also by the lack of appropriate sentiment dictionaries which would
not only contain standard language expressions but also slang words
and cusses.

An additional challenge was posed by emoticons which could not only
convey an evaluative meaning but also serve as politeness expressions
and were therefore ambiguous sentiment markers.  Furthermore, we
should also mention that the German language itself is more difficult
for automatic processing due to its relatively free word order,
ambiguous inflections, and a rich morphology.  And these problems
become even more aggravated when dealing with Twitter.

We also saw that the outcomes of our experiments crucially depended on
the definition of the sentiments that we applied and that different
CRF variants had different influence on the classification accuracy
for either interpretation.  Unfortunately, none of these models could
clearly outperform its rivals, but we could detect some general trends
for each of the proposed interpretations.  If these trends would also
hold whe using other feature sets and other corpora is one of the
questions that we want to address in future research.
\newpage
