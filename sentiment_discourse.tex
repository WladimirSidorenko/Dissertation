% FILE: main.tex  Version 2.1
% AUTHOR:
% Universität Duisburg-Essen, Standort Duisburg
% AG Prof. Dr. Günter Törner
% Verena Gondek, Andy Braune, Henning Kerstan
% Fachbereich Mathematik
% Lotharstr. 65., 47057 Duisburg
% entstanden im Rahmen des DFG-Projektes DissOnlineTutor
% in Zusammenarbeit mit der
% Humboldt-Universitaet zu Berlin
% AG Elektronisches Publizieren
% Joanna Rycko
% und der
% DNB - Deutsche Nationalbibliothek

\chapter{Discourse-Augmented Sentiment Analysis}


\section{Data Preparation}\label{sec:cgsa:data}

\section{Discourse-Aware Sentiment Analysis Methods}

% \done[inline]{\citet{Bickerstaffe:10}}

% \citet{Bickerstaffe:10} also considered the rating prediction task,
% addressing this problem with the minimum-spanning-tree (MST) SVM
% approach.  In the initial step of this method, they constructed a
% strongly connected graph whose vertices were associated with the most
% representative example (determined via the average all-pairs Tanimoto
% coefficient) of each star rating and the edge weights represented the
% Tanimoto distances between those nodes.  Afterwards, they determined
% the MST of this graph using the Kruskal's
% algorithm~\cite[see][pp.~567--574]{Cormen:09} and, finally,
% constructed a decision tree from this MST, replacing the MST vertices
% with binary SVM classifiers, which had to discern the respective
% rating groups. An evaluation on the four-star review corpus
% of~\citet{Pang:05} showed an improvement by up to~7\% over the
% previous state of the art, boosting it to 59.37\% average accuracy.


\todo[inline]{\citet{Pang:02}}
Pang et al. (2002) propose three different machine learning methods to
extract the SO of adjectives. Their results are above a
human-generated baseline, but the authors point out that discourse
structure is necessary to detect and exploit the rhetorical devices
used by the review authors

\todo[inline]{\citet{Riloff:03}}

\todo[inline]{\citet{Pang:04}}

One of the first discourse-aware approaches to coarse-grained
sentiment analysis was proposed by~\citet{Pang:04}.  In their work,
the authors partitioned the text of a movie review into subjective and
objective sentences using the min-cut method of~\citet{Blum:01}, and
then analyzed the sentences from the former group in order to classify
the overall polarity of the review.

Another multi-stage Na{\"i}ve Bayes procedure was proposed by
\citet{Pang:04}, who tried to classify the overall semantic
orientation of movie reviews (positive vs. negative) by first dividing
their sentences into subjective and objective ones (achieving 92\%
accuracy on this subtask) and then predicting the overall polarity of
a review using only subjective passages.  By pruning objective
clauses, the authors achieved statistically significant improvements
from~82.8 to~86.4\%.  The authors also experimented with the min-cut
method to encode the idea that nearby sentence will likely share the
same subjectivity class, but did not get any statistically significant
improvement over the discourse-unaware baseline.

Quote:

Fundamentally, it seems that some form of discourse analysis is
necessary (using more sophisticated techniques than our positional
feature mentioned above), or at least some way of determining the
focus of each sentence, so that one can decide when the author is
talking about the film itself. (Turney (2002) makes a similar point,
noting that for reviews, ``the whole is not necessarily the sum of the
parts''.)

\todo[inline]{}

Pang and Lee (2004) first showed that sentencelevel extraction can
improve document-level performance. They used a cascaded approach by
first filtering out objective sentences and performing subjectivity
extractions using a global min-cut inference. Afterward, the
subjective extracts were converted into inputs for the document-level
sentiment classifier. One advantage of their approach is that it
avoids the need for explicit subjectivity annotations. However, like
other cascaded approaches (e.g., Thomas et al. (2006), Mao and Lebanon
(2006)), it can be difficult to control how errors propagate from the
sentence-level subtask to the main document classification task.

\todo[inline]{\citet{Hu:04}, Section 3.6}


\todo[inline]{\citet{Mao:06}}
Other notable works on discourse-level sentiment analysis include
those of~\citet{Mao:06}, who used isotonic CRFs to predict the
polarity of single sentences and then used a nearest neighbors
classifier to induce the polarity of the whole document from these
predictions, observing an improvement over the traditional
bag-of-words approach.

\todo[inline]{\citet{Thomas:06}}

\todo[inline]{\citet{Polanyi:06}}

\todo[inline]{\citet{Snyder:07}}
Other notable works on discourse-augented sentiment analysis include
those of~\citet{Snyder:07}, who proposed a joint Good Grief model to
predict users' evaluations of different aspects of restaurants.  In
the first phase of this approach, they applied a set of
aspect-specific classifiers, each of which was supposed to predict
user's polarity towards one particular criterion.  In the next step,
the output of this ensemble was corrected by an additional linear
classifier, which was supposed to account for the overall agreement on
multiple aspects.

\todo[inline]{\citet{Voll:07}}

\citet{Voll:07} addressed the problem of classifying the polarity of
Epinions reviews.\footnote{\url{www.epinions.com}} For this purpose,
they applied the SO-CAL system (Semantic Orientation CALculator) to
adjectives appearing in the user posts, considering three ways of
pre-filtering this input: In the first (baseline) approach, they
invariably passed all adjectives found in the review to SO-CAL.  In
the second method, they only analyzed those words which appeared in
the top-most nuclei of the sentences.  These nuclei were determined
automatically by an RST parsing system \cite[SPADE; ][]{Soricut:03}.
Finally, in the third experiment, the authors trained a decision-tree
classifier, which predicted whether a particular sentence pertained to
the general topic of its document or not, and applied the sentiment
system only to those sentences which were classified as
topic-relevant.  With this last approach, \citeauthor{Voll:07} were
able to obtain their best results (69\% accuracy), getting their
second-best scores with the system which considered all adjectives.
The suboptimal results of the discourse-based approach were partially
explained by the limited performance of the automatic RST system.

\todo[inline]{\citet{McDonald:07}}

Another joint framework for predicting the polarity of both sentences
and documents was described by \cite{McDonald:07}.  In their work, the
authors designed a unified CRF system, in which the predicted labels
of the sentences formed a linear chain and simultaneously were
connected to the label node of the whole document which had to be
predicted along with sentence orientations.

\todo[inline]{\citet{Sadamitsu:08}}

\todo[inline]{\citet{Somasundaran:08,Somasundaran:08a}}

\citet{Somasundaran:08,Somasundaran:08a} introduced the concept of
\emph{opinion frames} which represented a pair of subjective
statements (arguments or sentiments) that were related to each via the
\emph{same} or \emph{alternative} link.  In the former case, both
statements had to refer to the same target, while, in the latter case,
these targets had to represent mutually exclusive alternatives.

\todo[inline]{\citet{Somasundaran:09a,Somasundaran:09b}}

Another work, which apparently comes closest to ours, is that
of~\citet{Somasundaran:09a}, who tried to unite the notions of opinion
frames~\cite{Somasundaran:08} with the \emph{dialogue act
  theory}~\cite{Bunt:12} within a single framework.  In particular,
the authors projected the polarity annotation of the
AMI~corpus~\cite{Carletta:05} onto the dialogue segments, and then
used a two-stage approach similar to the one proposed
by~\citet{Snyder:07}, in which they first separately classified
polarity of each dialogue leaf, and then used an integer linear
programming module to correct the cases which violated the overall
agreement of opinion frames.

\done[inline]{\citet{Yessenalina:10}}

Similarly, \citet{Yessenalina:10} proposed a latent-variable approach
in which they tried to predict the overall polarity of a document by
jointly selecting a subset of sentences which were most indicative of
document's polarity and predicting the document's class (either
positive or negative) based on this subset.  To achieve this goal, the
authors adopted the latent-SVM method of \citet{Yu:09}, training a
linear classifier on individual sentences with latent classes and
making the final prediction using 30\% of the sentences which this
classifier was most sure about.  To account for discourse phenomena,
they also enhanced feature representations of analyzed sentences with
attributes extracted from their preceding clauses and added
characteristics of the whole document to the model.  This way,
\citet{Yessenalina:10} attained 93.22\% accuracy on the movie review
corpus of \citet{Pang:04} and scored 77.09\% on the collection of
congressional floor debates introduced by \citet{Thomas:06}.

\done[inline]{\citet{Trivedi:13}}

Later on, \citet{Trivedi:13} refined this approach by introducing
special \emph{connector-augmented} transition features, which
reflected whether an explicit connector at the beginning of a sentence
implied a \emph{continuation} or \emph{shift} of the polarity of the
preceding discourse unit.  These attributes turned out to be extremely
useful, boosting the prediction accuracy on the movie review corpus
of~\citet{Maas:11} from 88.21 to 91.36\% in comparison with the
connector-unaware model.

\todo[inline]{\citet{Heerschop:11}}

\todo[inline]{\citet{Zhou:11}}

\todo[inline]{\citet{Zirn:11}}

Zirn et al. (2011) use an RST parser in a Markov Logic Network, with
the goal of making polarity predictions at the sub-sentence level,
rather than improving document-level pre- diction. None of the prior
work considers the sort of recurrent compositional model presented
here.

\todo[inline]{\citet{Mukherjee:12}}

\todo[inline]{\citet{Wang:13}}

Wang and Wu (2013) use manually-annotated discourse parses in
combination with a sentiment lexicon, which is automatically updated
based on the dis- course structure.

\todo[inline]{\citet{Chenlo:13}}

\todo[inline]{\citet{Chenlo:14}}

\done[inline]{\citet{Bhatia:15}}

Other ways of incorporating discourse structure into an opinion mining
application were proposed by~\citet{Bhatia:15} who presented two
methods for augmenting a sentiment analysis system with discourse
information:
\begin{inparaenum}[(i)]
\item discourse depth reweighting (DDR) and
\item rhetorical recursive neural network (R2N2).
\end{inparaenum}

In the former approach, the authors first computed the sentiment score
$\lambda_i$ of each elementary discourse unit $i$ as:
\begin{equation*}
  \lambda_i = \max\left(0.5, 1 - d_i/6\right),
\end{equation*}
where $d_i$ stands for the depth of the $i$-th EDU in the discourse
tree of the whole document.  Afterwards, they estimated the overall
polarity of the document~$\Psi$ by summing up the linear predictions
for each discourse unit ($\theta^T\cdot\mathbf{w}_i$), scaling these
prediction by the factor~$\lambda_i$:
\begin{equation*}
  \Psi = \sum_i\lambda_i\mathbf{\theta}^T\cdot\mathbf{w}_i = \mathbf{\theta}^T\cdot\sum_i\lambda_i\mathbf{w}_i,
\end{equation*}
In this equation, $\mathbf{\theta}$ represents a vector of polarity
scores ($-1$ for negative terms and $+1.$ for positive entries) for
all word in the vocabulary (as obtained from the sentiment lexicon
of~\citet{Wilson:05}), and $w_i$ denotes the bag-of-words
representation of the $i$-th EDU.

In the R2N2 method, \citet{Bhatia:15} adopted the RNN approach
of~\citet{Socher:13} and recursively computed polarity scoreb of each
discourse $i$ unit as follows:
\begin{equation*}
  \psi_i = tanh\left(K_n^{(r_i)} \psi_{n(i)} + K_s^{(r_i)}\psi_{s(i)} \right),
\end{equation*}
where $K_n^{(r_i)}$ and $K_s^{(r_i)}$ denote the nucleus an satellite
coefficients associated with the rhetorical relation $(r_i)$, and
$\psi_{n(i)}$ and $\psi_{s(i)}$ represent the sentiment scores of the
nucleus and satellite nodes of the $i$-th discourse unit.  With this
method, the authors achieved 84.1\% two-class prediction accuracy on
the moview review corpus of~\citet{Pang:04} and 85.6\% on the dataset
of~\citet{Socher:13}.

\todo[inline]{Lazaridou, 2013}

\section{Evaluation}

\section{Summary and Conclusions}
