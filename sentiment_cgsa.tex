\chapter{Coarse-Grained Sentiment Analysis}\label{sec:snt:cgsa}

\section{Coarse-Grained Sentiment Analysis with Lexicon-Based Methods}

\section{Coarse-Grained Sentiment Analysis with Traditional Supervised
  Machine Learning Frameworks}

\section{Coarse-Grained Sentiment Analysis with Deep Neural Networks}

\section{Evaluation}
\subsection{Effect of Distant Supervision}
\subsection{Effect of Normalization}

\section{Language and Domain Adaptation}

\section{Related Work}

\textbf{Lexicon-Based Methods}

Bruce and Wiebe, 2004

Hu and Liu, 2004

\citet{Kim:04} experimented with three different methods of
determining the overall polarity of a sentence:
\begin{inparaenum}[(i)]
  \item by multiplying the signs of polar terms found in sentence,
  \item taking their sum, and
  \item taking the geometric mean of polarity scores;
\end{inparaenum}
finding that the first and last options worked best for the Document
Undestanding Corpus.\footnote{\url{http://duc.nist.gov/}}

% Intensity Classification
Polanyi and Zaenen (2006)

\citet{Kennedy:06}

Taboada and Grieve (2004)

Taboada, Anthony, and Voll (2006)

Finally, a veritably seminal work on lexicon-based approaches was
introduced by~\citet{Taboada:11} who presented a manually compiled
sentiment lexicon,\footnote{The authors hand-annotated all occurrences
  of adjectives, nouns, and verbs found in a corpus of 400 Epinions
  reviews with ordinal categories ranging from -5 to 5 which reflected
  the semantic orientation of a term (positive vs. negative) and its
  polar strength (weak vs. strong).} and used this resource to compute
the overall \emph{semantic orientation} (SO) of text.  Inspired by the
ideas of~\citet{Polanyi:06}, to make this calculation more precise,
the authors also incorporated a set of additional heuristic rules by
changing the prior SO values of negated, itensified, and downtoned
terms, ignoring irrealis sentences, and adjusting weights of specific
sections of a document.  An extensive evaluation of this method showed
a superior performance of the manual lexicon in comparison with other
polarity lists including the Subjectivity Dictionary~\cite{Wilson:05},
Maryland Polarity Set~\cite{Mohammad:09}, and
\textsc{SentiWordNet}~\cite{Esuli:06c}.  Moreover, the authors also
proved the robustness of their SO computation procedure for other
topics and text genres, hypothesizing that lexion-based approaches
were more domain-independent than traditional supervised learning
techniques.

\textbf{Traditional Supervised Machine Learning Frameworks}

One of the first attempts to analyze message-level sentiments on
Twitter was made by \citet{Go:09}.  For their experiments, the authors
collected a set of 1,600,000 tweets containing smileys.  Based on
these emoticons, they automatically derived polarity classes for these
messages (positive or negative) and used them to train a Na\"{\i}ve
Bayes, MaxEnt, and SVM classifier.  The best $F$-score for this
two-class classification problem could be achieved by the last system
and run up to 82.2\%.

Similar work was also done by \citet{Pak:10} who used the Na\"{\i}ve
Bayes approach to differentiate between neutral, positive, and
negative microblogs; and \citet{Barbosa:10} who gathered a collection
of 200,000 tweets, subsequently analyzing them with three publicly
available sentiment web-services and training an SVM classifier on the
results of these predictors.  In a similar way, \citet{Agarwal:11}
compared a simple unigram-based SVM approach with two other
full-fledged systems, one which relied on a rich set of manually
defined features, and another used partial tree
kernels~\cite{Moschitti:06}.  The authors evaluated these methods on a
commercially acquired corpus of 8,753 foreign-language tweets, which
were automatically translated into English, finding that a combination
of these methods worked best for both two- and three-way prediction
tasks.

The state-of-the-art results for message level polarity prediction on
tweets were established by~\citet{Mohammad:13}, whose system (a
supervised SVM classifier) used a rich set of various features
including word and character n-grams, PoS statistics, Brown
clusters~\cite{Brown:92}, etc., and also strongly benefitted from
automatic corpus-based polarity lists---Sentiment~140 and NRC
Hashtag~\cite{Mohammad:12,Kiritchenko:14}.  This approach ranked first
at the SemEval competition~2013~\cite{Nakov:13} and anchieved the
fourth place on the rerun of this task one year
later~\cite{Rosenthal:14}, being outperformed by the supervised
logistic regression approach of~\citet{Miura:14}, who used a heavy
preprocessing of the data and a special balancing scheme for
underrepresented classes.  Later on, these results were further
improved by the apporaches of~\citet{Hagen:15} and \citet{Deriu:16},
which both relied on ensembles of multiple independent classifiers.

\cite{Nakagawa:10}

Wiebe 2002, Riloff 2003

Wiebe, Bruce, \& O'Hara 1999
Hatzivassiloglou \& Wiebe 2000
Wiebe 2000;
Wiebe et al. 2002
Yu \& Hatzivassiloglou 2003

\textbf{Deep Neural Networks}

\citet{Yessenalina:11}

A real breakthrough in the use of deep neural networks for the
sentence-level sentiment analysis happened with the pioneering work
of~\citet{Socher:11}, who first introduced a recursive autoencoder
(RAE).  In this system, the authors obtained a fixed-width vector
representation for complex phrases $\vec{v}$ by recursively merging
the vectors of adjacent tokens (say $\vec{w}_1$ and $\vec{w}_2$),
first multiplying these vectors with a compositional matrix $W$ and
then applying a non-linear function ($softmax$) to the resulting
product:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}\right)
\end{align*}
Using a max-margin classifier on top of the resulting phrase
representation, \citet{Socher:11} could improve the state-of-the-art
results on predicting the sentence-level polarity of user's blog
posts~\cite{Potts:10} and also outperformed the system
of~\citet{Nasukawa:03} on the MPQA data set~\cite{Wiebe:05}.

Later on, \citet{Socher:12} further improved these scores with the
help of a recursive matrix-vectors space model (RMVSM), in which each
word was associated with a 2-tuple of a vector and matrix---e.g.,
$(\vec{w}_1, W_1)$ and $(\vec{w}_2, W_2)$---and the compositionality
function was redefined as follows:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  W_2\cdot\vec{w}_1\\
  W_1\cdot\vec{w}_2
  \end{bmatrix}\right)
\end{align*}

\citet{Wang:15}

\textbf{Language and Domain Adaptation}

One of the first works which pointed out the importance of domain
adaptation in sentiment analysis was presented by~\citet{Aue:05}.  In
their experiments, the authors trained separate SVM classifiers on
four different document collections: movie revies, book reviews,
customer feedback from a product support service, and a knowledge base
data survey; finding that each classifier performed best when the
training and test domains were the same.  In order to find an optimal
way of overcoming this domain specificity, \citet{Aue:05} tried out
four different options:
\begin{inparaenum}[(i)]
\item\label{sent-cgsa:lst:rel-wrk1} training a classifier on all but the target domain and applying
  it to the latter;
\item using the same procedure as in~\ref{sent-cgsa:lst:rel-wrk1}, but
  limiting the features only to those which also frequently appeared
  in the target texts;
\item taking an ensemble of individual classifiers each of which was
  trained on a different domain; and, finally,
\item using a minimal subset of labeled in-domain data to train a
  Na{\"i}ve Bayes system with the Expectation-Maximization
  \cite[EM;][]{Dempster:77} algorithm.
\end{inparaenum}
The authors found the ensemble and EM options working best in these
cross-domain settings, achieving an accuracy of up to 82.39\% on new
unseen text genres.

\cite{Blitzer:07} Andreevskaia and Bergler, 2008


\section{Summary and Conclusions}\label{slsa:subsec:conclusions}
