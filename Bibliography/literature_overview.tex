\documentclass[a4paper,11pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Packages
\usepackage{paralist}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Variables and Methods

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Documents
\begin{document}
\section{Pre-Processing}

\section{Sentiment Analysis}
\subsubsection{Turney: Thumbs Up or Thumbs Down? Semantic Orientation Applied to
               Unsupervised Classification of Reviews\cite{Turney-02}}

\subsubsection{Pang, Lee, and Vaithyanathan: Thumbs up? Sentiment Classification using Machine
                  Learning Techniques\cite{Pang-Lee-02}}

This article deals with the problem of sentiment classification of movie
reviews.  For this task, the authors explore three popular machine learning
techniques, namely Naive Bayes, maximum entropy, and support vector machines.
They compare these techniques against each other and also against a
human-based baseline classifier which relied on a deductively constructed
dictionary of polarity terms.

For their training and testing purposes, the authors created a corpus of 752
negative and 1301 positive reviews.  Discrimination between subjective and
objective articles was left out of the scope of this study.  To establish a
classifier baseline, the authors first asked two graduate students to
introspectively devise two lists of polarity terms each -- one containing
positive and one containing negative polarity markers.  A simple decision
procedure than counted occurrences of terms from these lists in each review in
the corpus selection and assigned to these reviews the class of the list with
most items found.  This procedure showed a classification accuracy of 58\% for
the lists of the first student and 64\% for the second.  A manually refined
list with fewer items, however, allowed to increase the baseline to 69\%.

In their machine learning experiments, the authors represented each document
as a word vector.  They tested different experimental setups, including simple
unigrams with either presence flag or frequency counts, bigram and
bigram+unigram binary presence flags.  The authors also tried to restrict the
features solely to adjectives and to combine unigram presence features with
their parts-of-speech or position in text.  The best result was then achieved
by a SVM classifier which only used unigram presence markers as its features.
It should be said, that for unigrams, the authors also used the negation
heuristics proposed by Das and Chen \cite{Das-Chen-01} and added the
\texttt{NOT\_} prefix to every word between a negation and a punctuation mark.
This heuristics, however, was not used for bigrams, since bigrams were
considered to be a sufficient source of context information on their own.

In general, sentiment classification task was said to be harder than
conventional topic-classification.  The authors also noted the fact, that
their system experienced serious difficulties due to the lack of discourse
information.  Because sentences describing the plot of the movies often
expressed opposite polarity than one of the author's real attitude to the
described movie.

\subsubsection{Bing and Liu: Mining and summarizing customer reviews\cite{Bing-Liu-04}}
In their article, Bing and Liu \cite{Bing-Liu-04} propose a system for
automatically mining subjective judgements on product features in customer
reviews.  This system searches the reviews for frequently occurring mentions
of prominent features of specific products by using an association mining
algorithm.  The algorithm extracts nouns and noun phrases which frequently
occur across multiple documents.  Implicit features and features which are not
represented by nouns are not taken into consideration.  Extracted feature
candidates are then pruned off if their components do not occur in a specific
order in sentences or if some features appear to be full subsets of other
ones.

In the following processing step, text passages surrounding feature candidates
are searched for adjectives, since words of this part-of-speech are considered
to be the most obvious indicators of subjective opinions.  Found adjectives
are classified into subjective and objective ones.  The subjective class is
thereby subdivided into positive and negative subclasses depending on the type
of adjectives' polarity.  Both subjectivity and polarity are determined by
using WordNet information.  For this, two manually constructed sets of
adjectives with opposite polarities are used.  Synonyms and antonyms of
adjectives found in text are searched for occurrences of any adjectives from
one of the two seed sets.  If such an occurrence is found, the text adjective
is added to the respective seed set or its counterpart depending on whether
the occurrence was found in synonyms or antonyms.  The algorithm stops when
both seed sets stop growing or when the list of text adjectives is exhausted.
Adjectives which were not assigned to any of the seed cluster during this
procedure are assumed to have neutral polarity and are skipped.

In the next step, sentences which don not contain any frequent features but
which contain at least one opinion adjective are analyzed.  The aim of this
analysis is to find less frequent features of a product which still can be
important for the user.  It is conjectured by the authors that less frequent
features will usually be expressed by nouns or noun phrases which are nearest
to the opinion word.

Further, overall polarity of sentences containing frequent and less frequent
product features is determined.  Here three different possible cases are
distinguished:
\begin{enumerate}
  \item The user likes or dislikes most or all the features in one sentence.
  \item The user likes or dislikes most of the features in one sentence, but
    there is an equal number of positive and negative opinion words.
  \item All the other cases.
\end{enumerate}
For case 1, the dominant orientation of a sentence is easily identified as
average orientation of adjectives used in this sentence.  For case 2, the
orientation of effective opinions of features is used instead.  Effective
opinion is assumed to be the most related opinion for a feature.  For case 3,
the orientation of the opinion sentence is assumed to be the same as the
orientation of previous opinion sentence unless the former sentence is
introduced by a contrastive connector like ``but'' or ``however'' in which
case the orientation is swapped to the opposite.

In the final step, a review summary is generated for each sentence containing
a subjective opinion on a frequent or less frequent feature of a product.  The
summary generation procedure generally consists of two steps:
\begin{itemize}
  \item For each discovered feature, related opinion sentences are put into
    positive and negative categories according to the opinion sentences'
    orientations.

  \item All features are ranked according to the frequency of their
    appearances in the reviews.
\end{itemize}

\subsubsection{Scheible and Sch\"utze: Sentiment Relevance\cite{Scheible-13}}

In their article, Scheible and Sch\"utze propose a novel concept for
measuring the importance of a single sentence for an overall sentiment
polarity of a document and call this concept \textit{sentiment
  relevance}.

The authors deviate from the well-established notion of sentence
subjectivity by showing that not every sentence which appears to be
subjective may contribute to document's polarity and vice versa an
apparently objective sentence may be highly sentiment relevant for a
document.  They illustrate this with an example form the movie review
database (IMDB).  In a review, a sentence describing movie's plot and
acting characters can be highly subjective containing strong polarity
markers like, for example, ``Bruce Banner, a genetics researcher with
a \textit{tragic} past, suffers a \textit{horrible} accident'', but
this sentence does not say much about author's attitude to the
``Hulk'' movie which it describes.  On the contrary, a sentence like
``The movie won a Golden Globe'' is relevant for the overall movie
review despite that it merely states an objective fact.

To confirm their assumption that sentiment relevance and subjectivity
are different concepts, the authors created a corpus (henceforth
\textit{SR-corpus}) of sentences from 125 randomly selected documents
from the movie review dataset \cite{Pang-02}.  Each of these sentences
was manually annotated for S-relevance.  Another corpus (the
\textit{P\&L} corpus) was automatically constructed from the
subjectivity dataset provided by Pang and Lee \cite{Pang-Lee-04}.  In
the latter data collection, all subjectivity markers were automaticlly
considered as S-relevent and all objective markers were regarded as
S-nonrelevant.  The authors than showed than a MaxEnt classifier
performed significantly worse if it was trained on one corpus and
tested on the other than if it was trained and tested on the data
taken from the same set.

Since no large collections of annotated S-relevance data are available
yet, Scheible and Sch\"utze propose two different semi-supervised
techniques for doing such classification automatically.  These
techniques are
\begin{inparaenum}[\itshape a)\upshape]
  \item distant supervision and
  \item transfer learning.
\end{inparaenum} In their experimental setup, the authors use three kinds of features:
\begin{enumerate}
  \item \textit{Named Entity} features (NE) which are binary features
    indicating the presence of an actor, character, or staff member
    mention in a sentence.  Named entity detection and resolution is
    performed on the basis of lexicons obtained from the IMDB metadate
    database;\label{li-ne-feat}
  \item \textit{Sequential} features (SQ) which effectively repeat the
    former features by transferring them to the following sentence in
    document.  It means that if sentence $s_i$ had feature
    \texttt{<character>}, then the following sentence $s_{i+1}$ will
    automatically get a sequential feature \texttt{<character$+1$>};
  \item \textit{Semantic} features (SEM) which are semantic classes of
    nouns or verbs which are obtained from \texttt{CoreLex} and
    \texttt{VerbNet}.  These features are added to sentences depending
    on the words which appear in them.
\end{enumerate}
Additionally to that, to account for discourse coherence factors, the
authors represent the whole document as a graph with $n + 2$ nodes
where $n$ is the number of sentences and the two additional nodes
(called source and sink) represent the classes S-relevance and
S-nonrelevance, respectively.  The weights $ind(s, x)$ between a
sentence $s$ and the source/sink vertex $x$ are measured as the number
of actors and staff mentions in the sentence when $x = SR$ and the
number of characters mentions when $x = SNR$.  It means that if a
sentence $s$ has more actors mentions in it than characters mentions,
it is more likely to be S-relevant and therefore has a higher weight
attached to the edge connecting it with the $x_{sr}$ verex than with
the $x_{nsr}$ vertex.  The coherence factor between two consecutive
sentences $i$ and $j$ is estimated as $assoc(s_i, s_j) = c / (j -
i)^2$ where $c$ is a parameter which is set to $1$ in distant
supervision setup and empirically found to bring best results when set
to $0.8$ in transfer learning.

In distant supervision, the authors first assign a relevance class to
each sentence according to the majority of NE features found in it.
They than train a MaxEnt classifier on these noisy labels and impose
the MinCut discourse constraint on it.  These re-labeled data serve as
training set for the next iteration of MaxEnt training.  In this next
iteration the authors than look how different features affect the
model.  The best total performace with average $F_1 = 67.0$ is then
achieved by a distantly supervised MaxEnt classifier with activated
features SQ and NE.

In the transfer learning setup, the authors use the P\&L corpus as
their training data, though the labels of this corpus rather
correspond to the subjectivity measure than to S-relevance.  This
time, they first classify the data using a supervised classifier and
then use MinCut to smooth the sequences.  The individual weight
$ind(s,x)$ on the edge between sentence $s$ and class $x$ is set to
the estimate $p(x|s)$ returned by the supervised classifier.  The
parameter $c$ of the MinCut model is tuned using the run count method.
The authors establish a baseline by obtaining a classifier with a
simple bag-of-words representation of sentence.  The addition of
semantic information and smoothing with MinCut discourse constraint
gives an improvement over the baseline by 12.6\% with an averaged
$F_1$ equal to $82.5\%$.

\section{Markov Logic Network}

\section{Discourse Analysis}

\nocite{*} \bibliography{bibliography} % relative to the project's
root directory \bibliographystyle{plain}
\end{document}
