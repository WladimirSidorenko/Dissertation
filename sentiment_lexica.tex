% FILE: sentiment_lexica.tex  Version 0.01
% AUTHOR: Uladzimir Sidarenka

% This is a modified version of the file main.tex developed by the
% University Duisburg-Essen, Duisburg, AG Prof. Dr. GÃ¼nter TÃ¶rner
% Verena Gondek, Andy Braune, Henning Kerstan Fachbereich Mathematik
% Lotharstr. 65., 47057 Duisburg entstanden im Rahmen des
% DFG-Projektes DissOnlineTutor in Zusammenarbeit mit der
% Humboldt-Universitaet zu Berlin AG Elektronisches Publizieren Joanna
% Rycko und der DNB - Deutsche Nationalbibliothek

\section{Sentiment Lexica}\label{sec:snt:lex}

The first avenue that we are going to explore with the help of the
obtained corpus data is an automatic prediction of polar words or,
speaking in terms of our guidelines, emotional expressions.  To this
end, we will first present an updated version of our dataset in
Subsection~\ref{subsec:snt-lex:data} in which our experts revised the
annotations of words and idioms that were present in the existing
German sentiment lexica (GSL), but were not marked as emo-expressions
in our data, and, vice versa, were annotated as polar terms in the
corpus but absent in the analyzed polarity lists.  Afterwards, we will
evaluate the aforementioned lexical resources on the updated corpus
data in order to obtain a baseline for our subsequent experiments.
Since literally all of the current GSL were created using an automatic
translation of English opinion lists with a manual post-editing of
translated entries, we will then look whether the original methods
that were used initially for creating the English source files would
yield better results when applied to German data directly.  Finally,
in the concluding step, we will analyze if one of most popular areas
of research in contemporary CL---distributed vector representations of
words \cite{Mikolov:13}---could be a perspective way for deriving new
domain-specific polarity lists without labeled data.  We will
summarize and draw conclusions in the last part of this section, also
discussing which of the presented methods performed best on our
corpus.

\subsection{Data}\label{subsec:snt-lex:data}

In the following experiments, we will use the updated version 0.1.0 of
the sentiment dataset introduced in the previous section.  The main
changes included in this version are:
\begin{inparaenum}[\itshape a\upshape)]
  \item the revision of the annotated emotional expressions, and
  \item the addition of the boolean attributes \emph{subjective-fact}
    and \emph{uncertain} to the annotation scheme of these elements.
\end{inparaenum}

To accomplish the fortmer objective, we compared the labelings of the
opinionated terms in the first release of our corpus (version 0.0.1
presented previously) with the entries from the existing German
sentiment lexica: SentiWS \cite{Remus:10}, German Polarity Clues
\cite{Waltinger:10}, and the Zurich Polarity List \cite{Clematide:10}.
Similarly to the adjudication procedure used earlier, we automatically
highlighted the differences between the corpus labels and the entries
of these resources, letting our experts resolve the emerged conflicts.

As it turned out, many of the contradicting cases stemmed from the
missing annotations of subjective facts, such as ``Tod''
(\emph{death}), ``Anstieg'' (\emph{surge}), ``duften'' (\emph{to
  scent}) etc., in our dataset and the presence of these entities in
the analyzed polarity lists.  As a golden-mean solution to this
discrepancy, we added a special feature \emph{subjective-fact} to the
attribute set of emotional expressions and explicitly asked our coders
to annotate subjectively connotated facts as emotional expressions,
setting the value of the introduced attribute to \texttt{true}.

The remaining differences were mainly due to polysemous words whose
meaning in the corpus was not always subjective; errors in the lexica
(GPC, for example, featured such auxiliary words as ``sein'' (\emph{to
  be}), ``wer'' (\emph{who}), or ``aus'' (\emph{from}) as polar
entries); and, finally, numerous borderline cases which were difficult
to resolve even in group discussions.  Examples of such challenging
cases were words and idioms such as ``verbieten'' (\emph{to
  prohibit}), ``verwechseln'' (\emph{to confuse}), ``Hauen und
Stechen'' (\emph{hewing and stabbing}).  To address the last issue, we
again introduced an additional attribute, called \emph{uncertain},
into the annotation scheme of emotional expressions and asked our
experts to mark such borderline instances with this attribute.

The statistics on the total number of the labeled elements and their
agreement in the updated version are shown in
Table~\ref{tbl:snt-lex:ucrp-agrmnt}:

\begin{table*}[thb!]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.7\tabcolsep} \scriptsize
    \begin{tabular}{|p{0.15\textwidth}| % first columm
        *{10}{>{\centering\arraybackslash}p{0.05\textwidth}|}} % next ten columns
      \hline
          \multirow{2}{0.2\textwidth}{\bfseries Element} &
          \multicolumn{5}{c|}{Binary $\kappa$} & %
          \multicolumn{5}{c|}{Proportional $\kappa$}\\\cline{2-11}
          & $M_1$ & $A_1$ & $M_2$ & $A_2$ & $\mathbf{\kappa}$ %
          & $M_1$ & $A_1$ & $M_2$ & $A_2$ & $\mathbf{\kappa}$\\\hline

          EExpression &  &  &  & & \textbf{} & &  &  &  & \textbf{}\\\hline
    \end{tabular}
    \egroup
  \end{center}
  \captionof{table}{Inter-annotator agreement on the emotional
    expression in the updated corpus.\\ {\small ($M1$ -- number of
      tokens with matching labels in the first annotation, $A1$ --
      total number of tokens labeled with that class in the first
      annotation, $M2$ -- number of tokens with matching labels in the
      second annotation, $A2$ -- total number of tokens labeled with
      that class in the second annotation)}}
  \label{tbl:snt-lex:ucrp-agrmnt}
\end{table*}

\subsection{Baseline: Existing German Lexica}

To obtain a raw estimate of the expected scores for the prediction of
subjective expressions, we then evaluated existing sentiment lexica
for German on the updated corpus data.  The most prominent of these
resources are:
\begin{itemize}
\item the \textbf{German Polarity Clues} (GPC) list of
  \citet{Waltinger:10}, which comprises 10,141 subjective entries
  automatically translated from the English lexica \emph{Subjectivity
    Clues} \cite{Wilson:05} and \emph{SentiSpin} \cite{Takamura:05}
  with a subsequent manual rechecking of these translations and
  several synonyms and negated terms added by the authors;

\item the \textbf{SentiWS} (SWS) lexicon introduced by
  \citet{Remus:10}, which includes 1,818 positively and 1,650
  negatively connotated entries, also providing their part-of-speech
  tags and inflection forms (resulting in a total of 32,734 word
  forms).  Similarly to the GPC, the authors used an English sentiment
  resource---the \emph{General Inquirer} list of \citet{Stone:66}---
  to bootstrap the entries for their lexicon, manually revising these
  automatic translations afterwards.  In addition to that,
  \citet{Remus:10} also expanded their set with words and phrases
  frequently co-occurring with positive and negative seed lexemes
  using collocation information obtained from a corpus of 10,200
  customer reviews or extracted from the German Collocation Dictionary
  \cite{Quasthoff:10};

\item finally, the \textbf{Zurich Polarity List} proposed by
  \citet{Clematide:10}, which comprises 8,000 subjective entries taken
  from GermaNet synsets \cite{Hamp:97}.  These synsets were manually
  annotated with their prior polarities by human experts.  Since the
  authors, however, found the number of polar adjectives obtained that
  way insufficient for running further classification experiments,
  they automatically enriched their lexicon with more attributive
  terms by analyzing conjoined collocations from a corpus as it was
  done by \citet{Hatzivassi:97}.
\end{itemize}

In order to evaluate these resources on our dataset, we represented
each of the above lexicons as a trie \cite{Knuth:73} with the lexicon
entries corresponding to paths in that graph and the polarity
class(es) of these entries stored at the respective terminal leaf
nodes of the paths.  We then applied the standard match operation by
checking the trie against the lemmatized corpus tokens.  All
lemmatizations were done using the \texttt{TreeTagger}
\cite{Schmid:95}, and the match was performed in a case-insensitive
mode.  Moreover, since all analyzed lexica only included plain lexical
terms, we also ignored smileys during this comparison, concentrating
on the evaluation of standard vocabulary words instead.

That way, we estimated the precision, recall, and $F1$-score of the
positive, negative, and neutral polarity classes of each lexicon by
separately computing these figures on each corpus file (99--109
tweets) and then obtaining the mean and standard deviation of these
scores over all files at the end.  In addition to that, we also
calculated the macro- and micro-averaged $F1$-scores over all three
classes.  Following the standard practice for computing these terms,
we estimated the macro-average as the mean of the $F1$-scores for all
three classes, and obtained the micro-average by taking the harmonic
mean of the micro-averaged precision and recall calculated over all
true positives, false positives, and false negatives.  The results of
these computations are shown in
Table~\ref{snt-lex:tbl:gsl-res}.\footnote{For the sake of these
  experiments, we have excluded the auxiliary words ``aus''
  (\emph{from}), ``der'' (\emph{the}), ``keine'' (\emph{no}),
  ``nicht'' (\emph{not}), ``sein'' (\emph{to be}), ``was''
  (\emph{what}), and ``wer'' (\emph{who}) as well as their inflection
  forms from the German Polarity Clues lexicon as these entries
  significantly worsened the evaluation results.}

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{|p{0.162\columnwidth}| % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}|} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}|}} % last two columns
      \hline
          \multirow{2}*{\bfseries Lexicon} & %
          \multicolumn{3}{c|}{Positive Expressions} & %
          \multicolumn{3}{c|}{Negative Expressions} & %
          \multicolumn{3}{c|}{Neutral Terms} & %
          \multirow{2}{0.068\columnwidth}{\centering Macro\newline $F1$} & %
          \multirow{2}{0.068\columnwidth}{\centering Micro\newline $F1$}\\\cline{2-10}

          & Precision & Recall & $F1$ & %
          Precision & Recall & $F1$ & %
          Precision & Recall & $F1$ & & \\\hline
      %% \multicolumn{9}{|c|}{\cellcolor{cellcolor}Existing Lexica}\\\hline

      GPC & 21.8\stddev{6.9} & 50.2\stddev{11.2} & 29.9\stddev{7.7} & %
      19.7\stddev{7} & 43.1\stddev{10.7} & 26.5\stddev{7.7} & %
      98.1\stddev{0.5} & 93.1\stddev{1.1} & 95.6\stddev{0.5} & %
      50.7\stddev{4.5} & 91.3\stddev{1}\\

      SWS & 35.6\stddev{12.8} & 41.7\stddev{11.3} & 37.3\stddev{9.9} & %
      48.5\stddev{17.1} & 32\stddev{11.3} & \textbf{37.6}\stddev{11.9} & %
      97.6\stddev{0.5} & 97.7\stddev{1} & 97.6\stddev{0.5} & %
      \textbf{57.5}\stddev{6.3} & 95.3\stddev{0.9}\\

      ZPL & 39.5\stddev{11.8} & 31.9\stddev{9} & 34.7\stddev{9} & %
      37.4\stddev{14.7} & 25.6\stddev{9.5} & 29.8\stddev{10.7} & %
      97.3\stddev{0.6} & 98.4\stddev{0.4} & 97.8\stddev{0.3} & %
      54.1\stddev{5.1} & 95.6\stddev{0.6}\\

      GPC $\cap$ SWS $\cap$ ZPL & \textbf{53.5}\stddev{12.6} & %
      35.4\stddev{10.5} & \textbf{41.7}\stddev{9.9} & %

      \textbf{61.2}\stddev{17.7} & 22\stddev{9.2} & 31.5\stddev{11.4} & %
      97.3\stddev{0.6} & \textbf{99.2}\stddev{0.3} & \textbf{98.2}\stddev{0.3} & %
      57.1\stddev{5.6} & \textbf{96.4}\stddev{0.6}\\

      GPC $\cup$ SWS $\cup$ ZPL & 21\stddev{6.5} & \textbf{52.5}\stddev{11.1} & 29.6\stddev{7.6} & %
      20\stddev{7.3} & \textbf{48.3}\stddev{10.8} & 27.7\stddev{8.3} & %
      \textbf{98.3}\stddev{0.5} & 92.6\stddev{1.1} & 95.4\stddev{0.5} & %
      50.9\stddev{4.6} & 90.9\stddev{1}\\\hline
    \end{tabular}
    \egroup
    \caption{Evaluation of the existing German sentiment lexica.\\ {\small (GPC -- German Polarity
        Clues \cite{Waltinger:10}, SWS -- SentiWS \cite{Remus:10}, ZPL
        -- Zurich Polarity Lexicon \cite{Clematide:10})}}
    \label{snt-lex:tbl:gsl-res}
  \end{center}
\end{table}

As can be seen from the table, the intersection of all three lexica
achieves the best results on both positive and neutral classes, also
yielding the best scores in terms of the macro-averaged $F$-measure.
One of the main reasons for this success is the relatively high
precision of this list for all but the neutral polarity class, where
it is outperformed by the union of the three resources.  Not
surpisingly, the union also shows the highest recall of positive and
negative terms among all compared polarity sets.

Regarding the figures attained by the individual lexica, the best
results here are achieved by the SentiWS polarity list
\cite{Remus:10}, which not only shows the highest $F1$-score at
recognizing negative expressions but also achieves the best
micro-averaged $F1$-result for all classes.

At the same time, we alco can observe that the deviation of the scores
on different files is relatively high.  In order to see whether this
skewness of the distribution had significantly affected the net
outcome, we additionally recomputed all results on the whole corpus.
The updated figures are shown in Table~\ref{snt-lex:tbl:gsl-res-full}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{|p{0.162\columnwidth}| % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}|} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}|}} % last two columns
      \hline
          \multirow{2}*{\bfseries Lexicon} & %
          \multicolumn{3}{c|}{Positive Expressions} & %
          \multicolumn{3}{c|}{Negative Expressions} & %
          \multicolumn{3}{c|}{Neutral Terms} & %
          \multirow{2}{0.068\columnwidth}{\centering Macro\newline $F1$} & %
          \multirow{2}{0.068\columnwidth}{\centering Micro\newline $F1$}\\\cline{2-10}

          & Precision & Recall & $F1$ & %
          Precision & Recall & $F1$ & %
          Precision & Recall & $F1$ & & \\\hline
      %% \multicolumn{9}{|c|}{\cellcolor{cellcolor}Existing Lexica}\\\hline

      GPC & 21.71 & 50.56 & 30.38 & %
       20.1 & 43.13 & 27.42 & %
       98.14 & 93.12 & 95.56 & %
       51.12 & 91.32\\

      SWS & 34.3 & 42.24 & 37.86 & %
       49.06 & 32.32 & \textbf{38.96} & %
       97.61 & 97.69 & 97.65 & %
       \textbf{58.16} & 95.33\\

      ZPL & 39.76 & 31.9 & 35.4 & %
       38 & 25.95 & 30.84 & %
       97.27 & 98.36 & 97.8 & %
       54.68 & 95.59\\

      GPC $\cap$ SWS $\cap$ ZPL & \textbf{54.04} & 35.57 & \textbf{42.9} & %
       \textbf{62.22} & 22.15 & 32.7 & %
       97.27 & \textbf{99.14} & \textbf{98.2} & %
       57.92 & \textbf{96.41}\\

      GPC $\cup$ SWS $\cup$ ZPL & 20.92 & \textbf{52.8} & 29.96 & %
       20.36 & \textbf{48.63} & 28.7 & %
       \textbf{98.3} & 92.61 & 95.37 & %
       51.35 & 90.88\\\hline
    \end{tabular}
    \egroup
    \caption{Evaluation of the existing German sentiment lexica on the
      complete corpus.\\ {\small (GPC -- German Polarity Clues
        \cite{Waltinger:10}, SWS -- SentiWS \cite{Remus:10}, ZPL --
        Zurich Polarity Lexicon \cite{Clematide:10})}}
    \label{snt-lex:tbl:gsl-res-full}
  \end{center}
\end{table}

As we can see from the table, the relative placement of the
best-performing systems is the same as in
Table~\ref{snt-lex:tbl:gsl-res}.  Moreover, the absolute scores are
only minimally higher (zero to one percent) than the mean estimates
computed in the previous step.  Thus, even despite their high
variance, the averages of the results computed on single files are
still reliable indicators of the quality of recognized opinionated
terms.  For the sake of brevity, we will therefore only compute the
former metric (mean and standard deviation of the scores on single
files), refraining from estimating the latter scores (i.e., total
statistics on the corpus as a whole).

\subsection{Lexicon Generation Using a Lexical Database}

Since SentiWS \cite{Remus:10} was the best compared stand-alone
lexicon, a natural question that arises in this case is whether the
original method used for creating its underlying Engish source data --
the SentiWordNet lexicon of \citet{Esuli:06b} -- would also produce a
comparatively good polar term list when applied to German data
directly.

To check this hypothesis, we have re-implemented the original system
of the authors of SentiWordNet and applied it to the German equivalent
of the English WordNet \cite{Miller:95} -- the GermaNet database
\cite{Hamp:97}.

The results of this method are shown in Table~\ref{snt-lex:tbl:swn-res}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize \small
    \begin{tabular}{|p{0.21\columnwidth}| % first columm
        *{8}{>{\centering\arraybackslash}m{0.1\columnwidth}|}} % next nine columns
      \hline
          \multirow{2}*{\bfseries Element} & \multicolumn{3}{c|}{Positive
        Expressions} & %
      \multicolumn{3}{c|}{Negative Expressions} & %
      \multirow{2}*{Macro-$F1$} & %
      \multirow{2}*{Micro-$F1$}\\\cline{2-7}

      & Precision & Recall & $F1$ & Precision & Recall & $F1$ & & \\\hline
      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Existing Lexica}\\\hline

      SentiWordNet$^{\mathrm{ternary}}_{\mathrm{Rocchio}}$ & 67.09\stddev{22.16} &
      14.74\stddev{7.79} & 11.73\stddev{5.59} & 4.57\stddev{4.57} &
      5.88\stddev{5.4} & 2.48\stddev{2.37} & 21.08\stddev{2.15} &
      96.04\stddev{0.72}\\

      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Our Method}\\\hline
    \end{tabular}
    \egroup
    \caption{Classification results.\\ {\small (GPC -- German Polarity
        Clues \cite{Waltinger:10}, SWS -- SentiWS \cite{Remus:10}, ZPL
        -- Zurich Polarity Lexicon \cite{Clematide:10})}}
    \label{snt-lex:tbl:swn-res}
  \end{center}
\end{table}

\subsection{Corpus-based Lexicon Induction}

Another popular alternative to the Ontology-based methods is lexicon
induction on the basis of actual corpus data.  Considering that
Twitter vocabulary is typically very different from the entries that
are usually included into standard-language dictionaries, applying
this strategy directly to tweets might potentially significantly
outperform the results of both translated resources and polar term
lists generated from GermaNet.

To check this hypothesis, we have re-implemented the Ising Spin system
of \citet{Takamura:05} -- one of the arguably most competitive methods
for unsupervised lexicon induction -- and applied it to the German
Twitter snapshot of \cite{Scheffler:14}.

The results of this method are shown in Table~\ref{snt-lex:tbl:ispn-res}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize \small
    \begin{tabular}{|p{0.21\columnwidth}| % first columm
        *{8}{>{\centering\arraybackslash}m{0.1\columnwidth}|}} % next nine columns
      \hline
          \multirow{2}*{\bfseries Element} & \multicolumn{3}{c|}{Positive
        Expressions} & %
      \multicolumn{3}{c|}{Negative Expressions} & %
      \multirow{2}*{Macro-$F1$} & %
      \multirow{2}*{Micro-$F1$}\\\cline{2-7}

      & Precision & Recall & $F1$ & Precision & Recall & $F1$ & & \\\hline
      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Existing Lexica}\\\hline

      Ising Spin Model & \stddev{} & \stddev{} & \stddev{} & \stddev{}
      & \stddev{} & \stddev{} & \stddev{} & \stddev{}\\\hline

      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Our Method}\\\hline
    \end{tabular}
    \egroup
    \caption{Classification results.\\ {\small (GPC -- German Polarity
        Clues \cite{Waltinger:10}, SWS -- SentiWS \cite{Remus:10}, ZPL
        -- Zurich Polarity Lexicon \cite{Clematide:10})}}
    \label{snt-lex:tbl:ispn-res}
  \end{center}
\end{table}

\subsection{Lexicon Generation Using Neural Word Embeddings}

A new family of lexicon induction methods builds on learned vector
representations of words -- the neural word embeddings
\cite{Mikolov:13}.

The results of this method are shown in Table~\ref{snt-lex:tbl:w2v}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize \small
    \begin{tabular}{|p{0.21\columnwidth}| % first columm
        *{8}{>{\centering\arraybackslash}m{0.1\columnwidth}|}} % next nine columns
      \hline
          \multirow{2}*{\bfseries Element} & \multicolumn{3}{c|}{Positive
        Expressions} & %
      \multicolumn{3}{c|}{Negative Expressions} & %
      \multirow{2}*{Macro-$F1$} & %
      \multirow{2}*{Micro-$F1$}\\\cline{2-7}

      & Precision & Recall & $F1$ & Precision & Recall & $F1$ & & \\\hline
      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Existing Lexica}\\\hline

      SentiWordNet$^{\mathrm{ternary}}_{\mathrm{Rocchio}}$ & 67.09\stddev{22.16} &
      14.74\stddev{7.79} & 11.73\stddev{5.59} & 4.57\stddev{4.57} &
      5.88\stddev{5.4} & 2.48\stddev{2.37} & 21.08\stddev{2.15} &
      96.04\stddev{0.72}\\

      Ising Spin Model & \stddev{} & \stddev{} & \stddev{} & \stddev{}
      & \stddev{} & \stddev{} & \stddev{} & \stddev{}\\\hline

      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Our Method}\\\hline
    \end{tabular}
    \egroup
    \caption{Classification results.\\ {\small (GPC -- German Polarity
        Clues \cite{Waltinger:10}, SWS -- SentiWS \cite{Remus:10}, ZPL
        -- Zurich Polarity Lexicon \cite{Clematide:10})}}
    \label{snt-lex:tbl:w2v}
  \end{center}
\end{table}

\subsection{Related Work}

Among the first who addressed the problem of the automatic generation
of sentiment lexica were \citet{Hatzivassi:97}.  In their work, the
authors relied on the hypothesis that coordinatively conjoined terms
often share the same semantic orientation while adversatively linked
words rather express opposite polarities.  To test this conjecture,
they automatically extracted all pairs of conjoined adjectives from
the Wall Street Journal (WSJ) corpus and represented those adjectives
as nodes in a graph.  The arcs weights of this graph were to show the
strength and direction by which two coordinatively conjoined terms
influenced each others' polarity.  To derive these weights,
\citeauthor{Hatzivassi:97} trained a log-linear regression model on
those pairs of terms in which both nodes belonged to a manually
labeled seed set of 1,336 adjectives and then let this model predict
the weights for the rest of the arcs.  In the final step, the
resulting graph was partitioned into two clusters -- that of positive
and that of negative terms -- which were subsequently used to enrich
the initial seed set.
%% This method, enhanced by the possibility of recognizing gradable
%% adjectives, was later used in the classification experiments of
%% \citet{Hatzivassi:00} to predict subjective and objective sentences in
%% the WSJ.

A different way of bootstrapping polarity terms from large text
corpora was proposed by \citet{Turney:03}.  Following
\citeauthor{Turney:02}'s original approach for classifying reviews
\citep{Turney:02}, the authors generated a polarity lexicon by first
taking a seed set of 14 a priory known polar adjectives (seven
negative and seven positive ones) and then expading this set with the
words that had the strongest pointwise mutual information associations
with the chosen seeds.  The PMI scores were computed as the log ratio
between the number of times a new word $w$ appeared with any of the
seed terms, divided by the total number of search hits for the
complete seed set.  As search hits, the authors considered the number
of relevant documents returned by the \texttt{AltaVista} search engine
for the given queries.  This system attained an accuracy of 82.84\% on
the General Inquirer Lexicon \citep{Stone:66} and correctly predicted
polar terms from the lexicon of \cite{Hatzivassi:97} in 87.13\% of the
cases. %% This method could also be further improved by using cosine
%% similarities between word vectors from an LSA matrix instead of
%% web-based PMIs.

Other notable works on corpus-based lexicon induction include
\citet{Kanayama:06}, who enhanced the method of
\citeauthor{Hatzivassi:97} by incorporating iter-sentential coherence
relations.  \citet{Kaji:07} also followed a corpus-based approach as
they mined opinionated sentences from HTML pages using structural and
linguistic clues and then extracted new polar terms from these
sentences, considering words having the highest PMI association scores
with the rest of the sentences as subjective.

One of the allegedly first attempts to derive a sentiment lexicon from
a lexical ontology instead of a corpus was made by \citet{Kim:04}.  In
their work, the authors expanded an initial seed set of 34 adjectives
and 44 verbs to a list of 18,192 presumably polar terms by iteratively
enriching the seed words with their synonyms and antonyms from the
\texttt{WordNet} database \citep{Miller:95}. In a similar way,
\citet{Godbole:07} enriched their topic-specific seed sets of polar
expressions by following the synonymy and antonymy links in
\texttt{WordNet} and exponentially decreasing the probability of a new
word being polar with the growing path length from a seed term.

%% Another interesting finding of this paper was surpassed by using
%% the cosine similarities between word vectors in an LSA matrix
%% instead of PMI scores from web crawlers.

%% More precisely, the social orientation PMI (SO-PMI) of a new word
%% $w$ was defined as: \begin{equation*} \text{SO-PMI}(w) =
%% \log_2\bigg(\frac{\prod_{p\in\mathcal{P}}\text{hits}(w\text{NEAR}p)%
%% \prod_{n\in\mathcal{N}}\text{hits}(w\text{NEAR}n)}%
%% {\prod_{p\in\mathcal{P}}\text{hits}(p)\prod_{n\in\mathcal{N}}\text{hits}(n)}\bigg) \end{equation*}
%% where $\mathcal{P}$ and $\mathcal{N}$ are the seed sets of positive
%% and negative adjectives respectively and
%% $\text{hits}(\cdot\text{NEAR}\cdot)$ is the number

%% The authors evaluated the accuracy of their model on the General
%% Inquirer lexicon \cite{Stone:66}.  Its final results (81.9\%) were
%% comparable to the figures obtained by \citet{Turney:03} in their
%% method (82.84\%) and significantly outperformed the precision of
%% the approach proposed by \citet{Kamps:04} (73.4 versus 70.8).

Another purely dictionary-based method was described by
\citet{Esuli:06b}.  The authors automatically assigned positive,
negative, and neutral polarity scores to synsets in \texttt{WordNet}
by iteratively expanding seed sets for each of these three polarity
classes using a committee of binary and multiclass classifiers.  At
each step of this process, these classifiers were trained on the seed
sets obtained from the previous runs and then used to predict whether
further synsets immediately accessible from the recently expanded
seeds would belong the same polarity class as their seed term.

\citet{Takamura:05} attempted to unite corpus- and dictionary-based
approaches into a single framework.  To this end, the authors adopted
the Ising spin model from the statistical mechanics and represented
all words found in \texttt{WordNet}, in the Wall Street Journal, and
the Brown corpus as nodes in a graph.  The edges of this graph
represented associativity links and were established between any two
words, if one of these word was a synonym, an antonym, or a hyponym of
the other one or appeared in its gloss or context.  Taking into
account the a priori known polarities of some of the terms, the Ising
model then tried to find an approximation of the most likely polarity
combination of all terms in the graph over all possible polarity
assignments.

\subsubsection{Domain-Specific Sentiment Lexica}

\citet{Chetviorkin:14} obtained a set of possible subjective terms
from English and Russian microblogs by using an ensemble of supervised
machine learning classifiers that had previously been trained on a
manually annotated corpus of movie reviews.  In order to determine the
prior polarity of the extracted terms, the authors first calculated
approximate polarity scores of the processed messages using general
polarity lexicons and then took these rough estimates as prior
polarity expectations of the candidate expressions.  The posterior
scores of these expressions were computed using the Ising spin model
in a similar way to the approach proposed by \citet{Takamura:05}.  The
resulting lexicon comprised 2,772 words for Russian and 2,786 lexical
items for English.

\subsubsection{Sentiment Lexica for German}

One of the first attempts to create a sentiment dictionary for German
was made by \citet{Remus:10}.  In their work, the authors
automatically translated the General Inquirer lexicon \citep{Stone:66}
into German.  In order to incorporate domain-specific knowledge into
this translated resource, they also collected a set of terms that were
strongly associated with positive or negative product reviews and
added this set to their translations.  Finally, the resulting lexicon
was enriched with inflection forms and frequent collocations of the
translated expressions.  The authors automatically estimated polarity
scores of their obtained 3,648 lexicon lemmas as the difference
between their positive and negative PMIs (i.e. PMI scores between the
collected terms and the pre-defined sets of positive or negative
expressions).

Similar work was also done by \citet{Waltinger:10} who compiled a list
of 10,141 subjective terms by first automatically translating the
Subjectivity Clues lexicon \citep{Wilson:05} and then manually
reassessing polarity scores of the translated items.  The author
expanded this list with the most frequent German synonyms of the
obtained terms and also added a set of negated opinionated phrases
(e.g. \emph{nicht schlecht} (\emph{not bad})).  The resulting
dictionary was used as a source of features for an automatic
classification of the polarity of Amazon reviews and showed superior
accuracy in comparison with other automatically translated resources.

A slightly different approach was taken by \citet{Clematide:10} who
manually annotated synsets from \emph{GermaNet} \cite{Hamp:97} with
their prior polarities and polarity strengths.  A subsequently
conducted study revealed, however, that the stock of polar adjectives
obtained in this way was insufficient for doing proper subjectivity
analysis of literary texts.  In order to overcome this problem, the
authors followed the idea of \citet{Hatzivassi:97} and extracted an
additional set of 918 adjectives which frequently co-occurred in
conjoined phrases with subjective terms from their dictionary.  After
manually inspecting each of the candidate terms, the authors added
this pruned set of new adjectives to their original lexicon.  The
total size of this final dictionary run up to 7,432 entries, 2,779 of
which were positive.

\subsection{Summary and Conclusions}

In this section, we presented the first attempt of a practical
evaluation of our corpus.  In doing so, we addressed the task of
automatic prediction of polar terms (emotional expressions) with the
help of sentiment dictionaries.  To obtain a rough baseline estimate,
we first evaluated the quality of the existing sentiment lists for
German: German Polarity Clues~\cite{Waltinger:10},
SentiWS~\cite{Remus:10}, and Zurich Polarity List~\cite{Clematide:10}.
We showed that \ldots achieved the best quality, reaching an average
$F1$-score of \ldots on recognizing positive expressions and \ldots on
predicting negative polar terms.

In the next step, we analyzed whether the methods that were used for
creating the original English resources whose translations formed the
basis of the German lexica could yield better results than the
manually revised tranlated lists when applied to German data directly.

The first family of lexicon generation methods that we looked into
used Ontology information about lexical items: words' definitions,
examples, and lexical links, to find subjectively connotated terms.
In particular, we reimplemented the original method of
\citet{Esuli:06b} and applied it to the German lexical database --
GermaNet \cite{Hamp:97}.  The results of this approach turned out to
be relatively low, only achievening \ldots for positive expressions
and \ldots for negative polar terms.  These rather low scores can be
explained by the scarceness of GermaNet definitions on the one hand
and the unconventional vocabulary used on Twitter.

Another popular approach to an unsupervised induction of sentiment
lexica relies on the cooccurrence information about the words taken
directly from corpus.  One of the most popular methods from this
category is the Ising spin model adopted from the statistical
mechanics which interprets words as magnetic spins in a crystal grid
and tries to derive the most probable orientation of these spins in a
magnetic field.  This model was first applied to the needs of
computational linguistics by \citet{Takamura:05}, who induced a
sentiment lexicon for English using \ldots corpus data.  We have
reimplemented this approach in our program suite and applied to the
German Twitter snapshop of \citet{Scheffler:14}.  The results of this
approach are shown in Table~\ref{snt-lex:tbl:ispn-res}.

A different way of incorporating corpus data is to encode the
cooccurrence statistics directly into word information, representing
the latter as vectors.  We explored this direction in the final part
of this section, first obtaining word2vec embeddings for tokens from
the aforementioned snapshot and then applying clustering algorithms to
these representations.

Our results show that \ldots.

\newpage
