\documentclass[a4paper,11pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{paralist}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\labelenumii}{\theenumi.\arabic{enumii} }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Research Proposal for PhD-Thesis}
\author{Uladzimir Sidarenka}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle

%% zwischen Kap 1 und 2 wäre wahrscheinlich eines der Art "Background:
%% Microblogging / Twitter" nicht schlecht.

%% Ich würde empfehlen jetzt für jede subsection einen längeren Absatz
%% einzufügen (viertel / halbe Seite), der die Zielsetzung dieser
%% subsection angibt: was soll hier passieren, was sind die
%% Arbeitsschritte für Dich, ggf.: wie ist die Relation dieser subsection
%% zu den bisherigen Abschnitten. So dass man sich insgesamt ein Bild von
%% der gesamten Dissertation machen kann, auch wenn man bisher nichts von
%% ihr gehört hat. Je konkreter das wird, umso besser - zum Beispiel
%% kannst Du vielleicht in (4) schon mal kurz skizzieren, was denn
%% discourse-level sentiment von dem bisher besprochenen sentence-level
%% sentiment unterscheidet.  In (2) kannst Du schon mal aufschreiben, was
%% Deine Datengrundlage ist (Zusammensetzung des Korpus).  Auch das
%% Einarbeiten konkreter Beispiele wäre nicht schlecht, zB für die
%% Sentimentannotation.

%% Das Ziel des ganzen Dokuments wäre: Stell Dir vor, Du machst eine
%% Inhaltsangabe für einen Computerlinguisten, der das Vokabular
%% beherrscht, aber Dein Projekt nicht kennt. Wenn er das proposal
%% gelesen hat, sollte er einen klaren Eindruck davon bekommen, was Du
%% machst.

%% (Optional kannst Du bei den subsections auch angeben, ob Teile der
%% Arbeit bereits erledigt sind.)

%% Meinst Du das lässt sich in erster Version bis Anfang der Woche
%% machen? Dann können wir ja zusammen noch etwas weiter dran basteln.

\section{Introduction}

The explosive expansion of online communication means (OCM) and social
networks in recent decades has lead to a change in the language in
which people communicate with each other via these means.  While
communication partners still are striving to express their opinions,
attitudes, thoughts, and moods in their full generality, they often do
not have reach enough possibilities for doing that in OCM as they
would have them in everyday speech.  One, for example, cannot show her
vein via facial expression or voice tonality in Twitter, neither there
is wide room for kinesics in Facebook posts.  As a consequence, new
language forms and meanings started appearing in online talks --
smileys replaced users' faces, exclamation marks replaced the voice
etc.

This work is supposed to be a bottom-up study of ways and means how
people express their polar attitudes or sentiments in online
communication.  The object of this study are users' discourses on
Twitter.  The aim of the work is to detect and determine how polar
attitudes of people are expressed in these discourses and what and
first of all how different levels of the language interact with each
other when people express their opinions.

We begin with the lowest level of natural language processing and
investigate based on character and word segments how widely the
language of OCM deviates from the standard language form.  In section
\ref{section-preprocessing} we also propose a normalization procedure
for mitigating spurious effects of casual writings to ease the work of
further NLP modules.  After a short description of intermediate
language analysis modules, we explore in section
\ref{section-sentsentiment} how polar attitudes of people are
expressed on sentential and subsentential level.  Since polar opinion
expressions however do not solely confine themselves to the sentence
level only, we take one step further and explore how relations between
different discourse segements help us detect more polar expressions as
well as sources and target of already detected sentiments.

\section{Corpus}

To begin our survey, we start with the preparation of a corpus in
which information relevant to sentiment extraction is annotated.  The
aim of this annotation is:
\begin{itemize}
  \item to determine whether polar judgements of users can be reliably
    annotated by people thus proving the amenability of this task for
    automatic computer analysis (this hypothesis has already been
    proven by Wiebe (\cite{TWilsonDiss08}) for English newspaper
    texts, now we prove this for German social media discourses);

  \item to evaluate existing systems and resources used for extracting
    German sentiments;

  \item to detect by which grammatical and rhetorical structures polar
    opinions are expressed;

  \item to find out which lexical, grammatical, and rhetorical clues
    might help us detect sentiments automatically.
\end{itemize}

For the above goals, we decided to annotate the following lexical
clues in the corpus:
\begin{itemize}
  \item \textit{emotional expressions} (lexical items which inherently
    possess some polar connotation in their meaning);
  \item \textit{intensifiers} and \textit{diminishers} (lexical items
    which may shift or vice versa reduce primary polar meaning of
    emotional expressions);
  \item \textit{negations} (lexical or grammatical which on their own
    can turn the polarity of an expression to its complete opposite).
\end{itemize}
The selection of these attributes was inspired by the annotation
scheme used for Layer 2 of the multi-layer sentiment corpus MLSA
(cf. \cite{MLSA}) and should primarily serve for evaluation of
existing sentiment lexica, as well as training basis for further
machine learning sentiment extraction system.

To train and test the performance of the actual sentiment extraction
system, we have also annotated spans of texts encompassing the
following elements of polar opinions:
\begin{itemize}
  \item the actual \textit{sentiment}s encompassing all the relevant
    information pertaining to subjective attitude;
  \item sentiment \textit{target}s which are actual subjects of
    particular judgements;
  \item sentiment \textit{source}s -- these elements represent
    immediate authors of polar opinions.
\end{itemize}

In order to keep the corpus as balanced as possible but at the same
time to capture the maximal number of sentimental expressions, we have
assembled it from three equal parts:
\begin{enumerate}
\item the first part was represented by sentences which had at least
  one of their words present in the sentiment lexicon SentiWS
  (cf. \cite{SentiWS});

\item the second part consisted of sentences which did not have any of
  the lexicon words but contained some emoticon or multiple
  repetitions of exclamation marks;

\item finally, the conluding part was randomly sampled from the rest
  of our larger data collection in order to see how often sentiments
  are expressed in messages not complying with the former two
  criteria.
\end{enumerate}

All of the above three parts were drawn in equal shares of 111
messages from four different data collections, namely:
\begin{enumerate}
  \item a corpus of generic users' tweets gathered in April 2013;
  \item a corpus of users' tweets pertaining to the election of the
    Pope Francis;
  \item a corpus of users' tweets pertaining to general political
    tweets gathered in spring 2013;
  \item a corpus of users' tweets pertaining to the federal election
    in Germany in autumn 2013.
\end{enumerate}
The total size of the corpus then run up to $3 * 4 * 111 = 1332$
messages.

This corpus has been annotated by ... annotators.  In a conluding step
after annotation is finished, we also plan to measure the
inter-annotator agreement and to see what annotation elements have
caused most difficulties for the annotators.

\textbf{Open Questions:} Manfred, maybe it would make sense to add
discourse level annotation already to this corpus? We than would have
all the annotation and corpus questions solved in one place.


\section{Preprocessing}\label{section-preprocessing}

In order to prepare the text for further NLP preprocessing with
established automatic text analysis tools, we decided to perform some
normalization steps prior to the analysis since many of the utilized
tools were not adjusted to the peculiarities of OCM texts a priori.

For doing so, we performed a study of what words could be considered
as out-of-vocabulary (OOV) and which part of them would require
normalization.  This study showed us (cf. \cite{Sidorernko}) that
approximately 20~\% of all input tokens on average were considered as
OOV by \texttt{TreeTagger} (\cite{Schmid}) and \texttt{hunspell}.

\section{Sentence-Level Sentiment Analysis}\label{section-sentsentiment}

\section{Discourse-Level Sentiment Analysis}

\section{Discourse-augmented Sentiment Analysis}

\bibliographystyle{plain} \bibliography{bibliography}
\end{document}
