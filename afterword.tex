\chapter*{Afterword}
\addcontentsline{toc}{chapter}{Afterword}

It is hard to believe, but we have finally reached the home stretch of
our \thepage-page long marathon and, preparing the final spurt, we
should first recall the main milestones that we have seen along this
way:
\begin{itemize}
\item As you might remember, we started off by summarizing the history
  of sentiment analysis, going back to its very origins in the ancient
  Greek philosophy and tracing its development to the present day;

\item Afterwards, to see what the current state of the art in opinion
  mining would yield on German Twitter, we created a corpus of
  $\approx8,000$ German tweets, collecting these messages for four
  different topics (federal elections, papal conclave, general
  political discussions, and casual everyday conversations).  To
  ensure a good recall of opinionated statements in the resulting
  dataset, we grouped all microblogs into three formal categories
  (tweets with a polar term from the SentiWS lexicon, messages
  containing a smiley, and all remaining microblogs) and sampled an
  equal number of tweets (666) from each of these three categories for
  each of the four topics.  After annotating this corpus in three
  steps (initial, adjudication, and final), we attained a reliable
  level of inter-annotator agreement for all elements (sentiments,
  sources, targets, polar terms, diminishers, negations, and
  intensifiers), finding that both selection criteria (topics and
  formal traits) had a significant impact on the distribution of
  sentiments and polar terms and the reliability of their annotation;

\item Then, at the first checkpoint, we compared exisiting German
  sentiment lexicons, which were translated from English resources and
  revised by human experts, with lexicons that were generated
  automatically from scratch with the help of existing
  dictionary\mbox{-,} corpus\mbox{-,} and word-embedding--based
  methods.  An evaluation of these approaches on our corpus showed
  that semi-automatically translated polarity lists were generally
  better than the automatically induced ones, reaching 0.587
  macro-\F{} and attaining 0.955 micro-\F{}--score for the prediction
  of polar terms.  Furthermore, among fully automatic methods,
  dictionary-based systems showed stronger results than their corpus-
  and word-embedding--based competitors, yielding 0.479 macro-\F{} and
  0.962 micro-\F{}.  We could, however, improve on the latter metric
  (pushing it to 0.963) with our proposed linear projection solution,
  in which we first found a line that maximized the mutual distance
  between the projections of seed vectors with opposite semantic
  orientations and then projected the embeddings of all remaining
  words on that line, considering the distance of these projections to
  the median as the respective polarity scores;

\item In Chapter \ref{chap:fgsa}, we turned our attention to the
  aspect-based sentiment analysis, in which we tried to predict the
  spans of sentiments, targets, and holders of opinions using two most
  popular approaches to this task---conditional random fields and
  recurrent neural networks.  We obtained our best results (0.287
  macro-\F{}) with the first-order linear-chain CRFs.  We could,
  however, increase these scores by using alternative topologies of
  CRFs (second-order linear-chain and semi-Markov CRFs) and also boost
  the macro-averaged \F{} to 0.38 by taking a narrower interpretation
  of the sentiment spans (in which we only assigned the
  \textsc{Sentiment} tag to polar terms).  Further evaluation of these
  methods proved the utility of the text normalization step (which
  raised the macro-\F{} of the CRF-method by almost 3\%) and
  task-specific word embeddings with the least-squares fallback (which
  improved the macro-\F{}--score of the GRU system by 1.4\%);

\item Afterwards, in Chapter~\ref{chap:cgsa}, we addressed one of the
  most popular objective in contemporary sentiment
  analysis---message-level polarity prediction (which we called
  coarse-grained sentiment analysis [CGSA]).  To get a better overview
  of the numerous existing systems, we compared three larger families
  of CGSA methods---dictionary-, machine-learning--, and
  deep-learning--based ones, finding that the last two groups
  performed significantly better than the lexicon-based approaches
  (the best macro-\F{}--scores of machine- and deep-learning methods
  run up to 0.677 and 0.69 respectively, whereas the best
  lexicon-based solution [\citeauthor{Hu:04}, \citeyear{Hu:04}] only
  reached 0.641 macro-\F{}).  In addition to that, we improved the
  results of many of these approaches by changing their default
  configuration (\eg{} abandoning polarity changing rules of
  lexicon-based systems, using altenative classifiers for ML-based
  classifiers, or taking the least-squares embeddings for DL-based
  methods).  Apart from numerous reimplementations of popular existing
  algorithms, we also proposed our own solution---lexicon-based
  attention (LBA), in which we tried to unite the lexicon and
  deep-learning paradigms by taking a bidirectional LSTM network and
  explicitly pointing its attention to the polar terms that appeared
  in the analyzed tweets.  With this solution, we not only
  outperformed all allternative DL systems, but also improved on the
  scores of ML-based classifiers, attaining 0.69 macro-\F{} and 0.73
  micro-\F{} on the PotTS corpus.  Similarly to the findings of the
  previous chapter, we observed a strong positive effect of text
  normalization and task-specific embeddings with the least-squares
  approximation;

\item Finally, in the last part, we tried to improve the results of
  the proposed LBA method by making it aware of the discourse
  structure.  For this purpose, we segmented all microblogs from the
  PotTS and SB10k corpora into elementary discourse units,
  individually analyzing each of these segments with our CGSA
  classifier, and then estimated the overall polarity of a tweet by
  joining the polarity scores of its EDUs over the RST tree.  We
  proposed three different ways of doing this joining (latent CRFs,
  latent-marginalized CRFs, and Recursive Dirichlet Process),
  obtaining better results than existing discourse-aware sentiment
  methods and also outperfoming the original discourse-unaware
  baseline.  In the concluding experiments, we further improved these
  scores by using manually annotated RST trees and richer subsets of
  discourse relations.
\end{itemize}

\section*{Conclusions}
\addcontentsline{toc}{section}{Conclusions}

Now that we have gone past all these landmarks, it is time to unbag
the list of the questions which we had asked ourselves at the
beginning of this endeavor, and try to answer them again, equipped
with all knowledge that we have acquired during our run.  Here we go:

\begin{itemize}
  \item\textbf{Can we apply NLP methods devised for standard English
    to German Twitter?}

    Yes, we can, but the success of these approaches might
    significantly vary depending on the task, the size and the
    reliability of the training data, as well as the evaluation metric
    that we use. For example, dictionary-based lexicon methods
    achieved fairly good results on their objective, but this success
    was mostly due to the high quality of the \textsc{GermaNet}
    annotation.  On the other hand, our manually labeled PotTS corpus
    was evidently too small for aspect-based sentiment systems, which
    failed to generalize to unseen tweets despite their high scores on
    the training messages.  Message-level sentiment approaches, vice
    versa, seemed to be quite happy with the size of the training set,
    attaining good results on both datasets (PotTS and SB10k).
    Nevertheless, we again experienced a lack of data while working on
    discourse-aware enhancements, many of which hit the same ceiling
    of the macro-averaged \F{}-scores.

    Apart from these difficulties arising from insufficient data, we
    also noticed a significant degradation of the scores for systems
    whose original tasks and evaluation metrics were different from
    ours.  For example, the lexicon generation method of
    \citet{Esuli:05} was originally designed to assign polarity scores
    to all \emph{synsets} found in the \textsc{WordNet} and not to
    produce a list of polar \emph{words}.  Similarly, the RNTN
    approach of \citet{Socher:13} was trained and evaluated on all
    syntactic subtrees of a document and not only at the top document
    level.  Likewise, the system of~\citet{Yessenalina:11} was devised
    for ordinal logisitc regression and not multi-class prediction, as
    in our case.  As a result, all these approaches showed lower
    scores than their competitors in our evaluation, even though they
    are undoubtedly well suited for their original data and tasks.

    Due to the high diversity of methods, metrics, and tasks, it is
    difficult to provide a general recipe for transferring existing
    English sentiment systems to German Twitter, but we still would
    like to formulate at least a few rules of thumb, which came up
    during our experiments:
    \begin{itemize}
      \item\textbf{Prefer methods which are closest to your training
        objective} and which were trained under similar conditions
        w.r.t. the amount of data, their class distribution and
        domain;
      \item\textbf{Put every single setting of these methods into
        question}---bear in mind that things which work well in the
        original cases are not guaranteed to work in your
        situation.\footnote{In this respect, it is important to
          realize that every classification task is merely an attempt
          to solve a system of equations, so that methods which are
          good at solving one system might completely fail to solve
          many other equations.}  The more options you try, the better
        will be your results;
      \item\textbf{Try using manually labeled resources for your
        target domain}, if they are available, but pay attention at
        the quality of their annotation---it often matters more than
        the corpus size;
      \item If you have manually annotated data, \textbf{prefer
        machine-learning methods to hard-coded rules}--- the former
        will penalize their bad components automatically by
        themselves;
      \item\textbf{Do not use randomly initialized word embeddings for
        deep-learning systems}---intialize them with language-model
        vectors (which are cheap to obtain) and approximate
        task-specific representations of unobserved tokens with the
        least-squares fallback.  Otherwise, your model might get stuck
        in a very bad local optimum.
    \end{itemize}

  \item\textbf{Which groups of methods are best suited for which
    sentiment tasks?}

    Based on our evaluation, we can answer to this question as
    follows:
    \begin{itemize}
      \item\emph{Sentiment lexicon generation} is more amenable to
        dictionary-based solutions, provided that there exists a
        sufficiently big, reliably annotated lexical taxonomy for
        these systems.  If there is no such resource, one should
        better resort to word-embedding based algorithms;

      \item With a limited amount of training data, \emph{aspect-based
        sentiment analysis} can be better addressed with probabilistic
        graphical models such as conditional random fields with
        hand-crafted features;

      \item On the other hand, plain \emph{message-level sentiment
        analysis} can be efficiently tackled with machine- and
        deep-learning algorithms (first of all SVM, logistic
        regression, and RNN);

      \item But probabilistic graphical models strike back at
        \emph{discourse-aware sentiment methods}, yieding fairly good
        scores with both directed and undirected models and even
        outperforming pure neural-network solutions, although the
        margin of these improvements is not always large.
    \end{itemize}

    Thus, probabilistic model can still hold their ground when it
    comes to structured prediction, but the difference of these
    algorithms from and their improvements upon neural networks are
    gradually vanishing.

  \item\textbf{How much do word- and discourse-level analyses affect
    message-level sentiment classification?}

    Our evaluation in Section~\ref{cgsa:subsec:eval:lexicons} showed
    that the macro-averaged \F{}-scores of our proposed lexicon-based
    attention method varied by up to 14\% (from 0.64 to 0.69
    macro-\F{} on the PotTS corpus, and from 0.44 to 0.58 on SB10k)
    depending on the lexicon used by this approach.  At the same,
    discourse enhancements could only improve the results of LBA by at
    most 1.5\% percent (from 0.677 to 0.678 on PotTS, and from 0.557
    to 0.572 on SB10k).  Although it appears at first glance that the
    lexicon component is more important to a sentiment system, such
    conclusion would be generally incorrect, because
    \begin{inparaenum}[(a)]
      \item a full-fledged sentiment solution should take into account
        both levels (words and discourse) and
      \item these relative results might look different if we expand
        the analyzed domain to longer documents or complete
        discoussion threads.
    \end{inparaenum}

  \item\textbf{Does text normalization help sentiment systems?}

    Yes, it definitely does.  As we could see in
    Chapters~\ref{chap:fgsa} and \ref{chap:cgsa}, normalization
    significantly improves the quality of aspect-based and
    message-level sentiment analysis, boosting the results on the
    former task by up to 4\% (cf.
    Table~\ref{snt-fgsa:tbl:normalization}) and improving the
    macro-averaged \F{}-measure of coarse-grained methods by up to
    25\% (cf. Table~\ref{snt-cgsa:tbl:res-no-normalization});

    %% The only question that remained unanswered in this context is
    %% which normalization steps exactly improve the scores of sentiment
    %% systems.
    %% \begin{table}[htb!]
    %%   \begin{center}
    %%     \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    %%     \begin{tabular}{p{0.162\columnwidth} % first columm
    %%         *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
    %%         *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
    %%       \toprule
    %%       \multirow{2}*{\bfseries Method} & %
    %%       \multicolumn{3}{c}{\bfseries Positive} & %
    %%       \multicolumn{3}{c}{\bfseries Negative} & %
    %%       \multicolumn{3}{c}{\bfseries Neutral} & %
    %%       \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}$^{+/-}$} & %
    %%       \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
    %%       \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

    %%       & Precision & Recall & \F{} & %
    %%       Precision & Recall & \F{} & %
    %%       Precision & Recall & \F{} & & \\\midrule

    %%       \multicolumn{12}{c}{\cellcolor{cellcolor}PotTS}\\
    %%       %% with normalization
    %%       w/ normalization & 0.76 & 0.84 & 0.79 & %
    %%       0.6 & 0.56 & 0.58 & %
    %%       0.75 & 0.68 & 0.72 & %
    %%       0.69 & 0.73\\
    %%       %% without replacement of Twitter-specific phenomena
    %%       w/o replacement of Twitter-specific phenomena & \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{}\\
    %%       %% without spelling correction
    %%       w/o spelling correction & \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{}\\
    %%       %% without slang normalization
    %%       w/o slang normalization & \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{}\\
    %%       %% without normalization
    %%       w/o normalization & 0.58\negdelta{0.18} & 0.77\negdelta{0.07} & 0.66\negdelta{0.13} & %
    %%       0.54\negdelta{0.06} & 0.53\negdelta{0.03} & 0.54\negdelta{0.04} & %
    %%       0.63\negdelta{0.12} & 0.37\negdelta{0.31} & 0.46\negdelta{0.26} & %
    %%       0.6\negdelta{0.09} & 0.58\negdelta{0.15}\\

    %%       \multicolumn{12}{c}{\cellcolor{cellcolor}SB10k}\\
    %%       %% with normalization
    %%       w/ normalization & 0.6 & 0.72 & 0.66 & %
    %%       0.47 & 0.42 & 0.44 & %
    %%       0.84 & 0.8 & 0.82 & %
    %%       0.55 & 0.73\\
    %%       %% without replacement of Twitter-specific phenomena
    %%       w/o replacement of Twitter-specific phenomena & \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{}\\
    %%       %% without spelling correction
    %%       w/o spelling correction & \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{}\\
    %%       %% without slang normalization
    %%       w/o slang normalization & \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{} & \negdelta{} & %
    %%       \negdelta{} & \negdelta{}\\
    %%       %% without normalization
    %%       w/o normalization & 0.24\negdelta{0.36} & 0.86\posdelta{0.14} & 0.38\negdelta{0.28} & %
    %%       0.45\negdelta{0.02} & 0.45\posdelta{0.03} & 0.45\posdelta{0.01} & %
    %%       0.69\negdelta{0.15} & 0.01\negdelta{0.79} & 0.02\negdelta{0.8} & %
    %%       0.41\negdelta{0.14} & 0.27\negdelta{0.46}\\\bottomrule
    %%     \end{tabular}
    %%     \egroup
    %%     \caption{LBA$^{(1)}$ results with different text normalization steps}
    %%     \label{afterword:tbl:lba-normalization-steps}
    %%   \end{center}
    %% \end{table}

  \item\textbf{Can we do better than these methods?}

    Yes, we can: with our proposed linear projection algorithms we
    improved the macro-averaged results of exisitng lexicon-generation
    methods; be redefining the topology of CRF graphs we increased the
    scores of aspect-based approaches; our lexicon-based attention
    network outperformed many of its competitors on message-level
    classification; and, finally, with latent-marginalized CRFs and
    Recursive Dirichlet Process we could surpass the discourse-unware
    baseline and other existing discourse-aware sentiment solutions.
\end{itemize}


%% https://github.com/WladimirSidorenko/CRFSuite
%% https://github.com/WladimirSidorenko/DiscourseSegmenter
%% https://github.com/WladimirSidorenko/SentiLex
%% https://github.com/WladimirSidorenko/CGSA
%% https://github.com/WladimirSidorenko/PotTS
%% https://github.com/WladimirSidorenko/TextNormalization
\section*{Contributions}
\addcontentsline{toc}{section}{Contributions}

Apart from answering the above questions and pushing the state of the
art on several major sentiment tasks , we have paved also released the
Potsdam Twitter Sentiment dataset, which is available at
\url{https://github.com/WladimirSidorenko/PotTS}, and open-sourced all

\section*{Limitations}
\addcontentsline{toc}{section}{Limitations}
