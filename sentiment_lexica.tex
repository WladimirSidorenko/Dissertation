% FILE: sentiment_lexica.tex  Version 0.01
% AUTHOR: Uladzimir Sidarenka

% This is a modified version of the file main.tex developed by the
% University Duisburg-Essen, Duisburg, AG Prof. Dr. Günter Törner
% Verena Gondek, Andy Braune, Henning Kerstan Fachbereich Mathematik
% Lotharstr. 65., 47057 Duisburg entstanden im Rahmen des
% DFG-Projektes DissOnlineTutor in Zusammenarbeit mit der
% Humboldt-Universitaet zu Berlin AG Elektronisches Publizieren Joanna
% Rycko und der DNB - Deutsche Nationalbibliothek

\section{Sentiment Lexica}\label{sec:snt:lex}

The first avenue that we are going to explore with the help of the
obtained corpus data is an automatic prediction of polar words or,
speaking in terms of our guidelines, emotional expressions.  To this
end, we will first present an updated version of our dataset in
Subsection~\ref{subsec:snt-lex:data} in which our experts revised the
annotations of words and idioms that were present in the existing
German sentiment lexica (GSL), but were not marked as emo-expressions
in our data and, vice versa, were annotated as polar terms in the
corpus, but absent in the analyzed polarity lists.  Afterwards, we
will evaluate the aforementioned lexical resources on the updated
corpus data in order to obtain a baseline for our subsequent
experiments.  Since literally all of the current GSL were created
using an automatic translation of English opinion lists with a manual
post-editing of the translated entries, we will then look whether the
original methods that were used initially for creating the English
source files would yield better results when applied to German data
directly.  Finally, in the concluding step, we will analyze if one of
most popular areas of research in contemporary CL---distributed vector
representations of words \cite{Mikolov:13}---could be a perspective
way for deriving new domain-specific polarity lists without labeled
data.  We will summarize and draw conclusions in the last part of this
section, also discussing which of the presented methods performed best
on our corpus and what the reasons for this success could be.

\subsection{Data}\label{subsec:snt-lex:data}

In the following experiments, we will use the updated version 0.1.0 of
the sentiment dataset introduced in the previous section.  The main
changes included in this version are:
\begin{inparaenum}[\itshape a\upshape)]
  \item the revision of the annotated emotional expressions, and
  \item the addition of the boolean attributes \emph{subjective-fact}
    and \emph{uncertain} to the annotation scheme of these elements.
\end{inparaenum}

To accomplish the fortmer objective, we compared the labelings of the
opinionated terms in the first release of our corpus (version 0.0.1
presented previously) with the entries from the existing German
sentiment lexica: SentiWS \cite{Remus:10}, German Polarity Clues
\cite{Waltinger:10}, and the Zurich Polarity List \cite{Clematide:10}.
Similarly to the adjudication procedure used earlier, we automatically
highlighted the differences between the corpus labels and the entries
of these resources, letting our experts resolve the emerged conflicts.

As it turned out, many of the contradicting cases stemmed from a
different treatment of polar facts, such as ``Tod'' (\emph{death}),
``Anstieg'' (\emph{surge}), ``duften'' (\emph{to scent}) etc.: these
entities were not labeled as emo-expressions in the full annotation of
our dataset, but were still present as opinionated terms in the
analyzed polarity lists.  Since these words represented a special
class of polar clues due to their unequivocal objective nature, we
decided to introduce a special feature called \emph{subjective-fact}
into the attribute set of emotional expressions, explicitly asking our
coders to annotate such terms with the emo-expression tag, but setting
the value of the newly added attribute to \texttt{true}.

The remaining differences were mainly due to polysemous words whose
meaning in the corpus was not always subjective; errors in the lexica
(GPC, for example, featured such auxiliary terms as ``sein'' (\emph{to
  be}), ``wer'' (\emph{who}), or ``aus'' (\emph{from}) as opinionated
entries); and, finally, numerous borderline cases which were difficult
to resolve even in group discussions.  Examples of such challenging
cases were words and idioms such as ``verbieten'' (\emph{to
  prohibit}), ``verwechseln'' (\emph{to confuse}), ``Hauen und
Stechen'' (\emph{hewing and stabbing}).  To address this issue, we
again introduced an additional attribute, called \emph{uncertain},
into the annotation scheme of emotional expressions and asked our
experts to mark such borderline instances with this attribute.

The statistics on the total number of the labeled elements and their
agreement in the updated version are shown in
Table~\ref{tbl:snt-lex:ucrp-agrmnt}:

\begin{table*}[thb!]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.7\tabcolsep} \scriptsize
    \begin{tabular}{|p{0.15\textwidth}| % first columm
        *{10}{>{\centering\arraybackslash}p{0.05\textwidth}|}} % next ten columns
      \hline
          \multirow{2}{0.2\textwidth}{\bfseries Element} &
          \multicolumn{5}{c|}{Binary $\kappa$} & %
          \multicolumn{5}{c|}{Proportional $\kappa$}\\\cline{2-11}
          & $M_1$ & $A_1$ & $M_2$ & $A_2$ & $\mathbf{\kappa}$ %
          & $M_1$ & $A_1$ & $M_2$ & $A_2$ & $\mathbf{\kappa}$\\\hline

          EExpression &  &  &  & & \textbf{} & &  &  &  & \textbf{}\\\hline
    \end{tabular}
    \egroup
  \end{center}
  \captionof{table}{Inter-annotator agreement on the emotional
    expression in the updated corpus.\\ {\small ($M1$ -- number of
      tokens with matching labels in the first annotation, $A1$ --
      total number of tokens labeled with that class in the first
      annotation, $M2$ -- number of tokens with matching labels in the
      second annotation, $A2$ -- total number of tokens labeled with
      that class in the second annotation)}}
  \label{tbl:snt-lex:ucrp-agrmnt}
\end{table*}

\subsection{Baseline I: Existing German Lexica}

In order to obtain a raw estimate of the expected scores for the
prediction of subjective expressions, we then evaluated existing
sentiment lexica for German on the updated corpus data.  The most
prominent of these resources are:
\begin{itemize}
\item the \textbf{German Polarity Clues} (GPC) list of
  \citet{Waltinger:10}, which comprises 10,141 subjective entries
  automatically translated from the English sentiment lexica
  \emph{Subjectivity Clues} \cite{Wilson:05} and \emph{SentiSpin}
  \cite{Takamura:05} with a subsequent manual rechecking of these
  translations and several synonyms and negated terms added by the
  authors;

\item the \textbf{SentiWS} (SWS) lexicon introduced by
  \citet{Remus:10}, which includes 1,818 positively and 1,650
  negatively connotated entries, also providing their part-of-speech
  tags and inflections (resulting in a total of 32,734 word forms).
  Similarly to the GPC, the authors used an English sentiment
  resource---the \emph{General Inquirer} list of \citet{Stone:66}---
  to bootstrap the entries for their lexicon, manually revising these
  automatic translations afterwards.  In addition to that,
  \citet{Remus:10} also expanded their polarity set with words and
  phrases frequently co-occurring with positive and negative seed
  lexemes using collocation information obtained from a corpus of
  10,200 customer reviews or extracted from the German Collocation
  Dictionary \cite{Quasthoff:10};

\item finally, the \textbf{Zurich Polarity List} proposed by
  \citet{Clematide:10}, which comprises 8,000 subjective entries taken
  from GermaNet synsets \cite{Hamp:97}.  These synsets were manually
  annotated with their prior polarities by human experts.  Since the
  authors, however, found the number of polar adjectives obtained that
  way insufficient for running further classification experiments,
  they automatically enriched their lexicon with more attributive
  terms by analyzing conjoined collocations from a corpus as it was
  done by \citet{Hatzivassi:97}.
\end{itemize}

In order to evaluate these resources on our dataset, we represented
each of the above lexicons as a trie \cite[pp. 492--512]{Knuth:98}
with the lexicon entries corresponding to paths in that graph and the
polarity class(es) of these entries stored at the respective terminal
leaf nodes of the paths.  We then applied the standard match operation
by simultaneously checking the trie against contiguous runs of both
original and lemmatized corpus tokens.  All lemmatizations were done
using the \texttt{TreeTagger} \cite{Schmid:95}, and the match
operation was made case-insensitive.  Furthermore, since all analyzed
lexica only included plain textual terms, we also ignored smileys
during this comparison, solely concentrating on common lexical terms.

In this way, we estimated the precision, recall, and $F1$-score of the
positive, negative, and neutral polarity classes of each lexicon by
separately computing these figures on each corpus file (99--109
tweets) and then obtaining the mean and standard deviation of these
scores over all files in the dataset.  In addition to that, we also
calculated the macro- and micro-averaged results over all polarity
classes.  Following the standard practice for computing these terms,
we estimated the macro-average as the mean of the $F1$-scores over all
three types (positive, negative, and neutral), and obtained the
micro-average by taking the harmonic mean of precision and recall
calculated over all true positives, false positives, and false
negatives found in the corpus.  The results of these computations are
shown in Table~\ref{snt-lex:tbl:gsl-res}.\footnote{For the sake of
  these experiments, we have excluded the auxiliary words ``aus''
  (\emph{from}), ``der'' (\emph{the}), ``keine'' (\emph{no}),
  ``nicht'' (\emph{not}), ``sein'' (\emph{to be}), ``was''
  (\emph{what}), and ``wer'' (\emph{who}) as well as their inflection
  forms from the German Polarity Clues lexicon as these entries
  significantly worsened the evaluation results.}

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{|p{0.162\columnwidth}| % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}|} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}|}} % last two columns
      \hline
          \multirow{2}*{\bfseries Lexicon} & %
          \multicolumn{3}{c|}{Positive Expressions} & %
          \multicolumn{3}{c|}{Negative Expressions} & %
          \multicolumn{3}{c|}{Neutral Terms} & %
          \multirow{2}{0.068\columnwidth}{\centering Macro\newline $F1$} & %
          \multirow{2}{0.068\columnwidth}{\centering Micro\newline $F1$}\\\cline{2-10}

          & Precision & Recall & $F1$ & %
          Precision & Recall & $F1$ & %
          Precision & Recall & $F1$ & & \\\hline
      %% \multicolumn{9}{|c|}{\cellcolor{cellcolor}Existing Lexica}\\\hline

      GPC & 24\stddev{7.9} & 46.4\stddev{11.2} & 31.1\stddev{8.2} & %
      22\stddev{8.1} & 41.6\stddev{10.6} & 28.1\stddev{8.3} & %
      98\stddev{0.5} & 94.3\stddev{1} & 96.1\stddev{0.5} & %
      51.8\stddev{4.7} & 92.3\stddev{0.9}\\

      SWS & 35.9\stddev{13.2} & 38.7\stddev{11.6} & 36\stddev{9.9} & %
      49\stddev{17.1} & 31.3\stddev{10.9} & \textbf{37.2}\stddev{11.6} & %
      97.5\stddev{0.5} & 97.8\stddev{1} & 97.7\stddev{0.5} & %
      \textbf{56.9}\stddev{6.2} & 95.4\stddev{0.9}\\

      ZPL & 36.4\stddev{12.3} & 21.3\stddev{7.2} & 26.3\stddev{8} & %
      41.1\stddev{15.6} & 23.3\stddev{8.7} & 29\stddev{10} & %
      97\stddev{0.6} & 98.7\stddev{0.3} & 97.8\stddev{0.3} & %
      51.1\stddev{4.4} & 95.7\stddev{0.6}\\

      GPC $\cap$ SWS $\cap$ ZPL & \textbf{54}\stddev{12.8} & %
      30.6\stddev{10.5} & \textbf{38.1}\stddev{10.1} & %

      \textbf{61.7}\stddev{17.8} & 21.6\stddev{9.1} & 31.2\stddev{11.3} & %
      97.1\stddev{0.6} & \textbf{99.3}\stddev{0.3} & \textbf{98.2}\stddev{0.3} & %
      55.8\stddev{5.4} & \textbf{96.4}\stddev{0.6}\\

      GPC $\cup$ SWS $\cup$ ZPL & 23.3\stddev{7.6} & \textbf{48.5}\stddev{11.2} & %
      30.9\stddev{8.2} & %

      22\stddev{8} & \textbf{46.1}\stddev{10.8} & 29.1\stddev{8.5} & %
      \textbf{98.1}\stddev{0.5} & 93.9\stddev{1} & 96\stddev{0.5} & %
      52\stddev{4.8} & 92\stddev{0.9}\\\hline
    \end{tabular}
    \egroup
    \caption{Evaluation of the existing German sentiment lexica.\\
      {\small (GPC -- German Polarity Clues \cite{Waltinger:10}, SWS
        -- SentiWS \cite{Remus:10}, ZPL -- Zurich Polarity Lexicon
        \cite{Clematide:10})}}
    \label{snt-lex:tbl:gsl-res}
  \end{center}
\end{table}

As can be seen from the table, the intersection of all three lexica
achieves the best results on both positive and neutral classes, also
yielding the best scores in terms of the micro-averaged $F$-measure.
One of the main reasons for this success is a relatively high
precision of this list for all but the neutral polarity class, where
it is outperformed by the union of the three resources.  Not
surpisingly, the union also shows the highest recall of positive and
negative terms among all compared polarity sets.

Regarding the figures attained by the individual lexica, the best
results here are achieved by the SentiWS list \cite{Remus:10}, which
not only shows the highest $F1$-score for negative expressions but
also achieves the best macro-averaged $F1$-result on all classes.

At the same time, we also can observe that the deviation of the scores
on different files is relatively high.  In order to see whether this
skewness of the distribution could significantly affect the net
outcome, we additionally recomputed all results on the whole corpus.
The updated figures are shown in Table~\ref{snt-lex:tbl:gsl-res-full}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{|p{0.162\columnwidth}| % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}|} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}|}} % last two columns
      \hline
          \multirow{2}*{\bfseries Lexicon} & %
          \multicolumn{3}{c|}{Positive Expressions} & %
          \multicolumn{3}{c|}{Negative Expressions} & %
          \multicolumn{3}{c|}{Neutral Terms} & %
          \multirow{2}{0.068\columnwidth}{\centering Macro\newline $F1$} & %
          \multirow{2}{0.068\columnwidth}{\centering Micro\newline $F1$}\\\cline{2-10}

          & Precision & Recall & $F1$ & %
          Precision & Recall & $F1$ & %
          Precision & Recall & $F1$ & & \\\hline
      %% \multicolumn{9}{|c|}{\cellcolor{cellcolor}Existing Lexica}\\\hline

      GPC & 23.83 & 46.8 & 31.58 & %
       22.37 & 41.58 & 29 & %
       98.01 & 94.27 & 96.1 & %
       52.26 & 91.32\\

      SWS & 34.36 & 39.23 & 36.63 & %
       49.56 & 31.45 & \textbf{38.48} & %
       97.5 & 97.84 & 97.67 & %
       \textbf{57.59} & 95.38\\

      ZPL & 36.4 & 21.22 & 26.81 & %
       41.57 & 23.14 & 29.73 & %
       96.96 & 98.73 & 97.84 & %
       51.46 & 95.67\\

      GPC $\cap$ SWS $\cap$ ZPL & \textbf{54.52} & 30.74 & \textbf{39.31} & %
       \textbf{62.92} & 21.68 & 32.25 & %
       97.13 & \textbf{99.25} & \textbf{98.18} & %
       56.58 & \textbf{96.39}\\

      GPC $\cup$ SWS $\cup$ ZPL & 23.11 & \textbf{48.86} & 31.38 & %
       22.42 & \textbf{46.05} & 30.16 & %
       \textbf{98.14} & 93.87 & 95.96 & %
       52.5 & 91.99\\\hline
    \end{tabular}
    \egroup
    \caption{Evaluation of the existing German sentiment lexica on the
      complete corpus.\\ {\small (GPC -- German Polarity Clues
        \cite{Waltinger:10}, SWS -- SentiWS \cite{Remus:10}, ZPL --
        Zurich Polarity Lexicon \cite{Clematide:10})}}
    \label{snt-lex:tbl:gsl-res-full}
  \end{center}
\end{table}

As we can see from the figures, the relative placement of the
best-performing systems is the same as in the previous evaluation.
Moreover, the absolute results are only minimally higher (by typically
at most one percent) than the mean scores computed in the previous
step.  Thus, even despite their high variance, the average figures
computed over single corpus files are still a reliable indicator of
the quality of the recognized opinionated terms.  For the sake of
brevity, we will therefore only present the former metric (the mean
and standard deviation of the scores estimated over individual files),
refraining from computing the statistics on the whole corpus at once.

\subsection{Baseline II: Automatic Lexicon Generation}

Since all of the presented works rely on a machine translation step
with a manual correction of the entries, they clearly fall into the
category of semi-automatic approaches.  Unlike fully automated
methods, such systems typically yield more precise results, which,
however, come at the cost of tedious human efforts.  In order to see
to which extent these efforts really pay off in practice on the
lexicon generation (LG) task at hand, we additionally decided to
evaluate the most popular lexicon induction approaches that work
completely automatically without any assistance from human experts.

According to \citet[p. 79]{Liu:12}, most of such automatic LG systems
can be divided into dictionary- and corpus-based ones.  The former
methods try to derive polarity lists using monolingual thesauri or
lexical databases such as the Macquarie Dictionary \cite{Bernard:86}
or WordNet \cite{Miller:95}.  A clear advantage of these approaches is
their relatively good precision as they operate on carefully verified
data enriched with hand-crafted meta information.  At the same time,
this verification can become a drawback for the recall on the domains
where the language changes occur very rapidly, and new terms are being
coined in a flash.

An opposite situation is observed with the corpus-based approaches:
These methods typically rely on inherently noisy datasets and,
consequently, suffer from a low precision.  This disadvantage,
however, is more than compensated for by the abundance of training
material available to these methods and the persistent up-to-date
state of these data, which can hugely boost the recall.

In order to see which of the above paradigms bears more potential for
generating high-quality polarity lists for Twitter---the
dictionary-based methods with a supposedly higher precision or the
corpus-based approaches with an allegedly better recall---we decided
to re-implement the most popular systems from both families and to
evaluate them on our corpus.  The results of these computations are
presented in the following sections.

\subsubsection{Dictionary-Based Lexicon Generation}

The presumably first dictionary-based approach was proposed by
\citet{Hu:04}.  In their work on sentiment classification and
summarization of cutomer reviews, the authors determined semantic
orientation of adjectives (which were supposed to be the most relevant
part-of-speech class for mining opinions) by taking a seed set of
words with known subjective polarities and propagationg these values
to the synonyms of these terms found in WordNet \cite{Miller:95}.  A
similar procedure was applied to antonymic relations too.  However, in
this case, the polarity orientation was reversed during the
propagation.  This expansion continued until no more adjective could
be reached via the synonymy-antonymy (SA) links, thus covering all
stronly-connected SA components around the seed words in the WordNet
graph.  Unfortunately, no intrinsic evaluation of the resulting
lexicon was performed in this work---the authors only report their
results on recognizing subjective sentences and classifying their
polarity, where they attain an average \F-score of 0.667 and 0.842
respectively.

With various modifications, the core idea of propagating the semantic
orientation through a graph was adopted in almost all of the following
dictionary-based works:

\cite{Kim:06}

An alternative way of bootstrapping polarities for open-class words
was proposed by \cite{Kamps:04}.  The authors estimated the
orientation of the given term by computing the difference between the
shortest path lengths of this word to the prototypic positive and
negative lexemes.  For example, the orientation of an adjective was
computed as
\begin{equation*}
  POL(w) = \frac{d(w, \textrm{bad}) - d(w, \textrm{good})}%
  {d(\textrm{bad}, \textrm{good})},
\end{equation*}
where $d(w_1, w_2)$ denotes the length of the shortest synonym path
between the words $w_1$ and $w_2$ found in WordNet.  \cite{Kamps:04}
evaluated the accuracy of their method on the General Inquirer lexicon
\cite{Stone:66} by comparing the terms with non-zero scores to the
entries from this resource, getting 68.19\% of correct predictions on
a set of 349 terms.

\cite{Esuli:06b}


\cite{Blair-Goldensohn:08}

\cite{Mohammad:09}

\cite{Rao:09}

\cite{Hassan:10}

\cite{Dragut:10}

The results of this method are shown in Table~\ref{snt-lex:tbl:swn-res}.

To check this hypothesis, we have re-implemented the original system
of the authors of SentiWordNet and applied it to the German equivalent
of the English WordNet \cite{Miller:95} -- the GermaNet database
\cite{Hamp:97}.


\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize \small
    \begin{tabular}{|p{0.21\columnwidth}| % first columm
        *{8}{>{\centering\arraybackslash}m{0.1\columnwidth}|}} % next nine columns
      \hline
          \multirow{2}*{\bfseries Element} & \multicolumn{3}{c|}{Positive
        Expressions} & %
      \multicolumn{3}{c|}{Negative Expressions} & %
      \multirow{2}*{Macro-$F1$} & %
      \multirow{2}*{Micro-$F1$}\\\cline{2-7}

      & Precision & Recall & $F1$ & Precision & Recall & $F1$ & & \\\hline
      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Existing Lexica}\\\hline

      SentiWordNet$^{\mathrm{ternary}}_{\mathrm{Rocchio}}$ & 67.09\stddev{22.16} &
      14.74\stddev{7.79} & 11.73\stddev{5.59} & 4.57\stddev{4.57} &
      5.88\stddev{5.4} & 2.48\stddev{2.37} & 21.08\stddev{2.15} &
      96.04\stddev{0.72}\\

      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Our Method}\\\hline
    \end{tabular}
    \egroup
    \caption{Classification results.\\ {\small (GPC -- German Polarity
        Clues \cite{Waltinger:10}, SWS -- SentiWS \cite{Remus:10}, ZPL
        -- Zurich Polarity Lexicon \cite{Clematide:10})}}
    \label{snt-lex:tbl:swn-res}
  \end{center}
\end{table}

\subsection{Corpus-based Lexicon Induction}

Another popular alternative to the Ontology-based methods is lexicon
induction on the basis of actual corpus data.  Considering that
Twitter vocabulary is typically very different from the entries that
are usually included into standard-language dictionaries, applying
this strategy directly to tweets might potentially significantly
outperform the results of both translated resources and polar term
lists generated from GermaNet.

To check this hypothesis, we have re-implemented the Ising Spin system
of \citet{Takamura:05} -- one of the arguably most competitive methods
for unsupervised lexicon induction -- and applied it to the German
Twitter snapshot of \cite{Scheffler:14}.

The results of this method are shown in Table~\ref{snt-lex:tbl:ispn-res}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize \small
    \begin{tabular}{|p{0.21\columnwidth}| % first columm
        *{8}{>{\centering\arraybackslash}m{0.1\columnwidth}|}} % next nine columns
      \hline
          \multirow{2}*{\bfseries Element} & \multicolumn{3}{c|}{Positive
        Expressions} & %
      \multicolumn{3}{c|}{Negative Expressions} & %
      \multirow{2}*{Macro-$F1$} & %
      \multirow{2}*{Micro-$F1$}\\\cline{2-7}

      & Precision & Recall & $F1$ & Precision & Recall & $F1$ & & \\\hline
      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Existing Lexica}\\\hline

      Ising Spin Model & \stddev{} & \stddev{} & \stddev{} & \stddev{}
      & \stddev{} & \stddev{} & \stddev{} & \stddev{}\\\hline

      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Our Method}\\\hline
    \end{tabular}
    \egroup
    \caption{Classification results.\\ {\small (GPC -- German Polarity
        Clues \cite{Waltinger:10}, SWS -- SentiWS \cite{Remus:10}, ZPL
        -- Zurich Polarity Lexicon \cite{Clematide:10})}}
    \label{snt-lex:tbl:ispn-res}
  \end{center}
\end{table}

\subsection{Lexicon Generation Using Neural Word Embeddings}

A new family of lexicon induction methods builds on learned vector
representations of words -- the neural word embeddings
\cite{Mikolov:13}.

The results of this method are shown in Table~\ref{snt-lex:tbl:w2v}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize \small
    \begin{tabular}{|p{0.21\columnwidth}| % first columm
        *{8}{>{\centering\arraybackslash}m{0.1\columnwidth}|}} % next nine columns
      \hline
          \multirow{2}*{\bfseries Element} & \multicolumn{3}{c|}{Positive
        Expressions} & %
      \multicolumn{3}{c|}{Negative Expressions} & %
      \multirow{2}*{Macro-$F1$} & %
      \multirow{2}*{Micro-$F1$}\\\cline{2-7}

      & Precision & Recall & $F1$ & Precision & Recall & $F1$ & & \\\hline
      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Existing Lexica}\\\hline

      SentiWordNet$^{\mathrm{ternary}}_{\mathrm{Rocchio}}$ & 67.09\stddev{22.16} &
      14.74\stddev{7.79} & 11.73\stddev{5.59} & 4.57\stddev{4.57} &
      5.88\stddev{5.4} & 2.48\stddev{2.37} & 21.08\stddev{2.15} &
      96.04\stddev{0.72}\\

      Ising Spin Model & \stddev{} & \stddev{} & \stddev{} & \stddev{}
      & \stddev{} & \stddev{} & \stddev{} & \stddev{}\\\hline

      \multicolumn{9}{|c|}{\cellcolor{cellcolor}Our Method}\\\hline
    \end{tabular}
    \egroup
    \caption{Classification results.\\ {\small (GPC -- German Polarity
        Clues \cite{Waltinger:10}, SWS -- SentiWS \cite{Remus:10}, ZPL
        -- Zurich Polarity Lexicon \cite{Clematide:10})}}
    \label{snt-lex:tbl:w2v}
  \end{center}
\end{table}

\subsection{Related Work}

Among the first who addressed the problem of the automatic generation
of sentiment lexica were \citet{Hatzivassi:97}.  In their work, the
authors relied on the hypothesis that coordinatively conjoined terms
often share the same semantic orientation while adversatively linked
words rather express opposite polarities.  To test this conjecture,
they automatically extracted all pairs of conjoined adjectives from
the Wall Street Journal (WSJ) corpus and represented those adjectives
as nodes in a graph.  The arcs weights of this graph were to show the
strength and direction by which two coordinatively conjoined terms
influenced each others' polarity.  To derive these weights,
\citeauthor{Hatzivassi:97} trained a log-linear regression model on
those pairs of terms in which both nodes belonged to a manually
labeled seed set of 1,336 adjectives and then let this model predict
the weights for the rest of the arcs.  In the final step, the
resulting graph was partitioned into two clusters -- that of positive
and that of negative terms -- which were subsequently used to enrich
the initial seed set.
%% This method, enhanced by the possibility of recognizing gradable
%% adjectives, was later used in the classification experiments of
%% \citet{Hatzivassi:00} to predict subjective and objective sentences in
%% the WSJ.

A different way of bootstrapping polarity terms from large text
corpora was proposed by \citet{Turney:03}.  Following
\citeauthor{Turney:02}'s original approach for classifying reviews
\citep{Turney:02}, the authors generated a polarity lexicon by first
taking a seed set of 14 a priory known polar adjectives (seven
negative and seven positive ones) and then expading this set with the
words that had the strongest pointwise mutual information associations
with the chosen seeds.  The PMI scores were computed as the log ratio
between the number of times a new word $w$ appeared with any of the
seed terms, divided by the total number of search hits for the
complete seed set.  As search hits, the authors considered the number
of relevant documents returned by the \texttt{AltaVista} search engine
for the given queries.  This system attained an accuracy of 82.84\% on
the General Inquirer Lexicon \citep{Stone:66} and correctly predicted
polar terms from the lexicon of \cite{Hatzivassi:97} in 87.13\% of the
cases. %% This method could also be further improved by using cosine
%% similarities between word vectors from an LSA matrix instead of
%% web-based PMIs.

Other notable works on corpus-based lexicon induction include
\citet{Kanayama:06}, who enhanced the method of
\citeauthor{Hatzivassi:97} by incorporating iter-sentential coherence
relations.  \citet{Kaji:07} also followed a corpus-based approach as
they mined opinionated sentences from HTML pages using structural and
linguistic clues and then extracted new polar terms from these
sentences, considering words having the highest PMI association scores
with the rest of the sentences as subjective.

One of the allegedly first attempts to derive a sentiment lexicon from
a lexical ontology instead of a corpus was made by \citet{Kim:04}.  In
their work, the authors expanded an initial seed set of 34 adjectives
and 44 verbs to a list of 18,192 presumably polar terms by iteratively
enriching the seed words with their synonyms and antonyms from the
\texttt{WordNet} database \citep{Miller:95}. In a similar way,
\citet{Godbole:07} enriched their topic-specific seed sets of polar
expressions by following the synonymy and antonymy links in
\texttt{WordNet} and exponentially decreasing the probability of a new
word being polar with the growing path length from a seed term.

%% Another interesting finding of this paper was surpassed by using
%% the cosine similarities between word vectors in an LSA matrix
%% instead of PMI scores from web crawlers.

%% More precisely, the social orientation PMI (SO-PMI) of a new word
%% $w$ was defined as: \begin{equation*} \text{SO-PMI}(w) =
%% \log_2\bigg(\frac{\prod_{p\in\mathcal{P}}\text{hits}(w\text{NEAR}p)%
%% \prod_{n\in\mathcal{N}}\text{hits}(w\text{NEAR}n)}%
%% {\prod_{p\in\mathcal{P}}\text{hits}(p)\prod_{n\in\mathcal{N}}\text{hits}(n)}\bigg) \end{equation*}
%% where $\mathcal{P}$ and $\mathcal{N}$ are the seed sets of positive
%% and negative adjectives respectively and
%% $\text{hits}(\cdot\text{NEAR}\cdot)$ is the number

%% The authors evaluated the accuracy of their model on the General
%% Inquirer lexicon \cite{Stone:66}.  Its final results (81.9\%) were
%% comparable to the figures obtained by \citet{Turney:03} in their
%% method (82.84\%) and significantly outperformed the precision of
%% the approach proposed by \citet{Kamps:04} (73.4 versus 70.8).

\citet{Kim:04} used a similar approach to determine the polarity of
new words (adjectives, nouns, and verbs), given a small seed set of
terms with known orientation.  More precisely, the likelihood of a
word $w$ belonging to the polarity class $c \in {\textrm{postive},
  \textrm{negative}, \textrm{neutral}}$ was computed as:
\begin{equation*}
  P(c|w) = \argmax_{c}P(c)\prod_{k=1}^{m}{P(f_k|c)^{\textrm{count}(f_k,\textrm{synset}(w))}},
\end{equation*}
where $f_k$ stands for the $k$-th feature (seed word) of sentiment
class $c$ which is also a member of the synonym set of $w$, and
$\textrm{count}(f_k,\textrm{synset}(w))$ means the total number of
occurrences of $f_k$ in the synonym set of $w$.  $P(c)$ is the number
of words in class $c$ divided by the total number of words considered.
The authors, however, did not attempt to generate a lexicon with this
method but only used it to classify new words encountered in reviews.

Another lexical sentiment resource (WordNet-Affect) was proposed by
\citet{Strapparava:04} who manually compiled a list of 1,903
subjective terms and projected these polarities to the respective
synononyms set in WordNet.  The resulting database included 2,874
synsets with a total of 4,787 words.

Another purely dictionary-based method was described by
\citet{Esuli:06b}.  The authors automatically assigned positive,
negative, and neutral polarity scores to synsets in \texttt{WordNet}
by iteratively expanding seed sets for each of these three polarity
classes using a committee of binary and multiclass classifiers.  At
each step of this process, these classifiers were trained on the seed
sets obtained from the previous runs and then used to predict whether
further synsets immediately accessible from the recently expanded
seeds would belong the same polarity class as their seed term.

\citet{Takamura:05} attempted to unite corpus- and dictionary-based
approaches into a single framework.  To this end, the authors adopted
the Ising spin model from the statistical mechanics and represented
all words found in \texttt{WordNet}, in the Wall Street Journal, and
the Brown corpus as nodes in a graph.  The edges of this graph
represented associativity links and were established between any two
words, if one of these word was a synonym, an antonym, or a hyponym of
the other one or appeared in its gloss or context.  Taking into
account the a priori known polarities of some of the terms, the Ising
model then tried to find an approximation of the most likely polarity
combination of all terms in the graph over all possible polarity
assignments.

\subsubsection{Domain-Specific Sentiment Lexica}

\citet{Chetviorkin:14} obtained a set of possible subjective terms
from English and Russian microblogs by using an ensemble of supervised
machine learning classifiers that had previously been trained on a
manually annotated corpus of movie reviews.  In order to determine the
prior polarity of the extracted terms, the authors first calculated
approximate polarity scores of the processed messages using general
polarity lexicons and then took these rough estimates as prior
polarity expectations of the candidate expressions.  The posterior
scores of these expressions were computed using the Ising spin model
in a similar way to the approach proposed by \citet{Takamura:05}.  The
resulting lexicon comprised 2,772 words for Russian and 2,786 lexical
items for English.

\subsubsection{Sentiment Lexica for German}

One of the first attempts to create a sentiment dictionary for German
was made by \citet{Remus:10}.  In their work, the authors
automatically translated the General Inquirer lexicon \citep{Stone:66}
into German.  In order to incorporate domain-specific knowledge into
this translated resource, they also collected a set of terms that were
strongly associated with positive or negative product reviews and
added this set to their translations.  Finally, the resulting lexicon
was enriched with inflection forms and frequent collocations of the
translated expressions.  The authors automatically estimated polarity
scores of their obtained 3,648 lexicon lemmas as the difference
between their positive and negative PMIs (i.e. PMI scores between the
collected terms and the pre-defined sets of positive or negative
expressions).

Similar work was also done by \citet{Waltinger:10} who compiled a list
of 10,141 subjective terms by first automatically translating the
Subjectivity Clues lexicon \citep{Wilson:05} and then manually
reassessing polarity scores of the translated items.  The author
expanded this list with the most frequent German synonyms of the
obtained terms and also added a set of negated opinionated phrases
(e.g. \emph{nicht schlecht} (\emph{not bad})).  The resulting
dictionary was used as a source of features for an automatic
classification of the polarity of Amazon reviews and showed superior
accuracy in comparison with other automatically translated resources.

A slightly different approach was taken by \citet{Clematide:10} who
manually annotated synsets from \emph{GermaNet} \cite{Hamp:97} with
their prior polarities and polarity strengths.  A subsequently
conducted study revealed, however, that the stock of polar adjectives
obtained in this way was insufficient for doing proper subjectivity
analysis of literary texts.  In order to overcome this problem, the
authors followed the idea of \citet{Hatzivassi:97} and extracted an
additional set of 918 adjectives which frequently co-occurred in
conjoined phrases with subjective terms from their dictionary.  After
manually inspecting each of the candidate terms, the authors added
this pruned set of new adjectives to their original lexicon.  The
total size of this final dictionary run up to 7,432 entries, 2,779 of
which were positive.

\subsection{Summary and Conclusions}

In this section, we presented the first attempt of a practical
evaluation of our corpus.  In doing so, we addressed the task of
automatic prediction of polar terms (emotional expressions) with the
help of sentiment dictionaries.  To obtain a rough baseline estimate,
we first evaluated the quality of the existing sentiment lists for
German: German Polarity Clues~\cite{Waltinger:10},
SentiWS~\cite{Remus:10}, and Zurich Polarity List~\cite{Clematide:10}.
We showed that \ldots achieved the best quality, reaching an average
$F1$-score of \ldots on recognizing positive expressions and \ldots on
predicting negative polar terms.

In the next step, we analyzed whether the methods that were used for
creating the original English resources whose translations formed the
basis of the German lexica could yield better results than the
manually revised tranlated lists when applied to German data directly.

The first family of lexicon generation methods that we looked into
used Ontology information about lexical items: words' definitions,
examples, and lexical links, to find subjectively connotated terms.
In particular, we reimplemented the original method of
\citet{Esuli:06b} and applied it to the German lexical database --
GermaNet \cite{Hamp:97}.  The results of this approach turned out to
be relatively low, only achievening \ldots for positive expressions
and \ldots for negative polar terms.  These rather low scores can be
explained by the scarceness of GermaNet definitions on the one hand
and the unconventional vocabulary used on Twitter.

Another popular approach to an unsupervised induction of sentiment
lexica relies on the cooccurrence information about the words taken
directly from corpus.  One of the most popular methods from this
category is the Ising spin model adopted from the statistical
mechanics which interprets words as magnetic spins in a crystal grid
and tries to derive the most probable orientation of these spins in a
magnetic field.  This model was first applied to the needs of
computational linguistics by \citet{Takamura:05}, who induced a
sentiment lexicon for English using \ldots corpus data.  We have
reimplemented this approach in our program suite and applied to the
German Twitter snapshop of \citet{Scheffler:14}.  The results of this
approach are shown in Table~\ref{snt-lex:tbl:ispn-res}.

A different way of incorporating corpus data is to encode the
cooccurrence statistics directly into word information, representing
the latter as vectors.  We explored this direction in the final part
of this section, first obtaining word2vec embeddings for tokens from
the aforementioned snapshot and then applying clustering algorithms to
these representations.

Our results show that \ldots.

\newpage
