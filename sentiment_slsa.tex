\chapter{Sentence-Level Sentiment Analysis}\label{sec:snt:slsa}

\section{Sentence-Level Sentiment Analysis with Deep Neural Networks}

\section{Sentence-Level Sentiment Analysis with Deep Neural Networks}

\section{Evaluation}

\section{Related Work}

Wiebe 2002, Riloff 2003

\citet{Kim:04} experimented with three different methods of
determining the overall polarity of a sentence:
\begin{inparaenum}[(i)]
  \item multiplying the signs of polar terms found in sentence,
  \item taking their sum, and
  \item taking the geometric mean of polarity scores;
\end{inparaenum}
finding that the first and last options worked best for the Document
Undestanding Corpus.\footnote{\url{http://duc.nist.gov/}}

One of the first attempts to analyze message-level sentiments on
Twitter was made by \citet{Go:09}.  For their experiments, the authors
collected a set of 1,600,000 tweets containing smileys.  Based on
these emoticons, they automatically derived polarity classes for these
messages (positive or negative) and used them to train a Na\"{\i}ve
Bayes, MaxEnt, and SVM classifier.  The best $F$-score for this
two-class classification problem could be achieved by the last system
and run up to 82.2\%.

Similar work was also done by \citet{Pak:10}, who used the Na\"{\i}ve
Bayes approach to differentiate between neutral, positive, and
negative microblogs; and \citet{Barbosa:10}, who gathered a collection
of 200,000 tweets, subsequently analyzing them with three publicly
available sentiment web-services and training an SVM classifier on the
results of these services.  In a similar way, \citet{Agarwal:11}
compared a simple unigram-based SVM approach with two other
full-fledged systems, one which relied on a rich set of manually
defined features, and another used partial tree
kernels~\cite{Moschitti:06}.  The authors evaluated these methods on a
commercially acquired corpus of 8,753 foreign-language tweets, which
were automatically translated into English, finding that a combination
of these methods worked best for both two- and three-way prediction
tasks.

The state-of-the-art results for message level polarity prediction on
tweets were established by~\citet{Mohammad:13}, whose system (a
supervised SVM classifier) used a rich set of various features
including word and character n-grams, PoS statistics, Brown
clusters~\cite{Brown:92}, etc., and also strongly benefitted from
automatic corpus-based polarity lists---Sentiment~140 and NRC
Hashtag~\cite{Mohammad:12,Kiritchenko:14}.  This approach ranked first
at the SemEval competition~2013~\cite{Nakov:13} and anchieved the
fourth place on the rerun of this task one year
later~\cite{Rosenthal:14}, being outperformed by the supervised
logistic regression approach of~\citet{Miura:14}, who used a heavy
preprocessing of the data and a special balancing scheme for
underrepresented classes.  Later on, these results were further
improved by the apporaches of~\citet{Hagen:15} and \citet{Deriu:16},
which both relied on ensembles of multiple independent classifiers.

\citet{Kobayashi:07}
Wiebe, Bruce, \& O'Hara 1999
Hatzivassiloglou \& Wiebe 2000
Wiebe 2000;
Wiebe et al. 2002
Yu \& Hatzivassiloglou 2003


\citet{Yessenalina:11}

A real breakthrough in the use of deep neural networks for the
sentence-level sentiment analysis happened with the pioneering work
of~\citet{Socher:11}, who first introduced a recursive autoencoder
(RAE).  In this system, the authors obtained a fixed-width vector
representation for complex phrases $\vec{v}$ by recursively merging
the vectors of adjacent tokens (say $\vec{w}_1$ and $\vec{w}_2$),
first multiplying these vectors with a compositional matrix $W$ and
then applying a non-linear function ($softmax$) to the resulting
product:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}\right)
\end{align*}
Using a max-margin classifier on top of the resulting phrase
representation, \citet{Socher:11} could improve the state-of-the-art
results on predicting the sentence-level polarity of user's blog
posts~\cite{Potts:10} and also outperformed the system
of~\citet{Nasukawa:03} on the MPQA data set~\cite{Wiebe:05}.

Later on, \citet{Socher:12} further improved these scores with the
help of a recursive matrix-vectors space model (RMVSM), in which each
word was associated with a 2-tuple of a vector and matrix---e.g.,
$(\vec{w}_1, W_1)$ and $(\vec{w}_2, W_2)$---and the compositionality
function was redefined as follows:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  W_2\cdot\vec{w}_1\\
  W_1\cdot\vec{w}_2
  \end{bmatrix}\right)
\end{align*}

\citet{Wang:15}

\section{Summary and Conclusions}\label{slsa:subsec:conclusions}
