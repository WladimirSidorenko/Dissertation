\documentclass[11pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Packages
\usepackage{amsmath}
\usepackage{breakcites}
\usepackage[authoryear]{natbib}
\usepackage{paralist}
\usepackage{titling}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Commands
\newcommand{\ienocomma}{\textit{i.e.}}
\newcommand{\ie}{\ienocomma,}
\newcommand{\eg}{\textit{e.g.},}
\newcommand{\F}[0]{$F_1$}
\newcommand{\markable}[1]{\texttt{#1}}
\newcommand{\attribute}[1]{\emph{\texttt{#1}}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\renewcommand{\cite}{\citep}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Lengths
\setlength{\droptitle}{-7em}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Variables
\author{Uladzimir Sidarenka}
\title{  {\large Summary of the Dissertation}\\[0.5em]
  {\Large ``Sentiment Analysis of German Twitter''}}
\date{\vspace{-3ex}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Document
\begin{document}
\maketitle

Online communication services have not only revolutionized our
everyday life (allowing us to stay in touch with friends, exchange
ideas with colleagues, and get latest news from celebrities and
politicians without leaving the desktop) but also opened the doors to
numerous new dangers, such as cyber-attacks, online bullying, and
deliberate manipulation of public opinion---dangers that can hardly be
averted manually due to the immense amount of data exchanged on online
platforms.  Therefore, if we want to protect our society from these
looming threats, we urgently need more robust, higher-quality natural
language processing (NLP) applications, which can recognize such
menaces automatically, by analyzing uncensored texts.  Unfortunately,
most NLP programs today have been created for standard language, as we
know it from newspapers, or, in the best case, adapted to the
specifics of English social media.

My dissertation reduces the existing deficit by entering the new
frontier of German online communication and addressing one of its most
prolific forms---users' conversations on Twitter.  In particular, it
explores the ways and means by how people express their opinions on
this service, examines current approaches to automatic mining of these
feelings, and proposes new sentiment analysis techniques, which
outperform state-of-the-art methods.

I perform these tasks on three different linguistic levels:
\begin{itemize}
  \item\emph{subsentential}, in which I try to predict the polarity of
    single words, and automatically determine the main components of
    an opinion (its target, holder, and the actual evaluative
    expression) within a sentence;

  \item\emph{sentential}, where I consider the whole microblog as a
    single statement and classify its overall semantic orientation;

  \item and, finally, \emph{suprasentential}, in which I try to
    improve on message-level sentiment classification by inferring the
    overall polarity of a tweet from the semantic orientation of its
    single sentences, taking into account their semantic and pragmatic
    links (discourse relations).
\end{itemize}
At each of these levels, I evaluate the most prominent classes of
existing systems, seek to outperform their scores with my own
solutions, and estimate the effect of different hyper-parameters and
input settings on the net results of classification.

\section{Research Questions}

By doing so, I hope to find answers to the following questions:
\begin{itemize}
\item Can we apply opinion-mining methods devised for standard English
  to German Twitter?
\item Which groups of approaches are best suited for which sentiment
  analysis tasks?
\item How much do word- and discourse-level analyses affect
  message-level sentiment classification?
\item Does text normalization help analyze sentiments?
\item Can we do better than existing systems?
\end{itemize}

\section{Outline of this Work}

To get these answers, I proceed in the following way:

\subsection{Chapter~1: Introduction to Sentiment Analysis}

In Chapter~1, I give a brief introduction to the main goals and
definitions of sentiment analysis, and make a digression into the
history of this field, tracing its roots back to the ancient
philosophy, and following the course if its development through the
main milestones in psychology, narratology, and linguistics.  In the
final sections, I elaborate on the recent advances in automatic
opinion mining of social media, and pay special attention to the
current state of the art in sentiment analysis of Twitter.

\subsection{Chapter~2: Sentiment Corpus}

Afterwards, in Chapter~2, I introduce the Potsdam Twitter Sentiment
(PotTS) corpus---a collection of $7,992$ German microblogs pertaining
to the German federal elections, papal conclave, general political
discussions, and casual everyday conversations.

I obtained messages for the first three groups by tracking the public
Twitter API between March and September 2013, using extensive lists of
hand-picked keywords, which, in my opinion, best described these
topics.  As casual everyday posts, I considered microblogs from the
German Twitter Snapshot~\cite{Scheffler:14}, which comprises
$\approx97\%$ of all German tweets (17M messages) posted in April
2013.

To ensure a representative number of subjective opinions in the
resulting corpus, I grouped all downloaded microblogs pertaining to
the same topic into three formal classes:
\begin{inparaenum}[(i)]
  \item tweets that contained a polar term from the sentiment lexicon
    SentiWS~\cite{Remus:10};
  \item messages that had an emoticon; and, finally,
  \item all remaining microblogs.
\end{inparaenum}
Afterwards, I sampled an equal number of tweets (666) from each of the
four topics, from each of these three formal categories.

In the next step, I defined an annotation scheme---a list of elements
that had to be annotated by human experts.  These were:
\begin{itemize}
  \item\emph{sentiment}s, which were specified as polar subjective
    evaluative opinions about people, entities, or events;

  \item\emph{target}s, which represented entities or events being
    evaluated;

  \item\emph{source}s, which denoted the immediate author(s) or
    holder(s) of an opinion;

  \item\emph{polar term}s, which were words or idioms that had a
    distinguishable evaluative lexical meaning;

  \item\emph{intensifier}s, which represented elements that increased
    the expressivity and subjective sense of polar terms;

  \item\emph{diminisher}s, which vice versa reduced the strength of a
    polar expression;

  \item and, finally, \emph{negation}s, which completely reversed the
    semantic orientation of a polar item.
\end{itemize}

The annotation process was performed in three steps: first, two
linguists labeled one half of the data after only minimal training;
then, I automatically highlighted their divergent analyses and asked
them to resolve these differences; finally, the annotators continued
with the analysis of the remaining files.

To estimate the inter-rater reliability, I introduce two modified
versions of the popular $\kappa$-metric~\cite{Cohen:60}---binary and
proportional kappa---which differ in the way how they treat multiple
annotations of the same tokens (\eg{} if one phrase was labeled as
\emph{sentiment} several times by one annotator) and how they deal
with partial matches of different annotations (\eg{} if two
\emph{sentiment}s labeled by different linguists agree on some, but
not all of their tokens).  Using these measures, I estimate the
inter-annotator agreement (IAA) of the experts at different stages of
their work.  This study shows that, initially, both assistants could
hardly agree on the mere notion of targeted opinions, but their
disagreements could be resolved with the help of the adjudication
procedure, which was applied in step two.  Despite a small drop of the
IAA scores in the final stage, all $\kappa$-values still remained at
the level of at least moderate reliability.

Finally, I demonstrate that the initial selection criteria (topics and
formal classes) had a strong impact on the number and agreement of
annotated sentiments and polar terms, with tweets about federal
elections and messages without prefiltered topics being the most
prolific sources of these elements.

\subsection{Chapter~3: Sentiment Lexicons}

In Chapter~3, I turn to the first subsentential sentiment analysis
task, in which I try to predict the polarity (positive, negative, or
neutral) of single words in the text.

For this purpose, I first evaluate existing manually-curated German
sentiment lexicons---SentiWS \cite{Remus:10}, German Polarity Clues
\cite{Waltinger:10}, and the Zurich Polarity List
\cite{Clematide:10}---by directly comparing their entries with the
annotated polar terms in the PotTS corpus.  This evaluation shows that
the last resource (Zurich Polarity List) outperforms the first two
competitors in terms of both macro- and micro-averaged \F{}-scores,
reaching 0.589 and 0.955 \F{} respectively.

Afterwards, I compare three major groups of automatic
lexicon-generation methods: dictionary-, corpus-, and
word-embedding--based ones.  The first of these groups induces
polarity lists from a manually annotated lexical taxonomy
\cite[\textsc{GermaNet}; ][]{Hamp:97}---a graph whose nodes represent
sets of synonyms (synsets), which are connected to each other by
semantic relations (meronymy-holonymy, hypernymy-hyponymy, etc.).
Corpus-based algorithms, on the contrary, infer sentiment lexicons
from collocation statistics computed on raw, unlabeled data, by taking
a set of terms with known semantic orientation (seed terms) and
looking for other words that frequently co-occur with these seeds.  In
the same vein, word-embedding--based methods generate new polarity
lists from words whose vector representations (which are automatically
learned by a classifier that predicts the occurrence of other words in
the nearby context) are most similar to the embeddings of seed terms.

Besides evaluating existing methods, I propose four new
word-embedding--based approaches, which induce sentiment lexicons by
clustering word vectors using
\begin{inparaenum}[(i)]
\item the method of $k$-nearest neighbors,
\item nearest centroids,
\item principal component analysis, and
\item my own linear projection algorithm.
\end{inparaenum}
In the last of these systems, I determine a line that maximizes the
distance between the projections of seed embeddings with opposite
semantic orientations, and then project the vectors of all remaining
terms on this line, considering the distance of these projections to
the origin as the respective polarity scores.  My approach surpasses
all existing alternatives in terms of the micro-averaged \F{}-score,
reaching 0.963 micro-\F{}, and yields a very competitive macro-\F{}
(0.462), outperforming all corpus- and word-embedding--based solutions
except for the method of $k$-nearest neighbors.

As confirmed by the evaluation, the linear-projection and $k$-NN
algorithms are the only viable lexicon-generation approaches that
dispense with any manually annotated linguistic resources and whose
results are at least close to the scores of manually-curated and
dictionary-based sentiment lexicons.  Furthermore, additional
experiments demonstrate that the quality of almost all automatically
generated polarity lists can be further improved by using larger and
less ambiguous sets of initial seed terms.

\subsection{Chapter~4: Aspect-Based Sentiment Analysis}

Another subsentential sentiment analysis task---automatic prediction
of targets, holders, and text spans of opinions---is addressed in
Chapter~4.  Following the current state of the art, I consider this
objective as a sequence-labeling (SL) problem and approach it with two
most popular SL techniques: conditional random fields~\cite[CRFs;
][]{Lafferty:01} and recurrent neural networks \cite[RNNs;
][]{Hochreiter:97}.  For the former method, I devise an extensive set
of features, which reflect different lexical, morphological, and
syntactic traits of the input.  Using these attributes, CRFs achieve
0.287 macro-\F{} for the three classes (sentiments, sources, and
targets) on the PotTS test set.  An additional ablation test shows
that complex features, which simultaneously reflect lexical and
syntactic properties, are most useful for predicting sentiment spans;
pure lexical traits are most helpful when classifying sources; and
syntactic attributes work best when predicting the target of an
opinion.

Unfortunately, recurrent neural networks yield lower results than
CRFs, with the best scores (0.27 macro-\F{}) achieved by the
long-short term memory network~\cite[LSTM; ][]{Hochreiter:97}.  Their
accuracy, however, can be improved by initializing word embeddings
with word2vec vectors~\cite{Mikolov:13} and optimizing these values
along with other network parameters during the training.  In addition
to this, I propose a new fallback strategy based on the method of the
ordinary least squares, which allows to approximate trained
task-specific vector representations of words that have word2vec
embeddings, but do not occur in the sentiment training data.  This
strategy improves the macro-averaged scores of RNN systems by up to
3.2\%.

In the concluding sections, I investigate whether using alternative
structures of computational graphs for CRFs and RNNs and providing
unnormalized text to these methods as input can improve their results.
As it turns out, CRFs benefit from increasing the order of their graph
dependencies, and yield better scores when classifying spans of the
text (hypernodes) rather than individual tokens; RNNs, on the
contrary, achieve their best scores with the standard first-order
linear-chain topology.  Both approaches, however, strongly profit from
text normalization, showing a quality increase by almost 4\% when this
preprocessing step is active.

%% Finally, I compare two different approaches to the definition of
%% \emph{sentiment} spans: \emph{broad} and \emph{narrow}, in which I
%% assign the \emph{sentiment} tag either to minimal complete
%% syntactic unit (typically noun or verb phrase) that simultaneously
%% comprise both the target of an evaluation and the evaluative phrase
%% (\emph{broad}), or to the polar term only.  A comparison of both
%% schemes shows that the former approach leads to better
%% classification scores for \emph{source}s and \emph{target}s,
%% whereas the second take is more amenable to the prediction of
%% \emph{sentiment}s.

\subsection{Chapter~5: Message-Level Sentiment Analysis}

Afterwards, in Chapter~5, I turn to a sentential task---message-level
sentiment classification, whose goal is to predict the polarity
(positive, negative, or neutral) of a single tweet.  In this part, I
again compare the most prominent groups of methods: dictionary-,
machine-learning--, and deep-learning--based ones; and evaluate them
on two different German corpora: my own PotTS dataset and the SB10k
Twitter corpus~\cite{Cieliebak:17}.

This comparison shows that English dictionary-based methods are least
suitable for being applied to German data due to the language
specifics of their manually defined rules.  Machine-learning systems,
however, are mostly language-independent and can be equally well
applied to English and German microblogs.  A more diverse situation is
observed for deep-learning classifiers, which perform relatively poor
when trained with randomly initialized word vectors, but surpass many
other methods when used with word2vec embeddings, especially in
combination with the least-square fallback.

Apart from this, I also propose my own deep-learning--based system---a
bidirectional recurrent neural network with lexicon-based attention
(LBA), which, in addition to the standard LSTM recurrence and
traditional attention mechanism~\cite{Bahdanau:14}, assigns greater
importance to terms that have higher polarity scores in the sentiment
lexicon, as well as their syntactic dependents, which might serve as
contextual modifiers of these terms.  My algorithm attains 0.69
macro-\F{} and 0.73 micro-\F{} on the PotTS corpus, outperforming all
other competitors on this dataset, and comes very close to the best
overall scores on the SB10k test data.

An extensive evaluation of all methods once again confirms the utility
of text normalization and the linear-projection lexicon, which yields
better results than all other polarity lists on the PotTS corpus when
used in combination with the LBA classifier, reaching 0.69~macro-\F
and 0.73~micro-\F{} on PotTS and attaining 0.55~macro-\F{} and
0.73~micro-\F{} on the SB10k data.  An interesting finding, however,
is that distant supervision (\ie{} training on tweets whose labels are
automatically inferred from emoticons) does not necessarily improve
the results of message-level sentiment classification.

\subsection{Chapter~6: Discourse-Aware Sentiment Analysis}

Finally, in Chapter~6, I improve the accuracy of my proposed LBA
classifier by making it aware of the discourse structure of tweets.
For this purpose, I automatically split all microblogs from the PotTS
and SB10k corpora into elementary discourse units (EDUs) and construct
a rhetorical structure tree of these segments \cite{Mann:88} with the
English discourse parser DPLP \cite{Ji:14}, which has been retrained
on the Potsdam Commentary Corpus \cite{Stede:14}.

Afterwards, I let the LBA system predict the polarity scores of single
EDUs, and infer the overall polarity of a tweet from the polarity of
its units.  In particular, I first evaluate three existing
discourse-aware sentiment methods (the system of
\citeauthor{Wang:13}~[\citeyear{Wang:13}] and two approaches
introduced by~\citeauthor{Bhatia:15}~[\citeyear{Bhatia:15}]) on all
microblogs that have more than one discourse segment, and then improve
on the results of these approaches with three new alternative
solutions: latent and latent-marginalized conditional random fields
and a recursive Dirichlet process.

In the final steps, I estimate the effect of different base sentiment
classifiers (\ie{} downstream systems that are used to predict the
polarity scores of single EDUs) and check the utility of different
discourse relations, which are taken into account by discourse-aware
systems.  These experiments show that my proposed methods yield
consistently good results with almost all settings, outperforming
their competitors in the prevailing majority of the cases.

\section{Conclusions}

Based on these experiments, I come to the following conclusions
regarding the initially posed questions:

\begin{itemize}
  \item\textbf{Can we apply opinion mining methods devised for
    standard English to German Twitter?}

    Yes, we can, but the success of these approaches might
    significantly vary depending on the task, the size and the
    reliability of the training data, as well as the evaluation metric
    in use.

  \item\textbf{Which groups of approaches are best suited for which
    sentiment tasks?}

    Based on my evaluation, I answer this question as follows:
    \begin{itemize}
      \item\emph{Sentiment lexicon generation} is more amenable to
        dictionary-based solutions and word-embedding--based systems;

      \item With a limited amount of training data, \emph{aspect-based
        sentiment analysis} can be better addressed with probabilistic
        graphical models, such as conditional random fields;

      \item \emph{Message-level sentiment analysis}, however, can be
        efficiently tackled with machine- and deep-learning--based
        algorithms;

      \item But probabilistic graphical models strike back at
        \emph{discourse-aware sentiment prediction}.
    \end{itemize}

  \item\textbf{How much do word- and discourse-level analyses affect
    message-level sentiment classification?}

    As shown by the experiments in Chapters~5 and 6, the
    macro-averaged \F{}-scores of my proposed lexicon-based attention
    classifier vary by up to 14\% (from 0.44 to 0.58 on SB10k)
    depending on the lexicon used by this system.  At the same,
    discourse enhancements only improved the results of LBA by at most
    1.5\% percent (from 0.557 to 0.572 on SB10k).

  \item\textbf{Does text normalization help analyze sentiments?}

    Yes, it definitely does.  Text normalization significantly
    improves the quality of aspect-based and message-level sentiment
    analyses, boosting the results on the former task by up to 4\% and
    increasing the macro-averaged \F{}-measure on the latter objective
    by up to 25\%.

  \item\textbf{Can we do better than existing systems?}

    Yes, we can with the proposed linear-projection and lexicon-based
    attention classifiers, latent and latent-marginalized CRFs, as
    well as the recursive Dirichlet algorithm.
\end{itemize}

% Bibliography
\bibliographystyle{apalike}
\bibliography{../bibliography}

\end{document}
