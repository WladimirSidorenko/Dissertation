\chapter*{Afterword}
\addcontentsline{toc}{chapter}{Afterword}

It is hard to believe, but we have also reached the home stretch of
our \thepage-page long journey and, preparing the final spurt, it is
probably the best time for us to remember the main milestones that we
passed along the way:
\begin{itemize}
\item As you might remember, we started off with by summarizing the
  history of sentiment analysis, going back to its very origins in
  ancient Greece and tracing the course of its development to the
  present day;
\item Afterwords, to see what the current state of the art in opinion
  mining would yield on German Twitter, we have created a corpus of
  German microblogs, collecting a set of $\approx8,000$ messages which
  pertained to four different topics (federal elections, papal
  conclave, general political discussions, and casual small talk).
  Moreover, to ensure a good recall of opinionated statements in the
  resulting dataset we preselected the messages for our corpus using
  three formal criteria (presence of a polar term from a sentiment
  lexicon, presence of a smiley, all remaining tweets), finding that
  both criteria (topic and polar elements) had a significant impact on
  both distribution of sentiments and reliability of their annotation;
\item Then, at the first checkpoint, we compared exisiting German
  sentiment lexicons, which were translated from English resources and
  then revised by human experts, with lexicons that were from scratch
  with the help of existing dictionary--, corpus--, and
  word-embedding--based methods, finding that translated lexicons had
  a better quality than automatically induced polarity lists.  In the
  latter group, we found dictionary-based systems performing best
  among all automatic methods.  We could, however, improve on the
  automatic results with our proposed linear projection solution, in
  which we first found a line that maximized the mutual distance
  between the projections of seed terms with opposite semantic
  orientations and then projected the vectors of remaining words on
  that line, considering the distance to the mean as the respective
  polarity score.

  three families of
  lexicon induction methods (dictionary--, corpus--, and
  word-embedding--based), finding that manually curated translations
  of English resources achieved best results on our corpus, followed
  by dictionary-based systems, which could surpass corpus-based
  methods;
\item We could, however, outperform existing automatic lexicon methods
  with our proposed linear-projection solution, in which we first
  found a line that maximized mutual distance between the projections
  of seed vetors with opposite semantic orientations and and then
  projected the vexctors of remaining terms on this line, considering
  their distance to the mean as the respective polarity score;
\item In Chapter \ref{chap:fgsa}, we turned our attention to the task
  of aspect-level sentiment analysis, in which we tried to predict the
  spans of sentiments, targets, and holders of opinions using
  conditional random fields and recurrent neural networks;
\item Then, we addressed one of the most popular ;
\end{itemize}
