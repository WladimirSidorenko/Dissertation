{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"supertitle\">\n",
    "Presentation of the Ph.D. Thesis\n",
    "</div>\n",
    "\n",
    "<div class=\"title\">\n",
    "Sentiment Analysis of German Twitter\n",
    "</div>\n",
    "\n",
    "<div class=\"author\">\n",
    "Uladzimir Sidarenka<br/>\n",
    "(Wladimir Sidorenko)\n",
    "</div>\n",
    "\n",
    "<div class=\"institution\">\n",
    "University of Potsdam\n",
    "</div>\n",
    "\n",
    "<div class=\"date\">\n",
    "July 12, 2019\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 class=\"chapter\">Chapter I: Introduction</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<strong><em>Sentiment Analysis</em></strong> is a field of knowledge that deals with the analysis of emotions, sentiments, evaluations, and attitudes (Liu, 2012).\n",
    "\n",
    "<img src=\"img/may_tears.jpg\"/>\n",
    "<img src=\"img/trump_anger.jpg\"/>\n",
    "<img src=\"img/erdogan.jpg\"/>\n",
    "<img src=\"img/merkel_laugh.jpg\"/>\n",
    "<img src=\"img/macron_putin.jpg\"/>\n",
    "<img src=\"img/medvedev_rain.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Main difficulties of sentiment analysis:\n",
    "* There can be different opinions (good, bad, mixed);\n",
    "* That can be expressed by different people (children and adults, common people and celebrities);\n",
    "* On different topics (family, work, politics);\n",
    "* In different languages (English, Chinese, German);\n",
    "* In different communication channels (newspapers, emails, social media);\n",
    "* At different language levels (via words, sentences [utterances], or a complete discourse)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<strong><em>Twitter</em></strong> is an American online news and social networking service on which users post and interact with short messages (up to 140 [280] characters) known as <em>\"tweets\"</em>. Registered users can post, like, and retweet tweets, whereas unregistered users can only read them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**TODO:** German Sentiment + Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Research Questions\n",
    "\n",
    "* Can we apply opinion mining methods devised for standard English to German Twitter?\n",
    "\n",
    "* Which groups of approaches are best suited for which sentiment tasks?\n",
    "\n",
    "* By how much do word- and discourse-level analyses affect message-level sentiment classification?\n",
    "\n",
    "* Does text normalization help analyze sentiments?\n",
    "\n",
    "* Can we do better than existing methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to answer these questions, we need:\n",
    "\n",
    "* data;\n",
    "* baselines;\n",
    "* solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 class=\"chapter\">Chapter II: Data</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenges\n",
    "\n",
    "* Get as many sentiments as possible;\n",
    "\n",
    "* Still try keeping the bias of the data sample low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Selection Criteria\n",
    "\n",
    "* Content-Related Criteria (based on the information contained in the tweet);\n",
    "\n",
    "* Formal Criteria (based on the form of tweet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Content-Related Criteria\n",
    "\n",
    "As criteria that could help us get more opinions, we considered topic and form of the\n",
    "tweets, assuming that some subjects, especially social or political issues, would be more\n",
    "amenable to subjective statements. Because we started creating the corpus in spring 2013,\n",
    "obvious choices of opinion-rich topics to us were the papal conclave, which took place in\n",
    "March of that year, and the German federal elections, which were held in autumn. Since\n",
    "both of these events implied some form of voting, we decided to counterbalance the election\n",
    "specifics by including general political discussions as the third subject in our dataset. Finally,\n",
    "to obey the second principle, i.e., to keep the corpus bias low, we sampled the rest of the\n",
    "data from casual everyday conversations without any prefiltering.\n",
    "\n",
    "* Federal Elections 2013;\n",
    "* Papal Conclave 2013;\n",
    "* General Political Discussions;\n",
    "* Everyday Conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Formal Criteria\n",
    "\n",
    "In the next step, I divided all tweets of the same topic into three groups based on the\n",
    "following formal criteria:\n",
    "* I put all messages that contained at least one polar term from the sentiment lexicon\n",
    "of Remus et al. (2010) into the first group;\n",
    "* Microblogs that did not satisfy the first condition, but had at least one exclamation\n",
    "mark or emoticon were allocated to the second group;\n",
    "* All remaining microblogs were assigned to the third category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Annotation Scheme\n",
    "\n",
    "The annotation scheme for the corpus includes the following elements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **sentiments**, which were defined as *polar subjective evaluative opinions about people, entities, or events*, e.g.:\n",
    "<pre>\n",
    "   <sentiment>Mir hat die letzte Folge von Games of Thrones gar nicht gefallen.</sentiment>\n",
    "   <translation><sentiment>I absolutely didn't like the last episode of Game of Thrones.</sentiment></translation>\n",
    "</pre>\n",
    "With this element, we associated the following types of attributes, which also had to be specified by the experts:\n",
    "  * ***polarity*** with possible values *positive*, *negative*, and *comparative*;\n",
    "  * ***intensity*** with possible values *weak*, *medium*, and *strong*;\n",
    "  * as well as the boolean attribute ****sarcasm***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **targets**, which are entities or events evaluated by opinions.\n",
    "<pre>\n",
    "   [Mir hat [die letzte Folge von Games of Thrones]_target gar nicht gefallen.]_sentiment\n",
    "   [I absolutely didn't like [the last episode of Game of Thrones]_target.]_sentiment\n",
    "</pre>\n",
    "This tag has the following attributes:\n",
    "  * the boolean property ***preferred***, which, however, only had to be used in comparative opinions;\n",
    "  * and two link-attributes ***anaph-ref***, which connects an a pronomial target to its antecedent, and ***sentiment-ref***, which links the target to its respective opinion if the target is located at the intersection of two sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **sources**, which mark the immediate authors of opinions.\n",
    "<pre>\n",
    "   [[Mir]_source hat [die letzte Folge von Games of Thrones]_target gar nicht gefallen.]_sentiment\n",
    "   [[I]_source absolutely didn't like [the last episode of Game of Thrones]_target.] sentiment\n",
    "</pre>\n",
    "and have only one possible attributes, ***sentiment-ref***, defined in the same way as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the three core opinion-level elements (sentiments, sources, and targets), I also defined a set of word-level items that had to be labeled by the annotators and were supposed to ease automatic sentiment analysis.  These were:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ***polar terms***, which are defined as words or idioms that have a distinguishable evaluative\n",
    "lexical meaning (e.g., \"ekelhaft\" [*disgusting*], \"lieben\" [*to love*], \"Held\" [*hero*], \"wie die Pest meiden\" [*to\n",
    "avoid like the pest*]).\n",
    "<pre>\n",
    "   [[Mir]_source hat [die letzte Folge von Games of Thrones]_target gar nicht [gefallen]_polar_term.]_sentiment\n",
    "   [[I]_source absolutely didn't [like]_polar_term [the last episode of Game of Thrones]_target.] sentiment\n",
    "</pre>\n",
    "The attributes of these elements are:\n",
    "  * polarity (*positive*, *negative*);\n",
    "  * intensity (*weak*, *medium*, *strong*);\n",
    "  * sarcasm (*true*, *false*);\n",
    "  * subjective-fact (*true*, *false*);\n",
    "  * uncertain (*true*, *false*);\n",
    "  * sentiment-ref."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ***intensifiers*** and ***diminishers*** are elements that increase (reduce) the expressivity and subjective sense of a polar term (e.g., \"sehr\" [*very*], \"super\" [*super*], \"stark\" [*strongly*], \"kaum\" [*hardly*]).\n",
    "The attributes of these elements are:\n",
    "  * degree (*medium*, *strong*);\n",
    "  * polar-term-ref."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* and, finally, ***negations***, which are grammatical or lexical means that reverse the semantic orientation of a polar term. (e.g., \"nicht\" [*not*]).\n",
    "<pre>\n",
    "   [[Mir]_source hat [die letzte Folge von Games of Thrones]_target gar [nicht]_negation [gefallen]_polar_term.]_sentiment\n",
    "   [[I]_source absolutely did[n't]_negation [like]_polar_term [the last episode of Game of Thrones]_target.] sentiment\n",
    "</pre>\n",
    "The only attribute of this element is:\n",
    "  * polar-term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "For estimating the inter-annotator agreement, I adopted the popular κ metric [Cohen, 1960]. Following the standard practice, I computed this term as:\n",
    "\n",
    "$$\\kappa = \\frac{p_o - p_c}{1 - p_c},$$\n",
    "\n",
    "where $p_o$ denotes the observed agreement and $p_c$ is the agreement by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **observed agreement** is normally estimated as:\n",
    "$$p_o = \\frac{T − A_1 + M_1 − A_2 + M_2}{T},$$\n",
    "where $T$ means the total number of tokens; $A_1$ and $A_2$ are the numbers of tokens annotated by the first and second annotator respectively; and $M_1$ and $M_2$ denote the numbers of tokens with matching annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **agreement by chance** is computed as:\n",
    "$$p_c = c_1\\times c_2 + (1 - c_1)\\times(1 - c_2),$$\n",
    "where c 1 and c 2 are the proportions of tokens annotated with the given class in the first and\n",
    "second annotation respectively, i.e., $c_1 = \\frac{A_1}{T}$ and $c_2 = \\frac{A_2}{T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p class=\"annotator1\">Annotation 1:</p>\n",
    "[Mein Vater hasst [dieses schöne Buch] sentiment .] sentiment\n",
    "\n",
    "[My father hates [this nice book] sentiment .] sentiment\n",
    "\n",
    "<p class=\"annotator2\">Annotation 2:</p>\n",
    "Mein [Vater hasst [dieses schöne Buch] sentiment .]_sentiment\n",
    "\n",
    "My [father hates [this nice book] sentiment .]_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary $\\kappa$**\n",
    "\n",
    "$A_1 = 10$\n",
    "\n",
    "$A_2 = 10$\n",
    "\n",
    "$M_1 = A_1$\n",
    "\n",
    "$M_2 = A_2$\n",
    "\n",
    "$\\kappa = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proportional $\\kappa$**\n",
    "\n",
    "$A_1 = 7$\n",
    "\n",
    "$A_2 = 6$\n",
    "\n",
    "$M_1 = 6$\n",
    "\n",
    "$M_2 = 6$\n",
    "\n",
    "$\\kappa = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stage I: Initial Annotation\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "      <tr>\n",
    "    <td>Element</td>\n",
    "    <td colspan=5>Binary $\\kappa$</td>\n",
    "    <td colspan=5>Proportional $\\kappa$</td>\n",
    "          </tr>\n",
    "<tr>\n",
    "<td></td>\n",
    "<td>$M_1$ </td>\n",
    "<td>$A_1$ </td>\n",
    "<td>$M_2$ </td>\n",
    "<td>$A_2$ </td>\n",
    "<td>$\\kappa$</td>\n",
    "<td>$M_1$</td>\n",
    "<td>$A_1$</td>\n",
    "<td>$M_2$</td>\n",
    "<td>$A_2$</td>\n",
    "<td>$\\kappa$</td>\n",
    "</tr>\n",
    "  </thead>\n",
    "<tbody>\n",
    "    <tr>\n",
    "<td>Sentiment</td>\n",
    "<td>4,215</td>\n",
    "<td>7,070</td>\n",
    "<td>3,484</td>\n",
    "<td>9,827</td>\n",
    "<td>38.05</td>\n",
    "<td>3,269</td>\n",
    "<td>6,812</td>\n",
    "<td>3,269</td>\n",
    "<td>9,796</td>\n",
    "<td>31.21</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>Target</td>\n",
    "<td>1,103</td>\n",
    "<td>1,943</td>\n",
    "<td>1,217</td>\n",
    "<td>4,162</td>\n",
    "<td>35.48</td>\n",
    "<td>898</td>\n",
    "<td>1,905</td>\n",
    "<td>898</td>\n",
    "<td>4,148</td>\n",
    "<td>26.85</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>Source</td>\n",
    "<td>159</td>\n",
    "<td>445</td>\n",
    "<td>156</td>\n",
    "<td>456</td>\n",
    "<td>34.53</td>\n",
    "<td>153</td>\n",
    "<td>439</td>\n",
    "<td>153</td>\n",
    "<td>456</td>\n",
    "<td>33.75</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>Polar Term</td>\n",
    "<td>1,951</td>\n",
    "<td>2,854</td>\n",
    "<td>2,029</td>\n",
    "<td>3,188</td>\n",
    "<td>64.29</td>\n",
    "<td>1,902</td>\n",
    "<td>2,851</td>\n",
    "<td>1,902</td>\n",
    "<td>3,180</td>\n",
    "<td>61.36</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>Intensifier</td>\n",
    "<td>57</td>\n",
    "<td>101</td>\n",
    "<td>59</td>\n",
    "<td>123</td>\n",
    "<td>51.71</td>\n",
    "<td>57</td>\n",
    "<td>101</td>\n",
    "<td>57</td>\n",
    "<td>123</td>\n",
    "<td>50.81</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>Diminisher</td>\n",
    "<td>3</td>\n",
    "<td>10</td>\n",
    "<td>3</td>\n",
    "<td>8</td>\n",
    "<td>33.32</td>\n",
    "<td>3</td>\n",
    "<td>10</td>\n",
    "<td>3</td>\n",
    "<td>8</td>\n",
    "<td>33.32</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "<td>Negation</td>\n",
    "<td>21</td>\n",
    "<td>63</td>\n",
    "<td>21</td>\n",
    "<td>83</td>\n",
    "<td>28.69</td>\n",
    "<td>21</td>\n",
    "<td>63</td>\n",
    "<td>21</td>\n",
    "<td>83</td>\n",
    "<td>28.69</td>\n",
    "    </tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stage II: Adjudication Step\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "      <tr>\n",
    "    <td>Element</td>\n",
    "    <td colspan=5>Binary $\\kappa$</td>\n",
    "    <td colspan=5>Proportional $\\kappa$</td>\n",
    "          </tr>\n",
    "<tr>\n",
    "<td></td>\n",
    "<td>$M_1$ </td>\n",
    "<td>$A_1$ </td>\n",
    "<td>$M_2$ </td>\n",
    "<td>$A_2$ </td>\n",
    "<td>$\\kappa$</td>\n",
    "<td>$M_1$</td>\n",
    "<td>$A_1$</td>\n",
    "<td>$M_2$</td>\n",
    "<td>$A_2$</td>\n",
    "<td>$\\kappa$</td>\n",
    "</tr>\n",
    "  </thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>8,198</td>\n",
    "<td>8,530</td>\n",
    "<td>8,260</td>\n",
    "<td>14,034</td>\n",
    "<td>67.92</td>\n",
    "<td>7,435</td>\n",
    "<td>8,243</td>\n",
    "<td>7,435</td>\n",
    "<td>13,714</td>\n",
    "<td>61.94</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Target</td>\n",
    "<td>3,088</td>\n",
    "<td>3,407</td>\n",
    "<td>2,814</td>\n",
    "<td>5,303</td>\n",
    "<td>65.66</td>\n",
    "<td>2,554</td>\n",
    "<td>3,326</td>\n",
    "<td>2,554</td>\n",
    "<td>5,212</td>\n",
    "<td>57.27</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Source</td>\n",
    "<td>573</td>\n",
    "<td>690</td>\n",
    "<td>545</td>\n",
    "<td>837</td>\n",
    "<td>72.91</td>\n",
    "<td>539</td>\n",
    "<td>676</td>\n",
    "<td>539</td>\n",
    "<td>833</td>\n",
    "<td>71.12</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Polar Term</td>\n",
    "<td>3,164</td>\n",
    "<td>3,298</td>\n",
    "<td>3,261</td>\n",
    "<td>4,134</td>\n",
    "<td>85.68</td>\n",
    "<td>3,097</td>\n",
    "<td>3,290</td>\n",
    "<td>3,097</td>\n",
    "<td>4,121</td>\n",
    "<td>82.64</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Intensifier</td>\n",
    "<td>111</td>\n",
    "<td>219</td>\n",
    "<td>113</td>\n",
    "<td>180</td>\n",
    "<td>56.01</td>\n",
    "<td>111</td>\n",
    "<td>219</td>\n",
    "<td>111</td>\n",
    "<td>180</td>\n",
    "<td>55.51</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Diminisher</td>\n",
    "<td>9</td>\n",
    "<td>16</td>\n",
    "<td>10</td>\n",
    "<td>16</td>\n",
    "<td>59.37</td>\n",
    "<td>9</td>\n",
    "<td>16</td>\n",
    "<td>9</td>\n",
    "<td>15</td>\n",
    "<td>58.05</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Negation</td>\n",
    "<td>68</td>\n",
    "<td>84</td>\n",
    "<td>67</td>\n",
    "<td>140</td>\n",
    "<td>60.21</td>\n",
    "<td>67</td>\n",
    "<td>83</td>\n",
    "<td>67</td>\n",
    "<td>140</td>\n",
    "<td>60.03</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stage III: Final Annotation\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "      <tr>\n",
    "    <td>Element</td>\n",
    "    <td colspan=5>Binary $\\kappa$</td>\n",
    "    <td colspan=5>Proportional $\\kappa$</td>\n",
    "          </tr>\n",
    "<tr>\n",
    "<td></td>\n",
    "<td>$M_1$ </td>\n",
    "<td>$A_1$ </td>\n",
    "<td>$M_2$ </td>\n",
    "<td>$A_2$ </td>\n",
    "<td>$\\kappa$</td>\n",
    "<td>$M_1$</td>\n",
    "<td>$A_1$</td>\n",
    "<td>$M_2$</td>\n",
    "<td>$A_2$</td>\n",
    "<td>$\\kappa$</td>\n",
    "</tr>\n",
    "  </thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>14,748</td>\n",
    "<td>15,929</td>\n",
    "<td>14,969</td>\n",
    "<td>26,047</td>\n",
    "<td>65.03</td>\n",
    "<td>13,316</td>\n",
    "<td>15,375</td>\n",
    "<td>13,316</td>\n",
    "<td>25,352</td>\n",
    "<td>58.82</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Target</td>\n",
    "<td>5,765</td>\n",
    "<td>6,629</td>\n",
    "<td>5,292</td>\n",
    "<td>9,852</td>\n",
    "<td>64.76</td>\n",
    "<td>4,789</td>\n",
    "<td>6,462</td>\n",
    "<td>4,789</td>\n",
    "<td>9,659</td>\n",
    "<td>56.61</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Source</td>\n",
    "<td>966</td>\n",
    "<td>1,207</td>\n",
    "<td>910</td>\n",
    "<td>1,619</td>\n",
    "<td>65.99</td>\n",
    "<td>898</td>\n",
    "<td>1,180</td>\n",
    "<td>898</td>\n",
    "<td>1,604</td>\n",
    "<td>64.1</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Polar Term</td>\n",
    "<td>5,574</td>\n",
    "<td>5,989</td>\n",
    "<td>5,659</td>\n",
    "<td>7,419</td>\n",
    "<td>82.83</td>\n",
    "<td>5,441</td>\n",
    "<td>5,977</td>\n",
    "<td>5,441</td>\n",
    "<td>7,395</td>\n",
    "<td>80.29</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Intensifier</td>\n",
    "<td>192</td>\n",
    "<td>432</td>\n",
    "<td>194</td>\n",
    "<td>338</td>\n",
    "<td>49.97</td>\n",
    "<td>192</td>\n",
    "<td>432</td>\n",
    "<td>192</td>\n",
    "<td>338</td>\n",
    "<td>49.71</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Diminisher</td>\n",
    "<td>16</td>\n",
    "<td>30</td>\n",
    "<td>17</td>\n",
    "<td>34</td>\n",
    "<td>51.55</td>\n",
    "<td>16</td>\n",
    "<td>30</td>\n",
    "<td>16</td>\n",
    "<td>33</td>\n",
    "<td>50.78</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Negation</td>\n",
    "<td>111</td>\n",
    "<td>132</td>\n",
    "<td>110</td>\n",
    "<td>243</td>\n",
    "<td>58.87</td>\n",
    "<td>110</td>\n",
    "<td>131</td>\n",
    "<td>110</td>\n",
    "<td>242</td>\n",
    "<td>58.92</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inter-Annotator Agreement on Attributes\n",
    "\n",
    "<table class=\"narrow\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <td>Element</td>\n",
    "      <td>Polarity $\\kappa$</td>\n",
    "      <td>Intensity $\\alpha$</td>\n",
    "    </tr>\n",
    "  </thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>58.8 </td>\n",
    "<td>73.54</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Polar Term</td>\n",
    "<td>87.12</td>\n",
    "<td>78.79</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Qualitative Analysis\n",
    "\n",
    "<div class=\"example\">\n",
    "  <p class=\"annotator1\">Annotation 1:</p>\n",
    "  @TinaPannes immerhin ist die #afd nicht dabei ,\n",
    "  <div class=\"translation\">@TinaPannes anyway the #afd is not there ,</div>\n",
    "\n",
    "  <p class=\"annotator2\">Annotation 2:</p>\n",
    "  @TinaPannes <sentiment><target>immerhin ist die #afd nicht dabei</target> ,</sentiment>\n",
    "\n",
    "  <div class=\"translation\">@TinaPannes <sentiment><target>anyway the #afd is not there</target> ,</sentiment></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<example>\n",
    "<p class=\"annotator1\">Annotation 1:</p>\n",
    "<sentiment>Koalition wirft der SPD <target>Blockadehaltung</target> vor</sentiment>\n",
    "<div class=\"translation\"><sentiment>Coalition accuses the SPD of <target>blocking politics</target></sentiment></div>\n",
    "\n",
    "<p class=\"annotator2\">Annotation 2:</p>\n",
    "<sentiment>Koalition wirft [der SPD] target Blockadehaltung vor</sentiment>\n",
    "<div class=\"translation\"><sentiment>Coalition accuses [the SPD] target of blocking politics</sentiment></div>\n",
    "</example>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<example>\n",
    "<p class=\"annotator1\">Annotation 1:</p>\n",
    "Syrien vor dem Angriff&mdash;bringen diese Bomben den Frieden?\n",
    "\n",
    "<div class=\"translation\">Syria facing an attack&mdash;will these bombs bring peace?</div>\n",
    "\n",
    "<p class=\"annotator2\">Annotation 2:</p>\n",
    "Syrien vor dem <polarterm>Angriff</polarterm>&mdash;bringen diese <polarterm>Bomben</polarterm> polar-term den [Frieden] polar-term ?\n",
    "\n",
    "<div class=\"translation\">Syria facing an <polarterm>attack</polarterm>&mdash;will these <polarterm>bombs</polarterm> polar-term bring [peace] polar-term ?</div>\n",
    "</example>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Effect of the Selection Criteria\n",
    "\n",
    "<img src=\"images/sentiment_stat.png\" class=\"heatmap\"><img src=\"images/emo-expression_stat.png\" class=\"heatmap\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Effect of the Selection Criteria\n",
    "\n",
    "<img src=\"images/sentiment_agreement.png\" class=\"heatmap\"/><img src=\"images/emo-expression_agreement.png\" class=\"heatmap\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Effect of the Selection Criteria\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Selection Criteria</td>\n",
    "<td colspan=\"4\">Correlation Coefficients</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> # of elements </td>\n",
    "<td> agreement </td>\n",
    "<td> # of elements </td>\n",
    "<td> agreement</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Federal Elections</td>\n",
    "<td>0.312</td>\n",
    "<td>0.169</td>\n",
    "<td>0.356</td>\n",
    "<td>0.289</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Papal Conclave</td>\n",
    "<td>0.149</td>\n",
    "<td>0.124</td>\n",
    "<td>0.182</td>\n",
    "<td>0.264</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Political Discussions</td>\n",
    "<td>0.195</td>\n",
    "<td>0.148</td>\n",
    "<td>0.218</td>\n",
    "<td>0.244</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>General Conversations</td>\n",
    "<td>0.183</td>\n",
    "<td>0.19</td>\n",
    "<td>0.372</td>\n",
    "<td>0.452</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Polar Terms</td>\n",
    "<td>0.445</td>\n",
    "<td>0.352</td>\n",
    "<td>0.38</td>\n",
    "<td>0.301</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Emoticons</td>\n",
    "<td>0.127</td>\n",
    "<td>0.096</td>\n",
    "<td>0.47</td>\n",
    "<td>0.615</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Random</td>\n",
    "<td>0.216</td>\n",
    "<td>0.134</td>\n",
    "<td>0.143</td>\n",
    "<td>0.138</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "* I have presented a comprehensive, manually labeled corpus of 7,992 German tweets;\n",
    "* All microblogs in this collection were sampled from four different topics (political elections, papal conclave, general political conversations, and everyday smalltalk);\n",
    "* Afterwards, two human experts annotated these messages with sentiments, sources, targets, polar terms, their inensifiers, diminishers, and negations;\n",
    "* As it turned out, marking these elements poses a significant challenge even to professional linguists, with their mutual inter-annotator agreement on sentiments hardly reaching 35% (which is generally considered as a low IAA);\n",
    "* These difficulties, however, can be largely reduced if we let the annotators resolve their contradicting  cases (the IAA on sentiments, in this case, rises to);\n",
    "* After adjudication, the inter-rater reliability remains at a constantly high level (sentiment IAA $\\approx$ 59%);\n",
    "* The remaining disagreement cases mostly represent ambiguous or controversial statements;\n",
    "* Finally, we could see a significant correlation between the initial selection criteria and the number and reliability of annotated sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 class=\"chapter\">Chapter III: Lexicons</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lexicon Types\n",
    "\n",
    "Main types of sentiment lexicons are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* manual;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* semi-automatic;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* automatic:\n",
    "  * dictionary-based;\n",
    "  * corpus-based;\n",
    "  * word-embedding&ndash;based ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Semi-Automatic Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **German Polarity Clues** (GPC; Waltinger, 2010), which contains 10,141 polar terms\n",
    "from the English sentiment lexicons Subjectivity Clues (Wilson et al., 2005) and Sen-\n",
    "tiSpin (Takamura et al., 2005) that were automatically translated into German and\n",
    "then manually revised by the author. Apart from that, Waltinger also manually en-\n",
    "riched these translations with their frequent synonyms and 290 negated phrases; 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **SentiWS** (SWS; Remus et al., 2010), which includes 1,818 positively and 1,650 nega-\n",
    "tively connoted terms along with their part-of-speech tags and inflections, which results\n",
    "in a total of 32,734 word forms. As in the previous case, the authors obtained the initial\n",
    "entries for their resource by translating an English polarity list (the General Inquirer\n",
    "lexicon) and then manually correcting these translations. In addition to this, they\n",
    "expanded the translated set with words and phrases that frequently co-occurred with\n",
    "positive and negative seed terms in a corpus of 10,200 customer reviews or in the\n",
    "German Collocation Dictionary (Quasthoff, 2010); 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* and, finally, the only the lexicon that was not obtained through translation—the\n",
    "**Zurich Polarity List** (ZPL; Clematide and Klenner, 2010), which features 8,000 sub-\n",
    "jective entries extracted from GermaNet synsets (Hamp and Feldweg, 1997). These\n",
    "synsets had been manually annotated by human experts with their prior polarities.\n",
    "Since the authors, however, found the number of polar adjectives obtained this way\n",
    "to be insufficient for their classification experiments, they automatically enriched this\n",
    "lexicon with more attributive terms, using the collocation method of Hatzivassiloglou\n",
    "and McKeown (1997)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Results of Semi-Automatic Lexicons\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Lexicon</td>\n",
    "<td colspan=\"3\">Positive Expressions</td>\n",
    "<td colspan=\"3\">Negative Expressions</td>\n",
    "<td colspan=\"3\">Neutral Terms</td>\n",
    "<td rowspan=\"2\">Macro-$F_1$</td>\n",
    "<td rowspan=\"2\">Micro-$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tr>\n",
    "<td>GPC</td>\n",
    "<td>0.209</td>\n",
    "<td>0.535</td>\n",
    "<td>0.301</td>\n",
    "<td>0.195</td>\n",
    "<td>0.466</td>\n",
    "<td>0.275</td>\n",
    "<td>0.983</td>\n",
    "<td>0.923</td>\n",
    "<td>0.952</td>\n",
    "<td>0.509</td>\n",
    "<td>0.906 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SWS</td>\n",
    "<td>0.335</td>\n",
    "<td>0.435</td>\n",
    "<td>0.379</td>\n",
    "<td>0.484</td>\n",
    "<td>0.344</td>\n",
    "<td>0.402</td>\n",
    "<td>0.977</td>\n",
    "<td>0.975</td>\n",
    "<td>0.976</td>\n",
    "<td>0.586</td>\n",
    "<td>0.952</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ZPL</td>\n",
    "<td>0.411</td>\n",
    "<td>0.424</td>\n",
    "<td>0.417</td>\n",
    "<td>0.38</td>\n",
    "<td>0.352</td>\n",
    "<td>0.366</td>\n",
    "<td>0.977</td>\n",
    "<td>0.979</td>\n",
    "<td>0.978</td>\n",
    "<td>0.587</td>\n",
    "<td>0.955 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GPC $\\cap$ SWS $\\cap$ ZPL</td>\n",
    "<td><b>0.527</b></td>\n",
    "<td>0.372</td>\n",
    "<td><b>0.436</b></td>\n",
    "<td><b>0.618</b></td>\n",
    "<td>0.244</td>\n",
    "<td>0.35</td>\n",
    "<td>0.973</td>\n",
    "<td><b>0.99</b></td>\n",
    "<td><b>0.982</b></td>\n",
    "<td><b>0.589</b></td>\n",
    "<td><b>0.964</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GPC $\\cup$ SWS $\\cup$ ZPL</td>\n",
    "<td>0.202</td>\n",
    "<td>0.562</td>\n",
    "<td>0.297</td>\n",
    "<td>0.195</td>\n",
    "<td>0.532</td>\n",
    "<td>0.286</td>\n",
    "<td>0.985</td>\n",
    "<td>0.917</td>\n",
    "<td>0.95</td>\n",
    "<td>0.51</td>\n",
    "<td>0.901 </td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary-Based Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hu and Liu (2004);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Blair-Goldensohn et al. (2008);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kim and Hovy (2004, 2006);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Esuli and Sebastiani (2006a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rao and Ravichandran (2009):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Results of Dictionary-Based Methods\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Lexicon</td>\n",
    "<td rowspan=\"2\"># of Terms</td>\n",
    "<td colspan=\"3\">Positive Expressions</td>\n",
    "<td colspan=\"3\">Negative Expressions</td>\n",
    "<td colspan=\"3\">Neutral Terms</td>\n",
    "<td rowspan=\"2\">Macro-$F_1$</td>\n",
    "<td rowspan=\"2\">Micro-$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tr>\n",
    "<td>Seed Set</td>\n",
    "<td>20</td>\n",
    "<td>0.771</td>\n",
    "<td>0.102</td>\n",
    "<td>0.18</td>\n",
    "<td>0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.999</td>\n",
    "<td>0.981</td>\n",
    "<td>0.398</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HL</td>\n",
    "<td>5,745</td>\n",
    "<td>0.161</td>\n",
    "<td>0.266</td>\n",
    "<td>0.2</td>\n",
    "<td>0.2</td>\n",
    "<td>0.133</td>\n",
    "<td>0.16</td>\n",
    "<td>0.969</td>\n",
    "<td>0.96</td>\n",
    "<td>0.965</td>\n",
    "<td>0.442</td>\n",
    "<td>0.93</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BG</td>\n",
    "<td>1,895</td>\n",
    "<td>0.503</td>\n",
    "<td>0.232</td>\n",
    "<td>0.318</td>\n",
    "<td>0.285</td>\n",
    "<td>0.093</td>\n",
    "<td>0.14</td>\n",
    "<td>0.968</td>\n",
    "<td>0.991</td>\n",
    "<td>0.979</td>\n",
    "<td>0.479</td>\n",
    "<td>0.959</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>KH</td>\n",
    "<td>356</td>\n",
    "<td>0.716</td>\n",
    "<td>0.159</td>\n",
    "<td>0.261</td>\n",
    "<td>0.269</td>\n",
    "<td>0.044</td>\n",
    "<td>0.076</td>\n",
    "<td>0.965</td>\n",
    "<td>0.997</td>\n",
    "<td>0.981</td>\n",
    "<td>0.439</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ES</td>\n",
    "<td>39,181</td>\n",
    "<td>0.042</td>\n",
    "<td>0.564</td>\n",
    "<td>0.078</td>\n",
    "<td>0.033</td>\n",
    "<td>0.255</td>\n",
    "<td>0.059</td>\n",
    "<td>0.981</td>\n",
    "<td>0.689</td>\n",
    "<td>0.81</td>\n",
    "<td>0.315</td>\n",
    "<td>0.644</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RR$_{mincut}$</td>\n",
    "<td>8,060</td>\n",
    "<td>0.07</td>\n",
    "<td>0.422</td>\n",
    "<td>0.12</td>\n",
    "<td>0.216</td>\n",
    "<td>0.073</td>\n",
    "<td>0.109</td>\n",
    "<td>0.972</td>\n",
    "<td>0.873</td>\n",
    "<td>0.92</td>\n",
    "<td>0.383</td>\n",
    "<td>0.849</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RR$_{lbl-prop}$</td>\n",
    "<td>1,105</td>\n",
    "<td>0.567</td>\n",
    "<td>0.176</td>\n",
    "<td>0.269</td>\n",
    "<td>0.571</td>\n",
    "<td>0.046</td>\n",
    "<td>0.085</td>\n",
    "<td>0.965</td>\n",
    "<td>0.997</td>\n",
    "<td>0.981</td>\n",
    "<td>0.445</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>AR</td>\n",
    "<td>23</td>\n",
    "<td>0.768</td>\n",
    "<td>0.1</td>\n",
    "<td>0.176</td>\n",
    "<td>0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.999</td>\n",
    "<td>0.981</td>\n",
    "<td>0.397</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HL $\\cap$ BG $\\cap$ RR$_{lbl-prop}$</td>\n",
    "<td>752</td>\n",
    "<td>0.601</td>\n",
    "<td>0.165</td>\n",
    "<td>0.259</td>\n",
    "<td>0.567</td>\n",
    "<td>0.045</td>\n",
    "<td>0.084</td>\n",
    "<td>0.965</td>\n",
    "<td>0.997</td>\n",
    "<td>0.981</td>\n",
    "<td>0.441</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HL $\\cup$ BG $\\cup$ RR$_{lbl-prop}$</td>\n",
    "<td>6,258</td>\n",
    "<td>0.166</td>\n",
    "<td>0.288</td>\n",
    "<td>0.21</td>\n",
    "<td>0.191</td>\n",
    "<td>0.146</td>\n",
    "<td>0.165</td>\n",
    "<td>0.97</td>\n",
    "<td>0.958</td>\n",
    "<td>0.964</td>\n",
    "<td>0.446</td>\n",
    "<td>0.929</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus-Based Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Takamura et al. (2005);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Velikovich et al. (2010);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kiritchenko et al. (2014);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Severyn and Moschitti (2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Results of Corpus-Based Methods\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Lexicon</td>\n",
    "<td rowspan=\"2\"># of Terms</td>\n",
    "<td colspan=\"3\">Positive Expressions</td>\n",
    "<td colspan=\"3\">Negative Expressions</td>\n",
    "<td colspan=\"3\">Neutral Terms</td>\n",
    "<td rowspan=\"2\">Macro-$F_1$</td>\n",
    "<td rowspan=\"2\">Micro-$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tr>\n",
    "<td>Seed Set</td>\n",
    "<td>20</td>\n",
    "<td>0.771</td>\n",
    "<td>0.102</td>\n",
    "<td>0.18</td>\n",
    "<td>0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.999</td>\n",
    "<td>0.981</td>\n",
    "<td>0.398</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TKM</td>\n",
    "<td>920</td>\n",
    "<td>0.646</td>\n",
    "<td>0.134</td>\n",
    "<td>0.221</td>\n",
    "<td>0.565</td>\n",
    "<td>0.029</td>\n",
    "<td>0.055</td>\n",
    "<td>0.964</td>\n",
    "<td>0.998</td>\n",
    "<td>0.981</td>\n",
    "<td>0.419</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>VEL</td>\n",
    "<td>60</td>\n",
    "<td>0.764</td>\n",
    "<td>0.102</td>\n",
    "<td>0.18</td>\n",
    "<td>0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.999</td>\n",
    "<td>0.98</td>\n",
    "<td>0.398</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>KIR</td>\n",
    "<td>320</td>\n",
    "<td>0.386</td>\n",
    "<td>0.106</td>\n",
    "<td>0.166</td>\n",
    "<td>0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.996</td>\n",
    "<td>0.979</td>\n",
    "<td>0.393</td>\n",
    "<td>0.959</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>60</td>\n",
    "<td>0.68</td>\n",
    "<td>0.102</td>\n",
    "<td>0.177</td>\n",
    "<td>0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.999</td>\n",
    "<td>0.981</td>\n",
    "<td>0.397</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TKM $\\cap$ VEL $\\cap$ SEV</td>\n",
    "<td>20</td>\n",
    "<td>0.771</td>\n",
    "<td>0.102</td>\n",
    "<td>0.18</td>\n",
    "<td>0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.999</td>\n",
    "<td>0.981</td>\n",
    "<td>0.398</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TKM $\\cup$ VEL $\\cup$ SEV</td>\n",
    "<td>1,020</td>\n",
    "<td>0.593</td>\n",
    "<td>0.134</td>\n",
    "<td>0.218</td>\n",
    "<td>0.565</td>\n",
    "<td>0.029</td>\n",
    "<td>0.055</td>\n",
    "<td>0.964</td>\n",
    "<td>0.998</td>\n",
    "<td>0.98</td>\n",
    "<td>0.418</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NWE-Based Lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Tang et al. (2014);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Vo and Zhang (2016);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* <div class=\"new\">Nearest Centroids;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* <div class=\"new\">$k$-Nearest Neighbors;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* <div class=\"new\">Principal Component Analysis;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* <div class=\"new\">Linear Projection.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of NWE-Based Methods\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Lexicon</td>\n",
    "<td rowspan=\"2\"># of Terms</td>\n",
    "<td colspan=\"3\">Positive Expressions</td>\n",
    "<td colspan=\"3\">Negative Expressions</td>\n",
    "<td colspan=\"3\">Neutral Terms</td>\n",
    "<td rowspan=\"2\">Macro-$F_1$</td>\n",
    "<td rowspan=\"2\">Micro-$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tr>\n",
    "<td>Seed Set</td>\n",
    "<td>20</td>\n",
    "<td>0.771</td>\n",
    "<td>0.102</td>\n",
    "<td>0.18</td>\n",
    "<td>0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.999</td>\n",
    "<td>0.981</td>\n",
    "<td>0.398</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TNG</td>\n",
    "<td>1,600</td>\n",
    "<td>0.088</td>\n",
    "<td>0.153</td>\n",
    "<td>0.112</td>\n",
    "<td>0.193</td>\n",
    "<td>0.155</td>\n",
    "<td>0.172</td>\n",
    "<td>0.966</td>\n",
    "<td>0.953</td>\n",
    "<td>0.959</td>\n",
    "<td>0.414</td>\n",
    "<td>0.921</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>VO</td>\n",
    "<td>40</td>\n",
    "<td>0.117</td>\n",
    "<td>0.115</td>\n",
    "<td>0.116</td>\n",
    "<td>0.541</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.98</td>\n",
    "<td>0.971</td>\n",
    "<td>0.374</td>\n",
    "<td>0.944</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>NC</td>\n",
    "<td>5,200</td>\n",
    "<td>0.771</td>\n",
    "<td>0.102</td>\n",
    "<td>0.18</td>\n",
    "<td>0.568</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.999</td>\n",
    "<td>0.981</td>\n",
    "<td>0.398</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$k$-NN</td>\n",
    "<td>420</td>\n",
    "<td>0.486</td>\n",
    "<td>0.182</td>\n",
    "<td>0.265</td>\n",
    "<td>0.65</td>\n",
    "<td>0.091</td>\n",
    "<td>0.16</td>\n",
    "<td>0.966</td>\n",
    "<td>0.995</td>\n",
    "<td>0.98</td>\n",
    "<td>0.468</td>\n",
    "<td>0.961</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PCA</td>\n",
    "<td>40</td>\n",
    "<td>0.771</td>\n",
    "<td>0.102</td>\n",
    "<td>0.18</td>\n",
    "<td>0.529</td>\n",
    "<td>0.017</td>\n",
    "<td>0.033</td>\n",
    "<td>0.963</td>\n",
    "<td>0.999</td>\n",
    "<td>0.981</td>\n",
    "<td>0.398</td>\n",
    "<td>0.962</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LP</td>\n",
    "<td>6,340</td>\n",
    "<td>0.741</td>\n",
    "<td>0.156</td>\n",
    "<td>0.257</td>\n",
    "<td>0.436</td>\n",
    "<td>0.088</td>\n",
    "<td>0.147</td>\n",
    "<td>0.966</td>\n",
    "<td>0.998</td>\n",
    "<td>0.982</td>\n",
    "<td>0.462</td>\n",
    "<td>0.963</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Effect of Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* word2vec;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* task-specific;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* task-specific + word2vec;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* task-specific least-squares mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Effect of Word Embeddings\n",
    "\n",
    "<img src=\"img/potts_embeddings.png\" alt=\"t-SNE visualization of different word-embedding types\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Effect of Word Embeddings\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Lexicon</td>\n",
    "<td colspan=\"4\">Embedding Type</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>word2vec</td>\n",
    "<td>task-specific + word2vec</td>\n",
    "<td>task-specific + least squares</td>\n",
    "<td>task-specific</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>NC</td>\n",
    "<td>0.398</td>\n",
    "<td>0.398</td>\n",
    "<td><div class=\"best\">0.401</div></td>\n",
    "<td>0.399</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$k$-NN</td>\n",
    "<td><div class=\"best\">0.468</div></td>\n",
    "<td>0.43</td>\n",
    "<td>0.398</td>\n",
    "<td>0.392</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PCA</td>\n",
    "<td>0.398</td>\n",
    "<td>0.398</td>\n",
    "<td>0.404</td>\n",
    "<td><div class=\"best\">0.409</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LP</td>\n",
    "<td><div class=\"best\">0.462</div></td>\n",
    "<td>0.441</td>\n",
    "<td>0.398</td>\n",
    "<td>0.399</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Vector Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mean and length normalization $\\vec{v}^* = \\frac{\\frac{\\vec{v}}{\\left\\lVert\\vec{v}\\right\\rVert} - \\vec{\\mu}^*}{\\vec{\\sigma}^*}$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mean normalization: $\\vec{v}^* = \\frac{\\vec{v} - \\vec{\\mu}}{\\vec{\\sigma}}$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* length normalization: $\\vec{v}^* = \\frac{\\vec{v}}{\\left\\lVert\\vec{v}\\right\\rVert}$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* no normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Vector Normalization\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">SLG Method</td>\n",
    "<td colspan=\"4\">Vector Normalization</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>mean normalization + length normalization</td>\n",
    "<td>mean normalization</td>\n",
    "<td>length normalization</td>\n",
    "<td>no normalization</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>NC</td>\n",
    "<td>0.398</td>\n",
    "<td>0.398</td>\n",
    "<td>0.398</td>\n",
    "<td>0.398</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>$k$-NN</td>\n",
    "<td>0.468</td>\n",
    "<td>0.418</td>\n",
    "<td>0.467</td>\n",
    "<td>0.417</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PCA</td>\n",
    "<td>0.398</td>\n",
    "<td>0.396</td>\n",
    "<td>0.398</td>\n",
    "<td>0.396</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LP</td>\n",
    "<td>0.462</td>\n",
    "<td>0.416</td>\n",
    "<td>0.461</td>\n",
    "<td>0.442</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Seed Sets\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td>Seed Set</td>\n",
    "<td>Cardinality</td>\n",
    "<td>Part of Speech</td>\n",
    "<td>Examples</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Hu and Liu (2004)</td>\n",
    "<td>14 positive, 15 negative, and 10 neutral terms</td>\n",
    "<td>adjectives</td>\n",
    "<td class=\"example\">fantastisch, lieb, sympathisch, böse, dumm, schwierig</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Kim and Hovy (2004)</td>\n",
    "<td>60 positive, 60 negative, and 60 neutral terms</td>\n",
    "<td>any</td>\n",
    "<td class=\"example\">fabelhaft, Hoffnung, lieben, hässlich, Missbrauch, töten</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Esuli and Sebastiani (2006)</td>\n",
    "<td>16 positive, 35 negative, and 4,122 neutral terms</td>\n",
    "<td>any</td>\n",
    "<td>angenehm, ausgezeichnet, freundlich, arm, bedauernswert, dürftig</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Remus (2010)</td>\n",
    "<td>12 positive, 12 negative, and 10 neutral terms</td>\n",
    "<td>adjectives</td>\n",
    "<td>gut, schön, richtig, schlecht, unschön, falsch</td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Seed Sets on Dictionary-Based Methods\n",
    "<img src=\"img/sentilex-dict-alt-seed-sets.png\" als=\"Effect of Seed Sets on Dictionary-Based Methods\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Seed Sets on Corpus-Based Methods\n",
    "<img src=\"img/sentilex-crp-alt-seed-sets.png\" als=\"Effect of Seed Sets on Corpus-Based Methods\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of Seed Sets on NWE-Based Methods\n",
    "<img src=\"img/sentilex-nwe-alt-seed-sets.png\" als=\"Effect of Seed Sets on Dictionary-Based Methods\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* semi-automatic translations of common English polarity lists\n",
    "  notably outperform purely automatic SLG methods, which are applied\n",
    "  to German data directly;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* despite their allegedly worse ability to accommodate new\n",
    "  domains, dictionary-based approaches are still better than\n",
    "  corpus-based systems;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* a potential weakness of these algorithms though is their\n",
    "  dependence on manually\n",
    "  annotated linguistic resources, which might not necessarily be\n",
    "  present for every language;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* in this regard, a viable alternative to dictionary-based methods\n",
    "  are SLG systems that induce polar lexicons from neural word\n",
    "  embeddings;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* with at least two of such methods ($k$-NN and linear\n",
    "  projection), I was able to establish a new state of the art for\n",
    "  the macro- and micro-averaged \\F-scores of automatically induced\n",
    "  sentiment lexicons;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* I also checked how different types of embeddings affected the\n",
    "  performance of NWE-based SLG systems, noticing that the $k$-NN and\n",
    "  linear projection methods worked best with standard word2vec\n",
    "  vectors, while nearest centroids and PCA yielded better results when\n",
    "  using task-specific representations;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* all NWE-based approaches benefit from\n",
    "  mean-scaling and length normalization of the input vectors, getting\n",
    "  an improvement by up to 5% in their macro-averaged $F_1$-scores;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* finally, an extensive evaluation of various sets of seed terms\n",
    "  revealed that the results of almost all tested SLG algorithms\n",
    "  crucially depend on the quality of their initial seeds, with larger\n",
    "  and more balanced seed sets typically leading to much higher scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter IV: Aspect-Based Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Given an input sentence $x_1, x_2, \\ldots, x_n$, we need to automatically find textual spans of sentiments, sources, and targets, i.e. to assign a label $y_i\\in{SNT, SRC, TRG, NON}$ to each token $x_i$ in the sentence.\n",
    "\n",
    "<div class=\"example\">\n",
    "  TODO provide an example\n",
    "  <div class=\"translation\">TODO</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since this problem involves simultaneous prediction of tags for multiple inter-connected random variables ($y_1, y_2, \\ldots, y_n$), it is commonly considered as a structured-prediction task, viz. as a sequence-labeling problem (SLP), and addressed with two common SLP methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* probabilistic graphical model (conditional random fields [CRFs]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* recurrent neural networks (long-short term memory [LSTM] or gated recurrent units [GRU])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "<div class=\"example\">\n",
    "  TODO: provide an example\n",
    "  <div class=\"translation\">TODO</div>\n",
    "</div>\n",
    "\n",
    "Possible ways to measure the prediction quality of label spans:\n",
    "\n",
    "1. Exact match;\n",
    "\n",
    "2. Binary overlap;\n",
    "\n",
    "3. Proportional overlap (Johansson and Moschitti, 2010):\n",
    "Given two sets of manually and automatically tagged spans ($\\mathcal{S}$ and\n",
    "$\\widehat{\\mathcal{S}}$, respectively), Johansson and Moschitti\n",
    "estimate the precision of automatic assignment as:\n",
    "$$P(\\mathcal{S}, \\widehat{\\mathcal{S}}) = \\frac{C(\\mathcal{S},\n",
    "    \\widehat{\\mathcal{S}})}{|\\widehat{\\mathcal{S}}|},$$\n",
    "where $C(\\mathcal{S},\\widehat{\\mathcal{S}})$ stands for the proportion\n",
    "of overlapping tokens across all pairs of manually ($s_i$) and\n",
    "automatically ($s_j$) annotated spans:\n",
    "$$C(\\mathcal{S}, \\widehat{\\mathcal{S}}) = \\sum_{s_i \\in\n",
    "    \\mathcal{S}}\\sum_{s_j \\in \\widehat{\\mathcal{S}}}c(s_i, s_j),$$\n",
    "and the $|\\widehat{\\mathcal{S}}|$ term denotes the total number of\n",
    "spans automatically labeled with the given tag.\n",
    "Similarly, the recall of this assignment is estimated as:\n",
    "$$R(\\mathcal{S}, \\widehat{\\mathcal{S}}) = \\frac{C(\\mathcal{S},\n",
    "    \\widehat{\\mathcal{S}})}{|\\mathcal{S}|}.$$\n",
    "Using these two values, one can normally compute the $F_1$-measure as:\n",
    "$$F_1 = 2\\times\\frac{P \\times R}{P + R}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Random Fields\n",
    "\n",
    "TODO: Definition\n",
    "\n",
    "Conditional random fields are an undirected discriminative probabilistic graphical model, which tries to find the most likely "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "For my experiments, I devised the following types of features\n",
    "* **formal**, which included:\n",
    "  * the initial three characters of each token,\n",
    "  * its last three characters,\n",
    "  * general spelling class of the word (e.g., alphanumeric, digit, or punctuation);\n",
    "* **morphological**, which encompassed:\n",
    "  * the part-of-speech tag of the analyzed token,\n",
    "  * grammatical case and gender of inflectable PoS types,\n",
    "  * degree of comparison for adjectives,\n",
    "  * mood, tense, and person forms of verbs;\n",
    "* **lexical**, which comprised:\n",
    "  * the actual lemma and form of the token (using one-hot encoding),\n",
    "  * its polarity class (positive, negative, or neutral), obtained from the Zurich Polarity Lexicon (Clematide and Klenner, 2010);\n",
    "* **syntactic**, which were:\n",
    "  * the dependency relation via which token $x_i$ was connected to its parent,\n",
    "  * two binary attributes that showed whether the previous token in the sentence was the parent or the child of the current word,\n",
    "  * the dependency relation of the previous token in the sentence to its parent + the dependency relation of the current token to its ancestor,\n",
    "  * the dependency link of the next token + the dependency relation of the current token to its parent;\n",
    "* **lexico-syntactic**, which included:\n",
    "  * the lemma of syntactic parent;\n",
    "  * the part-of-speech tag and polarity class of the grandparent in the syntactic tree;\n",
    "  * the lemma of the child node + dependency relation between the current token and its child;\n",
    "  * the PoS tag of the child node + its dependency relation + the PoS tag of the current token;\n",
    "  * the lemma of the child node + its dependency relation + the lemma of the current token;\n",
    "  * the overall polarity of syntactic children, which was computed by summing up the polarity scores of all immediate dependents, and checking whether     the resulting value was greater, less than, or equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Data Set</td>\n",
    "<td colspan=\"3\">Sentiment</td>\n",
    "<td colspan=\"3\">Source</td>\n",
    "<td colspan=\"3\">Target</td>\n",
    "<td rowspan=\"2\">Macro-$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision </td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Training Set</td>\n",
    "<td>0.949</td>\n",
    "<td>0.908</td>\n",
    "<td>0.928</td>\n",
    "<td>0.903</td>\n",
    "<td>0.87</td>\n",
    "<td>0.886</td>\n",
    "<td>0.933</td>\n",
    "<td>0.865</td>\n",
    "<td>0.898</td>\n",
    "<td>0.904</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Test Set</td>\n",
    "<td>0.37</td>\n",
    "<td>0.28</td>\n",
    "<td>0.319</td>\n",
    "<td>0.305</td>\n",
    "<td>0.244</td>\n",
    "<td>0.271</td>\n",
    "<td>0.304</td>\n",
    "<td>0.244</td>\n",
    "<td>0.271</td>\n",
    "<td>0.287</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Analysis (Ablation Tests)\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Element</td>\n",
    "<td rowspan=\"2\">Original $F_1$-Score </td>\n",
    "<td colspan=\"2\">$F_1$-Score after Feature Removal</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Formal</td>\n",
    "<td>Morphological</td>\n",
    "<td>Lexical</td>\n",
    "<td>Syntactic</td>\n",
    "<td>Lexico-Syntactic</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>0.346</td>\n",
    "<td>0.343\\negdelta0.003</td>\n",
    "<td>0.344\\negdelta0.002</td>\n",
    "<td>0.326\\negdelta0.02</td>\n",
    "<td>0.345\\negdelta0.001</td>\n",
    "<td>0.324\\negdelta0.022</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Source</td>\n",
    "<td>0.309</td>\n",
    "<td>0.321\\posdelta0.012</td>\n",
    "<td>0.313\\posdelta0.004</td>\n",
    "<td>0.265\\negdelta0.044</td>\n",
    "<td>0.359\\posdelta0.05</td>\n",
    "<td>0.271\\negdelta0.038</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Target</td>\n",
    "<td>0.26</td>\n",
    "<td>0.282\\posdelta0.022</td>\n",
    "<td>0.252\\negdelta0.008</td>\n",
    "<td>0.263\\posdelta0.003</td>\n",
    "<td>0.233\\negdelta0.027</td>\n",
    "<td>0.263\\posdelta0.003</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Analysis (Top-10 Features)\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Rank</td>\n",
    "<td colspan=\"2\">State Features</td>\n",
    "<td colspan=\"2\">Transition Features</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Feature </td>\n",
    "<td>Score </td>\n",
    "<td>Feature </td>\n",
    "<td>Score</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>prntLemma=meiste $\\rightarrow$ TRG</td>\n",
    "<td>18.68</td>\n",
    "<td>NON $\\rightarrow$ TRG</td>\n",
    "<td>-7.01</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>prntLemma=rettungsschirme $\\rightarrow$ TRG</td>\n",
    "<td>18.3</td>\n",
    "<td>NON $\\rightarrow$ SRC</td>\n",
    "<td>-6.85</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3</td>\n",
    "<td>initChar=sty $\\rightarrow$ NON</td>\n",
    "<td>-16.04</td>\n",
    "<td>NON $\\rightarrow$ SNT</td>\n",
    "<td>-5.39</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td>\n",
    "<td>form=meisten $\\rightarrow$ NON</td>\n",
    "<td>15.99</td>\n",
    "<td>TRG $\\rightarrow$ SRC</td>\n",
    "<td>-2.99</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>5</td>\n",
    "<td>prntLemma=urlauberin $\\rightarrow$ SNT</td>\n",
    "<td>14.74</td>\n",
    "<td>NON $\\rightarrow$ NON</td>\n",
    "<td>2.69</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>6</td>\n",
    "<td>lemma=anfechten  $\\rightarrow$ SNT</td>\n",
    "<td>14.07</td>\n",
    "<td>SRC $\\rightarrow$ NON</td>\n",
    "<td>-2.59</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>7</td>\n",
    "<td>form=thomasoppermann  $\\rightarrow$ TRG</td>\n",
    "<td>13.44</td>\n",
    "<td>SNT $\\rightarrow$ SNT</td>\n",
    "<td>2.54</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>8</td>\n",
    "<td>form=bezeichnete $\\rightarrow$ SNT</td>\n",
    "<td>13.25</td>\n",
    "<td>TRG $\\rightarrow$ TRG</td>\n",
    "<td>2.31</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>9</td>\n",
    "<td>deprel[0]|deprel[1]=NK|AMS $\\rightarrow$ NON</td>\n",
    "<td>12.92</td>\n",
    "<td>SRC $\\rightarrow$ SRC</td>\n",
    "<td>2.19</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>10</td>\n",
    "<td>trailChar=te. $\\rightarrow$ NON</td>\n",
    "<td>12.77</td>\n",
    "<td>SRC $\\rightarrow$ TRG</td>\n",
    "<td>-2.07</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM\n",
    "\n",
    "The main components of a long-short term memory network are:\n",
    "\n",
    "* An **input gate** $\\vec{i}^{(t)}$, which controls how much input information will be used for the current prediction:\n",
    "$$\\vec{i}^{(t)} = \\sigma\\left(W_i\\cdot \\vec{x}^{(t)} + U_i \\cdot \\vec{h}^{(t-1)} + \\vec{b}_i\\right);$$\n",
    "where $\\sigma$ denotes the sigmoid function; $W_i$, $U_i$, and $\\vec{b_i}$ represent model's parameters; and $\\vec{x}^{(t)}$ and $\\vec{h}^{(t-1)}$ stand for the input and previous hidden states respectively;\n",
    "\n",
    "* A **forget gate** $\\vec{f}^{(t)}$, which controls how much previous information will be used for the current prediction:\n",
    "$$\\vec{f}^{(t)} = \\sigma\\left(W_i\\cdot \\vec{x}^{(t)} + U_i \\cdot \\vec{h}^{(t-1)} + \\vec{b}_i\\right);$$\n",
    "\n",
    "* An **intermediate update vector** $\\widetilde{c}^{(t)}$:\n",
    "$$\\widetilde{c}^{(t)} = tanh\\left(W_c\\cdot \\vec{x}^{(t)} + U_c \\cdot \\vec{h}^{(t-1)} + \\vec{b}_c\\right);$$\n",
    "\n",
    "* The **final update vector** $\\vec{c}^{(t)}$, which is a combination of the intermediate and previous updates:\n",
    "$$\\vec{c}^{(t)} = \\vec{i}^{(t)} \\odot \\widetilde{c}^{(t)} + \\vec{f}^{(t)} \\odot \\vec{c}^{(t-1)};$$\n",
    "\n",
    "Using the final update, we can then compute the new **hidden state** $\\vec{h}^{(t)}$ and **output vector** $\\vec{o}^{(t)}$ for position $t$:\n",
    "$$\\vec{o}^{(t)} = \\sigma\\left(W_o\\cdot \\vec{x}^{(t)} + U_o \\cdot \\vec{h}^{(t-1)} + V_o \\cdot \\vec{c}^{(t)} + \\vec{b}_o\\right),$$\n",
    "$$\\vec{h}^{(t)} = \\vec{o}^{(t)} \\odot tanh(\\vec{c}^{(t)}).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GRU\n",
    "\n",
    "A gated recurrent unit network is fundamentally similar to LSTM in that it also features:\n",
    "\n",
    "* An **input gate** $\\vec{i}^{(t)}$:\n",
    "$$\\vec{i}^{(t)} = \\sigma\\left(W_i\\cdot \\vec{x}^{(t)} + U_i \\cdot \\vec{h}^{(t-1)} + \\vec{b}_i\\right);$$\n",
    "\n",
    "* A **forget gate** $\\vec{f}^{(t)}$:\n",
    "$$\\vec{f}^{(t)} = \\sigma\\left(W_i\\cdot \\vec{x}^{(t)} + U_i \\cdot \\vec{h}^{(t-1)} + \\vec{b}_i\\right);$$\n",
    "\n",
    "* An **update vector** $\\widetilde{c}^{(t)}$:\n",
    "$$\\widetilde{c}^{(t)} = tanh\\left(W_c\\cdot \\mathbf{x}^{(t)} + U_c \\cdot \\left(\\vec{f}^{(t)} \\odot \\vec{h}^{(t-1)}\\right)  + \\vec{b}_c\\right);$$\n",
    "\n",
    "But in contrast to LSTM, this update vector is used directly to compute the **hidden state** $\\vec{h}^{(t)}$, which simultaneously serves as the **output** of the model:\n",
    "$$  \\vec{h}^{(t)} = \\vec{i}^{(t)} \\odot \\vec{h}^{(t-1)} + \\left(\\vec{1} -\\vec{i}^{(t)}\\right) \\odot \\widetilde{c}^{(t)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Since the size of our tagset (four tags) was obviously to small for keeping relevant context information in the hidden and output states ($\\vec{h}^{(t)}$ and $\\vec{o}^{(t)}$), I set the size of these vectors to 100 and multiplied them with matrix $O\\in\\mathbb{R}^{4 \\times 100}$ to obtain the final prediction:\n",
    "$$\\hat{y}^{(t)} = argmax(O\\cdot\\vec{o}^{(t)}).$$\n",
    "\n",
    "Due to a high imbalance of targets classes (with most of the words having the tag NON), I **upsampled** subjective tweets by randomly repeating microblogs that contained seniments until I got an equal proportion of subjective and objective messages in the training set.  Furthermore, I used *hinge-loss* as optimized **objective function**:\n",
    "$$L = \\sum_{i}^{N}\\sum_{t=0}^{\\lvert\\vec{x}_i\\rvert}\\max\\left(0, c + \\max\\limits_{y'\\neq y}\\vec{p}_{t,y'} - \\vec{p}_{t,y}\\right) + \\alpha \\left\\lVert{}O\\right\\rVert^2_2$$\n",
    "\n",
    "Finally, I initialized the values of all matrix parameters to random **orthogonal** matrices and used **uniform He sampling** (He et al., 2015) for initializing the values of bias terms.  Afterwards, I ran the training for **256 epochs**, using **RMSProp algorithm** (Tielemann and Hinton, 2012) for optimization and selecting the parameters that yielded the highest macro-averaged $F_1$-score on the dev data during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Data Set</td>\n",
    "<td colspan=\"3\">Sentiment</td>\n",
    "<td colspan=\"3\">Source</td>\n",
    "<td colspan=\"3\">Target</td>\n",
    "<td>Macro-$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Training Set</td>\n",
    "<td>0.49<div class=\"stddev\">0.16</div></td>\n",
    "<td>0.75<div class=\"stddev\">0.01</div></td>\n",
    "<td>0.58<div class=\"stddev\">0.13</div></td>\n",
    "<td>0.45<div class=\"stddev\">0.05</div></td>\n",
    "<td>0.63<div class=\"stddev\">0.12</div></td>\n",
    "<td>0.52<div class=\"stddev\">0.08</div></td>\n",
    "<td>0.41<div class=\"stddev\">0.11</div></td>\n",
    "<td>0.73<div class=\"stddev\">0.06</div></td>\n",
    "<td>0.52<div class=\"stddev\">0.11</div></td>\n",
    "<td>0.54<div class=\"stddev\">0.11</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Test Set</td>\n",
    "<td>0.29<div class=\"stddev\">0.03</div></td>\n",
    "<td>0.31<div class=\"stddev\">0.11</div></td>\n",
    "<td>0.29<div class=\"stddev\">0.03</div></td>\n",
    "<td>0.25<div class=\"stddev\">0.02</div></td>\n",
    "<td>0.31<div class=\"stddev\">0.0</div></td>\n",
    "<td>0.27<div class=\"stddev\">0.01</div></td>\n",
    "<td>0.23<div class=\"stddev\">0.02</div></td>\n",
    "<td>0.25<div class=\"stddev\">0.05</div></td>\n",
    "<td>0.24<div class=\"stddev\">0.01</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Training Set</td>\n",
    "<td>0.51<div class=\"stddev\">0.08</div></td>\n",
    "<td>0.66<div class=\"stddev\">0.05</div></td>\n",
    "<td>0.57<div class=\"stddev\">0.03</div></td>\n",
    "<td>0.42<div class=\"stddev\">0.03</div></td>\n",
    "<td>0.62<div class=\"stddev\">0.05</div></td>\n",
    "<td>0.5<div class=\"stddev\">0.03</div></td>\n",
    "<td>0.47<div class=\"stddev\">0.11</div></td>\n",
    "<td>0.63<div class=\"stddev\">0.11</div></td>\n",
    "<td>0.52<div class=\"stddev\">0.04</div></td>\n",
    "<td>0.53<div class=\"stddev\">0.03</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Test Set</td>\n",
    "<td>0.3<div class=\"stddev\">0.01</div></td>\n",
    "<td>0.26<div class=\"stddev\">0.06</div></td>\n",
    "<td>0.28<div class=\"stddev\">0.03</div></td>\n",
    "<td>0.22<div class=\"stddev\">0.03</div></td>\n",
    "<td>0.28<div class=\"stddev\">0.02</div></td>\n",
    "<td>0.24<div class=\"stddev\">0.02</div></td>\n",
    "<td>0.24<div class=\"stddev\">0.03</div></td>\n",
    "<td>0.21<div class=\"stddev\">0.07</div></td>\n",
    "<td>0.22<div class=\"stddev\">0.03</div></td>\n",
    "<td>0.25<div class=\"stddev\">0.01</div></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Effect of Word Embeddings\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">RNN</td>\n",
    "<td colspan=\"3\">Sentiment</td>\n",
    "<td colspan=\"3\">Source</td>\n",
    "<td colspan=\"3\">Target</td>\n",
    "<td>Macro-$F_1$</td>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"11\" class=\"table-subheader\">Task-Specific Embeddings</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LSTM</td>\n",
    "<td>0.283</td>\n",
    "<td>0.288</td>\n",
    "<td>0.278</td>\n",
    "<td>0.293</td>\n",
    "<td>0.372</td>\n",
    "<td>0.328</td>\n",
    "<td>0.254</td>\n",
    "<td>0.27</td>\n",
    "<td>0.259</td>\n",
    "<td>0.288</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GRU</td>\n",
    "<td>0.287</td>\n",
    "<td>0.246</td>\n",
    "<td>0.263</td>\n",
    "<td>0.287</td>\n",
    "<td>0.405</td>\n",
    "<td>0.335</td>\n",
    "<td>0.252</td>\n",
    "<td>0.205</td>\n",
    "<td>0.216</td>\n",
    "<td>0.271</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"11\" class=\"table-subheader\">Least-Squares Embeddings</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LSTM</td>\n",
    "<td>0.268</td>\n",
    "<td>0.37</td>\n",
    "<td>0.307</td>\n",
    "<td>0.261</td>\n",
    "<td>0.414</td>\n",
    "<td>0.314</td>\n",
    "<td>0.223</td>\n",
    "<td>0.275</td>\n",
    "<td>0.245</td>\n",
    "<td>0.289</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GRU</td>\n",
    "<td>0.256</td>\n",
    "<td>0.341</td>\n",
    "<td>0.291</td>\n",
    "<td>0.267</td>\n",
    "<td>0.395</td>\n",
    "<td>0.318</td>\n",
    "<td>0.229</td>\n",
    "<td>0.262</td>\n",
    "<td>0.245</td>\n",
    "<td>0.285</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"11\" class=\"table-subheader\">Word2Vec Embeddings</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LSTM</td>\n",
    "<td>0.291</td>\n",
    "<td>0.329</td>\n",
    "<td>0.309</td>\n",
    "<td>0.2</td>\n",
    "<td>0.311</td>\n",
    "<td>0.244</td>\n",
    "<td>0.221</td>\n",
    "<td>0.219</td>\n",
    "<td>0.22</td>\n",
    "<td>0.257</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GRU</td>\n",
    "<td>0.273</td>\n",
    "<td>0.355</td>\n",
    "<td>0.301</td>\n",
    "<td>0.207</td>\n",
    "<td>0.353</td>\n",
    "<td>0.257</td>\n",
    "<td>0.213</td>\n",
    "<td>0.26</td>\n",
    "<td>0.233</td>\n",
    "<td>0.264</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Annotation Scheme (Example)\n",
    "\n",
    "**Broad Interpretation:**\n",
    "<div class=\"example\">\n",
    "  <div class=\"seniment\"><div class=\"target\">Francis</div> makes a <div class=\"intensifier\">very</div> <div class=\"emoexpression\">good</div> impression on <div class=\"source\">me</div>!<div class=\"emoexpression\">:)</div></div>\n",
    "\n",
    "  $\\rightarrow$\n",
    "\n",
    "  Francis/TRG makes/SNT a/SNT very/SNT good/SNT impression/SNT on/SNT me/SRC !/SNT :)/SNT\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Narrow Interpretation:**\n",
    "<div class=\"example\">\n",
    "  <div class=\"seniment\"><div class=\"target\">Francis</div> makes a <div class=\"intensifier\">very</div> <div class=\"emoexpression\">good</div> impression on <div class=\"source\">me</div>!<div class=\"emoexpression\">:)</div></div>\n",
    "\n",
    "  $\\rightarrow$\n",
    "\n",
    "  Francis/TRG makes/NON a/NON very/NON good/SNT impression/NON on/NON me/SRC !/NON :)/SNT\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Annotation Scheme (Results)\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Sentiment</td>\n",
    "<td colspan=\"3\">Source</td>\n",
    "<td colspan=\"3\">Target</td>\n",
    "<td>Macro-$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"11\">Broad Interpretation</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>CRF</td>\n",
    "<td>0.38</td>\n",
    "<td>0.32</td>\n",
    "<td>0.34</td>\n",
    "<td>0.3</td>\n",
    "<td>0.33</td>\n",
    "<td>0.31</td>\n",
    "<td>0.29</td>\n",
    "<td>0.23</td>\n",
    "<td>0.26</td>\n",
    "<td>0.31</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LSTM</td>\n",
    "<td>0.28</td>\n",
    "<td>0.29</td>\n",
    "<td>0.28</td>\n",
    "<td>0.29</td>\n",
    "<td>0.37</td>\n",
    "<td>0.33</td>\n",
    "<td>0.25</td>\n",
    "<td>0.27</td>\n",
    "<td>0.26</td>\n",
    "<td>0.29</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GRU</td>\n",
    "<td>0.29</td>\n",
    "<td>0.25</td>\n",
    "<td>0.26</td>\n",
    "<td>0.29</td>\n",
    "<td>0.4</td>\n",
    "<td>0.34</td>\n",
    "<td>0.25</td>\n",
    "<td>0.21</td>\n",
    "<td>0.22</td>\n",
    "<td>0.27</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"11\">Narrow Interpretation</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>CRF</td>\n",
    "<td>0.59</td>\n",
    "<td>0.64</td>\n",
    "<td>0.62</td>\n",
    "<td>0.26</td>\n",
    "<td>0.23</td>\n",
    "<td>0.24</td>\n",
    "<td>0.22</td>\n",
    "<td>0.20</td>\n",
    "<td>0.21</td>\n",
    "<td>0.36</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LSTM</td>\n",
    "<td>0.62</td>\n",
    "<td>0.65</td>\n",
    "<td>0.63</td>\n",
    "<td>0.3</td>\n",
    "<td>0.35</td>\n",
    "<td>0.32</td>\n",
    "<td>0.26</td>\n",
    "<td>0.14</td>\n",
    "<td>0.18</td>\n",
    "<td>0.38</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GRU</td>\n",
    "<td>0.62</td>\n",
    "<td>0.63</td>\n",
    "<td>0.62</td>\n",
    "<td>0.28</td>\n",
    "<td>0.33</td>\n",
    "<td>0.3</td>\n",
    "<td>0.23</td>\n",
    "<td>0.24</td>\n",
    "<td>0.23</td>\n",
    "<td>0.38</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "## Graph Structure of the Models\n",
    "\n",
    "* first- and higher-order linear chain CRFs;\n",
    "* first- and higher-order semi-Markov model;\n",
    "* tree-structured CRFs.\n",
    "\n",
    "TODO: Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graph Structure of the Models (Results)\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Element</td>\n",
    "<td colspan=\"9\">Structure</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>lcCRF$^1$</td>\n",
    "<td>lcCRF$^2$</td>\n",
    "<td>lcCRF$^3$</td>\n",
    "<td>lcCRF$^4$</td>\n",
    "<td>smCRF$^1$</td>\n",
    "<td>smCRF$^2$</td>\n",
    "<td>smCRF$^3$</td>\n",
    "<td>smCRF$^4$</td>\n",
    "<td>trCRF$^1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"10\">Training Set</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>0.928</td>\n",
    "<td>0.919</td>\n",
    "<td>0.922</td>\n",
    "<td>0.925</td>\n",
    "<td>0.931</td>\n",
    "<td>0.931</td>\n",
    "<td>0.933</td>\n",
    "<td>0.931</td>\n",
    "<td>0.906</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Source</td>\n",
    "<td>0.887</td>\n",
    "<td>0.876</td>\n",
    "<td>0.89</td>\n",
    "<td>0.901</td>\n",
    "<td>0.869</td>\n",
    "<td>0.886</td>\n",
    "<td>0.874</td>\n",
    "<td>0.878</td>\n",
    "<td>0.881</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Target</td>\n",
    "<td>0.898</td>\n",
    "<td>0.811</td>\n",
    "<td>0.816</td>\n",
    "<td>0.827</td>\n",
    "<td>0.813</td>\n",
    "<td>0.827</td>\n",
    "<td>0.815</td>\n",
    "<td>0.817</td>\n",
    "<td>0.876</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"10\">Development Set</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>0.345</td>\n",
    "<td>0.334</td>\n",
    "<td>0.332</td>\n",
    "<td>0.335</td>\n",
    "<td>0.395</td>\n",
    "<td>0.385</td>\n",
    "<td>0.389</td>\n",
    "<td>0.378</td>\n",
    "<td>0.331</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Source</td>\n",
    "<td>0.313</td>\n",
    "<td>0.32</td>\n",
    "<td>0.272</td>\n",
    "<td>0.304</td>\n",
    "<td>0.298</td>\n",
    "<td>0.282</td>\n",
    "<td>0.287</td>\n",
    "<td>0.291</td>\n",
    "<td>0.223</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Target</td>\n",
    "<td>0.258</td>\n",
    "<td>0.235</td>\n",
    "<td>0.24</td>\n",
    "<td>0.229</td>\n",
    "<td>0.287</td>\n",
    "<td>0.309</td>\n",
    "<td>0.301</td>\n",
    "<td>0.292</td>\n",
    "<td>0.243</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graph Structure of the Models (Results)\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Element</td>\n",
    "<td colspan=\"8\">Structure</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>lcLSTM$^1$</td>\n",
    "<td>lcLSTM$^2$</td>\n",
    "<td>lcLSTM$^3$</td>\n",
    "<td>lcGRU$^1$</td>\n",
    "<td>lcGRU$^2$</td>\n",
    "<td>lcGRU$^3$</td>\n",
    "<td>trLSTM$^1$</td>\n",
    "<td>trGRU$^1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"9\">Training Set</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Sentiment</td>\n",
    "<td>0.584</td>\n",
    "<td>0.559</td>\n",
    "<td>0.54</td>\n",
    "<td>0.57</td>\n",
    "<td>0.587</td>\n",
    "<td>0.606</td>\n",
    "<td>0.43</td>\n",
    "<td>0.518</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Source</td>\n",
    "<td>0.525</td>\n",
    "<td>0.458</td>\n",
    "<td>0.424</td>\n",
    "<td>0.503</td>\n",
    "<td>0.546</td>\n",
    "<td>0.548</td>\n",
    "<td>0.317</td>\n",
    "<td>0.372 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Target</td>\n",
    "<td>0.521</td>\n",
    "<td>0.513</td>\n",
    "<td>0.501</td>\n",
    "<td>0.519</td>\n",
    "<td>0.544</td>\n",
    "<td>0.605</td>\n",
    "<td>0.305</td>\n",
    "<td>0.425</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<tr>\n",
    "<td colspan=\"9\">Development Set</td>\n",
    "</tr>\n",
    "<td>Sentiment</td>\n",
    "<td>0.278</td>\n",
    "<td>0.285</td>\n",
    "<td>0.281</td>\n",
    "<td>0.335</td>\n",
    "<td>0.252</td>\n",
    "<td>0.253</td>\n",
    "<td>0.314</td>\n",
    "<td>0.292</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Source</td>\n",
    "<td>0.328</td>\n",
    "<td>0.314</td>\n",
    "<td>0.303</td>\n",
    "<td>0.263</td>\n",
    "<td>0.298</td>\n",
    "<td>0.306</td>\n",
    "<td>0.256</td>\n",
    "<td>0.262</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Target</td>\n",
    "<td>0.259</td>\n",
    "<td>0.218</td>\n",
    "<td>0.222</td>\n",
    "<td>0.216</td>\n",
    "<td>0.219</td>\n",
    "<td>0.188</td>\n",
    "<td>0.205</td>\n",
    "<td>0.193</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "## Effect of Text Normalization\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Data Set</td>\n",
    "<td colspan=\"3\">Sentiment</td>\n",
    "<td colspan=\"3\">Source</td>\n",
    "<td colspan=\"3\">Target</td>\n",
    "<td rowspan=\"2\">Macro-$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"11\">With Normalization</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>CRF</td>\n",
    "<td>0.376</td>\n",
    "<td>0.319</td>\n",
    "<td>0.345</td>\n",
    "<td>0.298</td>\n",
    "<td>0.33</td>\n",
    "<td>0.313</td>\n",
    "<td>0.293</td>\n",
    "<td>0.231</td>\n",
    "<td>0.258</td>\n",
    "<td>0.305</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LSTM</td>\n",
    "<td>0.283</td>\n",
    "<td>0.288</td>\n",
    "<td>0.278</td>\n",
    "<td>0.293</td>\n",
    "<td>0.372</td>\n",
    "<td>0.328</td>\n",
    "<td>0.254</td>\n",
    "<td>0.27</td>\n",
    "<td>0.259</td>\n",
    "<td>0.288</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GRU</td>\n",
    "<td>0.287</td>\n",
    "<td>0.246</td>\n",
    "<td>0.263</td>\n",
    "<td>0.287</td>\n",
    "<td>0.405</td>\n",
    "<td>0.335</td>\n",
    "<td>0.252</td>\n",
    "<td>0.205</td>\n",
    "<td>0.216</td>\n",
    "<td>0.271</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"11\">Without Normalization</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>CRF</td>\n",
    "<td>0.301</td>\n",
    "<td>0.278</td>\n",
    "<td>0.289</td>\n",
    "<td>0.276</td>\n",
    "<td>0.3</td>\n",
    "<td>0.287</td>\n",
    "<td>0.255</td>\n",
    "<td>0.23</td>\n",
    "<td>0.242</td>\n",
    "<td>0.273</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LSTM</td>\n",
    "<td>0.274</td>\n",
    "<td>0.252</td>\n",
    "<td>0.261</td>\n",
    "<td>0.284</td>\n",
    "<td>0.367</td>\n",
    "<td>0.32</td>\n",
    "<td>0.237</td>\n",
    "<td>0.241</td>\n",
    "<td>0.237</td>\n",
    "<td>0.273</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GRU</td>\n",
    "<td>0.266</td>\n",
    "<td>0.245</td>\n",
    "<td>0.252</td>\n",
    "<td>0.296</td>\n",
    "<td>0.369</td>\n",
    "<td>0.328</td>\n",
    "<td>0.232</td>\n",
    "<td>0.268</td>\n",
    "<td>0.245</td>\n",
    "<td>0.275</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary and Concusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* CRFs can learn meaningful weights for state- and\n",
    "  transition-features, although different features types might have\n",
    "  different effects on classification of opinion elements: whereas\n",
    "  sentiments benefited from all features used in our\n",
    "  experiments, sources profited most from lexical and\n",
    "  complex attributes, and targets were positively\n",
    "  influenced by morphological and syntactic features only;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Apart from that, we analyzed the effect of different embedding\n",
    "  types on the net results of RNN systems, finding that least-squares\n",
    "  embeddings yield the best overall scores for these methods;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Furthermore, even higher prediction scores for\n",
    "  \\markable{sentiment}s can be achieved by narrowing the spans of\n",
    "  these elements to polar terms.  This, however, might negatively\n",
    "  affect the classification of \\markable{source}s and\n",
    "  \\markable{target}s;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Even though context seems to play an important role, redefining\n",
    "  models' structures by increasing the order of their dependencies or\n",
    "  performing inference over trees instead of linear sequences does not\n",
    "  bring much improvement.  I could, however, still outperform the\n",
    "  results of traditional first-order linear-chain CRFs with their\n",
    "  first- and second-order semi-Markov modifications;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* In the final step, I estimated the effect of text normalization\n",
    "  by rerunning all experiments with original (unnormalized) tweets.\n",
    "  This test showed that preprocessing is an extremely helpful\n",
    "  procedure, which might improve the results of ABSA methods by up to\n",
    "  3%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter V: Message-Level Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Task\n",
    "\n",
    "Given an input tweet $t$, our task is to automatically determine the polarity (positive 😊, negative ☹️, or neutral 😐) of that message.\n",
    "\n",
    "<div class=\"example\">\n",
    "  TODO Provide an example\n",
    "  <div class=\"translation\">TODO</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "To estimate the quality of compared systems, I rely on two established evaluation metrics that are commonly used to measure the MLSA results:\n",
    "\n",
    "* **macro-averaged $F_1$-score** over two main polarity classes (positive and negative):\n",
    "$$F_1 = \\frac{F_{pos} + F_{neg}}{2} $$'\n",
    "\n",
    "* and **micro-averaged $F_1$-score** over all three semantic orientations (positive, negative, and neutral), which essentially corresponds to accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "As in the previous experiments, I\n",
    "* preprocessed all tweets with the text normalization system of Sidarenka et al. (2013},\n",
    "* tokenized them with the adjusted version of Christopher Potts' tokenizer,\n",
    "* lemmatized and assigned part-of-speech tags to these tokens with the TreeTagger of Schmid (1995),\n",
    "* and obtained morphological and syntactic analyses with the Mate dependency parser (Bohnet et al., 2013);\n",
    "* I again split the 70-10-20 percent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Preparation (Inference of Gold Labels)\n",
    "\n",
    "Since the PotTS corpus, however, did not provide explicit message-level gold labels, I used a simple heuristic rule to derive these labels from existing annotations.  In partucular:\n",
    "\n",
    "* I assigned the positive (negative) label to the microblogs that had exclusively positive (negative) sentiments;\n",
    "* Messages that did not have any sentiments, but had exclusively positive (negative) polar terms were also assigned the respective label;\n",
    "* Tweets that featured sentiments of both polarities or had both positive and negative polar terms were considered as mixed and skipped from our experiments;\n",
    "* Finally, all remaining microblogs were regarded as neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Preparation (Inference of Gold Labels)\n",
    "\n",
    "<div class=\"example\">\n",
    "Ich finde den Papst <div class=\"emoexpression positive\">putzig</div> <div class=\"emoexpression positive\">🙂</div>\n",
    "<div class=\"translation\">I find the Pope <div class=\"emoexpression positive\">cute</div> <div class=\"emoexpression positive\">🙂</div></div>\n",
    "<div class=\"label positive\">positive</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"example\">\n",
    "<div class=\"emoexpression negative\">typisch</div> Bayern kaum ist der neue Papst da und schon haben sie ihn <div class=\"emoexpression negative\">in der Tasche</div>&#8230;\n",
    "<div class=\"translation\"><div class=\"emoexpression negative\">typical</div></div> Bavaria The new Pope is hardly there, as they already have him <div class=\"emoexpression negative\">in their pocket</div>\n",
    "<div class=\"label negative\">negative</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"example\">\n",
    "Unser Park, unser Geld, unsere Stadt! -NICHT unser Finanzminister! <div class=\"emoexpression positive\">🙂</div> #schmid #spd #s21 #btw13\n",
    "<div class=\"translation\">Our park, our money, our city! -NOT our Finance Minister! <div class=\"emoexpression positive\">🙂</div> #schmid #spd #s21 #btw13</div>\n",
    "<div class=\"label positive erroneous\">positive</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"example\">\n",
    "Auf die Lobby-FDP von heute kann Deutschland verzichten&#8230;\n",
    "<div class=\"translation\">Germany can go without today's lobby FDP</div>\n",
    "<div class=\"label positive erroneous\">neutral</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Preparation (SB10k)\n",
    "\n",
    "The SB10k dataset comprises a total of 9,738 microblogs, which were sampled from a\n",
    "larger snapshot of 5M German tweets gathered between August and November 2013. To\n",
    "ensure lexical diversity and proportional polarity distribution in this corpus, the authors\n",
    "first grouped all posts of this snapshot into 2,500 clusters using k-means with unigram\n",
    "features. Afterwards, from each of these groups, they selected tweets that contained at least\n",
    "one positive or one negative term from the German Polarity Clues lexicon (Waltinger, 2010), and the n let three human experts annotate these microblogs with their message-level polarity (positive, negative, neutral, or mixed). Unfortunately, due to the restrictions of\n",
    "Twitter’s terms of use, I could only retrieve 7,476 tweets of this collection, which, however, still represents\n",
    "a substantial part of the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Preparation (Statistics)\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    " <caption><caption>Polarity class distribution in PotTS, SB10k<br/>\n",
    "(&#42; – the mixed polarity was excluded from our experiments)</caption></caption>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Dataset</td>\n",
    "<td colspan=\"4\">Polarity Class</td>\n",
    "<td colspan=\"2\">Label Agreement</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Positive} </td>\n",
    "<td>Negative</td>\n",
    "<td>Neutral</td>\n",
    "<td>Mixed*</td>\n",
    "<td>$\\alpha$ </td>\n",
    "<td>$\\kappa$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>PotTS</td>\n",
    "<td>3,380</td>\n",
    "<td>1,541</td>\n",
    "<td>2,558</td>\n",
    "<td>513</td>\n",
    "<td>0.66</td>\n",
    "<td>0.4</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SB10k</td>\n",
    "<td>1,717</td>\n",
    "<td>1,130</td>\n",
    "<td>4,629</td>\n",
    "<td>0</td>\n",
    "<td>0.39</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<!--\n",
    "<tr>\n",
    "<td>GTS</td>\n",
    "<td>3,326,829</td>\n",
    "<td>350,775</td>\n",
    "<td>19,453,669</td>\n",
    "<td>73,776</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "-->\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Methods\n",
    "\n",
    "* lexicon-based methods;\n",
    "* machine-learning&ensp;based methods;\n",
    "* deep-learing&ensp;based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lexicon-Based Methods\n",
    "\n",
    "Given a tweet $t$, a lexicon-based method determines the overall polarity of that tweet $y\\in\\{\\mathrm{positive},\\mathrm{negative},\\mathrm{neutral}\\}$ by summing the scores of polar terms (taken from a sentiment lexicon), possibly modifying these scores by some context factors (intensifiers, downtoners, negations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The most well-known lexicon-based systems are the methods of:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Hu and Liu (2004);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Taboada et al. (2011);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Musto et al. (2014);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* and  Kolchyna et al. (2015)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lexicon-Based Methods (Results)\n",
    "\n",
    "<table>\n",
    "    <caption>Results of lexicon-based MLSA methods<br/>\n",
    "HL &emsp; Hu and Liu (2004), TBD &emsp; Taboada et al. (2011), MST &emsp; Musto et al. (2014), JRK &emsp; Jurek\n",
    "et al. (2015), KLCH &emsp; Kolchyna et al. (2015)</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro-$F_1^{+/-}$</td>\n",
    "<td rowspan=\"2\">Micro-$F_1^$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision </td>\n",
    "<td>Recall </td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision </td>\n",
    "<td>Recall </td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision </td>\n",
    "<td>Recall </td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>HL</td>\n",
    "<td>0.75</td>\n",
    "<td>0.76</td>\n",
    "<td>0.76</td>\n",
    "<td>0.53</td>\n",
    "<td>0.43</td>\n",
    "<td>0.47</td>\n",
    "<td>0.67</td>\n",
    "<td>0.73</td>\n",
    "<td>0.69</td>\n",
    "<td>0.615</td>\n",
    "<td>0.685</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TBD</td>\n",
    "<td>0.77</td>\n",
    "<td>0.71</td>\n",
    "<td>0.74</td>\n",
    "<td>0.54</td>\n",
    "<td>0.39</td>\n",
    "<td>0.45</td>\n",
    "<td>0.63</td>\n",
    "<td>0.77</td>\n",
    "<td>0.69</td>\n",
    "<td>0.597</td>\n",
    "<td>0.674</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MST</td>\n",
    "<td>0.75</td>\n",
    "<td>0.72</td>\n",
    "<td>0.74</td>\n",
    "<td>0.48</td>\n",
    "<td>0.47</td>\n",
    "<td>0.48</td>\n",
    "<td>0.68</td>\n",
    "<td>0.72</td>\n",
    "<td>0.7</td>\n",
    "<td>0.606</td>\n",
    "<td>0.675</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>JRK</td>\n",
    "<td>0.6</td>\n",
    "<td>0.31</td>\n",
    "<td>0.41</td>\n",
    "<td>0.42</td>\n",
    "<td>0.2</td>\n",
    "<td>0.27</td>\n",
    "<td>0.43</td>\n",
    "<td>0.8</td>\n",
    "<td>0.56</td>\n",
    "<td>0.339</td>\n",
    "<td>0.467</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>KLCH</td>\n",
    "<td>0.71</td>\n",
    "<td>0.72</td>\n",
    "<td>0.71</td>\n",
    "<td>0.34</td>\n",
    "<td>0.17</td>\n",
    "<td>0.22</td>\n",
    "<td>0.66</td>\n",
    "<td>0.82</td>\n",
    "<td>0.73</td>\n",
    "<td>0.468</td>\n",
    "<td>0.651</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HL</td>\n",
    "<td>0.49</td>\n",
    "<td>0.62</td>\n",
    "<td>0.55</td>\n",
    "<td>0.27</td>\n",
    "<td>0.33</td>\n",
    "<td>0.3</td>\n",
    "<td>0.73</td>\n",
    "<td>0.62</td>\n",
    "<td>0.67</td>\n",
    "<td>0.421</td>\n",
    "<td>0.577</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TBD</td>\n",
    "<td>0.48</td>\n",
    "<td>0.6</td>\n",
    "<td>0.53</td>\n",
    "<td>0.24</td>\n",
    "<td>0.27</td>\n",
    "<td>0.25</td>\n",
    "<td>0.72</td>\n",
    "<td>0.63</td>\n",
    "<td>0.67</td>\n",
    "<td>0.393</td>\n",
    "<td>0.57</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MST</td>\n",
    "<td>0.45</td>\n",
    "<td>0.49</td>\n",
    "<td>0.47</td>\n",
    "<td>0.29</td>\n",
    "<td>0.35</td>\n",
    "<td>0.32</td>\n",
    "<td>0.7</td>\n",
    "<td>0.64</td>\n",
    "<td>0.67</td>\n",
    "<td>0.395</td>\n",
    "<td>0.568</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>JRK</td>\n",
    "<td>0.41</td>\n",
    "<td>0.39</td>\n",
    "<td>0.4</td>\n",
    "<td>0.36</td>\n",
    "<td>0.26</td>\n",
    "<td>0.3</td>\n",
    "<td>0.69</td>\n",
    "<td>0.75</td>\n",
    "<td>0.72</td>\n",
    "<td>0.351</td>\n",
    "<td>0.592</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>KLCH</td>\n",
    "<td>0.39</td>\n",
    "<td>0.22</td>\n",
    "<td>0.28</td>\n",
    "<td>0.34</td>\n",
    "<td>0.13</td>\n",
    "<td>0.19</td>\n",
    "<td>0.66</td>\n",
    "<td>0.86</td>\n",
    "<td>0.75</td>\n",
    "<td>0.235</td>\n",
    "<td>0.606</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon-Based Methods (An Error Made by the System of Taboada et al.)\n",
    "\n",
    "<div class=\"example\">\n",
    "Der beste Microsoft Knowledgebase-Artikel, den ich je gelesen habe.\n",
    "<div class=\"translation\">The best Microsoft-Knowledgebase article I've ever read.</div>\n",
    "Gold Label:<div class=\"label positive\">positive</div>\n",
    "Predicted Label:<div class=\"label neutral\">neutral*</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon-Based Methods (An Error Made by the System of Musto et al.)\n",
    "\n",
    "<div class=\"example\">\n",
    "Mensch Meier, Mensch Meier! Das sieht gut aus f&uuml;r die %User:\n",
    "<div class=\"translation\">Gosh Meier, Gosh Meier! It looks good for the %User:</div>\n",
    "Gold Label:<div class=\"label positive\">positive</div>\n",
    "Predicted Label:<div class=\"label neutral\">neutral*</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon-Based Methods (An Error Made by the System of Jurek et al.)\n",
    "\n",
    "<div class=\"example\">\n",
    "Normal bin ich ja nicht der mensch dwer sich beschwert wegen dem essen aber diese Pizza von Joeys&8230; boah wie ekelhaft\n",
    "<div class=\"translation\">Normally I'm not a person who complains about food but  this pizza from Joeys&8230; Boah it's so disgusting</div>\n",
    "Gold Label:<div class=\"label negative\">negative</div>\n",
    "Predicted Label:<div class=\"label positive\">positive*</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lexicon-Based Methods (Effect of Polarity Changing Factors)\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Effect of polarity-changing factors on lexicon-based MLSA methods\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"3\">Polarity-Changing Factors</td>\n",
    "<td colspan=\"10\">System Scores</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"2\">HL</td>\n",
    "<td colspan=\"2\">TBD</td>\n",
    "<td colspan=\"2\">MST</td>\n",
    "<td colspan=\"2\">JRK</td>\n",
    "<td colspan=\"2\">KLCH</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td> Macro-$F_1$</td>\n",
    "<td> Micro-$F_1$</td>\n",
    "<td> Macro-$F_1$</td>\n",
    "<td> Micro-$F_1$</td>\n",
    "<td> Macro-$F_1$</td>\n",
    "<td> Micro-$F_1$</td>\n",
    "<td> Macro-$F_1$</td>\n",
    "<td> Micro-$F_1$</td>\n",
    "<td> Macro-$F_1$</td>\n",
    "<td> Micro-$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"11\">\n",
    "PotTS\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>All</td>\n",
    "<td>0.615</td>\n",
    "<td>0.685</td>\n",
    "<td>0.593</td>\n",
    "<td>0.671</td>\n",
    "<td>0.606</td>\n",
    "<td>0.675</td>\n",
    "<td>0.339</td>\n",
    "<td>0.467</td>\n",
    "<td>0.468</td>\n",
    "<td>0.651</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Negation</td>\n",
    "<td>0.622</td>\n",
    "<td>0.691</td>\n",
    "<td>0.596</td>\n",
    "<td>0.672</td>\n",
    "<td>0.641</td>\n",
    "<td>0.7</td>\n",
    "<td>0.357</td>\n",
    "<td>0.473</td>\n",
    "<td>0.298</td>\n",
    "<td>0.463</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Intensification</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.595</td>\n",
    "<td>0.672</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.339</td>\n",
    "<td>0.467</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Other Modifiers</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.613</td>\n",
    "<td>0.684</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"11\">\n",
    "SB10k\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>All</td>\n",
    "<td>0.421</td>\n",
    "<td>0.577</td>\n",
    "<td>0.392</td>\n",
    "<td>0.569</td>\n",
    "<td>0.395</td>\n",
    "<td>0.568</td>\n",
    "<td>0.351</td>\n",
    "<td>0.592</td>\n",
    "<td>0.235</td>\n",
    "<td>0.606</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Negation</td>\n",
    "<td>0.415</td>\n",
    "<td>0.576</td>\n",
    "<td>0.395</td>\n",
    "<td>0.572</td>\n",
    "<td>0.381</td>\n",
    "<td>0.559</td>\n",
    "<td>0.316</td>\n",
    "<td>0.586</td>\n",
    "<td>0.218</td>\n",
    "<td>0.609</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Intensification</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.4</td>\n",
    "<td>0.576</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.352</td>\n",
    "<td>0.59</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Other Modifiers</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.406</td>\n",
    "<td>0.566</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML-Based Methods\n",
    "\n",
    "Given a tweet $t$, a machine-learning&ndash;based method determines the overall polarity of that tweet $y\\in\\{\\mathrm{positive},\\mathrm{negative},\\mathrm{neutral}\\}$ by multiplying the values of its features with their automatically learned coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representatives\n",
    "\n",
    "* One of the earliest ML-based systems for message-level sentiment classification was proposed by **Gamon (2004)**, who used an SVM classifier with linguistic and surface-level features, such as\n",
    "  * part-of-speech trigrams,\n",
    "  * context-free phrase-structure patterns,\n",
    "   * and part-of-speech information coupled with syntactic relations) to distinguish between positive and negative customer feedback;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Following by the successes of SVMs at various NLP tasks, these systems also rapidly gained ground at the initial runs of SemEval competition on sentiment analysis in (English) Twitter (Nakov et al., 2013; Rosenthal et al, 2014), with the most prominent representatives being:\n",
    "* the system by Mohammad et al. (Mohammad et al., 2013), which relied on an extensive set of features:\n",
    "  * character and token $n$-grams;\n",
    "  * Brown clusters (Brown et al., 1992);\n",
    "  * statistics on part-of-speech tags, punctuation marks, and elongated words;\n",
    "  * numerous sentiment-lexicon features, extracted from both manual and automatic lexicons;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* the system by G&uuml;nther and Furrer (&uuml;nther and Furrer, 2013), which also used on an extensive set of manually-defined attributes, including:\n",
    "  * original and lemmatized unigrams;\n",
    "  * word clusters;\n",
    "  * and lexicon features (only SentiWordNet [Esuli and Sebastiani, 2005])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML-Based Methods (Results)\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Results of machine-learning&ndash;based MLSA methods<br/>\n",
    "GMN &mdash; Gamon (2004), MHM &mdash; Mohammad et al. (2013), GNT &mdash; Günther et al. (2014)\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro$F_1$</td>\n",
    "<td rowspan=\"2\">Macro$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\">\n",
    "PotTS\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GMN</td>\n",
    "<td>0.67</td>\n",
    "<td>0.73</td>\n",
    "<td>0.7</td>\n",
    "<td>0.35</td>\n",
    "<td>0.15</td>\n",
    "<td>0.21</td>\n",
    "<td>0.6</td>\n",
    "<td>0.72</td>\n",
    "<td>0.66</td>\n",
    "<td>0.453</td>\n",
    "<td>0.617</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MHM</td>\n",
    "<td class=\"best\">0.79</td>\n",
    "<td>0.77</td>\n",
    "<td class=\"best\">0.78</td>\n",
    "<td class=\"best\">0.58</td>\n",
    "<td class=\"best\">0.56</td>\n",
    "<td class=\"best\">0.57</td>\n",
    "<td class=\"best\">0.73</td>\n",
    "<td class=\"best\">0.76</td>\n",
    "<td class=\"best\">0.74</td>\n",
    "<td class=\"best\">0.674</td>\n",
    "<td class=\"best\">0.727</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GNT</td>\n",
    "<td>0.71</td>\n",
    "<td class=\"best\">0.8</td>\n",
    "<td>0.75</td>\n",
    "<td>0.55</td>\n",
    "<td>0.45</td>\n",
    "<td>0.5</td>\n",
    "<td>0.68</td>\n",
    "<td>0.63</td>\n",
    "<td>0.65</td>\n",
    "<td>0.624</td>\n",
    "<td>0.673</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"12\">\n",
    "SB10k\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GMN</td>\n",
    "<td>0.65</td>\n",
    "<td>0.45</td>\n",
    "<td>0.53</td>\n",
    "<td>0.38</td>\n",
    "<td>0.08</td>\n",
    "<td>0.13</td>\n",
    "<td>0.72</td>\n",
    "<td class=\"best\">0.93</td>\n",
    "<td>0.81</td>\n",
    "<td>0.329</td>\n",
    "<td>0.699</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MHM</td>\n",
    "<td class=\"best\">0.71</td>\n",
    "<td class=\"best\">0.65</td>\n",
    "<td class=\"best\">0.68</td>\n",
    "<td class=\"best\">0.51</td>\n",
    "<td class=\"best\">0.4</td>\n",
    "<td class=\"best\">0.45</td>\n",
    "<td class=\"best\">0.8</td>\n",
    "<td>0.87</td>\n",
    "<td class=\"best\">0.84</td>\n",
    "<td class=\"best\">0.564</td>\n",
    "<td class=\"best\">0.752</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GNT</td>\n",
    "<td>0.67</td>\n",
    "<td>0.62</td>\n",
    "<td>0.64</td>\n",
    "<td>0.44</td>\n",
    "<td>0.28</td>\n",
    "<td>0.34</td>\n",
    "<td>0.78</td>\n",
    "<td>0.87</td>\n",
    "<td>0.82</td>\n",
    "<td>0.491</td>\n",
    "<td>0.724</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML-Based Methods (An Error Made by the System of Mohammad et al.)\n",
    "\n",
    "<div class=\"example\">\n",
    "das klingt richtig gut! Was f&uuml;r eine hast du denn? (uvu) %PosSmiley3\n",
    "<div class=\"translation\">It sounds really great. Which one do you have? (uvu) %PosSmiley3</div>\n",
    "Gold Label:<div class=\"label positive\">positive</div>\n",
    "Predicted Label:<div class=\"label neutral\">neutral*</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top-ranking features of this example:\n",
    "\n",
    "1. &#42; (neutral): 0.131225868029;\n",
    "2. &#42; (negative): -0.0840804221845;\n",
    "3. %PoS-CARD (neutral): 0.0833658576233;\n",
    "4. %PoS-ADJD (neutral): -0.069745190018;\n",
    "5. t-␣-n (positive): 0.0556721202587."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML-Based Methods (An Error Made by the System of G&uuml;nther et al.)\n",
    "\n",
    "<div class=\"example\">\n",
    "Den CDU-Wählern traue ich durchaus zu der FDP 8 bis 9% zu bescheren! Die sind so borniert, nicht nur in Niedersachsen!\n",
    "<div class=\"translation\">I don't put giving 8 to 9% to the FDP past the CDU-voters! They are so narrow-minded, not only in Lower Saxony!</div>\n",
    "Gold Label:<div class=\"label negative\">negative</div>\n",
    "Predicted Label:<div class=\"label positive\">positive*</div>\n",
    "</div>\n",
    "\n",
    "The resason for this misclassification is the prevalence of many general features (e.g., 8, nicht-nur_NEG, nur_NEG, etc.) and their strong bias towards the majority class in the PotTS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML-Based Methods (Feature-Ablation Test)\n",
    "\n",
    "<table>\n",
    "    <caption>\n",
    "        Results of the feature-ablation test for ML-based MLSA methods\n",
    "    </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"3\">Features</td>\n",
    "<td colspan=\"6\">System Scores</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"2\">GMN</td>\n",
    "<td colspan=\"2\">MHM</td>\n",
    "<td colspan=\"2\">GNT</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Macro-$F_1^{+/-}$</td>\n",
    "<td>Micro-$F$</td>\n",
    "<td>Macro-$F_1^{+/-}$</td>\n",
    "<td>Micro-$F$</td>\n",
    "<td>Macro-$F_1^{+/-}$</td>\n",
    "<td>Micro-$F$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"7\">\n",
    "PotTS\n",
    "</td>\n",
    "</tr>   \n",
    "<tr>\n",
    "<td>All</td>\n",
    "<td>0.453</td>\n",
    "<td>0.617</td>\n",
    "<td>0.674</td>\n",
    "<td>0.727</td>\n",
    "<td>0.624</td>\n",
    "<td>0.673</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Constituents</td>\n",
    "<td>0.388</td>\n",
    "<td>0.545</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--PoS Tags</td>\n",
    "<td>0.417</td>\n",
    "<td>0.607</td>\n",
    "<td>0.669</td>\n",
    "<td>0.721</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Character Features</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.671</td>\n",
    "<td>0.734</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Token Features</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.659</td>\n",
    "<td>0.704</td>\n",
    "<td>0.0</td>\n",
    "<td>0.366</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Automatic Lexicons</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.667</td>\n",
    "<td>0.717</td>\n",
    "<td>0.613</td>\n",
    "<td>0.666</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Manual Lexicons</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.665</td>\n",
    "<td>0.715</td>\n",
    "<td>0.617</td>\n",
    "<td>0.675</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"7\">\n",
    "SB10k\n",
    "</td>\n",
    "<tr>\n",
    "<tr>\n",
    "<td>All</td>\n",
    "<td>0.329</td>\n",
    "<td>0.699</td>\n",
    "<td>0.564</td>\n",
    "<td>0.752</td>\n",
    "<td>0.491</td>\n",
    "<td>0.724</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Constituents</td>\n",
    "<td>0.127</td>\n",
    "<td>0.646</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--PoS Tags</td>\n",
    "<td>0.301</td>\n",
    "<td>0.7</td>\n",
    "<td>0.57</td>\n",
    "<td>0.757</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Character Features</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.546</td>\n",
    "<td>0.753</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Token Features</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.559</td>\n",
    "<td>0.741</td>\n",
    "<td>0.046</td>\n",
    "<td>0.62</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Automatic Lexicons</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.54</td>\n",
    "<td>0.753</td>\n",
    "<td>0.517</td>\n",
    "<td>0.735</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>--Manual Lexicons</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td>0.553</td>\n",
    "<td>0.751</td>\n",
    "<td>0.51</td>\n",
    "<td>0.739</td>\n",
    "    </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML-Based Methods (Top-10 Features)\n",
    "\n",
    "<table>\n",
    "    <caption>\n",
    "       Top-10 features learned by ML-based MLSA methods<br />\n",
    "(sorted by the absolute values of their weights)\n",
    "    </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Rank</td>\n",
    "<td colspan=\"3\">GMN</td>\n",
    "<td colspan=\"3\">MHM</td>\n",
    "<td colspan=\"3\">GNT</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Feature</td>\n",
    "<td>Label</td>\n",
    "<td>Weight</td>\n",
    "<td>Feature</td>\n",
    "<td>Label</td>\n",
    "<td>Weight</td>\n",
    "<td>Feature</td>\n",
    "<td>Label</td>\n",
    "<td>Weight</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>1</td>\n",
    "<td>NK-ITJ|</td>\n",
    "<td>POS</td>\n",
    "<td>0.457</td>\n",
    "<td>*</td>\n",
    "<td>NEUT</td>\n",
    "<td>0.131</td>\n",
    "<td>hate</td>\n",
    "<td>NEG</td>\n",
    "<td>1.86 </td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>2</td>\n",
    "<td>DM-ITJ|</td>\n",
    "<td>POS</td>\n",
    "<td>0.334</td>\n",
    "<td>Last-%QMark-Cnt</td>\n",
    "<td>NEUT</td>\n",
    "<td>0.088</td>\n",
    "<td>sick</td>\n",
    "<td>NEG</td>\n",
    "<td>1.7</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>3</td>\n",
    "<td>V-DM-I</td>\n",
    "<td>POS</td>\n",
    "<td>0.244</td>\n",
    "<td>s-c</td>\n",
    "<td>NEG</td>\n",
    "<td>0.079</td>\n",
    "<td>kahretsinn</td>\n",
    "<td>NEG</td>\n",
    "<td>1.69</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>4</td>\n",
    "<td>N-NK-I</td>\n",
    "<td>POS</td>\n",
    "<td>0.24</td>\n",
    "<td>*-%possmiley</td>\n",
    "<td>POS</td>\n",
    "<td>0.067</td>\n",
    "<td>dasisaberschade</td>\n",
    "<td>NEG</td>\n",
    "<td>1.69</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>5</td>\n",
    "<td>MO-ITJ|</td>\n",
    "<td>POS</td>\n",
    "<td>0.211</td>\n",
    "<td>c-h-e-i-s</td>\n",
    "<td>NEG</td>\n",
    "<td>0.064</td>\n",
    "<td>Anziehen</td>\n",
    "<td>POS</td>\n",
    "<td>1.67</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>6</td>\n",
    "<td>A-DM-I</td>\n",
    "<td>POS</td>\n",
    "<td>0.196</td>\n",
    "<td>h-a-h</td>\n",
    "<td>POS</td>\n",
    "<td>0.064</td>\n",
    "<td>&#7452;</td>\n",
    "<td>POS</td>\n",
    "<td>1.65</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>7</td>\n",
    "<td>A-MO-I</td>\n",
    "<td>POS</td>\n",
    "<td>0.191</td>\n",
    "<td>t-&blank;-.</td>\n",
    "<td>NEG</td>\n",
    "<td>0.064</td>\n",
    "<td>p&auml;rchenabend</td>\n",
    "<td>POS</td>\n",
    "<td>1.65</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>8</td>\n",
    "<td>NK-ITJ</td>\n",
    "<td>POS</td>\n",
    "<td>0.165</td>\n",
    "<td>geil</td>\n",
    "<td>POS</td>\n",
    "<td>0.062</td>\n",
    "<td>derien❤️❤️</td>\n",
    "<td>POS</td>\n",
    "<td>1.65</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>9</td>\n",
    "<td>NK-$.</td>\n",
    "<td>NEUT</td>\n",
    "<td>0.16</td>\n",
    "<td>*-?</td>\n",
    "<td>NEUT</td>\n",
    "<td>0.062</td>\n",
    "<td>sch&ouml;n-nicht</td>\n",
    "<td>POS</td>\n",
    "<td>1.56</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>10</td>\n",
    "<td>DM-ITJ</td>\n",
    "<td>POS</td>\n",
    "<td>0.157</td>\n",
    "<td>?</td>\n",
    "<td>NEUT</td>\n",
    "<td>0.061</td>\n",
    "<td>applause</td>\n",
    "<td>POS</td>\n",
    "<td>1.5</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ML-Based Methods (Effect of Classifiers)\n",
    "\n",
    "<table>\n",
    "    <caption>\n",
    "       Results of ML-based MLSA methods with different classifiers\n",
    "    </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"3\">Classifier</td>\n",
    "<td colspan=\"6\">System Scores</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"2\">GMN</td>\n",
    "<td colspan=\"2\">MHM</td>\n",
    "<td colspan=\"2\">GNT</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Macro-$F_1^{+/-}$</td>\n",
    "<td>Micro-$F_1$</td>\n",
    "<td>Macro-$F_1^{+/-}$</td>\n",
    "<td>Micro-$F_1$</td>\n",
    "<td>Macro-$F_1^{+/-}$</td>\n",
    "<td>Micro-$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"7\">\n",
    "PotTS\n",
    "</td>\n",
    "</tr>   \n",
    "<tr>\n",
    "<td>SVM</td>\n",
    "<td>0.453</td>\n",
    "<td>0.617</td>\n",
    "<td>0.674</td>\n",
    "<td>0.727</td>\n",
    "<td>0.624</td>\n",
    "<td>0.673</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Naive Bayes</td>\n",
    "<td>0.432</td>\n",
    "<td>0.577</td>\n",
    "<td>0.635</td>\n",
    "<td>0.675</td>\n",
    "<td>0.567</td>\n",
    "<td>0.59</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Logistic Regression</td>\n",
    "<td>0.431</td>\n",
    "<td>0.612</td>\n",
    "<td>0.677</td>\n",
    "<td>0.741</td>\n",
    "<td>0.624</td>\n",
    "<td>0.688</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"7\">\n",
    "SB10k\n",
    "</td>\n",
    "</tr>   \n",
    "<tr>\n",
    "<td>SVM</td>\n",
    "<td>0.329</td>\n",
    "<td>0.699</td>\n",
    "<td>0.564</td>\n",
    "<td>0.752</td>\n",
    "<td>0.491</td>\n",
    "<td>0.724</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Naive Bayes</td>\n",
    "<td>0.351</td>\n",
    "<td>0.637</td>\n",
    "<td>0.516</td>\n",
    "<td>0.755</td>\n",
    "<td>0.453</td>\n",
    "<td>0.675</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Logistic Regression</td>\n",
    "<td>0.309</td>\n",
    "<td>0.693</td>\n",
    "<td>0.553</td>\n",
    "<td>0.772</td>\n",
    "<td>0.512</td>\n",
    "<td>0.75</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DL-Based Methods\n",
    "\n",
    "Given a tweet $t$, a deep-learning method determines the overall polarity of that tweet $y\\in\\{\\mathrm{positive},\\mathrm{negative},\\mathrm{neutral}\\}$ by taking the embedding vectors of its words as input, passing them through a neural network, which produces a vector of (unnormalized) probabilities for each possible polarity class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The most common representatives of DL-based systems are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. The **matrix-space** approach by Yessenalina and Cardie (2011):\n",
    "$$\\xi = \\vec{u}^\\top\\left(\\prod_{j=1}^{|x|}W_{w_j}\\right)\\vec{v},$$\n",
    "where $W_{w_j}\\in\\mathcal{R}^{m\\times{}m}$ is matrix representation of word $w_j$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The **deep recursive autoencoder** (RAE) method by Socher et al. (2011):\n",
    "$$\\vec{w}_p = softmax\\left(W\\begin{bmatrix}\n",
    "      \\vec{w}_l\\\\\n",
    "      \\vec{w}_r\n",
    "  \\end{bmatrix}\\right),$$\n",
    "where $\\vec{w}_p\\in\\mathbb{R}^n$ stands for the embedding of the parent node, and $\\vec{w}_l$ and $\\vec{w}_r$ represent the embeddings of its left and right dependents, and $W\\in\\mathbb{R}^{n\\times{}2n}$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3. The **matrix-vector recursive neural network** (MVRNN) by Socher et al. (2012):\n",
    "$$  \\vec{w}_p = tanh\\left(W_v \\begin{bmatrix}W_r\\vec{w}_l\\\\\n",
    "      W_l \\vec{w}_r\\end{bmatrix} \\right),\\\\\n",
    "  W_p = W_m \\begin{bmatrix}W_l;\\\\\n",
    "    W_r\\end{bmatrix};\n",
    "$$\n",
    "where, in addition to the previously defined word vectors $\\vec{w}_p$, $\\vec{w}_l$, and $\\vec{w}_r$, the authors also associated a matrix parameter with each of these words ($W_p$, $W_l$, $W_r$);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "4. The **recursive neural tensor network** (RNTN) by Socher et al. (2013):\n",
    "$$\\vec{w}_p = softmax\\left(\\begin{bmatrix}\n",
    "  \\vec{w}_l\\\\\n",
    "  \\vec{w}_r\n",
    "  \\end{bmatrix}^{\\top}V^{[1:d]}\\begin{bmatrix}\n",
    "  \\vec{w}_l\\\\\n",
    "  \\vec{w}_r\n",
    "  \\end{bmatrix}\n",
    "            + W\\begin{bmatrix}\n",
    "  \\vec{w}_l\\\\\n",
    "  \\vec{w}_r\n",
    "\\end{bmatrix}\\right),\n",
    "$$\n",
    "where $\\vec{w}_p$, $\\vec{w}_l$, $\\vec{w}_r$, and $W$ are defined as before, and $V$ is a $2n\\times{}2n\\times{}n$-dimensional tensor;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The **convolutional system** by Severyn and Moschitti (2015), which was the winner of the SemEval task in 2015;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "6. The **attention system** by Baziotis et al. (2017):\n",
    "<figure>\n",
    "<img src=\"img/baziotis.png\" alt=\"Architecture of the neural network proposed by Baziotis et al. (2017)\">\n",
    "<figcaption>\n",
    "Architecture of the neural network proposed by Baziotis et al. (2017)\n",
    "</figcaption>\n",
    "</figure>\n",
    "in which the attention coefficients are estimated as follows:\n",
    "$$\\vec{a} = \\sum_{i=1}^{|\\mathbf{x}|}a_i\\vec{h}_i,$$\n",
    "where\n",
    "$$a_i =\\frac{\\exp(e_i)}{\\sum_{j=1}^{|\\mathbf{x}|}\\exp(e_j)},$$\n",
    "such that\n",
    "$$e_i = tanh\\left(\\vec{\\alpha}\\vec{h}^{(2)}_i + \\beta_i\\right)$$\n",
    "\n",
    "The $\\vec{a}$ and $b$ terms in the above equations represent automatically learned attention parameters, and the $\\vec{h}^{(2)}_i$ represents the concatenated output of the second bidirectional LSTM layer produced at the $i$-th tweet token: $\\vec{h}_i^{(2)} = [\\vec{h}^{(2)}_{i_{\\rightarrow}}, \\vec{h}^{(2)}_{i_{\\leftarrow}}]\\in\\mathbb{R}^{300}$.\n",
    "\n",
    "In the final step, Baziotis et al. multiplied the output of the attention layer with matrix $W$ and selected the label with the highest resulting score:\n",
    "$$\\hat{y} = argmax\\left(softmax(W^\\top\\vec{a})\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "7. Apart from these numerous existing approaches, I have also proposed my own **lexicon-based attention system**, which builds upon the work of Baziotis et al. (2017). In this approach, in addition to the standard attention mechanism over positions\n",
    "$$\\vec{a} = \\sum_{i=1}^{|\\mathbf{x}|}a_i\\vec{h}_i,$$\n",
    "$$a_i =\\frac{\\exp(e_i)}{\\sum_{j=1}^{|\\mathbf{x}|}\\exp(e_j)},$$\n",
    "$$e_i = tanh\\left(\\vec{\\alpha}\\vec{h}^{(2)}_i + \\beta_i\\right),$$\n",
    "I have introduced two more types of attention: **lexicon-** and **context-based** one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the first of these types (i.e., **lexicon-based** attention), I estimate the relevance of the $i$-th tweet token as a globally normalized polarity score of this term (taking this score from the previously introduced linear projection lexicon):\n",
    "$$\\vec{b} = \\sum_{i=1}^{|\\mathbf{x}|}b_i\\vec{h}_i,$$\n",
    "where\n",
    "$$b_i = \\frac{\\exp(f_i)}{\\sum_{j=1}^{|\\mathbf{x}|}\\exp(f_j)},$$\n",
    "such that\n",
    "$$f_i = \\left\\{\n",
    "  \\begin{array}{ll}\n",
    "    \\tanh(abs(V[{w_i}]) + \\epsilon) & \\textrm{ if } w_i\\in V\\\\\n",
    "    \\tanh(\\epsilon) & \\, \\textrm{otherwise.} \\\\\n",
    "  \\end{array}\n",
    "  \\right.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since the semantic orientation of a polar term can be overturned by its context, I also use another type of atttention (**context-based** one), which is supposed to assign greater importance to such modifying context elements:\n",
    "$$ \\vec{c} = \\sum_{i=1}^{|\\mathbf{x}|}c_i\\vec{h}_i,$$\n",
    "where\n",
    "$$c_i =\\frac{\\exp(g_i)}{\\sum_{j=1}^{|\\mathbf{x}|}\\exp(g_j)},$$\n",
    "such that\n",
    "$$g_i =\\tanh\\left(C [\\vec{w}_i, \\vec{b}_p]^\\top\\right).$$\n",
    "\n",
    "The $C$ term in the above equation represents a model parameter (context matrix), and the $\\vec{b}_p$ value denotes the output of lexicon-based attention produced for the parent word $p$. That way, I hoped to better differentiate cases where a modifying element was changing the meaning of a polar term from the rest of the situations, e.g.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"example\">\n",
    "Ich mag den neuen Bundesminister <b>nicht</b>.\n",
    "<div class=\"translation\">I do <b>not</b> like the new federal minister.</div>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"example\">\n",
    "Ich gehe heute <b>nicht</b> ins Kino.\n",
    "<div class=\"translation\">I am <b>not</b> going to cinema today.</div>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, to make the final prediction, I concatenate the outputs of the three attention layers into a single matrix $A\\in\\mathbb{R}^{3\\times 100}$ and multiply it with a vector $\\vec{w}\\in\\mathbb{R}^{1\\times{}100}$, applying softmax normalization at the end:\n",
    "$$\\vec{o} = softmax\\left(A\\vec{w}^\\top\\right),$$\n",
    "where \n",
    "$$A = \\begin{bmatrix}\n",
    "    \\vec{a}\\\\\n",
    "    \\vec{b}\\\\\n",
    "    \\vec{c}\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"img/lba.png\" alt=\"Architecture of the neural network with lexicon- and context-based attention\">\n",
    "<figcaption>\n",
    "Architecture of the neural network with lexicon- and context-based attention\n",
    "</figcaption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DL-Based Methods (Results)\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Results of deep-learning&mdash;based MLSA methods<br/>\n",
    "Y&amp;C &mdash; Yessenalina and Cardie (2011), RAE &mdash; Recursive Auto-Encoder (Socher et al., 2011),\n",
    "MVRNN &mdash; Matrix-Vector RNN (Socher et al., 2012), RNTN &mdash; Recursive Neural-Tensor Network\n",
    "(Socher et al., 2013), SEV &mdash; Severyn and Moschitti (2015b), BAZ &mdash; Baziotis et al. (2017),\n",
    "LBA (1) &mdash; lexicon-based attention with one Bi-LSTM layer, LBA (2) &mdash; lexicon-based attention with\n",
    "two Bi-LSTM layers\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro-$F_1^{+/-}$</td>\n",
    "<td rowspan=\"2\">Micro-$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\">\n",
    "PotTS\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Y&amp;C</td>\n",
    "<td>0.45</td>\n",
    "<td>1.0</td>\n",
    "<td>0.62</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.308</td>\n",
    "<td>0.446</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.64</td>\n",
    "<td>0.78</td>\n",
    "<td>0.7</td>\n",
    "<td>0.38</td>\n",
    "<td>0.04</td>\n",
    "<td>0.08</td>\n",
    "<td>0.57</td>\n",
    "<td>0.68</td>\n",
    "<td>0.62</td>\n",
    "<td>0.389</td>\n",
    "<td>0.605</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MVRNN</td>\n",
    "<td>0.45</td>\n",
    "<td>1.0</td>\n",
    "<td>0.62</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.308</td>\n",
    "<td>0.446</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RNTN</td>\n",
    "<td>0.45</td>\n",
    "<td>0.87</td>\n",
    "<td>0.59</td>\n",
    "<td>0.19</td>\n",
    "<td>0.02</td>\n",
    "<td>0.03</td>\n",
    "<td>0.32</td>\n",
    "<td>0.1</td>\n",
    "<td>0.15</td>\n",
    "<td>0.312</td>\n",
    "<td>0.428</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.73</td>\n",
    "<td>0.79</td>\n",
    "<td>0.76</td>\n",
    "<td>0.41</td>\n",
    "<td>0.52</td>\n",
    "<td>0.46</td>\n",
    "<td>0.72</td>\n",
    "<td>0.55</td>\n",
    "<td>0.62</td>\n",
    "<td>0.608</td>\n",
    "<td>0.651</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.45</td>\n",
    "<td>1.0</td>\n",
    "<td>0.62</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.308</td>\n",
    "<td>0.446</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.82</td>\n",
    "<td>0.73</td>\n",
    "<td>0.77</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.56</td>\n",
    "<td>0.92</td>\n",
    "<td>0.69</td>\n",
    "<td>0.387</td>\n",
    "<td>0.662</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.45</td>\n",
    "<td>1.0</td>\n",
    "<td>0.62</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.308</td>\n",
    "<td>0.446</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"12\">\n",
    "SB10k\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Y&amp;C</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.62</td>\n",
    "<td>1.0</td>\n",
    "<td>0.77</td>\n",
    "<td>0.0</td>\n",
    "<td>0.622</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.63</td>\n",
    "<td>0.57</td>\n",
    "<td>0.6</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.75</td>\n",
    "<td>0.94</td>\n",
    "<td>0.83</td>\n",
    "<td>0.299</td>\n",
    "<td>0.721</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MVRNN</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.62</td>\n",
    "<td>1.0</td>\n",
    "<td>0.77</td>\n",
    "<td>0.0</td>\n",
    "<td>0.622</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RNTN</td>\n",
    "<td>0.2</td>\n",
    "<td>0.03</td>\n",
    "<td>0.05</td>\n",
    "<td>0.07</td>\n",
    "<td>0.01</td>\n",
    "<td>0.02</td>\n",
    "<td>0.62</td>\n",
    "<td>0.94</td>\n",
    "<td>0.75</td>\n",
    "<td>0.033</td>\n",
    "<td>0.594</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.62</td>\n",
    "<td>1.0</td>\n",
    "<td>0.77</td>\n",
    "<td>0.0</td>\n",
    "<td>0.622</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.75</td>\n",
    "<td>0.47</td>\n",
    "<td>0.58</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.71</td>\n",
    "<td>0.98</td>\n",
    "<td>0.83</td>\n",
    "<td>0.291</td>\n",
    "<td>0.72</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.72</td>\n",
    "<td>0.58</td>\n",
    "<td>0.64</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.74</td>\n",
    "<td>0.97</td>\n",
    "<td>0.84</td>\n",
    "<td>0.321</td>\n",
    "<td>0.737</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.76</td>\n",
    "<td>0.49</td>\n",
    "<td>0.6</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.72</td>\n",
    "<td>0.98</td>\n",
    "<td>0.83</td>\n",
    "<td>0.298</td>\n",
    "<td>0.723</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Based Methods (An Error Made by the System of Baziotis et al.)\n",
    "\n",
    "<div class=\"example\">\n",
    "Wollte meinen Kleiderschrank aufr&auml;umen ... sitze nun darin und singe Liebeslieder ...\n",
    "<div class=\"translation\">Wanted to clean up my wardrobe... Now sitting in it and singing love songs...</div>\n",
    "Gold Label:<div class=\"label neutral\">neutral</div>\n",
    "Predicted Label:<div class=\"label positive\">positive*</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL-Based Methods (An Error Made by the LBA System)\n",
    "\n",
    "<div class=\"example\">\n",
    "Gerade super Lust, mit Carls Haaren was zu machen aber ca 300 km Distanz halten mich davon ab.\n",
    "<div class=\"translation\">Wanted to clean up my wardrobe... Now sitting in it and singing love songs...</div>\n",
    "Gold Label:<div class=\"label neutral\">neutral</div>\n",
    "Predicted Label:<div class=\"label positive\">positive*</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DL-Based Methods (Effect of Word Embeddings)\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Results of deep-learning&ndash;based MLSA methods with pretrained word2vec vectors\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td>Macro-$F_1^{+/-}$</td>\n",
    "<td>Micro-$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\">\n",
    "PotTS\n",
    "<td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.61<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.61<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.61<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.22<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.01<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.03<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.48<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.72<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.32<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.07</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RNTN</td>\n",
    "<td>0.45</td>\n",
    "<td>0.82<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.59</td>\n",
    "<td>0.24<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.06<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.1<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.43<div class=\"posdelta\">0.09</div></td>\n",
    "<td>0.17<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.24<div class=\"posdelta\">0.09</div></td>\n",
    "<td>0.34<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.44<div class=\"negdelta\">0.01</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.73</td>\n",
    "<td>0.74<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.74<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.41</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.52</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.46</div></td>\n",
    "<td>0.56<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.29</div></td>\n",
    "<td>0.68<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.37<div class=\"negdelta\">0.24</div></td>\n",
    "<td>0.64<div class=\"negdelta\">0.01</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.82<div class=\"posdelta\">0.37</div></td>\n",
    "<td>0.72<div class=\"negdelta\">0.28</div></td>\n",
    "<td>0.77<div class=\"posdelta\">0.15</div></td>\n",
    "<td>0.62<div class=\"posdelta\">0.62</div></td>\n",
    "<td>0.49<div class=\"posdelta\">0.49</div></td>\n",
    "<td>0.55<div class=\"posdelta\">0.55</div></td>\n",
    "<td>0.68<div class=\"posdelta\">0.68</div></td>\n",
    "<td>0.85<div class=\"posdelta\">0.85</div></td>\n",
    "<td>0.76<div class=\"posdelta\">0.76</div></td>\n",
    "<td>0.66<div class=\"posdelta\">0.35</div></td>\n",
    "<td>0.73<div class=\"posdelta\">0.28</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.76<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.11</div></td>\n",
    "<td>0.79<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.6<div class=\"posdelta\">0.6</div></td>\n",
    "<td>0.56<div class=\"posdelta\">0.56</div></td>\n",
    "<td>0.58<div class=\"posdelta\">0.58</div></td>\n",
    "<td>0.75<div class=\"posdelta\">0.19</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.24</div></td>\n",
    "<td>0.72<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.69<div class=\"posdelta\">0.3</div></td>\n",
    "<td>0.73<div class=\"posdelta\">0.07</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.84<div class=\"posdelta\">0.39</div></td>\n",
    "<td>0.73<div class=\"negdelta\">0.27</div></td>\n",
    "<td>0.78<div class=\"posdelta\">0.16</div></td>\n",
    "<td>0.57<div class=\"posdelta\">0.57</div></td>\n",
    "<td>0.48<div class=\"posdelta\">0.48</div></td>\n",
    "<td>0.53<div class=\"posdelta\">0.53</div></td>\n",
    "<td>0.66<div class=\"posdelta\">0.66</div></td>\n",
    "<td>0.82<div class=\"posdelta\">0.82</div></td>\n",
    "<td>0.73<div class=\"posdelta\">0.73</div></td>\n",
    "<td>0.65<div class=\"posdelta\">0.34</div></td>\n",
    "<td>0.72<div class=\"posdelta\">0.27</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"12\">\n",
    "SB10k\n",
    "<td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.5<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.73<div class=\"posdelta\">0.16</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.35<div class=\"posdelta\">0.35</div></td>\n",
    "<td>0.06<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.1<div class=\"posdelta\">0.1</div></td>\n",
    "<td>0.8<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.35<div class=\"posdelta\">0.15</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.04</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RNTN</td>\n",
    "<td>0.0<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.62</td>\n",
    "<td>1.0<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.77<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.62<div class=\"posdelta\">0.03</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.64<div class=\"posdelta\">0.64</div></td>\n",
    "<td>0.58<div class=\"posdelta\">0.58</div></td>\n",
    "<td>0.61<div class=\"posdelta\">0.61</div></td>\n",
    "<td>0.51<div class=\"posdelta\">0.51</div></td>\n",
    "<td>0.21<div class=\"posdelta\">0.21</div></td>\n",
    "<td>0.3<div class=\"posdelta\">0.3</div></td>\n",
    "<td>0.76<div class=\"posdelta\">0.14</div></td>\n",
    "<td>0.89<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.82<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.45<div class=\"posdelta\">0.45</div></td>\n",
    "<td>0.72<div class=\"posdelta\">0.1</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.72<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.59<div class=\"posdelta\">0.12</div></td>\n",
    "<td>0.65<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.53<div class=\"posdelta\">0.53</div></td>\n",
    "<td>0.33<div class=\"posdelta\">0.33</div></td>\n",
    "<td>0.41<div class=\"posdelta\">0.41</div></td>\n",
    "<td>0.79<div class=\"posdelta\">0.08</div></td>\n",
    "<td>0.91<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.53<div class=\"posdelta\">0.24</div></td>\n",
    "<td>0.75<div class=\"posdelta\">0.03</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.6<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.72<div class=\"posdelta\">0.14</div></td>\n",
    "<td>0.66<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.47<div class=\"posdelta\">0.47</div></td>\n",
    "<td>0.42<div class=\"posdelta\">0.42</div></td>\n",
    "<td>0.44<div class=\"posdelta\">0.44</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.1</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.82<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.55<div class=\"posdelta\">0.23</div></td>\n",
    "<td>0.73<div class=\"negdelta\">0.01</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.72<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.57<div class=\"posdelta\">0.08</div></td>\n",
    "<td>0.64<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.55<div class=\"posdelta\">0.55</div></td>\n",
    "<td>0.39<div class=\"posdelta\">0.39</div></td>\n",
    "<td>0.46<div class=\"posdelta\">0.46</div></td>\n",
    "<td>0.79<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.9<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.55<div class=\"posdelta\">0.25</div></td>\n",
    "<td>0.75<div class=\"posdelta\">0.03</div></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DL-Based Methods (Effect of Word Embeddings)\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Results of deep-learning&ndash;based MLSA methods with least-squares embeddings\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td>Macro-$F_1^{+/-}$</td>\n",
    "<td>Micro-$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\">\n",
    "PotTS\n",
    "<td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.5<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.73<div class=\"posdelta\">0.16</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.35<div class=\"posdelta\">0.35</div></td>\n",
    "<td>0.06<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.1<div class=\"posdelta\">0.1</div></td>\n",
    "<td>0.8<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.35<div class=\"posdelta\">0.15</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.04</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RNTN</td>\n",
    "<td>0.0<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.62</td>\n",
    "<td>1.0<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.77<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.62<div class=\"posdelta\">0.03</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.64<div class=\"posdelta\">0.64</div></td>\n",
    "<td>0.58<div class=\"posdelta\">0.58</div></td>\n",
    "<td>0.61<div class=\"posdelta\">0.61</div></td>\n",
    "<td>0.51<div class=\"posdelta\">0.51</div></td>\n",
    "<td>0.21<div class=\"posdelta\">0.21</div></td>\n",
    "<td>0.3<div class=\"posdelta\">0.3</div></td>\n",
    "<td>0.76<div class=\"posdelta\">0.14</div></td>\n",
    "<td>0.89<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.82<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.45<div class=\"posdelta\">0.45</div></td>\n",
    "<td>0.72<div class=\"posdelta\">0.1</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.72<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.59<div class=\"posdelta\">0.12</div></td>\n",
    "<td>0.65<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.53<div class=\"posdelta\">0.53</div></td>\n",
    "<td>0.33<div class=\"posdelta\">0.33</div></td>\n",
    "<td>0.41<div class=\"posdelta\">0.41</div></td>\n",
    "<td>0.79<div class=\"posdelta\">0.08</div></td>\n",
    "<td>0.91<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.53<div class=\"posdelta\">0.24</div></td>\n",
    "<td>0.75<div class=\"posdelta\">0.03</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.6<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.72<div class=\"posdelta\">0.14</div></td>\n",
    "<td>0.66<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.47<div class=\"posdelta\">0.47</div></td>\n",
    "<td>0.42<div class=\"posdelta\">0.42</div></td>\n",
    "<td>0.44<div class=\"posdelta\">0.44</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.1</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.82<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.55<div class=\"posdelta\">0.23</div></td>\n",
    "<td>0.73<div class=\"negdelta\">0.01</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.72<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.57<div class=\"posdelta\">0.08</div></td>\n",
    "<td>0.64<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.55<div class=\"posdelta\">0.55</div></td>\n",
    "<td>0.39<div class=\"posdelta\">0.39</div></td>\n",
    "<td>0.46<div class=\"posdelta\">0.46</div></td>\n",
    "<td>0.79<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.9<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.55<div class=\"posdelta\">0.25</div></td>\n",
    "<td>0.75<div class=\"posdelta\">0.03</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"12\">\n",
    "SB10k\n",
    "<td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.5<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.73<div class=\"posdelta\">0.16</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.35<div class=\"posdelta\">0.35</div></td>\n",
    "<td>0.06<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.1<div class=\"posdelta\">0.1</div></td>\n",
    "<td>0.8<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.35<div class=\"posdelta\">0.15</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.04</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RNTN</td>\n",
    "<td>0.0<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.62</td>\n",
    "<td>1.0<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.77<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.62<div class=\"posdelta\">0.03</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.64<div class=\"posdelta\">0.64</div></td>\n",
    "<td>0.58<div class=\"posdelta\">0.58</div></td>\n",
    "<td>0.61<div class=\"posdelta\">0.61</div></td>\n",
    "<td>0.51<div class=\"posdelta\">0.51</div></td>\n",
    "<td>0.21<div class=\"posdelta\">0.21</div></td>\n",
    "<td>0.3<div class=\"posdelta\">0.3</div></td>\n",
    "<td>0.76<div class=\"posdelta\">0.14</div></td>\n",
    "<td>0.89<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.82<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.45<div class=\"posdelta\">0.45</div></td>\n",
    "<td>0.72<div class=\"posdelta\">0.1</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.72<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.59<div class=\"posdelta\">0.12</div></td>\n",
    "<td>0.65<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.53<div class=\"posdelta\">0.53</div></td>\n",
    "<td>0.33<div class=\"posdelta\">0.33</div></td>\n",
    "<td>0.41<div class=\"posdelta\">0.41</div></td>\n",
    "<td>0.79<div class=\"posdelta\">0.08</div></td>\n",
    "<td>0.91<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.53<div class=\"posdelta\">0.24</div></td>\n",
    "<td>0.75<div class=\"posdelta\">0.03</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.6<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.72<div class=\"posdelta\">0.14</div></td>\n",
    "<td>0.66<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.47<div class=\"posdelta\">0.47</div></td>\n",
    "<td>0.42<div class=\"posdelta\">0.42</div></td>\n",
    "<td>0.44<div class=\"posdelta\">0.44</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.1</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.82<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.55<div class=\"posdelta\">0.23</div></td>\n",
    "<td>0.73<div class=\"negdelta\">0.01</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.72<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.57<div class=\"posdelta\">0.08</div></td>\n",
    "<td>0.64<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.55<div class=\"posdelta\">0.55</div></td>\n",
    "<td>0.39<div class=\"posdelta\">0.39</div></td>\n",
    "<td>0.46<div class=\"posdelta\">0.46</div></td>\n",
    "<td>0.79<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.9<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.84<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.55<div class=\"posdelta\">0.25</div></td>\n",
    "<td>0.75<div class=\"posdelta\">0.03</div></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distant Supervision (Data Statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<thead>\n",
    " <caption>Polarity class distribution in PotTS, SB10k, and the German Twitter Snapshot (GTS)<br/>\n",
    "(&#42; – the mixed polarity was excluded from our experiments)</caption>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Dataset</td>\n",
    "<td colspan=\"4\">Polarity Class</td>\n",
    "<td colspan=\"2\">Label Agreement</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Positive} </td>\n",
    "<td>Negative</td>\n",
    "<td>Neutral</td>\n",
    "<td>Mixed*</td>\n",
    "<td>$\\alpha$ </td>\n",
    "<td>$\\kappa$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>PotTS</td>\n",
    "<td>3,380</td>\n",
    "<td>1,541</td>\n",
    "<td>2,558</td>\n",
    "<td>513</td>\n",
    "<td>0.66</td>\n",
    "<td>0.4</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SB10k</td>\n",
    "<td>1,717</td>\n",
    "<td>1,130</td>\n",
    "<td>4,629</td>\n",
    "<td>0</td>\n",
    "<td>0.39</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GTS</td>\n",
    "<td>3,326,829</td>\n",
    "<td>350,775</td>\n",
    "<td>19,453,669</td>\n",
    "<td>73,776</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "<td class=\"NA\">NA</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distant Supervision (Results)\n",
    "\n",
    "<table>\n",
    "<caption>Results of MLSA methods with distantly supervised data</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro-$F_1$</td>\n",
    "<td rowspan=\"2\">Micro-$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>GMN</td>\n",
    "<td>0.8<div class=\"posdelta\">0.13</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.39</div></td>\n",
    "<td>0.48<div class=\"negdelta\">0.22</div></td>\n",
    "<td>0.2<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.29<div class=\"posdelta\">0.14</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.79<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.63<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.36<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.49<div class=\"negdelta\">0.12</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MHM</td>\n",
    "<td>0.86<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.7<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.31<div class=\"negdelta\">0.27</div></td>\n",
    "<td>0.39<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.35<div class=\"negdelta\">0.22</div></td>\n",
    "<td>0.55<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.61<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.52<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.14</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GNT</td>\n",
    "<td>0.86<div class=\"posdelta\">0.15</div></td>\n",
    "<td>0.6<div class=\"negdelta\">0.2</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.26<div class=\"negdelta\">0.29</div></td>\n",
    "<td>0.31<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.28<div class=\"negdelta\">0.22</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.5<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.1</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.68<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.31<div class=\"negdelta\">0.3</div></td>\n",
    "<td>0.43<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.25<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.46<div class=\"posdelta\">0.45</div></td>\n",
    "<td>0.32<div class=\"posdelta\">0.29</div></td>\n",
    "<td>0.49<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.61<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.38<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.45<div class=\"negdelta\">0.09</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.87<div class=\"posdelta\">0.14</div></td>\n",
    "<td>0.51<div class=\"negdelta\">0.23</div></td>\n",
    "<td>0.64<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.27<div class=\"posdelta\">0.27</div></td>\n",
    "<td>0.49<div class=\"posdelta\">0.49</div></td>\n",
    "<td>0.35<div class=\"posdelta\">0.35</div></td>\n",
    "<td>0.55<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.58<div class=\"negdelta\">0.26</div></td>\n",
    "<td>0.56<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.49<div class=\"posdelta\">0.12</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.11</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.0<div class=\"negdelta\">0.82</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.72</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.77</div></td>\n",
    "<td>0.19<div class=\"negdelta\">0.43</div></td>\n",
    "<td>1.0<div class=\"posdelta\">0.51</div></td>\n",
    "<td>0.32<div class=\"negdelta\">0.23</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.68</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.85</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.76</div></td>\n",
    "<td>0.16<div class=\"negdelta\">0.5</div></td>\n",
    "<td>0.19<div class=\"negdelta\">0.43</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^(1)$</td>\n",
    "<td>0.48<div class=\"negdelta\">0.28</div></td>\n",
    "<td>0.88<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.62<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.25<div class=\"negdelta\">0.35</div></td>\n",
    "<td>0.23<div class=\"negdelta\">0.33</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.34</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.75</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.68</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.72</div></td>\n",
    "<td>0.43<div class=\"negdelta\">0.26</div></td>\n",
    "<td>0.44<div class=\"negdelta\">0.29</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^(2)$</td>\n",
    "<td>0.91<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.08<div class=\"negdelta\">0.65</div></td>\n",
    "<td>0.14<div class=\"negdelta\">0.64</div></td>\n",
    "<td>0.19<div class=\"negdelta\">0.38</div></td>\n",
    "<td>0.99<div class=\"posdelta\">0.51</div></td>\n",
    "<td>0.32<div class=\"negdelta\">0.21</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.66</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.82</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.73</div></td>\n",
    "<td>0.23<div class=\"negdelta\">0.42</div></td>\n",
    "<td>0.22<div class=\"negdelta\">0.5</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GMN</td>\n",
    "<td>0.71<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.27<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.4<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.11<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.15<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.96<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.82<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.27<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.02</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MHM</td>\n",
    "<td>0.77<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.4<div class=\"negdelta\">0.25</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.61<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.1<div class=\"negdelta\">0.3</div></td>\n",
    "<td>0.18<div class=\"negdelta\">0.27</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.97<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.82<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.35<div class=\"negdelta\">0.21</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.04</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GNT</td>\n",
    "<td>0.77<div class=\"posdelta\">0.1</div></td>\n",
    "<td>0.39<div class=\"negdelta\">0.23</div></td>\n",
    "<td>0.52<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.25<div class=\"negdelta\">0.19</div></td>\n",
    "<td>0.13<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.17<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.92<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.04</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.44<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.27<div class=\"negdelta\">0.51</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.25</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.59<div class=\"posdelta\">0.53</div></td>\n",
    "<td>0.34<div class=\"posdelta\">0.24</div></td>\n",
    "<td>0.78<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.62<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.69<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.14</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.64</td>\n",
    "<td>0.39<div class=\"negdelta\">0.19</div></td>\n",
    "<td>0.49<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.12<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.18<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.7<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.9<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.78<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.33<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.69<div class=\"negdelta\">0.03</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.24<div class=\"negdelta\">0.48</div></td>\n",
    "<td>1.0<div class=\"posdelta\">0.41</div></td>\n",
    "<td>0.38<div class=\"negdelta\">0.27</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.53</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.33</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.41</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.79</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.91</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.84</div></td>\n",
    "<td>0.19<div class=\"negdelta\">0.34</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.51</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.64<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.43<div class=\"negdelta\">0.29</div></td>\n",
    "<td>0.52<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.59<div class=\"posdelta\">0.12</div></td>\n",
    "<td>0.09<div class=\"negdelta\">0.33</div></td>\n",
    "<td>0.16<div class=\"negdelta\">0.28</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.93<div class=\"posdelta\">0.13</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.21</div></td>\n",
    "<td>0.69<div class=\"negdelta\">0.04</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.0<div class=\"negdelta\">0.72</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.57</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.64</div></td>\n",
    "<td>0.14<div class=\"negdelta\">0.41</div></td>\n",
    "<td>1.0<div class=\"posdelta\">0.61</div></td>\n",
    "<td>0.25<div class=\"negdelta\">0.21</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.79</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.9</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.84</div></td>\n",
    "<td>0.12<div class=\"negdelta\">0.43</div></td>\n",
    "<td>0.14<div class=\"negdelta\">0.61</div></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicons\n",
    "\n",
    "<div>\n",
    "<figure class=\"halfpage\">\n",
    "<img src=\"img/cgsa_potts_macro_lexicons.png\" alt=\"Macro F-Scores of MLSA Classifiers with Different Lexicons on the PotTS corus\">\n",
    "<figcaption>Macro-$F_1$</figcaption>\n",
    "</figure>\n",
    "<figure class=\"halfpage\">\n",
    "<img src=\"img/cgsa_potts_micro_lexicons.png\" alt=\"Micro F-Scores of MLSA Classifiers with Different Lexicons on the PotTS corus\">\n",
    "<figcaption>Micro-$F_1$</figcaption>\n",
    "</figure>\n",
    "Results of MLSA methods with different lexicons on the PotTS corpus\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicons\n",
    "\n",
    "<div>\n",
    "<figure class=\"halfpage\">\n",
    "<img src=\"img/cgsa_sb10k_macro_lexicons.png\" alt=\"Macro F-Scores of MLSA Classifiers with Different Lexicons on the SB10k corus\">\n",
    "<figcaption>Macro-$F_1$</figcaption>\n",
    "</figure>\n",
    "<figure class=\"halfpage\">\n",
    "<img src=\"img/cgsa_sb10k_micro_lexicons.png\" alt=\"Micro F-Scores of MLSA Classifiers with Different Lexicons on the SB10k corus\">\n",
    "<figcaption>Micro-$F_1$</figcaption>\n",
    "</figure>\n",
    "Results of MLSA methods with different lexicons on the SB10k corpus\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Results of MLSA methods without text normalization\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro-$F_1^{+/-}$</td>\n",
    "<td rowspan=\"2\">Micro-$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<tr>\n",
    "<td colspan=\"12\">PotTS</td>\n",
    "</tr>\n",
    "<td>HL</td>\n",
    "<td>0.63<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.3<div class=\"negdelta\">0.46</div></td>\n",
    "<td>0.4<div class=\"negdelta\">0.36</div></td>\n",
    "<td>0.46<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.29<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.36<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.41<div class=\"negdelta\">0.26</div></td>\n",
    "<td>0.77<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.38<div class=\"negdelta\">0.24</div></td>\n",
    "<td>0.464<div class=\"negdelta\">0.22</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TBD</td>\n",
    "<td>0.65<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.47</div></td>\n",
    "<td>0.36<div class=\"negdelta\">0.38</div></td>\n",
    "<td>0.46<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.27<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.41<div class=\"negdelta\">0.22</div></td>\n",
    "<td>0.83<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.55<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.348<div class=\"negdelta\">0.25</div></td>\n",
    "<td>0.457<div class=\"negdelta\">0.22</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MST</td>\n",
    "<td>0.63<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.29<div class=\"negdelta\">0.43</div></td>\n",
    "<td>0.4<div class=\"negdelta\">0.34</div></td>\n",
    "<td>0.47<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.39<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.42<div class=\"negdelta\">0.26</div></td>\n",
    "<td>0.77<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.4<div class=\"negdelta\">0.21</div></td>\n",
    "<td>0.47<div class=\"negdelta\">0.21</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>JRK</td>\n",
    "<td>0.44<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.22<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.29<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.14<div class=\"negdelta\">0.28</div></td>\n",
    "<td>0.06<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.08<div class=\"negdelta\">0.19</div></td>\n",
    "<td>0.36<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.7<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.47<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.19<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.36<div class=\"negdelta\">0.11</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>KLCH</td>\n",
    "<td>0.61<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.23<div class=\"negdelta\">0.49</div></td>\n",
    "<td>0.33<div class=\"negdelta\">0.38</div></td>\n",
    "<td>0.33<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.21<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.26<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.41<div class=\"negdelta\">0.25</div></td>\n",
    "<td>0.82</td>\n",
    "<td>0.55<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.3<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.44<div class=\"negdelta\">0.21</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GMN</td>\n",
    "<td>0.59<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.77<div class=\"posdelta\">0.04</div></td>\n",
    "<td>0.66<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.37<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.14<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.2<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.55<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.56<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.43<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.05</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MHM</td>\n",
    "<td>0.78<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.76<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.77<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.59<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.56<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.7<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.74<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.72<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.67<div class=\"negdelta\">0.006</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.007</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GNT</td>\n",
    "<td>0.68<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.8</td>\n",
    "<td>0.73<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.55</td>\n",
    "<td>0.43<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.48<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.67<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.62<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.61<div class=\"negdelta\">0.017</div></td>\n",
    "<td>0.65<div class=\"negdelta\">0.02</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Y&amp;C</td>\n",
    "<td>0.45</td>\n",
    "<td>1.0</td>\n",
    "<td>0.62</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.31</td>\n",
    "<td>0.45</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.46<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.98<div class=\"posdelta\">0.37</div></td>\n",
    "<td>0.62<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.22</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.0<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.63<div class=\"posdelta\">0.15</div></td>\n",
    "<td>0.05<div class=\"negdelta\">0.67</div></td>\n",
    "<td>0.09<div class=\"negdelta\">0.48</div></td>\n",
    "<td>0.31<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.46<div class=\"negdelta\">0.08</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MVRNN</td>\n",
    "<td>0.45</td>\n",
    "<td>0.92<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.6<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.08<div class=\"posdelta\">0.08</div></td>\n",
    "<td>0.01<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.01<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.26<div class=\"posdelta\">0.26</div></td>\n",
    "<td>0.03<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.06<div class=\"posdelta\">0.06</div></td>\n",
    "<td>0.31</td>\n",
    "<td>0.43<div class=\"negdelta\">0.02</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RNTN</td>\n",
    "<td>0.45</td>\n",
    "<td>0.93<div class=\"posdelta\">0.11</div></td>\n",
    "<td>0.61<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.29<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.01<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.01<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.4<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.07<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.12<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.31<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.45<div class=\"negdelta\">0.01</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.56<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.79<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.66<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.57<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.27</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.33<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.56<div class=\"negdelta\">0.08</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.65<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.62<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.62</td>\n",
    "<td>0.22<div class=\"negdelta\">0.27</div></td>\n",
    "<td>0.32<div class=\"negdelta\">0.23</div></td>\n",
    "<td>0.5<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.74<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.6<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.47<div class=\"negdelta\">0.19</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.16</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.58<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.77<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.66<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.63<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.37<div class=\"negdelta\">0.31</div></td>\n",
    "<td>0.46<div class=\"negdelta\">0.26</div></td>\n",
    "<td>0.6<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.58<div class=\"negdelta\">0.15</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.67<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.52<div class=\"negdelta\">0.21</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.19</div></td>\n",
    "<td>0.51<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.44<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.47<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.52<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.7<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.6<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.57<div class=\"negdelta\">0.15</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"12\">SB10k</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>HL</td>\n",
    "<td>0.41<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.42<div class=\"negdelta\">0.2</div></td>\n",
    "<td>0.42<div class=\"negdelta\">0.13</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.28<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.26<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.66<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.63<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.65<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.34<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.05</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TBD</td>\n",
    "<td>0.41<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.37<div class=\"negdelta\">0.23</div></td>\n",
    "<td>0.39<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.21<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.22<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.65<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.66<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.66<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.31<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.04</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MST</td>\n",
    "<td>0.4<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.32<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.35<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.26<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.3<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.28<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.65<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.68<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.67</td>\n",
    "<td>0.32<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.03</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>JRK</td>\n",
    "<td>0.4<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.42<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.41<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.36</td>\n",
    "<td>0.26</td>\n",
    "<td>0.3</td>\n",
    "<td>0.69</td>\n",
    "<td>0.72<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.71<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.36<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.006</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>KLCH</td>\n",
    "<td>0.42<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.21<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.28</td>\n",
    "<td>0.25<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.13</td>\n",
    "<td>0.17<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.66</td>\n",
    "<td>0.86</td>\n",
    "<td>0.75</td>\n",
    "<td>0.23<div class=\"negdelta\">0.005</div></td>\n",
    "<td>0.6<div class=\"negdelta\">0.002</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GMN</td>\n",
    "<td>0.48<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.31<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.37<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.27<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.07<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.11<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.69<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.9<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.78<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.24<div class=\"negdelta\">0.09</div></td>\n",
    "<td>0.64<div class=\"negdelta\">0.06</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MHM</td>\n",
    "<td>0.67<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.62<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.65<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.59<div class=\"negdelta\">0.08</div></td>\n",
    "<td>0.42<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.49<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.8</td>\n",
    "<td>0.88<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.84</td>\n",
    "<td>0.56<div class=\"negdelta\">0.002</div></td>\n",
    "<td>0.75<div class=\"negdelta\">0.001</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GNT</td>\n",
    "<td>0.42<div class=\"negdelta\">0.25</div></td>\n",
    "<td>0.21<div class=\"negdelta\">0.41</div></td>\n",
    "<td>0.28<div class=\"negdelta\">0.36</div></td>\n",
    "<td>0.25<div class=\"negdelta\">0.19</div></td>\n",
    "<td>0.13<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.17<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.66<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.86<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.75<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.22<div class=\"negdelta\">0.2</div></td>\n",
    "<td>0.604<div class=\"negdelta\">0.12</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Y&amp;C</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.62</td>\n",
    "<td>1.0</td>\n",
    "<td>0.77</td>\n",
    "<td>0.0</td>\n",
    "<td>0.62</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RAE</td>\n",
    "<td>0.46<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.62<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.53<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.18<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.02<div class=\"negdelta\">0.04</div></td>\n",
    "<td>0.03<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.77<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.82<div class=\"posdelta\">0.02</div></td>\n",
    "<td>0.79<div class=\"negdelta\">0.01</div></td>\n",
    "<td>0.28<div class=\"negdelta\">0.07</div></td>\n",
    "<td>0.66<div class=\"negdelta\">0.02</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MVRNN</td>\n",
    "<td>0.19</td>\n",
    "<td>0.01</td>\n",
    "<td>0.03</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.62</td>\n",
    "<td>0.97</td>\n",
    "<td>0.76</td>\n",
    "<td>0.01</td>\n",
    "<td>0.61</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RNTN</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.62</td>\n",
    "<td>1.0</td>\n",
    "<td>0.77</td>\n",
    "<td>0.0</td>\n",
    "<td>0.62</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>SEV</td>\n",
    "<td>0.58<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.39<div class=\"negdelta\">0.19</div></td>\n",
    "<td>0.47<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.23<div class=\"negdelta\">0.28</div></td>\n",
    "<td>0.05<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.08<div class=\"negdelta\">0.22</div></td>\n",
    "<td>0.7<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.92<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.8<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.27<div class=\"negdelta\">0.18</div></td>\n",
    "<td>0.67<div class=\"negdelta\">0.05</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>BAZ</td>\n",
    "<td>0.69<div class=\"negdelta\">0.03</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.16</div></td>\n",
    "<td>0.6<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.36<div class=\"negdelta\">0.17</div></td>\n",
    "<td>0.49<div class=\"posdelta\">0.16</div></td>\n",
    "<td>0.41</td>\n",
    "<td>0.79</td>\n",
    "<td>0.79<div class=\"negdelta\">0.12</div></td>\n",
    "<td>0.79<div class=\"negdelta\">0.05</div></td>\n",
    "<td>0.51<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.69<div class=\"negdelta\">0.06</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(1)}$</td>\n",
    "<td>0.24<div class=\"negdelta\">0.36</div></td>\n",
    "<td>0.86<div class=\"posdelta\">0.14</div></td>\n",
    "<td>0.38<div class=\"negdelta\">0.28</div></td>\n",
    "<td>0.45<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.45<div class=\"posdelta\">0.03</div></td>\n",
    "<td>0.45<div class=\"posdelta\">0.01</div></td>\n",
    "<td>0.69<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.01<div class=\"negdelta\">0.79</div></td>\n",
    "<td>0.02<div class=\"negdelta\">0.8</div></td>\n",
    "<td>0.41<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.27<div class=\"negdelta\">0.46</div></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LBA$^{(2)}$</td>\n",
    "<td>0.74<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.42<div class=\"negdelta\">0.15</div></td>\n",
    "<td>0.54<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.62<div class=\"posdelta\">0.07</div></td>\n",
    "<td>0.25<div class=\"negdelta\">0.14</div></td>\n",
    "<td>0.35<div class=\"negdelta\">0.11</div></td>\n",
    "<td>0.73<div class=\"negdelta\">0.06</div></td>\n",
    "<td>0.95<div class=\"posdelta\">0.05</div></td>\n",
    "<td>0.82<div class=\"negdelta\">0.02</div></td>\n",
    "<td>0.45<div class=\"negdelta\">0.1</div></td>\n",
    "<td>0.72<div class=\"negdelta\">0.03</div></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary and Concusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* I have compared three major families of message-level sentiment analysis methods: lexicon-, machine-learning&ndash; and deep-learning&ndash;based ones, finding that the last two groups significantly outperform lexicon-driven systems;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Surprisingly, among all compared lexicon methods, the most simple one (the classifier of Hu and Liu [2004]) produced the best macro- and micro-averaged $F_1$-results on the PotTS corpus (0.615 and 0.685 respectively) and also yielded the highest macro $F_1$-measure on the SB10k dataset (0.421). Other systems, however, could have improved their scores if they better handled the negation of polar terms (after switching off the negation component in the method of Musto et al., its macro-$F_1$ on the PotTS corpus increased to 0.641, surpassing the benchmark of Hu and Liu);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* as expected, the ML-based system of Mohammad et al. (2013)—the winner of the inaugural run of SemEval task in sentiment analysis of Twitter (Nakov et al., 2013)—also surpassed other ML competitors, achieving highly competitive results: 0.674 macro-and 0.727 micro-$F_1$ on the PotTS data, and 0.564 macro- and 0.752 micro-averaged$F_1$-measure on the SB10k test set;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* as in the previous case, however, these results could have been improved if the classifier dispensed with character-level and part-of-speech features and used logistic regression instead of SVM;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* a much more varied situation was observed with deep-learning–based systems, which frequently simply fell into always predicting the majority class for all tweets, but sometimes yielded extraordinarily good results as it was the case with our proposed lexicon-based attention system, which attained 0.69 macro-$F_1$ on the PotTS corpus and 0.55 macro $F_1$-score on the SB10k dataset (0.73 and 0.75 micro-F 1 respectively), setting a new state of the art for the former data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* speaking of word embeddings, we should note that almost all DL-based approaches showed fairly low scores when they used randomly initialized task-specific embeddings, but notably improved their results after switching to pre-trained word2vec vectors, and benefited even more from the least-squares fallback;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* against our expectations, we could not overcome the majority class pitfall of DL-based systems after adding more distantly supervised training data, which, in general, only lowered the scores of both ML- and DL-based methods. Since this result contradicts the findings of other authors, we hypothesize that this degradation is primarily due to the differences in the class distributions between automatically and manually labeled tweets;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* on the other hand, we could see that using more qualitative sentiment lexicons (especially manually curated and dictionary-based ones) resulted in further improvements for the systems that relied on this lexical resource;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* last but not least, we proved the utility of the text normalization step, which brought about significant improvements for all tested methods, as confirmed by our last ablation test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter VI: Discourse-Aware Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Examples\n",
    "\n",
    "<div class=\"example\">\n",
    "Wollte meinen Kleiderschrank aufr&auml;umen ... sitze nun darin und singe Liebeslieder ...\n",
    "<div class=\"translation\">Wanted to clean up my wardrobe... Now sitting in it and singing love songs...</div>\n",
    "Gold Label:<div class=\"label neutral\">neutral</div>\n",
    "Predicted Label:<div class=\"label positive\">positive*</div>\n",
    "</div>\n",
    "\n",
    "<div class=\"example\">\n",
    "Gerade super Lust, mit Carls Haaren was zu machen aber ca 300 km Distanz halten mich davon ab.\n",
    "<div class=\"translation\">Wanted to clean up my wardrobe... Now sitting in it and singing love songs...</div>\n",
    "Gold Label:<div class=\"label neutral\">neutral</div>\n",
    "Predicted Label:<div class=\"label positive\">positive*</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Approaches to Discourse Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rhetorical Structure Theory (Mann and Thompson, 1988), which divides the text into elementary discourse units (EDUs) and infers a hierarchical structure (typically a tree) between these units;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PDTB (Prasad et al., 2004), which analyzes the occurrences of (either explicitly mentioned or implicitly assumed) connectives (i.e., lexico-grammatic elements that connect two sentences) in the text and considers text sentences as arguments of these connectives;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SDRT (Lascarides and Asher, 2001), which is conceptually similar to RST in that it also assumes a hierarchical structure if the text, with the leaves of this structure representing Elementary Discourse Units.  But in contrast to the first theory, SDRT allows this structure to be a graph, not just a tree (i.e., a node can have multiple parents and there can be multiple links between the same pair of nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "* I split all microblogs from the PotTS and SB10k corpora into elementary discourse units with the ML-based discourse segmenter of Sidarenka et al (2015).  After filtering out all tweets that had only one EDU, I obtained 4,771 messages (12,137 segments) for PotTS and 3,763 microblogs (9,625 segments) for the SB10k corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the next step, I assigned polarity scores to the segments of these microblogs with the help of the lexicon-based attention classifier, analyzing each elementary unit in isolation, independently of the rest of the tweet.  We again used the same 70--10--20 split into training, development, and test sets as I did in the previous chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"img/dasa_potts_edu_distribution.png\" alt=\"EDU distribution in PotTS\">\n",
    "<figcaption>PotTS</figcaption>\n",
    "<img src=\"img/dasa_sb10k_edu_distribution.png\" alt=\"EDU distribution in SB10k\">\n",
    "<figcaption>SB10k</figcaption>\n",
    "</figure>\n",
    "<div>\n",
    "Distribution of elementary discourse units and polarity classes in the training and development sets of PotTS and SB10k\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"example\">\n",
    "[Guinness on Wheelchairs :]$_1$ [Das .]$_2$ [Ist .]$_3$ [Verdammt .]$_4$ [Noch .]$_5$ [Mal .]$_6$ [Einer .]$_7$ [Der .]$_8$ [Besten .]$_9$ [Werbespots .]$_{10}$ [Des .]$_{11}$ [Jahrzehnts .]$_{12}$ [( Auch ...]$_{13}$\n",
    "<div class=\"translation\">\n",
    "[Guinness on Wheelchairs :]$_1$ [This .]$_2$ [Is .]$_3$ [Gosh .]$_4$ [Darn .]$_5$ [It .]$_6$ [One .]$_7$ [Of .]$_8$ [The best .]$_9$ [Commercials .]$_{10}$ [Of .]$_{11}$ [The Decade .]$_{12}$ [( Also ...]$_{13}$</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, I derived RST trees for the segmented tweets with the DPLP dicsourse parser (Yi and Eisenstein, 2014) that I had previously retrained on the Potsdam Commentary Corpus (PCC 2.0; Stede and Neumann, 2014).  An example of such automatically derived RST tree is provided below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: convert twitter-rst.tex to png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DASA Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the **No-Discourse** baseline, in which I simply re-use the scores assigned by the LBA classifier to the whole message;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Last**, in which I determine the overall polarity of a tweet by taking the LBA scores assigned to its last EDU;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Root**, which is conceptually similar to **Last** with the only difference that it infers the polarity of the microblog from the root EDU in the discourse tree instead of the last segment;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the method of **Wang and Wu** (2013), who determine the semantic orientation of a document by taking a linear combination of the polarity scores of its EDUs and multiplying these scores with automatically learned coefficients:\n",
    "$$\\psi = \\sum_{i}p_i\\times d_i + b$$,\n",
    "where $\\psi$ is the final sentiment score of the whole document, $p_i$ is the sentiment score of the $i$-th EDU, and $d_i$ and $b$ are automatically learned model parameters, with the former term denoting the strength of the discourse relation via which $i$-th EDU is connected to its parent;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Discourse-Depth Reweighting** (DDR) by Bhatia et al. (2015), in which the authors estimate the relevance $\\lambda_i$ of each discourse unit $i$ as:\n",
    "$$\\lambda_i = \\max(0.5, 1. - \\frac{d_i}{6})$$\n",
    "where $d_i$ stands for the depth of the $i$-th EDU in the document's discourse tree.\n",
    "After estimating the sentiment score of each unit as:\n",
    "$$\\sigma_i = \\vec{\\theta}^{\\top}\\vec{w},$$\n",
    "where $\\vec{w}$ represents a vector of features, and $\\vec{\\theta}$ stands for the corresponding parameters, Bhatia et al. compute the overall polarity of a document $\\psi$ as:\n",
    "$$\\psi = \\sum_i\\lambda_i\\vec{\\theta}^{\\top}\\vec{w}_i = \\vec{\\theta}^{\\top}\\sum_i\\lambda_i\\vec{w}_i;$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Recurrent Rhetorical Neural Network** (R2N2; Bhatia et al., 2015), where the authos mainly follow the RNN approach by Socher et al. (2013) by recursively computing the polarity score of each discourse unit as:\n",
    "$$  \\psi_i = \\tanh\\left(K_n^{(r_i)} \\psi_{n(i)} + K_s^{(r_i)}\\psi_{s(i)} \\right)$$\n",
    "The $K_n^{(r_i)}$ and $K_s^{(r_i)}$ terms in the above equation stand for the nucleus and satellite coefficients associated with the rhetorical relation $r_i$, and $\\psi_{n(i)}$ and $\\psi_{s(i)}$ represent sentiment scores of the nucleus and satellite of the $i$-th vertex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to these existing (baseline) solutions, I also propose three **own** approaches:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **latent Conditional Random Fields** (LCRF):\n",
    "<figure>\n",
    "<img src=\"img/latent-crf-correct.png\" alt=\"Computational path of the probability of the correct label in latent CRF\">\n",
    "<figcaption>\n",
    "Computational path of the probability of the correct label in latent CRF\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/latent-crf-wrong.png\" alt=\"Computational path of the probability of the wrong label in latent CRF\">\n",
    "<figcaption>\n",
    "Computational path of the probability of the wrong label in latent CRF\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **latent Marginalized Conditional Random Fields** (LMCRF):\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/latent-mcrf-correct.png\" alt=\"Computational path of the probability of the correct label in latent marginalized CRF\">\n",
    "<figcaption>\n",
    "Computational path of the probability of the correct label in latent marginalized CRF\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "<figure>\n",
    "<img src=\"img/latent-mcrf-wrong.png\" alt=\"Computational path of the probability of the wrong label in latent marginalized CRF\">\n",
    "<figcaption>\n",
    "Computational path of the probability of the wrong label in latent marginalized CRF\n",
    "</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the **Recursive Dirichlet Process**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_{\\theta}(\\mathbf{x},\\mathbf{z})=p_{\\theta}(\\mathbf{x}|\\mathbf{z})p_{\\theta}(\\mathbf{z})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\theta_{max} = \\underset{\\theta}{\\operatorname{argmax}}p_{\\theta}(\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$p_{\\theta_{max}}(\\mathbf{z}|\\mathbf{x}) = \\frac{p_{\\theta_{max}(\\mathbf{x}, \\mathbf{z})}}{\\int p_{\\theta_{max}}(\\mathbf{x}, \\mathbf{z})\\mathrm{d}\\mathbf{z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$q_{\\phi}\\approx p_{\\theta_{max}}(\\mathbf{z}|\\mathbf{x})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective function:\n",
    "\n",
    "$$\\mathrm{ELBO}\\overset{\\Delta}{=}\\mathbb{E}_{q_{\\phi}(\\boldsymbol{z})}\\left[\\log p_{\\theta}(\\boldsymbol{x},\\boldsymbol{z}) - \\log q_{\\phi}(\\boldsymbol{z})\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I associate a random variable $z_{j_k} \\in \\mathbb{R}^{3}_{+}$ with every RST node $i$, which represents the probabilities of polarity classes (negative, neutral, and positive) for that node after seeing its $k$-th child:\n",
    "$$z_{j_k} \\sim Dir(\\boldsymbol{\\alpha}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set the initial values of these variables (i.e., $z_{j_0}$) to the scores predicted by the LBA classifier for EDUs and the root nodes, and set them to zeroes for abstract nodes.  Then when analyzing the $k$-th child of that node, I compute the score that comes from that child as follows:\n",
    "$$\\boldsymbol{z}^{*} = \\textrm{sparsemax}(M_r\\boldsymbol{z}_k^{\\top}),$$\n",
    "where the $M_r \\sim \\mathcal{N}_{3\\times3}(\\boldsymbol{\\mu}_r, \\boldsymbol{\\Sigma}_r)$ matrix reflects contextual changes introduced by discourse relation $r$ that holds between parent and child.  The initial priors for this parameter are:\n",
    "$$  \\boldsymbol{\\mu}_r=\\begin{bmatrix}\n",
    "  1 & 0 & 0\\\\\n",
    "  0 & 0.3 & 0\\\\\n",
    "  0 & 0 & 1\n",
    "  \\end{bmatrix}$$\n",
    "and\n",
    "$$\n",
    "  \\boldsymbol{\\Sigma}_r=\\begin{bmatrix}\n",
    "  1 & 1 & 1\\\\\n",
    "  1 & 1 & 1\\\\\n",
    "  1 & 1 & 1\n",
    "  \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I compute the $\\boldsymbol{\\alpha}$ parameters as:\n",
    "$$  \\boldsymbol{\\alpha}_{j_k} = \\boldsymbol{\\beta}\\odot\\boldsymbol{z}^*_k +\n",
    "  (\\boldsymbol{1} -\n",
    "  \\boldsymbol{\\beta})\\odot\\boldsymbol{z}_{j_{k-1}},\n",
    "$$\n",
    "where $\\boldsymbol{\\beta}\\in\\mathbb{R}^3$ is another multivariate\n",
    "random variable sampled from the Beta distribution $B(5., 5.)$, which\n",
    "controls the amount of information we want to pass from child to its\n",
    "parent.\n",
    "\n",
    "The only thing that I need to do to the above $\\boldsymbol{\\alpha}_{j_k}$ term before drawing the actual probability\n",
    "$\\boldsymbol{z}_{j_k}$ is to scale this vector by a certain amount in\n",
    "order to reduce the variance of the resulting Dirichlet\n",
    "distribution.\\footnote{Because if we keep the\n",
    "  $\\boldsymbol{\\alpha}_{j_k}$ vector from Equation~\\ref{dasa:eq:alpha}\n",
    "  unchanged, most of its values will be in the range $[0,\\ldots,1]$\n",
    "  which will lead to an extremely high variance of the Dirichlet\n",
    "  distribution.} In particular, we compute this scaling factor as\n",
    "follows:\n",
    "$$\\textrm{scale} = \\frac{\\xi \\times \\left(0.1 + \\cos\\left(\\boldsymbol{z}^*_k, \\boldsymbol{z}_{j_{k-1}}\\right)\\right)}{H\\left(\\boldsymbol{\\alpha}_{j_k}\\right)};$$\n",
    "where $\\xi$ is a model parameter sampled from a $\\chi^2$-distribution: $\\xi\\sim\\chi^2(34)$; 0.1 is a constant used to prevent zero scales in the cases when $\\cos\\left(\\mathbf{z}^*_k, \\mathbf{z}_{j_{k-1}}\\right)$ is zero; and $H\\left(\\boldsymbol{\\alpha}_{j_k}\\right)$ stands for the entropy of the $\\boldsymbol{\\alpha}_{j_k}$ vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to predict the final polarity label for the whole tweet, I simply draw a value from a Categorical distribution, using the value $\\boldsymbol{z}_{\\text{\\textsc{Root}}}$ as its parameter:\n",
    "$$y \\sim Cat(\\mathbf{z}_{\\text{Root}}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "   <img src=\"img/dirichlet-process.png\" alt=\"Probability distributions of polar classes computed by the Recursive Dirichlet Process\">\n",
    "    <figcaption>\n",
    "    Probability distributions of polar classes computed by the Recursive Dirichlet Process\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Probability distributions of polar classes computed by the Recursive Dirichlet Process\n",
    "\n",
    "(higher probability regions are highlighted in red; $\\boldsymbol{p}_{prnt}$ means the probability of the parent\n",
    "node [the values in the vector represent the scores for the negative, neutral, and positive polarities respectively]; $\\boldsymbol{p}_{chld}$ denotes the probability of the child; and $\\boldsymbol{\\alpha}$, $\\boldsymbol{\\mu}$, and $\\boldsymbol{\\sigma}^2$ represent the parameters of the resulting joint distribution shown in the simplices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "   <img src=\"img/rdp.png\" alt=\"A plate diagram of Recursive Dirichlet Process\">\n",
    "    <figcaption>\n",
    "    A plate diagram of Recursive Dirichlet Process\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "<table>\n",
    "<caption>\n",
    "Results of discourse-aware sentiment analysis methods<br/>\n",
    "(LCRF &mdash; latent conditional random fields, LMCRF &mdash; latent-marginalized conditional random fields, RDP &mdash; recursive Dirichlet process, DDR &mdash; discourse-depth reweighting~(Bhatia et al., 2015), R2N2 &mdash; rhetorical recursive neural network (Bhatia et al., 2015), WNG &mdash; (Wang et al, 2013), Last &mdash; polarity determined by the last EDU, Root &mdash; polarity determined by the root EDU(s), No-Discourse &mdash; discourse-unaware classifier)\n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td rowspan=\"2\">Method</td>\n",
    "<td colspan=\"3\">Positive</td>\n",
    "<td colspan=\"3\">Negative</td>\n",
    "<td colspan=\"3\">Neutral</td>\n",
    "<td rowspan=\"2\">Macro-$F_1$</td>\n",
    "<td rowspan=\"2\">Micro-$F_1$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "<td>Precision</td>\n",
    "<td>Recall</td>\n",
    "<td>$F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td colspan=\"12\">\n",
    "    PotTS\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LCRF</td>\n",
    "<td>0.76</td>\n",
    "<td>0.79</td>\n",
    "<td>0.77</td>\n",
    "<td>0.61</td>\n",
    "<td>0.53</td>\n",
    "<td>0.56</td>\n",
    "<td>0.7</td>\n",
    "<td>0.71</td>\n",
    "<td>0.71</td>\n",
    "<td>0.67</td>\n",
    "<td>0.709</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LMCRF</td>\n",
    "<td>0.77</td>\n",
    "<td>0.77</td>\n",
    "<td>0.77</td>\n",
    "<td>0.61</td>\n",
    "<td>0.54</td>\n",
    "<td>0.57</td>\n",
    "<td>0.69</td>\n",
    "<td>0.74</td>\n",
    "<td>0.72</td>\n",
    "<td>0.671</td>\n",
    "<td>0.712</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RDP</td>\n",
    "<td>0.73</td>\n",
    "<td>0.82</td>\n",
    "<td>0.77</td>\n",
    "<td>0.61</td>\n",
    "<td>0.56</td>\n",
    "<td>0.58</td>\n",
    "<td>0.73</td>\n",
    "<td>0.65</td>\n",
    "<td>0.69</td>\n",
    "<td>0.678</td>\n",
    "<td>0.706</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DDR</td>\n",
    "<td>0.73</td>\n",
    "<td>0.77</td>\n",
    "<td>0.75</td>\n",
    "<td>0.54</td>\n",
    "<td>0.59</td>\n",
    "<td>0.56</td>\n",
    "<td>0.69</td>\n",
    "<td>0.61</td>\n",
    "<td>0.65</td>\n",
    "<td>0.655</td>\n",
    "<td>0.674</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>R2N2</td>\n",
    "<td>0.74</td>\n",
    "<td>0.78</td>\n",
    "<td>0.76</td>\n",
    "<td>0.59</td>\n",
    "<td>0.53</td>\n",
    "<td>0.56</td>\n",
    "<td>0.68</td>\n",
    "<td>0.68</td>\n",
    "<td>0.68</td>\n",
    "<td>0.657</td>\n",
    "<td>0.692</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>WNG</td>\n",
    "<td>0.58</td>\n",
    "<td>0.79</td>\n",
    "<td>0.67</td>\n",
    "<td>0.61</td>\n",
    "<td>0.21</td>\n",
    "<td>0.31</td>\n",
    "<td>0.61</td>\n",
    "<td>0.57</td>\n",
    "<td>0.59</td>\n",
    "<td>0.487</td>\n",
    "<td>0.59</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Last</td>\n",
    "<td>0.52</td>\n",
    "<td>0.83</td>\n",
    "<td>0.64</td>\n",
    "<td>0.57</td>\n",
    "<td>0.17</td>\n",
    "<td>0.26</td>\n",
    "<td>0.61</td>\n",
    "<td>0.43</td>\n",
    "<td>0.5</td>\n",
    "<td>0.453</td>\n",
    "<td>0.549</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Root</td>\n",
    "<td>0.56</td>\n",
    "<td>0.73</td>\n",
    "<td>0.64</td>\n",
    "<td>0.58</td>\n",
    "<td>0.22</td>\n",
    "<td>0.32</td>\n",
    "<td>0.55</td>\n",
    "<td>0.54</td>\n",
    "<td>0.54</td>\n",
    "<td>0.481</td>\n",
    "<td>0.56</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>No-Discourse</td>\n",
    "<td>0.73</td>\n",
    "<td>0.82</td>\n",
    "<td>0.77</td>\n",
    "<td>0.61</td>\n",
    "<td>0.56</td>\n",
    "<td>0.58</td>\n",
    "<td>0.72</td>\n",
    "<td>0.66</td>\n",
    "<td>0.69</td>\n",
    "<td>0.677</td>\n",
    "<td>0.706</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td colspan=\"12\">\n",
    "    SB10k\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LCRF</td>\n",
    "<td>0.64</td>\n",
    "<td>0.69</td>\n",
    "<td>0.66</td>\n",
    "<td>0.45</td>\n",
    "<td>0.45</td>\n",
    "<td>0.45</td>\n",
    "<td>0.82</td>\n",
    "<td>0.79</td>\n",
    "<td>0.8</td>\n",
    "<td>0.557</td>\n",
    "<td>0.713</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LMCRF</td>\n",
    "<td>0.64</td>\n",
    "<td>0.69</td>\n",
    "<td>0.67</td>\n",
    "<td>0.45</td>\n",
    "<td>0.45</td>\n",
    "<td>0.45</td>\n",
    "<td>0.82</td>\n",
    "<td>0.79</td>\n",
    "<td>0.8</td>\n",
    "<td>0.56</td>\n",
    "<td>0.715</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>RDP</td>\n",
    "<td>0.64</td>\n",
    "<td>0.69</td>\n",
    "<td>0.66</td>\n",
    "<td>0.45</td>\n",
    "<td>0.45</td>\n",
    "<td>0.45</td>\n",
    "<td>0.82</td>\n",
    "<td>0.79</td>\n",
    "<td>0.8</td>\n",
    "<td>0.557</td>\n",
    "<td>0.713</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DDR</td>\n",
    "<td>0.59</td>\n",
    "<td>0.63</td>\n",
    "<td>0.61</td>\n",
    "<td>0.48</td>\n",
    "<td>0.44</td>\n",
    "<td>0.46</td>\n",
    "<td>0.77</td>\n",
    "<td>0.76</td>\n",
    "<td>0.77</td>\n",
    "<td>0.534</td>\n",
    "<td>0.681</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>R2N2</td>\n",
    "<td>0.64</td>\n",
    "<td>0.69</td>\n",
    "<td>0.66</td>\n",
    "<td>0.46</td>\n",
    "<td>0.45</td>\n",
    "<td>0.45</td>\n",
    "<td>0.81</td>\n",
    "<td>0.79</td>\n",
    "<td>0.8</td>\n",
    "<td>0.559</td>\n",
    "<td>0.713</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>WNG</td>\n",
    "<td>0.61</td>\n",
    "<td>0.63</td>\n",
    "<td>0.62</td>\n",
    "<td>0.46</td>\n",
    "<td>0.29</td>\n",
    "<td>0.36</td>\n",
    "<td>0.76</td>\n",
    "<td>0.82</td>\n",
    "<td>0.79</td>\n",
    "<td>0.488</td>\n",
    "<td>0.693</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Last</td>\n",
    "<td>0.56</td>\n",
    "<td>0.55</td>\n",
    "<td>0.56</td>\n",
    "<td>0.46</td>\n",
    "<td>0.29</td>\n",
    "<td>0.36</td>\n",
    "<td>0.73</td>\n",
    "<td>0.8</td>\n",
    "<td>0.76</td>\n",
    "<td>0.459</td>\n",
    "<td>0.661</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Root</td>\n",
    "<td>0.51</td>\n",
    "<td>0.55</td>\n",
    "<td>0.53</td>\n",
    "<td>0.4</td>\n",
    "<td>0.3</td>\n",
    "<td>0.35</td>\n",
    "<td>0.74</td>\n",
    "<td>0.76</td>\n",
    "<td>0.75</td>\n",
    "<td>0.438</td>\n",
    "<td>0.64</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>No-Discourse</td>\n",
    "<td>0.64</td>\n",
    "<td>0.69</td>\n",
    "<td>0.66</td>\n",
    "<td>0.45</td>\n",
    "<td>0.45</td>\n",
    "<td>0.45</td>\n",
    "<td>0.82</td>\n",
    "<td>0.79</td>\n",
    "<td>0.8</td>\n",
    "<td>0.557</td>\n",
    "<td>0.713</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Classifier\n",
    "\n",
    "Since the scores of the presented discourse-aware sentiment systems crucially depended on the accuracy of the base classifier (the one we use to assign sentiment scores to single EDUs), I decided to rerun the experiments using the best systems from the two other message-level sentiment anaysis groups (lexicon- and machine-learning&ndash;), namely:\n",
    "* the system of Hu and Liu (2004);\n",
    "* and the SVM classifier of Mohammad et al. (2013)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"img/dasa-potts-bc-macro-F1.png\" alt=\"Macro-$F_1$ Results on the PotTS corpus with Different Base Classifiers\">\n",
    "<figcaption>Macro-$F_1$</figcaption>\n",
    "<img src=\"img/dasa-potts-bc-micro-F1.png\" alt=\"Micro-$F_1$ Results on the PotTS corpus with Different Base Classifiers\">\n",
    "<figcaption>Micro-$F_1$</figcaption>\n",
    "</figure>\n",
    "<div>\n",
    "Results of discourse-aware sentiment analysis methods with different base classifiers on the PotTS corpus\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"img/dasa-sb10k-bc-macro-F1.png\" alt=\"Macro-$F_1$ Results on the SB10k corpus with Different Base Classifiers\">\n",
    "<figcaption>Macro-$F_1$</figcaption>\n",
    "<img src=\"img/dasa-sb10k-bc-micro-F1.png\" alt=\"Micro-$F_1$ Results on the SB10k corpus with Different Base Classifiers\">\n",
    "<figcaption>Micro-$F_1$</figcaption>\n",
    "</figure>\n",
    "<div>\n",
    "Results of discourse-aware sentiment analysis methods with different base classifiers on the SB10k corpus\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation Scheme\n",
    "\n",
    "Another factor that could significantly affect the results of discourse-aware sentiment methods was the set of discourse relations distinguished by the parsing system. On the one hand, this set considerably affected the quality of discourse parsing (with richer sets typically leading to lower accuracy); on the other hand, it was also important to the sentiment systems (with richer sets allowing them to distinguish more facets).  To check which of these factors had a greater impact on the net results of discourse-aware sentiment methods, I reran the experiments with the following alternative sets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<caption>\n",
    "RST relations used in the original Potsdam Commentary Corpus and different\n",
    "discourse-aware sentiment methods<br/>\n",
    "(default relation, which subsumes the rest of the links, is shown in boldface)    \n",
    "</caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td>Scheme</td>\n",
    "<td>Relation Set</td>\n",
    "<td>Equivalence Classes</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Bhatia et al.</td>\n",
    "<td>{Contrastive, <div class=\"default-rel\">Non-Contrastive}</td>\n",
    "<td>Contrastive &#x225D; {Antithesis, Antithesis-E, Comparison, Concession, Consequence-S, Contrast, Problem-Solution}.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Chenlo et al.</td>\n",
    "<td>{Attribution, Background, Cause, Comparison, Condition, Consequence, Contrast, Elaboration, Enablement, Evaluation, Explanation, Joint, Otherwise, Temporal, <div class=\"default-rel\">Other</div>}</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Heerschop et al.</td>\n",
    "<td>{Attribution, Background, Cause, Condition, Contrast, Elaboration, Enablement, Explanation, <div class=\"default-rel\">Other</div>}</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PCC</td>\n",
    "<td>{Antithesis, Background, Cause, Circumstance, Concession, Condition, Conjunction, Contrast, Disjunction, E-Elaboration, Elaboration, Enablement, Evaluation-N, Evaluation-S, Evidence, Interpretation, Joint, Justify,       List, Means, Motivation, Otherwise, Preparation, Purpose, Reason, Restatement, Restatement-MN, Result, Sequence, Solutionhood, Summary, Unconditional, Unless, Unstated-Relation}</td>\n",
    "<td></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Zhou et al.</td>\n",
    "<td>{Contrast, Condition, Continuation, Cause, Purpose, <div class=\"default-rel\">Other</div>}</td>\n",
    "<td>Contrast &#x225D; {Antithesis, Concession, Contrast, Otherwise<br/>\n",
    "    Continuation &#x225D; {Continuation, Parallel}<br/>\n",
    "    Cause &#x225D; {Evidence, Nonvolitional-Cause, Nonvolitional-Result, Volitional Cause, Volitional-Result}</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <caption>\n",
    "        Results of the DPLP parser on PCC 2.0 with different relation schemes\n",
    "    </caption>\n",
    "<thead>\n",
    "<tr>\n",
    "<td>Relation Scheme</td>\n",
    "<td>Span $F_1$</td>\n",
    "<td>Nuclearity $F_1$</td>\n",
    "<td>Relation $F_1$</td>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Bhatia et al.</td>\n",
    "<td>0.777</td>\n",
    "<td>0.512</td>\n",
    "<td>0.396</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Chenlo et al.</td>\n",
    "<td>0.769</td>\n",
    "<td>0.505</td>\n",
    "<td>0.362</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Heerschop et al.</td>\n",
    "<td>0.774</td>\n",
    "<td>0.51</td>\n",
    "<td>0.361</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PCC</td>\n",
    "<td>0.776</td>\n",
    "<td>0.534</td>\n",
    "<td>0.326</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Zhou et al.</td>\n",
    "<td>0.776</td>\n",
    "<td>0.501</td>\n",
    "<td>0.388</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"img/dasa-potts-macro-F1.png\" alt=\"Macro-$F_1$ Results on the PotTS corpus with Different Relation Schemes\">\n",
    "<figcaption>Macro-$F_1$</figcaption>\n",
    "<img src=\"img/dasa-potts-micro-F1.png\" alt=\"Micro-$F_1$ Results on the PotTS corpus with Different Relation Schemes\">\n",
    "<figcaption>Micro-$F_1$</figcaption>\n",
    "</figure>\n",
    "<div>\n",
    "Results of discourse-aware sentiment classifiers for different relation schemes on the\n",
    "PotTS corpus\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<img src=\"img/dasa-sb10k-macro-F1.png\" alt=\"Macro-$F_1$ Results on the SB10k corpus with Different Relation Schemes\">\n",
    "<figcaption>Macro-$F_1$</figcaption>\n",
    "<img src=\"img/dasa-sb10k-micro-F1.png\" alt=\"Micro-$F_1$ Results on the SB10k corpus with Different Relation Schemes\">\n",
    "<figcaption>Micro-$F_1$</figcaption>\n",
    "</figure>\n",
    "<div>\n",
    "Results of discourse-aware sentiment classifiers for different relation schemes on the\n",
    "sb10k corpus\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I have presented an overview of the most popular approaches to automatic discourse analysis (RST, PDTB, and SDRT) and explained why we think that one of these frameworks (Rhetorical Structure Theory) would be more amenable to the purposes of discourse-aware sentiment analysis than the others;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* to substantiate our claims and to see whether the lexicon-based attention system introduced in the previous chapter would indeed benefit from information on discourse structure, I segmented all microblogs from the PotTS and SB10k corpora into elementary discourse units using the SVM-based segmenter of Sidarenka et al. (2015} and parsed these messages with the RST parser of Ji and Eisenstein (2014), which had been previously retrained on the Potsdam Commentary Corpus (Stede and Neumann, 2014);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* afterwards, I estimated the results of existing discourse-aware sentiment methods (the systems of Wang and Wu (2015) and Bhatia et al. (2015) and also evaluated two simpler baselines (in which I predicted the semantic orientation of a tweet by taking the polarity of its last and root EDUs), getting the best results with the R2N2 solution (0.657 and 0.559 macro-\\F{} on PotTS and SB10k respectively);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I could, however, improve on these scores and also outperform the plain LBA system (although by a not very large margin) with three proposed discourse-aware sentiment solutions (latent and latent-marginalized conditional random fields and Recursive Dirichlet Process), pushing the macro-averaged $F_1$-score on PotTS up to 0.678 and increasing the result on SB10k to 0.56 macro-$F_1$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a subsequent evaluation of these approaches with different settings showed that the results of all discourse-aware methods largely correlate with the scores of the base sentiment classifier and also revealed an important drawback of the latent-marginalized CRFs, which failed to predict any positive or negative instance on the test set of the SB10k corpus when trained in combination with the lexicon-based approach of Hu and Liu (2004};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* nevertheless, almost all DASA solutions could improve their scores when tested on richer sets of discourse relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In my dissertation, I have presented a corpus of $\\approx8,000$ German tweets, which pertain to four different topics (federal elections, papal conclave, general political discussions, and casual everyday conversations) and were sampled according to three formal criteria (tweets containing a polar term, messages having a smiley, and all remaining microblogs).  After annotating the corpus in three steps (initial, adjudication, and final), I have attained a reliable level of inter-annotator agreement for all elements (sentiments, sources, targets, polar terms, downtoners, negations, and intensifiers), finding that both selection criteria (topics and formal traits) significantly affected the distribution of sentiments and polar terms and the reliability of their annotation;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then, I have compared existing German sentiment lexicons, which were translated from English resources and\n",
    "  revised by human experts, with lexicons that were generated automatically from scratch with the help of state-of-the-art dictionary-, corpus -, and word-embedding&ndash;based methods.  An evaluation of these approaches on our corpus showed that semi-automatically translated polarity lists were generally better than the automatically induced ones, reaching 0.587 macro-$F_1$ and attaining 0.955 micro-$F_1$&ndash; score on the prediction of polar terms.  Furthermore, among fully automatic methods, dictionary-based systems showed stronger results than their corpus- and word-embedding&ndash;based competitors, yielding 0.479 macro-$F_1$ and 0.962 micro-$F_1$.  We could, however, improve on the latter metric (pushing it to 0.963) with our proposed linear projection solution, in which we first found a line that maximized the mutual distance between the projections of seed vectors with opposite semantic orientations and then projected the embeddings of all remaining words on that line, considering the distance of these projections to the median as polarity scores of respective terms;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In Chapter 4, I turned the attention to the aspect-based sentiment analysis, in which I tried to predict the\n",
    "  spans of sentiments, targets, and holders of the opinions using two most popular approaches to this task&mdash;conditional random fields and recurrent neural networks.  I obtained my best results (0.287\n",
    "  macro-$F_1$) with the first-order linear-chain CRFs and could increase these scores by using alternative topologies of CRFs (second-order linear-chain and semi-Markov CRFs), also boosting the macro-averaged $F_1$to 0.38 by taking a narrower interpretation  of sentiment spans (in which I only assigned the *sentiment* tag to polar terms).  Further evaluation of these methods proved the utility of the text normalization step (which raised the macro-$F_1$ of the CRF-method by almost 3%) and task-specific word embeddings with the least-squares fallback (which improved the macro-$F_1$--score of the GRU system by 1.4%);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Afterwards, in Chapter 5, I addressed one of the most popular objective in contemporary sentiment analysis&mdash;**message-level sentiment analysis** (MLSA).  To get a better overview of the numerous existing systems, I compared three larger families of MLSA methods: **dictionary-**, **machine-learning&ndash;**, and **deep-learning&ndash;based** ones; finding that **the last two groups performed significantly better** than the lexicon-based approaches (the best macro-$F_1$&ndash;scores of machine- and deep-learning methods run up to 0.677 and 0.69 respectively, whereas the best lexicon-based solution [Hu and Liu, 2004] only reached 0.641 macro-$F_1$).  Apart from this, I improved the results of many reimplemented approaches by changing their default configuration (e.g., abandoning polarity changing rules of lexicon-based systems, using alternative classifiers in ML-based systems, or taking the least-squares embeddings for DL-based methods).  In addition to the numerous reimplementations of popular existing algorithms, I also my own solution&mdash;**lexicon-based attention** (LBA), in which I tried to unite the lexicon and deep-learning paradigms by taking a bidirectional LSTM network and explicitly pointing its attention to the polar terms that appeared in the analyzed messages.  With this solution, I not only outperformed all alternative DL systems but also improved on the scores of ML-based classifiers, attaining 0.69 macro-$F_1$ and 0.73 micro-$F_1$ on the PotTS corpus.  Similarly to the findings in the previous chapter, I observed a strong positive effect of text normalization and task-specific embeddings with the least-squares approximation;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, in the last part, I tried to improve the results of the proposed LBA solution by making it aware of the discourse structure.  For this purpose, I segmented all microblogs from the PotTS and SB10k corpora into elementary discourse units, individually analyzing each of these segments with our MLSA classifier, and then estimated the overall polarity of a tweet by joining the polarity scores of its EDUs over the RST tree.  We proposed three different ways of doing this joining (latent CRFs, latent-marginalized CRFs, and Recursive Dirichlet Process), obtaining better results than existing discourse-aware sentiment methods and also outperforming the original discourse-unaware baseline.  In the concluding experiments, we further improved these scores by using manually annotated RST trees and richer subsets of iscourse relations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Can we apply opinion mining methods devised for standard English to German Twitter?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we can, but the success of these approaches might significantly vary depending on the task, the size and the reliability of the training data, as well as the evaluation metric that we use.  I can, however, provide a few general rules of thumb:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Prefer methods that are closest to your training objective and that were trained under similar conditions w.r.t. the amount of data, their class distribution and domain;**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Put every single setting of these methods into question**;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Try using manually labeled resources for your target domain, if they are available, but pay attention to the quality of their annotation&mdash;it often matters more than the corpus size;**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Prefer machine-learning methods to hard-coded rules**&mdash;they will penalize their bad components automatically by themselves;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Do not use randomly initialized word embeddings for deep-learning systems**&mdashinitialize them with language-model vectors;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Which groups of approaches are best suited for which sentiment tasks?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Sentiment lexicon generation** is more amenable to dictionary-based solutions and my proposed word-embedding&ndash;based algorithms;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Aspect-based sentiment analysis** can be better addressed with probabilistic graphical models, such as conditional random fields;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * **Message-level sentiment analysis** can be efficiently tackled with both machine- and deep-learning approaches;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * Finally, **probabilistic graphical models** strike back at discourse-aware opinion mining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **How much do word- and discourse-level analyses affect message-level sentiment classification?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  My evaluation showed that the macro-averaged $F_1$-scores of our proposed lexicon-based attention system varied   by up to 14% (from 0.64 to 0.69 macro-$F_1$ on the PotTS corpus, and from 0.44 to 0.58 on SB10k) depending on     the lexicon used. At the same, discourse enhancements could only improve the results of LBA by at most 1.5%       percent (from 0.677 to 0.678 on PotTS, and from 0.557 to 0.572 on SB10k)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Does text normalization help analyze sentiments?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Yes, it definitely does. Normalization significantly improved the quality of aspect-based and message-level sentiment analyses, boosting the results on the former task by up to 4% and improving the macro-averaged $F_1$-measure of message-level sentiment methods by up to 25%;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Can we do better than existing approaches?**\n",
    "Yes, we can with the proposed:\n",
    "* **linear-projection algorithm**;\n",
    "* **alternative CRF topologies**;\n",
    "* **lexicon-based attention network**;\n",
    "* and **latent-marginalized CRFs** and **Recursive Dirichlet Process**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contributions\n",
    "\n",
    "* The **Potsdam Twitter Sentiment Corpus**: https://github.com/WladimirSidorenko/PotTS;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Lexicon Generation Methods**: https://github.com/WladimirSidorenko/SentiLex;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Text-Normalization Pipeline** and **Aspect-Based Sentiment Methods**: https://github.com/WladimirSidorenko/TextNormalization;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **MLSA Approaches**: https://github.com/WladimirSidorenko/CGSA;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Discourse-Aware Sentiment Systems**: https://github.com/WladimirSidorenko/DASA;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Discourse Segmenter**: https://github.com/WladimirSidorenko/DiscourseSegmenter;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Retrained Discourse Parser**: https://github.com/WladimirSidorenko/RSTParser;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Extened Version of RST Markup Tool**: https://github.com/WladimirSidorenko/RSTTool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
