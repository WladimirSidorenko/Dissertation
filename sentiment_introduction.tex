% FILE: sentiment.tex  Version 0.01
% AUTHOR: Uladzimir Sidarenka

% This is a modified version of the file main.tex developed by the
% University Duisburg-Essen, Duisburg, AG Prof. Dr. Günter Törner
% Verena Gondek, Andy Braune, Henning Kerstan Fachbereich Mathematik
% Lotharstr. 65., 47057 Duisburg entstanden im Rahmen des
% DFG-Projektes DissOnlineTutor in Zusammenarbeit mit der
% Humboldt-Universitaet zu Berlin AG Elektronisches Publizieren Joanna
% Rycko und der DNB - Deutsche Nationalbibliothek

Interpersonal communication is not only a way to share objective
information with other people but also a vibrant channel to convey
one's subjective thoughts, attitudes, and feelings.  It is, in fact,
this latter use which provides a personal touch to our discourses,
making them more grasping, more entertaining, and more living.  And it
is often this use which significantly influences our decisions,
preferences, and choices in everyday life.  Therefore, a high-quality
automatic analysis of the subjective part of information is typically
not less important than the extraction and analysis of objective
facts.

\section{Introduction to Sentiment Analysis}

The field of knowledge which deals with the analysis of people's
opinions, sentiments, evaluations, appraisals, attitudes, and emotions
towards particular entities mentioned in discourse is called
\emph{sentiment analysis} (SA) \citep{Liu:12}.  The definition of this
discipline, however, much like the definition of the term
\emph{sentiment} itself, is neither complete nor universally accepted.
The main reasons for this are
\begin{inparaenum}[\itshape i)\upshape]
  \item a frequently unclear delimitation of the subjective and
    objective components of information and
  \item the heteroginity of the language system to which SA methods
    are applied.
\end{inparaenum}

The former factor, for instance, makes it difficult to delimitate what
kinds of events and expressions should actually belong to the domain
of sentiment analysis and which ones shall rather be excluded from it.
A prominent example of such problematic borderline cases are the
so-called subjective facts, such as \emph{terrorist attacks} or
\emph{cancer drugs}, which some people perceive as emotionally laden
terms while others regard them as purely objective statements.

The latter factor complicates a precise definition of SA because
different levels of the language have their own concepts of
subjectivity, which in turn require different approaches and methods
to use.  Depending on the language level being analyzed, researchers
typically distinguish three different subtypes of sentiment analysis:
\begin{itemize}
  \item\emph{subsentential} of \emph{fine-grained} SA whose task is to
    determine and analyze specific subjective opinions and/or
    evaluations within single clauses,
  \item\emph{sentential} analysis which tries to ascribe a single
    polarity class to each sentence in a text, and
  \item\emph{suprasentential} or \emph{document-level} sentiment
    classification which seeks to determine the polarity and
    subjectivity classes of complete discourses.
\end{itemize}

Each of these subtypes has its own characteristic strengths and
weaknesses.  The fine-grained SA, for example, is typically considered
to be the ultimate goal of any opinion mining\footnote{Following
  \citet{Liu:12}, we do not make a distinction between the terms
  \emph{opinion mining} and \emph{sentiment analysis} and use both
  expressions interchangeably in this thesis.}  system as it aims at
the highest possible recall of all subjective expressions occurring in
a text.  At the same time, this task is unfortunately very challenging
even for human beings, as we will show in Section \ref{sec:corpus},
let alone computer programs.  The consequently low results often
intimidate researchers and prevent subsentential SA systems from being
used in industrial applications.

The last two disciplines (sentential and document-level analyses) can
be considered as approximations of the fine-grained SA.  They coarsen
the targeted recall down to the level of sentences or texts
respectively, trying to determine only one (the most prominent)
expression of subjectivity per analyzed unit.  In contrast to the
subsentential SA, these approaches typically yield better results at
the cost of sacrificing important details.

Due to these crucial disparities between different variants of
sentiment analysis, speaking of the difficulty or easiness of this
task for a specific domain in general is in the same way wrong as
making overall judgments about the amenability of that domain to the
whole natural language processing field: one always has to specify the
precise level of sentiment analysis whose difficulty is being analyzed
as she has to set apart a particular NLP task in order to assess its
complicatedness.

In this chapter, we will primarily concentrate on the most challenging
SA objective -- that of the fine-grained SA.  After a brief summary of
related work done on the opinion mining in general and sentiment
analysis of social media in particular, we will introduce a
comprehensive corpus of German tweets that has been created for the
purpose of this work.  A detailed inter-annotator agreement study of
this dataset will reveal which linguistic and extra-linguistic factors
significantly influence the distribution of sentiments in Twitter and
which of them cause utter confusion among human experts.  After
obtaining an upper bound on human performance, we will successively
compare with it the results attained by automatic sentiment systems,
starting with the most fundamental task of recognizing subjective
expressions and concluding with the ultimate goal of recognizing
textual spans of sentiments, their objects of evaluation
(\emph{targets}), and authors (\emph{sources}).

\subsection{Prehistory of the Field}
TODO

\subsection{Sentiment Analysis of Social Media}

One of the main problems that people working on opinion mining are
typically confronted with in the first place is that of the discourse
domain to analyze.  Since sentiment analysis is a highly
domain-dependent task \citep[cf.][]{Aue:05,Blitzer:07,Li:08} --
i.e. systems trained for specific topics and on specific text genres
do not neatly generalize to other subjects and other linguistic styles
-- a natural question that arises in this context is which of the
domains should be addressed first in that case.

While earlier sentiment works primarily dealt with narratives
\citep{Wiebe:90a,Wiebe:94} or newspaper articles
\citep{Wiebe:03,Wiebe:05,Bautin:08}, it soon became clear that social
media provides a much more fertile ground for mining people's
opinions.  The reason for this is the virtual absence of any
moderation on many modern CMC services.  This lack of censorship
allows users to be upfront about their feelings.  Combined with the
great popularity of social networks, such freedom of expressing one's
thoughts makes social media the arguably most prolific channel for
sharing (and mining) personal emotions.

It is therefore not surprising that the rapid emergence of numerous SM
websites was accompanied by an increasing interest in their content
from many NLP practitioners.  One of the first who attempted to
extract users' evaluations automatically from CMC services were
\citet{Das:01}.  The authors investigated the correlation between
users' attitudes on economic chat boards and stock prices for about
eight stocks.  For this purpose, they trained a collection of five
classifiers on a training set of 500 messages manually labeled with
three classes: \emph{buy} (positive evaluations), \emph{sell}
(negative evaluations), and \emph{null} (neutral statements).  With
these classifiers, they subsequently annotated the rest of the
downloaded 25,000 posts, taking the majority votes of the five systems
as the final labels, and scrutinized the interrelationship between
these predictions and the observed stock indices.
%% This study revealed that user chats strongly correlated with stock
%% developments in a reactive way: changes in stock prices had
%% typically lead to fluctuations of public opinions but not vica
%% versa.

\citet{Glance:05} also used sentiment analysis for business
intelligence purposes by mining users' opinions on Usenet newsgroups
with a set of hand-crafted rules. TODO: add more works

A slightly different domain and objective were addressed by
\citet{Turney:02} who performed a two-class classification of 410
Epinions comments, dividing customers' reviews of automobiles, banks,
movies, and travel destinations into \emph{recommended} (thumbs up)
and \emph{not recommended} (thumbs down) ones.  In order to make these
decisions, the proposed system first computed the sum of the pointwise
mutual information (PMI) scores between the adjectival and adverbial
phrases occurring in a review text and the word ``excellent'' and then
subtracted from this the PMIs of the found phrases in conjunction with
the word ``poor''.  Reviews with negative results were subsequently
considered as \emph{thumbs down} and comments with non-negative total
scores were correspondingly classified as \emph{thumbs up}.

According to \citet{Turney:02}, the most challenging type of reviews
to analyze in these experiments were the movie critiques (the accuracy
on this subset only reached 66\%, whereas, for banks and automobiles,
it attained 80 and 84\% respectively).  This finding was also
confirmed by \citet{Pang:02} who tried out several machine learning
classifiers on a manually annotated corpus of $\approx2,000$ write-ups
from the Internet Movie Database (IMDb).  The best results (82.9\%
accuracy) for the two-class classification task (\emph{positive}
versus \emph{negative}) were attained by an SVM system which used the
presence of unigrams as classification features.

Due to its high commercial impact, opinion mining of customer reviews
soon became one of the most prolific topics in the sentiment analysis
research. \cite{Dave:03}, for example, attempted to classify C|Net and
Amazon reviews of movies and electrical appliances as positive or
negative using the Na\"{\i}ve Bayes and SVM approaches.
%% The authors found that replacing certain entities, such as product
%% names, abbreviations, or numerals, with special meta-tokens and
%% using token n-grams of different lengths had a significant positive
%% effect on the accuracy of both classifiers.
TODO: add more works

With the increasing spread of blogging services and social networks,
sentiment researchers have also gradually shifted the focus of their
work to these nascent text genres. \citet{Mishne:05} and
\citet{Mishne:07}, for example, attempted to classify Livejournal
blogs according to the moods of their authors (e.g.  \emph{amused},
\emph{tired}, \emph{happy} etc.).  The gold labels of their
$\approx$~815,000 blog corpus were obtained automatically from blog
labels on the Livejournal website.  On this corpus, the authors
trained a supervised SVM classifier, reaching an average of 8\%
improvement over the 50\% baseline.

\citet{Chesley:06} also applied an SVM system in order to classify
user blogs into objective, positive and negative subjective ones.  The
authors derived their features from the Wiktionary and the InfoXtract
lexicon \citep{Srihari:03} and attained an accuracy increase of up to
15~\% over the majority class baseline.  This research direction was
further continued by \citet{Godbole:07}, who applied a lexicon-based
method to detect users' opinions in newspapers and blogs, and
\citet{Gill:08}, who conducted an inter-annotator agreement study of
mood detection in blogs, finding that raters' consensus on the
expressed emotions strongly increased with the length of the blogs.

Speaking of blog length, it should certainly be said that the
inception of the micro-blogging service Twitter in 2006 was a real
game changer to the opinion research of social media: the sudden
availability of huge amounts of data, the presence of all possible
social, national, and age groups as well as a high idiosyncracy of the
language used gave rise to numerous scientific publications worth to
be described in a separate subsection.

\subsection{Sentiment Analysis of Twitter}

One of the first attempts to analyze sentiments on Twitter was made by
\citet{Go:09}.  For their experiments, the authors collected a set of
1,600,000 tweets containing smileys.  Based on these emoticons, they
automatically derived polarity classes for these messages (positive or
negative) and used them to train a Na\"{\i}ve Bayes, a MaxEnt, and an
SVM classifier.  The best $F$-score for this two-class classification
problem could be achieved by the last system and run up to 82.2\%.

Similar work was done later by \citet{Pak:10} who used a Na\"{\i}ve
Bayes approach to differentiate between neutral, positive, and
negative microblogs. \citet{Barbosa:10} also gathered a collection of
200,000 tweets from three publicly available sentiment web-services
and then trained an SVM classifier to predict the subjectivity and the
polarity class of new unseen messages.

Works attempting a more fine-grained sentiment analysis on Twitter
usually try to derive a common polarity class for each message with
respect to a particular target that is mentioned in that microblog.

\citet{Jiang:11}, for instance, tried to classify the polarity of
microblogs pertaining to a predefined set of specific topics, like
\emph{Obama}, \emph{Google}, \emph{iPad} etc.  To this end, the
authors manually labeled a corpus of 1,939 messages and trained a
binary SVM model in order to predict the subjectivity and the polarity
of the tweets with respect to the given subjects.

This classifier could achieve an accuracy of 68.2\% for the
subjectivity classification and 85.6\% for the polarity prediction.
The $F$-score of this system for the latter task could further be
improved from 66\% to 68.3\% by incorporating the information about
the predicted polarity class of the re-tweets, replies, and other
microblogs posted by the same author.

\citet{Mitchell:13} broadened the set of possible targets by allowing
any named entities found in microblogs to be associated with a
specific polarity.  For that purpose, the authors combined a CRF-based
NER system with a sentiment predicting CRF by considering three
different possibilities of such combination: a pipeline approach, a
joint multi-layer model, and a single classifier with a combined
tagset.  The best scores on their corpora of 7,105 Spanish and 2,350
English tweets could be achieved with the joint and pipeline
approaches.  The accuracy of recognizing the opinionated named
entities amounted to 31\% for Spanish and 30.4\% for English.

Other notable works in this direction include \citet{Chunping:14} who
first applied a Na\"{\i}ve Bayes classifier to predict the
subjectivity class of microblogs and then sequentially used two CRF
models to predict the particular type of subjectivity (such as anger,
fear, happiness etc.) for message sentences.

Other notable works in this direction include \citet{Dong:14} who used
a recurrent neural network to predict the polarity class associated
with the opinion targets.  They, however, assumed the targets of
sentiments to be apriori known and only were interested whether a
positive or a negative judgement was made about them.

\citet{Derks:08}

The \texttt{SentiStrength} system proposed by \cite{Thelwall:12} used
an extensive list of 763 polar terms in order to predict positive and
negative scores for MySpace comments.  The manually assigned scores of
these terms were automatically fine-tuned during training using a
perceptron-like technique.  In addition to the core lexicon, the final
implementation of this system also utilized a set of heuristic methods
and auxiliary modules such as spelling correction algorithm,
dictionaries of booster words and negations as well as special rules
for emoticons, repeated letters, and exclamation marks.  It correctly
predicted positive emotions in 60.6~\% of the cases and attained
73.5~\% accuracy at predicting negative sentiment scores.  All
predictions were made at the level of complete messages.

