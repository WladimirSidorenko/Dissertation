% FILE: main.tex  Version 2.1
% AUTHOR:
% Universität Duisburg-Essen, Standort Duisburg
% AG Prof. Dr. Günter Törner
% Verena Gondek, Andy Braune, Henning Kerstan
% Fachbereich Mathematik
% Lotharstr. 65., 47057 Duisburg
% entstanden im Rahmen des DFG-Projektes DissOnlineTutor
% in Zusammenarbeit mit der
% Humboldt-Universitaet zu Berlin
% AG Elektronisches Publizieren
% Joanna Rycko
% und der
% DNB - Deutsche Nationalbibliothek

\chapter{Discourse-Aware Sentiment Analysis}

Although coarse-grained sentiment analysis methods do a fairly good
job at classifying the overall polarity of a message, putting their
best leg forward to incorporate the compositional principle into that
prediction, a crucial limitation of all these systems is that they
absolutely overlook the structural nature of their input by either
considering it as a single whole (\eg{} bag-of-features approaches) or
analyzing it as a monotone sequence of equally important elements
(\eg{} recurrent neural methods).  Unfortunately, both of these
solutions completely ignore the hierarchical principle of language
\cite{Saussure:90,Hjelmslev:70} which states that complex linguistic
units emerge from the bottom up, by putting together successively
bigger language elements: \eg{} morphemes are united together into
words, several words are joined together into sentences, and multiple
sentences eventually constitue a discourse.  Moreover, besides this
inherent structural hierarchy, even units of the same linguistic level
might play a different role and be of unequal importance to the
meaning of the higher-level whole when joined syntagmatically: for
example, in words, the root morpheme typically conveys more lexical
information than affixes; in sentences, the syntactic head normally
dominates its grammatic dependents; and a similar kind of imbalance
can also be observed in discourse, where one of the sentences
frequently expresses the gist of the whole text.

Exactly the lack of discourse information was one of the main reasons
for the misclassifications of the systems of \citet{Severyn:15},
\citet{Baziotis:17}, and our own LBA method shown in Examples
\ref{snt:cgsa:exmp:severyn-error}, \ref{snt:cgsa:exmp:baziotis-error},
and \ref{snt:cgsa:exmp:lba-error}.  Since none of these approaches
explicitly took the discourse structure into account (in the best
case, the inference simply proceeded from the level of words directly
to the level of message), we decided to check whether making the last
of these solutions (the LBA classifier) aware of discourse phenomena
would improve its results.  However, before we present our findings,
we first would like to take a short digression and give an overview of
the most popular approaches to discourse analysis that exist in the
literature nowadays.  Afterwards, in Section~\ref{sec:dasa:data}, we
will describe the way how we inferred and added discourse information
to the PotTS and SB10k data.  Then, in Section~\ref{sec:dasa:methods},
we will summarize the current state of the art in discourse-aware
sentiment analysis (DASA) and also present several own methods,
evaluating their results on the aforementioned corpora.  After
analyzing the effects of different common factors (such as various
subsets of discourse relations and the quality of the downstream
sentiment classifier), we will recap our results and draw conclusions
in the last section of this part.

\section{Discourse Analysis}\label{sec:dasa:theory}

Since the main focus of this chapter will be on \emph{discourse
  analysis}, we should first clarify what discourse analysis actually
means and which common ways there are to represent and analyze
discourse automatically.

In a nutshell, discourse analysis is an area of research which
explores and analyzes language phenomena beyond the sentence level
\cite{Stede:11}.  Although the scope of this research can be quite
large (ranging from the use of pronouns in a sentence to the logical
composition of the whole document), in our experiments we will
primarily concentrate on the coherence structure of the text, \ie{}
its segmentation into \emph{elementary discourse units}\todo{give a
  definition of EDUs as parenthesis} (EDUs) and induction of
(hierarchical) \emph{coherence relations} between these EDUs\todo{give
  a definition of coherence relations as parenthesis}.

Although the idea of splitting the discourse into smaller meaningful
pieces and inferring semantic links between these parts is anything
but new, dating back to the very origins of general linguistics
\cite{Aristotle:10} and in particular its structuralism branch
\cite{Saussure:90}, an especially big surge of interest in this field
happened in the 1970-s with the fundamental works of
\citet{vanDijk:72} and \citet{vanDijk:83}.  In their study of
different stategies of discourse comprehension, the authors introduced
the notion of local and global coherence, defining the former as a set
of ``rules and conditions for the well-formed concatenation of pairs
of sentences in a linearly ordered sequence'' and specifying the
latter as contraints on the macro-structure of the narrative
\cite[see][]{Hoey:83}.  Similar ideas were also proposed
by~\citet{Longacre:79,Longacre:96}, who considered the paragraph as a
unit of tagmemic grammar which was composed of multiple sentences
according to a predefined set of compositional principles.  Almost
contemporarily with these works, \citet{Winter:77} presented an
extensive study of various lexical means which could connect two
sentences and divided these means into two major groups:
\textsc{Matching} and \textsc{Logical Sequence}; depending on whether
they introduced sentences that were rather elaborating on the previous
content or adding new information to the preceding context.

The increased interest of traditional linguistics in text-level
analysis has rapidly spurred the attention of the broader NLP
community.  Among the first who stressed the importance of discourse
structure for automatic generation and understanding of texts was
\citet{Hobbs:79}, who argued that semantic links between sentences
were one the most important component for building a coherent text.
Similarly to \citeauthor{Winter:77}, \citeauthor{Hobbs:79} also
proposed a set of possible inter-sentence relations which comprised
\textsc{Elaboration}, \textsc{Parallel}, and \textsc{Contrast}.
Although this taxonomy was obviously too small to accommodate all
different semantic ties that could exist between two sentences, this
division had laid the foundations for many successful approaches to
automatic discourse processing that appeared later on.

\textbf{RST} One of the best-known such approaches---\emph{Rhetorical
  Structure Theory} or \emph{RST}---was presented by~\citet{Mann:88}.
Besides revising \citeauthor{Hobbs:79}' inventory of possible
discourse relations and expanding it to a set of 23 elements
(including new items such as \textsc{Evidence}, \textsc{Antithesis},
\textsc{Elaboration}, \textsc{Circumstance}, etc.), the authors also
introduced the distinction between nucleus-satellite (hypotactic) and
multinuclear (paratactic) links, depending on whether the arguments of
these relations were of different or equal importance to the content
of the whole text.  Based on this distinction, they formally described
each relation using a set of constraints on the \emph{Nucleus} (N),
\emph{Satellite} (S), \emph{the N+S combination}, and \emph{the
  effect} of the whole combination on the reader (R).  An excerpt from
the original description of the \textsc{Antithesis} link is shown in
Example~\ref{dasa:exmp:rst-evidence}

\begin{example}[Definition of the \textsc{Antithesis} Relation]\label{dasa:exmp:rst-evidence}
  \textbf{Relation Name:} \textsc{Antithesis}

  \textbf{Constraints on N:} W has positive regard for the situation
  presented in N

  \textbf{Constraints on S:} None

  \textbf{Constraints on the N+S Combination:} the situations presented
  in N and S are in contrast (\ie{} are
  \begin{inparaenum}[(a)]
  \item comprehended as the same in many respects,
  \item comprehended as differing in a few respects and
  \item compared with respect to one or more of these differences
  \end{inparaenum}
  ); because of an incompatibility that arises from the contrast, one
  cannot have positive regard for both the situations presented in N and
  S; comprehending S and the incompatibility between the situations
  presented in N and S increases R's positive regard for the situation
  presented in N

  \textbf{Effect:} R's positive regard for N is increased

  \textbf{Locus of the Effect:} N
\end{example}
With the help of these relations, \citeauthor{Mann:88} then defined
the general structure of a discourse as a projective (constituency)
tree whose nodes were either elementary discourse units or subtrees,
which were connected to each other via semantic edges (relations).

You can see an example of such tree from the Rhetorical Structure
Treebank \cite{Carlson:01a} in Figure~\ref{dasa:fig:rst-tree}.

\begin{figure*}[htbp!]
  \input{rst.tex}
\end{figure*}

Despite its immense popularity and practical utility \cite[see
][]{Marcu:98,Yoshida:14,Bhatia:15,Goyal:16}, RST has been often
criticized for the rigidness of the imposed tree constraint
\cite{Wolf:05} and arbitrariness of distinguished relations
\cite{Nicholas:94,Miltsakaki:04}.  As a result of this criticism, two
alternative approaches to automatic discourse analysis have entered
the stage.

\textbf{PDTB} One of these approaches---PDTB (named so after the Penn
Discourse Treebank \cite{Prasad:04})---was developed by the research
group of the University of Pennsylvania
\cite{Miltsakaki:04,Miltsakaki:04a,Prasad:08} and in essence
represents an \emph{underspecification of RST}, where, instead of
fully specifying the hierarchical structure of the text and providing
a comprehensive set of discourse relations, the authors mainly focused
on the grammatical and lexical means (connectives) that could link two
sentences.  Typical such means were coordinating or subordinating
conjunctions (\eg{} \emph{and}, \emph{because}, \emph{since}, etc.)
and discourse adverbials (\eg{} \emph{however}, \emph{otherwise},
\emph{as a result}, etc.).  According to the authors' definition, each
such connective had to accept two sentential arguments (\textsc{Arg1}
and \textsc{Arg2}) and express a semantic relationship (\emph{sense})
holding between these predicates.%% The choice
%% of these senses was explicitly restricted for each word: for
%% example, the set of possible senses for \emph{nonetheless} included
%% \textsc{Comparison}, \textsc{Conjunction},
%% \textsc{Contra-Expectation}, and \textsc{Contrast}.

Apart from \emph{explicitly} mentioned connectives, \citet{Prasad:04}
also allowed for situations where connective elements were missing
from the text but could be easily inferred by the reader.  They
denoted these cases as \emph{implicit} relations and asked the
annotators to label the arguments of such structures as well,
additionally, specifying the connective which, in their opinion, was
omitted.

Finally, if there was no connective at all, \citet{Prasad:04}
distinguished three different possibilities:
\begin{inparaenum}[(i)]
\item the coherence relation was either expressed by an alternative
  lexical means which made the connective redundant
  (\textsc{AltLex}), or
\item it was achieved by referring to the same entities in both
  arguments (\textsc{EntRel}), or
\item there was no coherence relation (\textsc{NoRel});
\end{inparaenum}
and asked the experts to annotate these situations respectively with
different tags.  Furthermore, in case of reported speech, they also
instructed the annotators to mark the authors of the statements as a
special \textsc{Attribution} span.

Example~\ref{dasa:exmp:pdtb-analysis} shows the previous fragment of
the WSJ corpus now annotated according to the PDTB scheme.  As we can
see from the analysis, the PDTB approach is indeed more flexible in
comparison with RST as it allows the discourse units (arguments) to
overlap, be disjoint or even embedded into other segments.  The
assignment of sense relations is also more straightforward and mainly
determined by the connective that links the arguments (\eg{} \emph{in
  fact}, \emph{while}, or \emph{if}).  But, at the same time, the
structure of this annotation is completely flat so that we can neither
infer which of the sentences plays the central role in the text nor
see the modification scope of supplementary statements.

\begin{example}[Example of PDTB Analysis]\label{dasa:exmp:pdtb-analysis}
  \fbox{Analysts said,} \argone[1]{profit for the dozen or so big drug
    makers, as a group, is estimated to have climbed between 11\% and
    14\%.}  \connective[1]{\textsc{implicit}$:=$in fact}
  \argtwo[1]{\connective[2]{\textsc{explicit}$:=$While}
    \argtwo[2]{that's not spectacular}}, \fbox{Neil Sweig, an analyst
    with Prudential Bache, said} \argtwo[1]{\argone[2]{\argone[3]{that
        the rate of growth will ``look especially good as compared to
        other companies} \connective[3]{\textsc{explicit}:
        if}\argtwo[3]{the economy turns downward}}}.''
\end{example}

\textbf{SDRT} Another alternarive to RST---Segmented Discourse
Representation Theory or SDRT---was proposed by \citet{Lascarides:01}.
Although developed from a completely different angle of view (the SDRT
authors mainly drew their inspiration from predicate logics, dynamic
semantics, and anaphora theory), this theory shares many of its
features with the standard rhetorical structure as it also assumes a
graph-like structure of the text, whose nodes are either elementary or
complex discourse units, and distinguishes between coordinating and
subordinating relations.  However, unlike RST, segmented discourse
representation explictly allows the text structure to be any graph and
not only a tree (\ie{} a discourse node can have multiple parents and
also be connected via multiple links to the same vertex), provided
that it does not have crossing dependencies (the right-frontier
constraint).  In this respect, SDRT can be viewed as a
\emph{structural relaxation of RST}.

We can also notice the relatedness of the two approaches by looking at
the SDRT analysis of the WSJ fragment in
Example~\ref{dasa:fig:sdrt-graph}.  Although the names of relations in
the presented graph differ from those used in RST, many of these links
have the same (or at least similar) meaning as the respective edges in
the first analysis: for example, the \textsc{Source} relation in SDRT
almost completely corresponds to the \textsc{Attribution} edge in RST,
and the \textsc{Contrast} link is similar to the RST's
\textsc{Comparison}.
%% These discrepancies between paratactic dependencies in SDRT and
%% their hypotactic equivalents in RST account for the lion's share of
%% the differences between the two discourse representations in
%% Figures~\ref{dasa:fig:rst-tree} and \ref{dasa:fig:sdrt-graph}.
%% Another dissimilarity stems from the different scopes of the
%% commentary \texttt{While that's not spectacular} assigned by SDRT and
%% RST: while the SDRT graph suggests that this opinion primarily relates
%% to the actual statement of Neil Sweig, RST tree widens the
%% modification scope of this opinion also to the fact of making this
%% statement.

\begin{figure}[htbp]
  \begin{center}
    \begin{tikzpicture}[>=triangle 45,semithick]
      \tikzstyle{edu}=[]; \tikzstyle{cdu}=[draw,shape=rectangle];
      \node[edu] (1a) at (1,0) {$\pi_{1a}$}; \node[edu] (1b) at (1,-2)
           {$\pi_{1b}$};

           \node[edu] (p'') at (7,0)  {$\pi''$};
           \node[edu] (p') at (5.5,-2)  {$\pi'$};
           \node[edu] (1g) at (8.5,-2)  {$\pi_{1g}$};

           \node[edu] (1e) at (4,-4)  {$\pi_{1e}$};
           \node[edu] (1f) at (7,-4)  {$\pi_{1f}$};

           \node[edu] (1c) at (2,-2)  {$\pi_{1c}$};
           \node[edu] (1d) at (4,-2)  {$\pi_{1d}$};

           \draw[->] (1a)  to node [auto] {Source} (1b);
           \draw[->] (1a)  to node [auto] {Narration} (p'');
           \draw[-]  (p'') to node [auto] {} (p');
           \draw[-]  (p'') to node [auto] {} (1g);
           \draw[->] (p')  to node [auto] {Precondition} (1g);
           \draw[-]  (p')  to node [auto] {} (1e);
           \draw[-]  (p')  to node [auto] {} (1f);
           \draw[->] (1e)  to node [auto] {Contrast} (1f);
           \draw[->] (p'') to node [xshift=-8mm,yshift=-0.35mm] {Commentary} (1c);
           \draw[->] (p'') to node [xshift=-0mm,yshift=0mm] {Source} (1d);
    \end{tikzpicture}
    \caption[SDRT graph example]{Example of an SDRT graph}
    \label{dasa:fig:sdrt-graph}
  \end{center}
\end{figure}

\textbf{Final choice} Because it was unclear which of these approaches
(RST, PDTB, or SDRT) would be more amenable to our sentiment
experiments, we made our decision by taking the following theoretical
and practical considerations into account: From theoretical
perspective, we wanted to have a strictly hierarchical discourse
structure for analyzed tweets so that we could infer the polarity of
the whole message by recursively accumulating the polarity scores from
single discourse units.  From practical aspect, since there was no
discourse parser readily available for German, we wanted to have a
maximal assortment of such parsers for English so that we could pick
one that would be easiest to retrain on German data.  Fortunately,
both of these concerns have lead us to the same solution---Rhetorical
Structure Theory, which, on the one hand, was the only formalism that
explicitly guaranteed a single root for the analyzed text, and, on the
other hand, also provided a wide variety of open-source parsing
systems at our disposal
\cite[\eg][]{Hernault:10,Feng:14,Ji:14,Yoshida:14,Joty:15}.

\section{Data Preparation}\label{sec:dasa:data}

To prepare the data for our experiments, we split all microblogs from
the PotTS and SB10k corpora into elementary discourse units using the
ML-based discourse segmenter of~\citet{Sidarenka:15}.  After filtering
out all tweets which had at most one segment,\footnote{Since the focus
  of this chapter is mainly on discourse phenomena, we skip all
  messages which consist of a single discourse segment as their
  overall polarity is unaffected by the discourse structure and can be
  normally determined with the standard disource-unaware sentiment
  techniques.} we assigned polarity scores to the EDUs of remaining
tweets (analyzing each segment in isolation) with the help of the
lexicon-based attention classifier introduced previously.  This left
us with a total of 4,771 microblogs (12,137 segments) for PotTS and
3,763 messages (9,625 segments) for the SB10k corpus.  We again used
the same 70\%-10\%-20\% split into training, development, and test
sets as we did in our earlier work on coarse-grained sentiment
analysis.

%% This figure was generated using the iPython notebook
%% `notebooks/dasa.ipynb`.
\begin{figure*}
  \centering
      {
        \centering
        \begin{subfigure}{0.7\textwidth}
          \centering
          \includegraphics[width=\linewidth]{img/dasa_potts_edu_distribution.png}
          \caption{PotTS}\label{dasa:fig:data-distribution-potts}
        \end{subfigure}
      }
      \centering
          {
            \centering
            \begin{subfigure}{0.7\textwidth}
              \centering
              \includegraphics[width=\linewidth]{img/dasa_sb10k_edu_distribution.png}
              \caption{SB10k}\label{dasa:fig:data-distribution-sb10k}
            \end{subfigure}
          }
          \caption[EDU distribution in PotTS and SB10k]{Distribution of
            Elementary Discourse Units and Polarity Classes in the Training and
            Development Sets of PotTS and
            SB10k}\label{dasa:fig:data-distribution}
\end{figure*}

Figure~\ref{dasa:fig:data-distribution} shows the distribution of
elementary discourse units and their polarity classes in the training
and development data of both datasets.  As we can see from the
graphics, most of the tweets in both corpora typically have two or
three segments, while messages with more EDUs are extremely rare,
which is also not surprising regarding that the maximum length of a
tweet is limited to 140 characters.  Nonetheless, even with this
restriction, there still are a few microblogs with 12 and 13 discourse
units.  Since it was somewhat surprising for us to see that many
segments in a single message, we decided to look at these messages in
more detail.  As it turned out, such high number of EDUs typically
arose from spurious punctuation marks, which were carelessly used in
texted messages and evidently confused our segmenter, which had been
trained on standard-language texts (see
Example~\ref{dasa:exmp:many-segments}).

\begin{example}[SB10k Tweet with 13 EDUs]\label{dasa:exmp:many-segments}
  \noindent\textup{\bfseries\textcolor{darkred}{Tweet:}} { \upshape
    [Guinness on Wheelchairs :]$_1$ [Das .]$_2$ [Ist .]$_3$ [Verdammt
      .]$_4$ [Noch .]$_5$ [Mal .]$_6$ [Einer .]$_7$ [Der .]$_8$
    [Besten .]$_9$ [Werbespots .]$_{10}$ [Des .]$_{11}$ [Jahrzehnts
      .]$_{12}$ [( Auch ...]$_{13}$ }\\
                  {\textup{[}Guinness on
                      Wheelchairs :\textup{]$_1$} \textup{[}This .\textup{]$_2$}
                    \textup{[}Is .\textup{]$_3$} \textup{[}Gosh .\textup{]$_4$}
                    \textup{[}Darn .\textup{]$_5$} \textup{[}It .\textup{]$_6$}
                    \textup{[}One .\textup{]$_7$} \textup{[}Of .\textup{]$_8$}
                    \textup{[}The best .\textup{]$_9$} \textup{[}Commercials
                      .\textup{]$_{10}$} \textup{[}Of .\textup{]$_{11}$} \textup{[}The
                      Decade .\textup{]$_{12}$} \textup{[}( Also ...\textup{]$_{13}$}}
\end{example}

Another noticeable trend which we can also observe in the data is that
the distribution of polarity classes in messages with multiple EDUs
largely corresponds to the frequencies of these semantic orientations
in the complete datasets: For example, the positive class still
dominates the PotTS corpus, whereas the neutral polarity constitues
the vast majority of the SB10k set.  At the same time, negative
microblogs again are the least represented group in both cases and
account for only 22\% of the former dataset and 16\% of the latter
tweebank.

To obtain RST trees for segmented messages, we retrained the DPLP
discourse parser of~\citet{Ji:14} on the Potsdam Commentary Corpus
\cite[PCC~2.0; ][]{Stede:14}, after converting all discourse relations
of this dataset to the binary scheme $\{$\textsc{Contrastive},
\textsc{Non-Contrastive}$\}$ as suggested
by~\citet{Bhatia:15}.\footnote{See Table~\ref{dasa:tbl:rst-rels} for
  more details regarding this mapping.}  In contrast to the original
DPLP implementation though, we did not use Brown cluster features
\cite{Brown:92}, since this resource was unavailable for German, nor
did we utilize the linear projection of the features, because the
released parser code was missing this component either.  In part due
to these modifications, but mostly because of the specifics of the
German language (richer morphology, higher lexical variety and
syntactic ambiguity), the results of the retrained model were also
considerably lower than the figures reported for the English RST
Treebank~\cite{Carlson:01a}, amounting to 0.777, 0.512, and 0.396~\F{}
for the span, nuclearity, and relation prediction on PCC~2.0 versus
82.08, 71.13, and 61.63~\F{} on the English corpus.\footnote{Following
  \citet{Ji:14}, we use the span-based evaluation metric
  of~\citet{Marcu:00}.}

Figure~\ref{dasa:fig:twitter-rst-tree} shows an example of an
automatically induced RST tree.  As we can see, the adapted parser can
correctly distinguish between contrastive and non-contrastive links,
but apparently struggles with the disambiguation of the nuclearity
status, assigning the highest significance to the initial discource
segment (``Mooooiiinn.''  [\emph{Hellloooo!}]), which is merely a
greeting, and weighing the second EDU (``Gegen solche N\"achte hilft
die beste Kur nicht.''  [\emph{Even the best cure won't help against
    such nights.}]) less than the third one (``Aber Kaffee!''
[\emph{But coffee!}]), although traditional RST would rather consider
both units as equally important and use the multi-nuclear
\textsc{Contrast} relation for them.

\begin{figure*}[htbp!]
  \input{twitter-rst.tex}
\end{figure*}

As to the distribution of the two coarse-grained discourse relations,
the \textsc{Non-\-Con\-tra\-sti\-ve} links clearly dominate both
corpora, accounting for 98\% of all discourse relations in PotTS and
SB10k (see Figure~\ref{dasa:fig:relation-distribution}).  By
comparison, the precentage of these relations in PCC~2.0 amounts to
90\% and also represents the prevailing majority of all semantic links
in this corpus.

\begin{figure*}
  \centering
      {
        \centering
        \begin{subfigure}{0.7\textwidth}
          \centering
          \includegraphics[width=\linewidth]{img/dasa_potts_rel_distribution.png}
          \caption{PotTS}\label{dasa:fig:relation-distribution-potts}
        \end{subfigure}
      }
      \centering
          {
            \centering
            \begin{subfigure}{0.7\textwidth}
              \centering
              \includegraphics[width=\linewidth]{img/dasa_sb10k_rel_distribution.png}
              \caption{SB10k}\label{dasa:fig:relation-distribution-sb10k}
            \end{subfigure}
          }
          \caption[Relation distribution in PotTS and SB10k]{Distribution of
            Discourse Relations in the Training and Development Sets of PotTS
            and SB10k}\label{dasa:fig:relation-distribution}
\end{figure*}

\section{Discourse-Aware Sentiment Analysis}\label{sec:dasa:methods}

% \done[inline]{\citet{Bickerstaffe:10}}

% \citet{Bickerstaffe:10} also considered the rating prediction task,
% addressing this problem with the minimum-spanning-tree (MST) SVM
% approach.  In the initial step of this method, they constructed a
% strongly connected graph whose vertices were associated with the most
% representative example (determined via the average all-pairs Tanimoto
% coefficient) of each star rating and the edge weights represented the
% Tanimoto distances between those nodes.  Afterwards, they determined
% the MST of this graph using the Kruskal's
% algorithm~\cite[see][pp.~567--574]{Cormen:09} and, finally,
% constructed a decision tree from this MST, replacing the MST vertices
% with binary SVM classifiers, which had to discern the respective
% rating groups. An evaluation on the four-star review corpus
% of~\citet{Pang:05} showed an improvement by up to~7\% over the
% previous state of the art, boosting it to 59.37\% average accuracy.

Now, with these prepared data at hand, we are all set to check whether
incorporating the additional discourse information into our
lexicon-based attention system would improve the quality of its
analysis.\todo{Redefine this objective} However, before we proceed
with this check, let us first revise the most prominent approaches to
discourse-aware sentiment classification that have been proposed in
the literature so far.

\begin{example}[Polarity reversal via discourse antithesis]\label{disc-snt:exmp-pang02}
  \noindent\upshape This film should be brilliant.  It sounds like a
  great plot, the actors are first grade, and the supporting cast is
  good as well, and Stallone is attempting to deliver a good
  performance.  However, it can't hold up. \cite{Pang:02}
\end{example}

As it turns out, even the very first works on opinion mining already
pointed out the importance of discourse phenomena for determining the
overall polarity of a text.  For example, in the seminal paper
of~\citet{Pang:02}, where the authors tried to predict the semantic
orientation of movie reviews, they quickly noticed the fact that it
was insufficient to rely on the mere presence or even majority of
polarity clues because these clues could any time be reversed by a
single counter-argument (see Example~\ref{disc-snt:exmp-pang02}).
\citet{Polanyi:06} also affirmed the important role of the text
structure, considering discourse connectors and relations as one of
the most significant context factors which could notably affect the
intensity and polarity of opinions.  To prove this claim, they
provided several convincing examples in which discourse links
considerably weakened the strength of a sentiment by introducing
concessive statements, or, vice versa, increased the persuasiveness of
an argument by elaborating on its details.

These observations have rapidly motivated NLP researchers to add a
discourse-analysis component to document-level sentiment classifiers.
One of the first such systems was presented by~\citet{Pang:04}, who
tried to improve the classification accuracy on the IMDB corpus by
explicitly pointing the sentiment predictor to the sentences which
another (discourse-aware) classifier had previously considered as
subjective.  To achieve this goal (\ie{} to distinguish subjective
statements from the objective ones), \citet{Pang:04} considered each
text as a graph whose vertices represented the sentences with their
automatically induced subjectivity scores and connected these vertices
via affinity edges to their immediate neighbors in the text.  After
adding two more abstract nodes, which stood for the subjective and
objective classes respectively, and linking all sentences to these two
vertices, the authors determined the minimum cut of that graph using
the Blum algorithm \cite{Blum:01} and partitioned it into two clusters
(subjective and objective) on this cut.

Although an obvious oversimplification, the core idea that locally
adjacent sentences had to share the same subjective properties (local
coherence) was dominating the following discourse-aware sentiment
research for almost a decade.  For example, \citet{Riloff:03} also
improved the accuracy of their Na{\"i}ve Bayes predictor of subjective
expressions by $\approx2\%$ after adding a set of discourse-related
coherence features.  Similarly, \citet{Hu:04} achieved better scores
on disambiguating users' polarities towards particular product
features after taking the information about the semantic orientation
of previous sentences into account.

Another important line of DASA research concentrated more on the joint
analysis of opinions, where, in addition to classifying each sentiment
in isolation, the authors also sought to maximize the ``total
happiness'' (or global coherence) of these assignments by ensuring
that similar or related judgements received the same or at least
agreeing polarity scores.  One of the most notable works in this
direction was done by \citet{Snyder:07}, who introduced the Good Grief
algorithm for predicting the satisfaction of a user with different
restaurant aspects.  Another important contribution was made by
\citet{Somasundaran:08a,Somasundaran:08}, who proposed the concept of
\emph{opinion frames} (OF)---a special data structure for representing
the relations between opinions in discourse.  Depending on the type of
these opinions (whether arguing~[\emph{A}] or sentiment~[\emph{S}]),
their polarity towards the target (whether positive~[\emph{P}] or
negative~[\emph{N}]), and semantic relationship between these targets
(whether alternative~[\emph{Alt}] or the same~[\emph{same}]), the
authors distinguished 32 types of possible frames: \emph{SPSPsame},
\emph{SPSNsame}, \emph{APAPalt}, etc., dividing them into reinforcing
and non-reinforcing ones.  Later on,
\citet{Somasundaran:09a,Somasundaran:09b} also presented two joint
inference frameworks (iterative classification and integer linear
programming) for determining the best configuration of all opinion
frames, achieving 77.72\% accuracy of frame prediction on the AMI
meeting corpus~\cite{Carletta:05}.

%% \done[inline]{\citet{Somasundaran:09a,Somasundaran:09b}}

%% In a later work, \citet{Somasundaran:09b,Somasundaran:09a} also
%% introduced a joint inference framework based on the Iterative
%% Classification Algorithm (ICA) and Integer Linear Programming (ILP)
%% for joinly predicting the best configuration of single opinions and
%% their frames.  In this approach, the authors first applied a local SVM
%% classifier to compute the probabilities of polarity classes (positive,
%% negative, or neutral) of individual dialog acts and then harnessed the
%% ICA and ILP systems to determine which of the predicted opinions were
%% connected via opinion frames and whether these frames were reinforcing
%% or not.  Given a perfect information about the opinion links, this
%% joint method outperformed the local classifier by more than 9
%% percentage points, reaching 77.72\% accuracy on the AMI meeting
%% corpus~\cite{Carletta:05}.

%% \done[inline]{\citet{Mao:06}}

%% \citet{Mao:06} proposed the idea of isotonic CRFs in which they
%% explicitly modeled the constraint that features which were stronger
%% associated with either polarity classes had to have higher
%% coefficients than less predictive attributes.  After proving that this
%% formalism also allowed to directly model the ordinal scale of
%% sentiment scores (with lower CRF outputs indicating the negativity of
%% a sentence, and higher scores showing its positive class), the authors
%% used this approach to model the sentiment flow in a document.  For
%% this purpose, they first predicted the polarity value for each
%% sentence of a document in isolation and then convolved these outputs
%% with a Gaussian kernel, getting a smoothed polarity curve for the
%% whole analyzed text at the end.
%% \done[inline]{\citet{Thomas:06}}

%% \citet{Thomas:06} enhanced an SVM-based sentiment classification
%% system for predicting speaker's attitude in political speeches with
%% information about the inter-speaker agreement, incorporating these
%% links into the global cost function.  Thanks to this change, the
%% authors achieved $\approx$4\% improvement in accuracy (from 66.05 to
%% 70.81\%) over the baseline classifer which analyzed each utterance in
%% isolation.

An attemt to reunite local and global coherence again was made by
\citet{McDonald:07}, who proposed a joint framework based on
\emph{latent variables} for simultaneously predicting the polarity of
a document and its constituent parts (which could be either paragraphs
or sentences).  For this purpose, the authors considered the semantic
orientation of the whole text and its single sentences as unobserved
variables in a Markov random field, connecting each such sentence node
to the vertices of its adjacent clauses and the overal polarity node
of the complete document, and then figuring out the best configuration
of all variables at the end.%%   A similar approach was also suggested
%% by~\citet{Sadamitsu:08}, who attained 82.74\% accuracy on predicting
%% the polarity of customer reviews with the help of hidden conditional
%% random fields.
\todo{Add that they required gold labels for both sentences and the document.}

Another latent-variable approach was presented by
\citet{Yessenalina:10}, who tried to predict the overall semantic
orientation of a document by first selecting a subset of the most
indicative sentences and then classifying the document (as either
positive or negative) with the help of this selection.  To achieve
this goal, the authors adopted the latent-SVM method of \citet{Yu:09}
by first training a linear classifier on individual sentences with
latent classes and then making the final prediction using 30\% of
those clauses which the classifier was most sure about.  With this
method, \citet{Yessenalina:10} attained 93.22\% accuracy on the movie
review corpus of \citet{Pang:04} and reached 77.09\% on a collection
of congressional floor debates \cite{Thomas:06}.

A common drawback of all these approaches though is their complete
ignoring of traditional discourse theories and, as a consequence of
this, too coarse approximation of discourse structure (which
essentially boils down to either connecting nearby sentences, or
creating a densely connected graph).  Among the first who tried to
overcome this omission were \citet{Voll:07}, who came up with two
different ways of making their lexicon-based sentiment calculator
\cite[SO-CAL; ][]{Taboada:11} aware of discourse information: In the
first method, they let the classifier analyze only the top-post
nucleus segment of each sentence (using automatically derived RST
trees for determining these EDUs).  In the second attempt, they
restricted the SO-CAL's input only to the clauses that had been
previously classified as pertaining to the main topic of the text.
Unfortunately, the RST-based solution did not work out as well as
expected and failed to outperform even the discourse-unaware baseline,
yielding 69\% precision on a corpus of Epinion reviews
\cite{Taboada:06}.  This mishap, however, could be partially due to
the fact that the authors completely ignored all nuclei and satellites
apart from the roots and also disregarded all inter-sentential
relations.  The topic-aware method, however, turned out to work fairly
well, achieving 73\% accuracy on this two-class prediction task.

Other ways of incorporating discourse information into sentiment
systems were explored by \citet{Heerschop:11}, who experimented with
three different approaches:
\begin{inparaenum}[(i)]
\item increasing the polarity scores of words which appeared near
  the end of the document,
\item assigning higher weights to tokens in the nuclei of RST trees
  rather than satellites, and, finally,
\item training a genetic algorithm, which learned separate scores for
  satellites and nuclei.
\end{inparaenum}
An evaluation of these methods on the movie review corpus
of~\citet{Pang:04} showed a better performance of the first option
(0.608 accuracy and 0,597 macro-\F).  The authors, however, could
significantly improve the results of the last classifier at the end,
after adding an offset to the decision boundary, which increased both
its accuracy and macro~\F-measure to 0.72.

Other notable works on RST-based sentiment prediction were done by
\citet{Zhou:11}, who used a set of heuristic rules to infer possible
polarity shifts of discourse units based on their nuclearity status
and parent links, which allowed them to attain a statistically
significant improvement in sentence-level polarity classification on
the NTCIR MOAT corpus~\cite{Xu:10}; \citet{Zirn:11}, who applied a
lexicon-based sentiment system to compute the polarity scores of
elementary discourse units, and then enforced the consistency of these
assignments over an automatically derived RST tree with the help of of
Markov logic contraints; and, finally, \citet{Wang:13}, who also first
computed polarity scores of isolated discourse units and then
estimated the polarity of the whole document by taking a linear
combination of these scores, multiplying them with automatically
learned coefficients.\footnote{Similarly to the approach
  of~\citet{Zirn:11}, these coefficients depended on the status of the
  segment in the RST tree (whether nucleus or sattelite) and relation,
  which connected the respective discourse node to the ancestor.}  %% A
%% similar system was also described by \citet{Chenlo:13,Chenlo:14}, who
%% used their model to analyze user blog posts, achieving significantly
%% better results on the TREC corpus \cite{Macdonald:09} than any
%% discourse-unaware baselines.

Among the most recent advances in RST-based opinion mining, we should
especially emphasize the work of \citet{Bhatia:15}, who proposed two
different sentiment systems:
\begin{itemize}
\item discourse depth reweighting (DDR)
\item and rhetorical recursive neural network (R2N2).
\end{itemize}
In the former approach, they estimated the relevance $\lambda_i$ of
each elementary discourse unit $i$ as:
\begin{equation*}
  \lambda_i = \max\left(0.5, 1 - d_i/6\right),
\end{equation*}
where $d_i$ stands for the depth of the $i$-th EDU in the document's
discourse tree; and computed the sentiment score $\sigma_i$ of that
unit as the dot product of its binary feature vector $\mathbf{w}_i$
(token unigrams) with the polarities of these features
$\boldsymbol{\theta}$ taken from a lexicon:
\begin{equation*}
  \sigma_i = \boldsymbol{\theta}^{\top}\mathbf{w}_i.
\end{equation*}
In the final step, the authors calculated the overall semantic
orientation of the document~$\Psi$ as the sum of the predicted
sentiment scores for all units, multiplying these weights by the
importance factors~$\lambda$:
\begin{equation*}
  \Psi = \sum_i\lambda_i\boldsymbol{\theta}^T\mathbf{w}_i = \boldsymbol{\theta}^T\sum_i\lambda_i\mathbf{w}_i,
\end{equation*}

In the R2N2 method, \citet{Bhatia:15} adopted the RNN approach
of~\citet{Socher:13} by recursively computing the polarity score of
each discourse unit $i$ as:
\begin{equation*}
  \psi_i = tanh\left(K_n^{(r_i)} \psi_{n(i)} + K_s^{(r_i)}\psi_{s(i)} \right),
\end{equation*}
where $K_n^{(r_i)}$ and $K_s^{(r_i)}$ denote the nucleus and satellite
coefficients associated with the rhetorical relation $r_i$, whereas
$\psi_{n(i)}$ and $\psi_{s(i)}$ represent the sentiment scores of the
respective nodes.  This system achieved formidable 84.1\% two-class
accuracy on the moview review corpus of~\citet{Pang:04} and reached
85.6\% on the dataset of~\citet{Socher:13}.

For the sake of completeness, we should note that there also exist
discourse-aware sentiment approaches which build upon PDTB and SDRT:
For example, \citet{Trivedi:13} presented a method based on the latent
structural SVM~\cite{Yu:09}, in which they represented each sentence
as a vector of features produced by function $\mathbf{f}(y,
\mathbf{x}_i, h_i)$, where $y\in\{-1, +1\}$ denotes the potential
polarity of the document, $h_i \in \{0, 1\}$ represents the assumed
subjectivity class of sentence $i$, and $\mathbf{x}_i$ stands for the
surface form of the $i$-th sentence.  One such feature, for example,
could be \texttt{<+1, ``love'', 1>}, which would indicate that the
token ``love'' appears in a supposedly subjective sentence of a
positive document. However, because the true assignment of the
variables $h_i$ was unknown, the authors regarded this value as a
latent variable and tried to predict the most likely semantic
orientation of the document $\hat{y}$ by finding the maximum product
of weight and feature vectors over all possible assignments of
$\mathbf{h}$, \ie{}:
\begin{equation*}
  \hat{y} =
  \argmax_y\left(\max_{\mathbf{h}}\mathbf{w}^{\top}\mathbf{f}(y,
  \mathbf{x}, \mathbf{h})\right).
\end{equation*}
However, to ensure that these assignments were still coherent, the
authors additionally extended their original feature set with special
\emph{transitional} attributes, which indicated whether two adjacent
sentences were likely to share the same subjectivity given the
discourse connective between them.  With the help of these features,
they could improve the prediction accuracy on the movie review corpus
of~\citet{Maas:11} from 88.21 to 91.36\% in comparison with the
connector-unaware model.

The first step towards an SDRT-inspired approach was made by
\citet{Asher:08}, who presented an annotation scheme and pilot corpus
of English and French texts labeled with their discourse structure
according to the SDRT theory and augmented with an additional
sentiment layer.  In particular, the authors asked the annotators to
ascribe one of four opinion categories (reporting, judgement, advice,
or sentiment) along with their subcategories (e.g., inform, assert,
blame, recommend) to each discourse unit that had at least one
opinionated word from a sentiment lexicon.  Afterwards, they showed
that with a simple set of rules, one could easily propagate opinions
through the discourse graphs, increasing their strengths or reversing
their polarity, depending on the type of discourse relations that
connected the segments.

In general, however, PDTB- and SDRT-based sentiment systems are much
less common than RST-solutions.  Because of this fact and since the
RST formalism was our favorite framework for discourse-aware
enhancement of the LBA system anyway , we decided to primarily
concentrate on this group of methods.  In particular, for the sake of
comparison, we have replicated the linear combination method of
\citet{Wang:13} and also reimplemented the DDR and R2N2 approaches
of~\citet{Bhatia:15} .  Furthermore, to see how these techniques would
perform with respect to much easier baselines, we additionally created
two simple systems which predicted the polarity of a message by only
considering its last or topmost nucleus EDU (henceforth \textsc{Last}
and \textsc{Root} respectively), and also estimated the results of the
original LBA approach by letting it analyze the whole tweets at once
without splitting these messages into elementary discourse units
(henceforth \textsc{No-Discourse}).  Apart from these existing and
baseline methods, we also propose several new DASA solutions which we
briefly descibe below.

\textbf{Latent CRF} In the first of these solutions, called
\emph{Latent Conditional Random Fields} or \emph{LCRF}, we consider
the problem of message-level sentiment analysis as an inference task
over an undirected graphical model, where the nodes of the model
represent polarity probabilities of elementary discourse units, and
the structure reflects the RST tree of these EDUs.  In particular, we
define CRF graph $\mathcal{G}\defeq(\mathcal{V}, \mathcal{E})$ as a
set of vertices $\mathcal{V}\defeq \mathcal{Y}\cup\mathcal{X}$, in
which $\mathcal{Y}\defeq\{y_{(i, j)}\mid i\in\{\text{\textsc{Root}},
1, 2, \ldots, T\}, j \in\{\text{\textsc{Negative}, \textsc{Neutral},
  \textsc{Positive}}\}\}$ represents (partially observed) random
variables (with $T$ standing for the number of EDUs in the tweet), and
$\mathcal{X}\defeq\{x_{(i, j)}\mid i\in\{\text{\textsc{Root}}, 0, 1,
\ldots, n-1\}, j \in[0,\ldots, 3]\}$ denotes the respective features
of these nodes (three polarity scores returned by the LBA classifier
for the $i$-th EDU plus an additional \emph{offset} feature whose
value is always \texttt{1} irrespectively of the input).  Since the
\textsc{Root} vertix of this graph does not have a corresponding
discourse segment, we use the polarity scores predicted by the LBA
classifier for the whole message as features of this node.

Factor edges $\mathcal{E}$ connect random variables to their
corresponding feature attributes and also link every pair of vertices
$(v_{(i,\cdot)},v_{(j,\cdot)})$ if node $i$ appears as the parent of
node $j$ in the dependency representation of the automatically
constructed RST tree.\footnote{In fact, we use two edges to connect
  each child to its parent, setting one of these factors to one and
  another to zero, depending on which discource relation links the two
  nodes---\textsc{Contrastive} or \textsc{Non-Contrastive}.} Following
the work of~\citet{Bhatia:15}, we obtain this representation using the
DEP-DT algorithm of~\citet{Hirao:13} with a minor modification that we
do not follow any satellite branches while computing the heads of
abstract RST nodes in Step 1 of this procedure
\cite[see][pp.~1516--1517]{Hirao:13}.

\begin{figure*}[thb]
  \centering \input{latent-crf}
  \caption{Example of an RST-Based Latent-CRF\\ {\small (random
      variables are shown as circles, fixed input parameters appear as
      rectangles, and observed values are displayed in
      gray)}}\label{dasa:fig:latent-crf}
\end{figure*}

Figure~\ref{dasa:fig:latent-crf} shows a real example of such
automatically induced CRF tree where we can already notice a few
tendencies regarding the obtained discourse graph: First of all, our
segmenter clearly tends to oversegment its input, also considering
conjoined predicates and adverbial subordinate clauses as separate
discourse units.  Even though this behavior violates the principles of
standard RST, it actually comes advantageous to our particular
sentiment application as it allows the base classifier to be more
fine-grained (and consequently more precise) in its predictions.  At
the same time, we again can see that the automatic parser has
difficulties with determining the correct nuclearity status of
discourse segments, putting the segment ``f\"uhlt sich fast an''
(\textit{almost feels}) in the top-most position, which we can hardly
call the right decision.  Finally, we also can observe that despite an
incorrect prediction of the polarity of the whole tweet (the LBA
system considers it as a negative message, although human experts
regarded it as neutral) our base classifier might still have better
guesses for single EDUs, giving us at least a hypothetical possibility
to overcome its general error.

Now, before we describe to the training of our model, let us briefly
recall that in the standard CRF learning we typically try to find
optimal model parameters $\boldsymbol{\theta}^*$ that maximize the
log-likelihood of label sequences $\mathbf{y}^{(i)}$ on the training
set $\mathcal{D}\defeq\left\{\left(\mathbf{x}^{(i)},
\mathbf{y}^{(i)}\right)\right\}_{i=1}^{N}$, \ienocomma:
\begin{equation*}
  \boldsymbol{\theta}^* = \argmax_{\boldsymbol{\theta}}\ell(\boldsymbol{\theta}) = \sum_{i=1}^{N}\log\left(p\left(\mathbf{y}^{(i)}\vert\mathbf{x}^{(i)}; \boldsymbol{\theta}\right)\right),\label{dasa:eq:crf-objective}
\end{equation*}
where conditional likelihood is normally estimated as:
\begin{equation*}
  p\left(\mathbf{y}^{(i)}\vert\mathbf{x}^{(i)}; \boldsymbol{\theta}\right) =
  \frac{\exp\left(\sum_{t=1}^{T_i}\sum_k\boldsymbol{\theta}_k\mathbf{f}_k\left(\mathbf{x}^{(i)}_t,\mathbf{y}^{(i)}_{t-t},\mathbf{y}^{(i)}_{t}\right)\right)}{Z}.
\end{equation*}
Adapting this equation to our RST-based structure, we obtain:
\begin{equation*}
  p\left(\mathbf{y}^{(i)}\vert\mathbf{x}^{(i)}; \boldsymbol{\theta}\right) =
  \frac{\exp\left(\sum_{t=0}^{T_i}\left[%
      \sum_v\boldsymbol{\theta}_v\mathbf{f}_v\left(\mathbf{x}^{(i)}_t,\mathbf{y}^{(i)}_{t}\right)
      + \sum_{c\in
        ch(t)}\sum_e\boldsymbol{\theta}_e\mathbf{f}_e\left(\mathbf{y}^{(i)}_{t},
      \mathbf{y}^{(i)}_{c}\right)\right]\right)}{Z},
\end{equation*}
where $ch(t)$ denotes children of node $t$, $v$ stands for the indices
of vertex features, and $e$ represents the indices of edge attributes.

A crucial problem with this formulation though is that, in our task,
only a small subset of labels from $\mathbf{y}^{(i)}$ (namely those
which pertain to the root node) are actually observed at the training
time, whereas the rest of the tags (those which pertain to EDUs) are
unknown.  We will denote these observed and hidden subsequences as
$\mathbf{y}_o^{(i)}$ and $\mathbf{y}_h^{(i)}$ respectively.  Using
this notation, we can specify our goal as finding model parameters
$\boldsymbol{\theta}^*$ which maximize the log-likelihood of \emph{known} labels,
\ienocomma:
\begin{equation*}
  \boldsymbol{\theta}^* =
  \argmax_{\boldsymbol{\theta}}\sum_{i=1}^{N}\log\left(p\left(\mathbf{y}_o^{(i)}\vert\mathbf{x}^{(i)};
  \boldsymbol{\theta}\right)\right).
\end{equation*}
However, it is still unclear what we shall do about hidden tags
$\mathbf{y}_h^{(i)}$, since the values of their features are yet
undefined.

One possible way to approach the problem of unobserved states is to
assume that any label sequence $\mathbf{y}_h^{(i)}$ might be true, and
then try to maximize parameters along the path which leads to the
maximum probability of the correct observed tag, \ienocomma:
\begin{align}
  \begin{split}
    \mathbf{y}^{(i)}&\defeq[\mathbf{y}_o^{(i)}, \mathbf{y}_h^{*(i)}]\text{, where}\\\label{dasa:eq:y_i}
    \mathbf{y}_h^{*(i)}&=\argmax_{\mathbf{y}_h^{(i)}}p\left(\mathbf{y}_o^{(i)}\vert\mathbf{x}^{(i)}\right).
  \end{split}
\end{align}
We can easily find this path with the max-product belief-propagation,
which is also used in the standard CRF inference.

Unfortunately, if we simply consider label sequence $\mathbf{y}^{(i)}$
from Equation~\ref{dasa:eq:y_i} as the ground truth and penalize all
labels which disagree with this sequence, we might overly commit
ourselves to the model's guess of unknown tags and unduly discriminate
against other possible assignments of hidden labels.  To mitigate this
effect, we can instead penalize only one other sequence, namely that
which maximizes the probability of an incorrect label at the observed
state:
\begin{align*}
  \mathbf{y}^{'(i)}&\defeq\argmax_{\mathbf{y}_o^{'(i)}\neq\mathbf{y}_o^{(i)}}p\left([\mathbf{y}_o^{'(i)},
    \mathbf{y}_h^{*(i)}]\vert\mathbf{x}^{(i)}\right)\text{,
    where}\\
  \mathbf{y}_h^{*(i)}&=\argmax_{\mathbf{y}_h^{(i)}}p\left(\mathbf{y}_o^{'(i)}\vert\mathbf{x}^{(i)}\right).
\end{align*}
Correspondingly, we redefine our objective function and, instead of
maximizing the log-likelihood of the training set, will now try to
maximize the difference between the likelihoods of correct and wrong
assignments:
\begin{align}
  \begin{split}
    \boldsymbol{\theta}^* &= \argmax_{\boldsymbol{\theta}}\sum_{i=1}^{N}\log\left(p\left(\mathbf{y}^{(i)}\right)\right) - \log\left(p\left(\mathbf{y}^{'(i)}\right)\right)\\
    &= \argmax_{\boldsymbol{\theta}}\sum_{i=1}^{N}\log\left(\exp\left(\boldsymbol{\theta}^{\top}\mathbf{f}(\mathbf{x}^{(i)},\mathbf{y}^{(i)})\right)\right) - \log\left(\exp\left(\boldsymbol{\theta}^{\top}\mathbf{f}(\mathbf{x}^{(i)},\mathbf{y}^{'(i)})\right)\right)\\
    &= \argmax_{\boldsymbol{\theta}}\sum_{i=1}^{N}\boldsymbol{\theta}^{\top}\left(\mathbf{f}(\mathbf{x}^{(i)},\mathbf{y}^{(i)}) - \mathbf{f}(\mathbf{x}^{(i)},\mathbf{y}^{'(i)})\right),\label{dasa:eq:hcrf-objective}
  \end{split}
\end{align}
where $\mathbf{f}(\mathbf{x}^{(i)},\mathbf{y}^{(i)})$ and
$\mathbf{f}(\mathbf{x}^{(i)},\mathbf{y}^{'(i)})$ are all features
associated with label sequences $\mathbf{y}^{(i)}$ and
$\mathbf{y}^{'(i)}$ respectively.

The only thing that we still need to do to the above objective is to
introduce a regularization term $\frac{1}{2}\norm{\boldsymbol{\theta}}^2$ in order
to prevent its divergence to infinity in cases when
$\mathbf{f}(\mathbf{x}^{(i)},\mathbf{y}^{(i)})$ and
$\mathbf{f}(\mathbf{x}^{(i)},\mathbf{y}^{'(i)})$ are perfectly
separable.  This brings us to our final formulation:
\begin{align}
  \boldsymbol{\theta}^* &=
  \argmin_{\boldsymbol{\theta}}\frac{1}{2}\norm{\boldsymbol{\theta}}^2 -
  \sum_{i=1}^{N}\boldsymbol{\theta}^{\top}\left(\mathbf{f}(\mathbf{x}^{(i)},\mathbf{y}^{(i)})
  - \mathbf{f}(\mathbf{x}^{(i)},\mathbf{y}^{'(i)})\right)
\end{align}
At this point, we can notice that the resulting function is similar to
the unconstrained minimization problem of structural SVM
\cite{Taskar:03}, and we indeed can piggyback on one of the many
efficient SVM-optimization methods to learn the parameters of our
model.  In particular, we use the block-coordinate Frank-Wolfe
algorithm~\cite{Lacoste-Julien:13}, running it for 1,000 epochs or
until convergence is reached, whichever of these events occurs
first.\todo{Outline the difference from the approaches of
  \citet{McDonald:07} and \citet{Trivedi:13}.}

\textbf{Latent Marginalized CRF} Another way to tackle unobserved
labels in the input is to use the fact that we can estimate the
probability of a subset of random variables from a joint distribution
over a larger set by marginalizing (summing) out the remaining terms
of this distribution, \ienocomma:
\begin{align*}
  p\left(E=e\right) &= \sum_h p\left(E=e, H=h\right).
\end{align*}
Applying this formula to our objective, we get:
\begin{align*}
  \begin{split}
    p\left(\mathbf{y}_o^{(i)}\right) &= \sum_{\mathbf{y}_h^{(i)}} p\left(\mathbf{y}_o^{(i)}, \mathbf{y}_h^{(i)}\right)\\
    &= \frac{\sum_{\mathbf{y}_h^{(i)}}\exp\left(\sum_{t=0}^{T_i}\left[%
        \sum_v\boldsymbol{\theta}_v\mathbf{f}_v\left(\mathbf{x}^{(i)}_t,\mathbf{y}^{(i)}_{t}\right)
        + \sum_{c\in
          ch(t)}\sum_e\boldsymbol{\theta}_e\mathbf{f}_e\left(\mathbf{y}^{(i)}_{t},
        \mathbf{y}^{(i)}_{c}\right)\right]\right)}{Z},
  \end{split}
\end{align*}
where $\mathbf{y}^{(i)}$ in the exponent is defined as before:
$\mathbf{y}^{(i)}\defeq[\mathbf{y}_o^{(i)}, \mathbf{y}_h^{(i)}]$.

This time again, we would like to maximize the probability of the
correct assignment, setting it apart from its closest competitor by at
least some margin.  Unfortunately, due to the summation over
$\mathbf{y}_h^{(i)}$, we cannot avail ourselves of the log-exp
cancellation trick that we used previously in
Equation~\ref{dasa:eq:hcrf-objective}.  Instead of this, we will
replace the difference of log-likelihoods by the raw ratio of marginal
probabilities:
\begin{align}
  \begin{split}
    \boldsymbol{\theta}^* &= \argmax_{\boldsymbol{\theta}}\sum_{i=1}^{N}\frac{p(\mathbf{y}^{(i)})}{p(\mathbf{y}^{'(i)})}\\
    &= \argmax_{\boldsymbol{\theta}}\sum_{i=1}^{N}
    \frac{\sum_{\mathbf{y}_h^{(i)}}\exp\left(\sum_{t=0}^{T_i}\left[%
        \sum_v\boldsymbol{\theta}_v\mathbf{f}_v\left(\mathbf{x}^{(i)}_t,\mathbf{y}^{(i)}_{t}\right)
        + \sum_{c\in
          ch(t)}\sum_e\boldsymbol{\theta}_e\mathbf{f}_e\left(\mathbf{y}^{(i)}_{t},
        \mathbf{y}^{(i)}_{c}\right)\right]\right)}{\sum_{\mathbf{y}_h^{(i)}}\exp\left(\sum_{t=0}^{T_i}\left[%
        \sum_v\boldsymbol{\theta}_v\mathbf{f}_v\left(\mathbf{x}^{(i)}_t,\mathbf{y}^{'(i)}_{t}\right)
        + \sum_{c\in
          ch(t)}\sum_e\boldsymbol{\theta}_e\mathbf{f}_e\left(\mathbf{y}^{'(i)}_{t},
        \mathbf{y}^{'(i)}_{c}\right)\right]\right)}\label{dasa:eq:hmcrf-objective}
  \end{split}
\end{align}
To simplify this expression, we introduce the following abbreviations:
\begin{align*}
  a &\defeq \exp\left(\sum_{t=0}^{T_i}\left[
    \sum_v\boldsymbol{\theta}_v\mathbf{f}_v\left(\mathbf{x}^{(i)}_t,\mathbf{y}^{(i)}_{t}\right)
    + \sum_{c\in
      ch(t)}\sum_e\boldsymbol{\theta}_e\mathbf{f}_e\left(\mathbf{y}^{(i)}_{t},
    \mathbf{y}^{(i)}_{c}\right)\right]\right),\\
  b &\defeq \exp\left(\sum_{t=0}^{T_i}\left[
    \sum_v\boldsymbol{\theta}_v\mathbf{f}_v\left(\mathbf{x}^{(i)}_t,\mathbf{y}^{'(i)}_{t}\right)
    + \sum_{c\in
      ch(t)}\sum_e\boldsymbol{\theta}_e\mathbf{f}_e\left(\mathbf{y}^{'(i)}_{t},
    \mathbf{y}^{'(i)}_{c}\right)\right]\right).
\end{align*}
Now, we can estimate the derivatives of functions $a$ and $b$ w.r.t. a
single parameter $\boldsymbol{\theta}_v$ as:
\begin{align*}
  \frac{\partial{}a}{\partial\boldsymbol{\theta}_v} &= a\sum_{t=0}^{T_i}\mathbf{f}_v\left(\mathbf{x}^{(i)}_t,\mathbf{y}^{(i)}_{t}\right)\propto\mathbb{E}_{\mathbf{y}^{(i)}}\mathbf{f}_v,\\
  \frac{\partial{}b}{\partial\boldsymbol{\theta}_v} &= b\sum_{t=0}^{T_i}\mathbf{f}_v\left(\mathbf{x}^{(i)}_t,\mathbf{y}^{(i)}_{t}\right)\propto\mathbb{E}_{\mathbf{y}^{'(i)}}\mathbf{f}_v;
\end{align*}
and analogously obtain:
\begin{align*}
  \frac{\partial{}a}{\partial\boldsymbol{\theta}_e} &= a\sum_{t=0}^{T_i}\sum_{c\in ch(t)}\mathbf{f}_e\left(\mathbf{y}^{(i)}_{t}, \mathbf{y}^{(i)}_{c}\right)\propto\mathbb{E}_{\mathbf{y}^{(i)}}\mathbf{f}_e,\\
  \frac{\partial{}b}{\partial\boldsymbol{\theta}_e} &= b\sum_{t=0}^{T_i}\sum_{c\in ch(t)}\mathbf{f}_e\left(\mathbf{y}^{'(i)}_{t}, \mathbf{y}^{'(i)}_{c}\right)\propto\mathbb{E}_{\mathbf{y}^{'(i)}}\mathbf{f}_e.
\end{align*}
With the help of these expressions, we can easily compute the gradient
of the objective function w.r.t. $\boldsymbol{\theta}$ by observing that:
\begin{align*}
  \nabla_{\boldsymbol{\theta}} &=
  \sum_{i=1}^{N}\frac{\sum_{\mathbf{y}_h^{(i)}}\nabla_{\boldsymbol{\theta}}a\sum_{\mathbf{y}_h^{(i)}}b
    -
    \sum_{\mathbf{y}_h^{(i)}}a\sum_{\mathbf{y}_h^{(i)}}\nabla_{\boldsymbol{\theta}}b}{\left(\sum_{\mathbf{y}_h^{(i)}}b\right)^2}.
\end{align*}
We again use the block-coordinate Frank-Wolfe method to optimize the
parameters of our model, but instead of pushing these parameters in
the direction
$\boldsymbol{\psi}\defeq\mathbf{f}(\mathbf{x}^{(i)},\mathbf{y}^{(i)})-\mathbf{f}(\mathbf{x}^{(i)},\mathbf{y}^{'(i)})$
(which is the derivative of latent CRF, see Algorithm~2 in
[\citeauthor{Lacoste-Julien:13}, \citeyear{{Lacoste-Julien:13}}]), now
force them in the direction of the above gradient.

It is probably easier to realize the difference of the two methods
(latent and latent marginalized CRFs) more vividly by looking at
Figure~\ref{dasa:fig:lcrf-vs-lmcrf}, in which we have highlighted the
paths that are used to compute the probabilities of correct and wrong
labels in both systems.  As we can see from the figure, LCRF considers
only a single label sequence that leads to the maximum probability the
correct tag at the only observed \textsc{Root} state and then
contrasts this sequence with the path that maximizes the probability
of an incorrect tag (in this case \textsc{NEG}) at the same
\textsc{Root} hypernode.  As opposed to this, LMCRF takes into account
all possible label configurations of elementary discourse units and
uses this total cumulative mass to estimate the probability of the
observed tag.

\begin{figure*}[thb]
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
    \resizebox{\hcrfwidth}{\hcrfheight}{
      \input{latent-crf-a}
    }
    \caption{Computational path of the probability of the correct
      label in latent CRF}
  \end{subfigure}
  ~
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
    \resizebox{\hcrfwidth}{\hcrfheight}{
      \input{latent-crf-b}
    }
    \caption{Computational path of the probability of a wrong label in
      latent CRF}
  \end{subfigure}\\[1em]
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
    \resizebox{\hcrfwidth}{\hcrfheight}{
      \input{latent-mcrf-a}
    }
    \caption{Computational path of the probability of the correct
      label in latent marginalized CRF}
  \end{subfigure}
  ~
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
    \resizebox{\hcrfwidth}{\hcrfheight}{
      \input{latent-mcrf-b}
    }
    \caption{Computational path of the probability of a wrong label in
      latent marginalized CRF}
  \end{subfigure}
  \caption[Confronted computational paths in LCRF and
    LMCRF]{Confronted computational paths in latent and
    latent-marginalized conditional random
    fields}\label{dasa:fig:lcrf-vs-lmcrf}
\end{figure*}

\textbf{Recursive Dirichlet Process} Finally, the last method that we
present in this chapter---\emph{Recursive Dirichlet Process} or
\emph{RDP}---goes a further step in the probabilistic direction by
assuming that not only the probabilities of discourse units but also
the parameters via which these probabilities are computed are random
variables.

In particular, we associate a variable
$\mathbf{z}_i\in\mathbb{R}_+^3$, s.t. $\norm{\mathbf{z}}_1 = 1$, with
every RST node $i$ (which in this case can be either an elementary
discource segment or an abstract span).\footnote{In contrast to the
  previous CRF approaches, this time, we depart from the dependency
  tree representation and adopt the discourse tree structure proposed
  by~\citet{Bhatia:15} for their R2N2 method instead.  In this
  structure, we keep all abstract nodes from the original RST
  constituency tree, but relink all satellites to the abstract parents
  of their nuclei.}  The value of this variable reflects the
multivariate probability of the three polarity classes
(\textsc{Negative}, \textsc{Neutral}, and \textsc{Positive}) for the
$i$-th node.  Since every element of $\mathbf{z}_i$ has to be
non-negative and their total sum must add up to one, it is natural to
assume that this variable is drawn from the Dirichlet distribution:
\begin{align*}
  \mathbf{z}_i \sim Dir(\boldsymbol{\alpha}).
\end{align*}
The only parameter accepted by this distribution, which simultaneously
controls both the means and the variances of all polarities, is vector
$\boldsymbol{\alpha}$.  Consequently, our primary goal now is to find a
way how to compute this parameter automatically for every discourse
node.

One obvious source of information for this computation would certainly
be the polarity scores predicted by the base classifiers for
elementary discourse segments, which we will henceforth denote as
$\boldsymbol{\alpha}_{i_0}\in\mathbb{R}^3_+$. Since these scores,
however, are only available for EDUs, we will initialize these priors
to zero vectors $\mathbf{0}$ for all remaining abstract nodes except
for the \textsc{Root}, for which we again will use the scores returned
by the LBA classifier for the whole tweet.

Another important factor which might significantly affect the polarity
of an RST node is the semantic orientation of its children and
discourse relations via which these children are connected to their
parent.  It is apparent that, in order to estimate the polarity
probability of the parent, we need to join these two factors together.
But how?

Let us first deal with the allegedly easier issue---namely, the
discourse links between parent and child.  To encode the different
nature of contrastive and non-contrastive connections, we will
introduce a separate matrix $M_r$ for each relation discourse relation
$r\defeq(\rho, \nu)$, where $\rho$ is the type of the relation
(\textsc{Con\-tra\-sti\-ve} or \textsc{Non-\-Con\-tra\-sti\-ve}) and
$\nu$ is the nuclearity status of the child (\textsc{Nucleus} or
\textsc{Satellite}). We sample these matrices from a multivariate
normal distribution:
\begin{align*}
  M_r \sim \mathcal{N}_{3\times3}(\boldsymbol{\mu}_r, \mathbbm{1}),
\end{align*}
setting the mean $\boldsymbol{\mu}_r$ to:
\begin{equation*}
  \boldsymbol{\mu}_r\defeq\begin{bmatrix}
  1 & 0 & 0\\
  0 & 0.3 & 0\\
  0 & 0 & 1
  \end{bmatrix}
\end{equation*}
for all non-contrastive relations, and swapping the first and the last
rows of this matrix for the contrastive ones.  The variance of this
distribution is set to all-ones matrix, independently of the discourse
relation.

Prior to computing the polarity scores of a RST node $i$, we will
multiply the probability vector of its child node $j$ with matrix
$M_r$ (where $r$ is the discource relation holding between parent and
child), projecting the result of this multiplication back to the
probability simplex using the sparsemax method of~\citet{Martins:16}:
\begin{align*}
  \mathbf{z}^*_j&= sparsemax\left(M_r\mathbf{z}_j^{\top}\right).
\end{align*}
As is obvious from this equation, each row of matrix $M_r$ is
responsible for propagating the respective polarity score of the child
node.  Thus, setting the second row to $[0., 0.3, 0.]$ will dampen th
neutral polarity of the child, which refelcts our wish that neutral
segments should have less impact on the overall polarity of a tweet if
other, strongly polar EDUs are present.  (At the same time, we still
want to assign neutral semantic orientation to the message if all of
its RST nodes are neutral.)  By swapping the rows, we wish to invert
the polarities of contrastive nodes, turning the negative semantic
orientation into a positive one and vice versa.

We sort all nodes of RST tree in reverse topological order and compute
their polarities bottom-up.  When arriving at node $i$, we iterate
over all children $j \in ch(i)$ of that node from left to right,
estimating the probability of polar classes at each state as follows:
\begin{align*}
  \boldsymbol{\alpha}_{i_j}&= \boldsymbol{\beta}\odot\mathbf{z}^*_j +
  (\mathbf{1} - \boldsymbol{\beta})\odot\mathbf{z}_{i_{j-1}},
\end{align*}
where $\boldsymbol{\beta}\in\mathbb{R}^3$ is another multivariate
random variable drawn from a Beta distribution $B(5., 5.)$, which
controls how much information from parent and child we want to see in
the updated state $\mathbf{z}_{i_j}$; and $\odot$ means elementwise
multiplication.

The only thing that we now need to do to the
$\boldsymbol{\alpha}_{i_j}$ vector before we draw the actual
probability distribution $\boldsymbol{z}_{i_j}$ from the corresponding
Dirichlet distribution $Dir(\boldsymbol{\alpha}_{i_j})$ is to scale
this vector by a certain factor.\footnote{Since all elements of
  $\boldsymbol{\alpha}_{i_j}$ will be in the range $[0,\ldots,1]$, the
  variance of the resulting probability will be extremely high, and
  the polarity scores drawn from this distribution will be almost
  random.} We compute this factor using the following formula:
\begin{align*}
  scale&= \frac{\xi \times \left(0.1 + \cos\left(\mathbf{z}^*_j,
    \mathbf{z}_{i_{j-1}}\right)\right)}{H\left(\boldsymbol{\alpha}_{i_j}\right)};
\end{align*}
where $\xi$ is a random model parameter drawn from a
$\chi^2$-distribution: $\xi\sim\chi^2(34)$; 0.1 is a constant used to
prevent zero scales in the cases when $\cos\left(\mathbf{z}^*_j,
\mathbf{z}_{i_{j-1}}\right)$ is equal to zero; and
$H\left(\boldsymbol{\alpha}_{i_j}\right)$ stands for the entropy of
the $\boldsymbol{\alpha}_{i_j}$ vector.

Although looking somewhat complicated, the intuition behind this
formula is very simple: the $\xi$ term encodes our belief in the
correctness of the model's prediction (the higher this value, the more
trust we have in the model); the cosine term measures the similarity
between the probabilities of parent and child (the more similar these
probabilities, the higher will be the scale); and, finally, the
entropy in the denominator tells us how uniform the vector
$\boldsymbol{\alpha}_{i_j}$ is (the more uniform the polarity scores
are, the less confidence we will have in the final outcome).

Now, with this term at hand, we are all set to draw the actual
probability of polar classes for the $i$-th node when seeing its
$j$-th child:
\begin{align*}
  \mathbf{z}_{i_{j}}\sim Dir(scale \times \boldsymbol{\alpha}_{i_j}).
\end{align*}
We subsequently use this result to estimate the polarity of the $i$-th
node when analyzing its next child $j+1$ ($\mathbf{z}_{i_{j+1}}$).
Once all children have been processed, we consider the final result
$\mathbf{z}_{i_{J}}$ as the final probability of polarity classes for
the $i$-th node and move on the next vertex in the BFS sorting.

\begin{figure}[htbp!]
  {
    \centering
    \includegraphics[width=\linewidth]{img/dirichlet-process.png}
  }
  \caption{Updated probabilities of parent nodes in Recursive Dirichlet
    Process}\label{dasa:fig:rdp-alplha}
\end{figure}

Finally, after arriving at the \textsc{Root} node and finishing
processing all of its descendents, we use the resulting term
$\mathbf{z}_{\text{\textsc{Root}}}$ as a parameter of a categorical
distribution to predict the final label $y$:
\begin{align*}
  y \sim Cat(\mathbf{z}_{\text{\textsc{Root}}}).
\end{align*}

This model with manually defined parameters reflects our prior belief
of what the joint probability of hidden and observed states $p(y,
\mathbf{z})$ might look like.  As it turns out, knowing this belief is
sufficient to derive another probability $q(\mathbf{z})$, which best
approximates the probability of latent variables.  In particular, we
define this distribution in the same way as above, but deprive it of
the last step (drawing of the observed label), and optimize its
parameters ($\boldsymbol{\theta}$) by maximizing the evidence lower
bound \cite[see][]{Ranganath:14}:
\begin{align*}
  \mathcal{L}\left(\boldsymbol{\theta}\right)
  &=\mathbb{E}_{q_{\boldsymbol{\theta}}(\mathbf{z})}%
  \left[\log\left(p(y, \mathbf{z})\right) -
    \log\left(q(\mathbf{z})\right)\right].
\end{align*}
We perform this optimization for 100 epochs, picking the parameter
values which yield best results for $q(\mathbf{z})$ on the set-aside
development data.

The results of our proposed and baseline methods are shown in
Table~\ref{dasa:tbl:res}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

      \multicolumn{12}{c}{\cellcolor{cellcolor}PotTS}\\

      %% General Statistics:
      LCRF & 0.76 & 0.79 & \textbf{0.77} & %
      \textbf{0.61} & 0.53 & 0.56 & %
      0.7 & 0.71 & 0.71 & %
      0.67 & 0.709\\

      %% General Statistics:
      LMCRF & \textbf{0.77} & 0.77 & \textbf{0.77} & %
      \textbf{0.61} & 0.54 & 0.57 & %
      0.69 & \textbf{0.74} & \textbf{0.72} & %
      0.671 & \textbf{0.712}\\

      %% General Statistics:
      RDP & 0.73 & 0.82 & \textbf{0.77} & %
      \textbf{0.61} & 0.56 & \textbf{0.58} & %
      \textbf{0.73} & 0.65 & 0.69 & %
      \textbf{0.678} & 0.706\\

      %% General Statistics:
      WNG & 0.58 & 0.79 & 0.67 & %
      \textbf{0.61} & 0.21 & 0.31 & %
      0.61 & 0.57 & 0.59 & %
      0.487 & 0.59\\

      %% General Statistics:
      DDR & 0.73 & 0.77 & 0.75 & %
      0.54 & \textbf{0.59} & 0.56 & %
      0.69 & 0.61 & 0.65 & %
      0.655 & 0.674\\

      %% General Statistics:
      R2N2 & 0.74 & 0.78 & 0.76 & %
      0.59 & 0.53 & 0.56 & %
      0.68 & 0.68 & 0.68 & %
      0.657 & 0.692\\

      %% General Statistics:
      \textsc{Last} & 0.52 & \textbf{0.83} & 0.64 & %
      0.57 & 0.17 & 0.26 & %
      0.61 & 0.43 & 0.5 & %
      0.453 & 0.549\\

      %% General Statistics:
      \textsc{Root} & 0.56 & 0.73 & 0.64 & %
      0.58 & 0.22 & 0.32 & %
      0.55 & 0.54 & 0.54 & %
      0.481 & 0.56\\

      %% General Statistics:
      \textsc{No-Discourse} & 0.73 & 0.82 & \textbf{0.77} & %
      \textbf{0.61} & 0.56 & \textbf{0.58} & %
      0.72 & 0.66 & 0.69 & %
      0.677 & 0.706\\

      \multicolumn{12}{c}{\cellcolor{cellcolor}SB10k}\\

      %% General Statistics:
      LCRF & \textbf{0.64} & \textbf{0.69} & 0.66 & %
      0.45 & \textbf{0.45} & 0.45 & %
      \textbf{0.82} & 0.79 & \textbf{0.8} & %
      0.557 & 0.713\\

      %% General Statistics:
      LMCRF & \textbf{0.64} & \textbf{0.69} & \textbf{0.67} & %
      0.45 & \textbf{0.45} & 0.45 & %
      \textbf{0.82} & 0.79 & \textbf{0.8} & %
      \textbf{0.56} & \textbf{0.715}\\

      %% General Statistics:
      RDP & \textbf{0.64} & \textbf{0.69} & 0.66 & %
      0.45 & \textbf{0.45} & 0.45 & %
      0.82 & 0.79 & \textbf{0.8} & %
      0.557 & 0.713\\

      WNG & 0.61 & 0.63 & 0.62 & %
      0.46 & 0.29 & 0.36 & %
      0.76 & \textbf{0.82} & 0.79 & %
      0.488 & 0.693\\

      %% General Statistics:
      DDR & 0.59 & 0.63 & 0.61 & %
      \textbf{0.48} & 0.44 & \textbf{0.46} & %
      0.77 & 0.76 & 0.77 & %
      0.534 & 0.681\\

      %% General Statistics:
      R2N2 & \textbf{0.64} & \textbf{0.69} & 0.66 & %
      0.46 & \textbf{0.45} & 0.45 & %
      0.81 & 0.79 & \textbf{0.8} & %
      0.559 & 0.713\\

      %% General Statistics:
      \textsc{Last} & 0.56 & 0.55 & 0.56 & %
      0.46 & 0.29 & 0.36 & %
      0.73 & 0.8 & 0.76 & %
      0.459 & 0.661\\

      %% General Statistics:
      \textsc{Root} & 0.51 & 0.55 & 0.53 & %
      0.4 & 0.3 & 0.35 & %
      0.74 & 0.76 & 0.75 & %
      0.438 & 0.64\\

      %% General Statistics:
      \textsc{No-Discourse} & \textbf{0.64} & \textbf{0.69} & 0.66 & %
      0.45 & \textbf{0.45} & 0.45 & %
      \textbf{0.82} & 0.79 & \textbf{0.8} & %
      0.557 & 0.713\\\bottomrule
    \end{tabular}
    \egroup
    \caption[Evaluation of DASA methods]{Evaluation of discourse-aware
      sentiment analysis methods\\ {\small LCRF~--~latent conditional random fields,
        LMCRF~--~latent marginalized conditional random fields,
        RDP~--~recursive Dirichlet process,
        WNG~--~\citet{Wang:13},
        DDR~--~discourse-depth reweighting~\cite{Bhatia:15},
        R2N2~--~rhetorical recursive neural network~\cite{Bhatia:15},
        \textsc{Last}~--~polarity determined by last EDU,
        \textsc{Root}~--~polarity determined by root EDU(s),
        \textsc{No-Discourse}~--~discourse-unaware classifier}}
    \label{dasa:tbl:res}
  \end{center}
\end{table}

\section{Evaluation}

\subsection{Base Classifier}

\subsection{Relation Scheme}

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.17\columnwidth} % first columm
        *{1}{>{\centering\arraybackslash}p{0.4\columnwidth}}
        *{1}{>{}p{0.4\columnwidth}}} % next two columns
      \toprule
      \textbf{Scheme} & \textbf{Relation Set} & \textbf{Equivalence Classes}\\\midrule

      \textsc{PCC} & \{ \textsc{Antithesis}, \textsc{Background},
      \textsc{Cause}, \textsc{Circumstance}, \textsc{Concession},
      \textsc{Condition}, \textsc{Conjunction}, \textsc{Contrast},
      \textsc{Disjunction}, \textsc{E-Elaboration},
      \textsc{Elaboration}, \textsc{Enablement},
      \textsc{Evaluation-N}, \textsc{Evaluation-S}, \textsc{Evidence},
      \textsc{Interpretation}, \textsc{Joint}, \textsc{Justify},
      \textsc{List}, \textsc{Means}, \textsc{Motivation},
      \textsc{Otherwise}, \textsc{Preparation}, \textsc{Purpose},
      \textsc{Reason}, \textsc{Restatement}, \textsc{Restatement-MN},
      \textsc{Result}, \textsc{Sequence}, \textsc{Solutionhood},
      \textsc{Summary}, \textsc{Unconditional}, \textsc{Unless},
      \textsc{Unstated-Relation}\} & \\

      \citet{Heerschop:11} & \{\textsc{Attribution},
      \textsc{Background}, \textsc{Cause}, \textsc{Condition},
      \textsc{Contrast}, \textsc{Elaboration}, \textsc{Enablement},
      \textsc{Explanation}, \textsc{\bfseries Other}\} & \\

      \citet{Zhou:11} & \{\textsc{Contrast}, \textsc{Condition},
      \textsc{Continuation}, \textsc{Cause}, \textsc{Purpose},
      \textsc{\bfseries Other}\} & \textsc{Contrast} $\defeq$
      \{\textsc{Antithesis}, \textsc{Concession}, \textsc{Otherwise},
      \textsc{Contrast}\};\newline \textsc{Continuation} $\defeq$
      \{\textsc{Continuation}, \textsc{Parallel}\};\newline
      \textsc{Cause} $\defeq$ \{\textsc{Evidence}, \textsc{Volitional
        Cause}, \textsc{Nonvolitional-Cause},
      \textsc{Volitional-Result}, \textsc{Nonvolitional-Result}\};\\

      \citet{Chenlo:13} & \{\textsc{Attribution}, \textsc{Background},
      \textsc{Cause}, \textsc{Comparison}, \textsc{Condition},
      \textsc{Consequence}, \textsc{Contrast}, \textsc{Elaboration},
      \textsc{Enablement}, \textsc{Evaluation}, "Explanation",
      \textsc{Joint}, \textsc{Otherwise}, \textsc{Temporal},
      \textsc{\bfseries Other}\} & \\

      \citet{Bhatia:15} & \{\textsc{Contrastive},
      \textsc{\bfseries Non-Contrastive}\} & \textsc{Contrastive} $\defeq$
      \{\textsc{Antithesis}, \textsc{Antithesis-E},
      \textsc{Comparison}, \textsc{Concession},
      \textsc{Consequence-S}, \textsc{Contrast},
      \textsc{Problem-Solution}\}.\\\bottomrule
    \end{tabular}
    \egroup
    \caption[RST relations used in different discourse-aware sentiment
      methods]{RST relations used in the original Potsdam Commentary
      Corpus and different discourse-aware sentiment methods\\ {\small
        (default relation [which subsumes the rest of the links] is
        highlighted in \textbf{boldface})}}
    \label{dasa:tbl:rst-rels}
  \end{center}
\end{table}

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.22\columnwidth} % first columm
        *{3}{>{\centering\arraybackslash}p{0.25\columnwidth}}} \toprule

      \textbf{Relation Scheme} & \textbf{Span \F{}} &
      \textbf{Nuclearity \F{}} & \textbf{Relation \F{}}\\\midrule

      \textsc{PCC} & 0.776 & 0.534 & 0.326\\

      \citet{Heerschop:11} & 0.774 & 0.51 & 0.361\\

      \citet{Zhou:11} & 0.776 & 0.501 & 0.388\\

      \citet{Chenlo:13} & 0.769 & 0.505 & 0.362\\

      \citet{Bhatia:15} & 0.777 & 0.512 & 0.396\\\bottomrule
    \end{tabular}
    \egroup
    \caption[Results of RST parser on PCC~2]{Results of automatic RST
      parser on PCC~2.0 with different relation schemes}
    \label{dasa:tbl:rst-rels}
  \end{center}
\end{table}

\section{Summary and Conclusions}
