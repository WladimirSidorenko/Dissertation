\chapter{Coarse-Grained Sentiment Analysis}\label{sec:snt:cgsa}

Having familiarized ourselves with the peculiarities of the creation
of a sentiment corpus, the different ways to automatically induce new
polarity lists, and the difficulties of fine-grained opinion mining,
we now move on to the presumably most popular sentiment analysis
objective---the coarse-grained analysis or CGSA, in which we need to
determine the overall polarity of a message.

Traditionally, this task has been addessed with either of three
popular method groups:
\begin{inparaenum}[(i)]
  \item lexicon-based approaches,
  \item machine-learning-based (ML) techniques, and
  \item deep-learning-based (DL) applications.
\end{inparaenum}
In this chapter, we are going to scrutinize the most prominent
representatives of each of these paradigms and also tackle a much more
ambitious goal, namely to check whether we can achieve results
comparable with the scores of these methods when the language of the
domain we train on is completely different from the language of the
test data.

We begin our comparison by first presenting the metrics that we will
use in our subsequent evaluation.  After a brief description of the
data preparation step, we proceed to the actual estimation of popular
lexicon-, ML-, and DL-based approaches, explaining and evaluating them
in Sections~\ref{sec:cgsa:lexicon-based}, \ref{sec:cgsa:ml-based},
and~\ref{sec:cgsa:dl-based} respectively.  Then,
in~Section~\ref{sec:cgsa:domain-adaptation}, we also show which
results can be obtained by using cross-lingual transfer, where we
train a classifier on English microblogs and then adapt this system to
German messages.  Finally, we conclude with an extensive evaluation of
different hyperparameters and settings (including various types of
sentiment lexicons, different kinds of word embeddings, the utility of
the text normalization step, and the impact of additional noisily
labeled training data), summarizing our results and recapping our
findings at the end of this chapter.

\section{Evaluation Metrics}\label{sec:cgsa:eval-metrics}

To estimate the quality of the compared systems, we will rely on two
established evaluation metrics which are commonly used for measuring
CGSA results: One of these metrics is the macro-averaged \F-score over
the two major polarity classes~(positive and negative): { \small%
  \begin{equation*}
    F_1 = \frac{F_{pos} + F_{neg}}{2}.
  \end{equation*}%
  \normalsize%
}%
This measure was first introduced by the organizers of the SemEval
competition~\cite{Nakov:13,Rosenthal:14,Rosenthal:15} and has become a
de facto standard not only for the SemEval dataset, but virtually for
all related coarse-grained sentiment tasks and corpora.

The second metric is the micro-averaged \F-score over all three
possible semantic orientations (positive, negative, and neutral),
which basically corresponds to the accuracy over the complete labeled
dataset~\cite[see][p.~577]{Manning:99}.  This measure both predates
and supersedes the SemEval evaluation as it had already been applied
in the very first works on coarse-grained opinion
mining~\cite{Wiebe:99,Das:01,Read:05,Kennedy:06,Go:09} and was again
reintroduced at the GermEval Shared Task on Sentiment
Analysis~2017~\cite{Wojatzki:17}.

Moreover, in addition to these two metrics, we will also give a
detailed information on precision, recall, and \F-scores of each
particular polarity class in order to get a better intuition about
precise strengths, weaknesses, and biases of each evaluated method.

\section{Data Preparation}\label{sec:cgsa:data}

Similarly to the data preparation steps used for fine-grained
sentiment analysis, we preprocessed all tweets involved in our
experiments with the text normalization system
of~\citet{Sidarenka:13}, tokenized them using the same adjusted
version of Potts'
tokenizer,\footnote{\url{http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py}}
lemmatized and assigned part-of-speech tags to these tokens with the
\texttt{TreeTagger} of \citet{Schmid:95}.  Moreover, like in the
previous chapter, we automatically obtained morphological features for
each word, and induced syntactic trees for each sentence with the help
of the \texttt{Mate} dependency parser
\cite{Bohnet:13}. % Apart from the
% PotTS dataset, we also applied this procedure to the microblogs of
% the German Twitter snapshot~\cite{Scheffler:14}, which will be used
% in our subsequent experiments on noisy supervision.

We again divided the PotTS corpus~\cite{Sidarenka:16} into a training,
development, and test set, using 70\% of the tweets for learning, 10\%
for tuning and picking the optimal hyperparameters, and the remaining
20\% for evaluating the results.  We inferred polarity labels for
these microblogs with a simple heuristic rule, assigning the positive
(negative) class to all messages which had exclusively positive
(negative) sentiments and skipping all microblogs that simultaneously
contained multiple opinions with opposite semantic orientations.  In
cases when a sentiment was absent, we applied a fallback strategy,
considering all tweets with only positive (negative) polar terms as
positive (negative), disregarding messages which featured expressions
from both polarity classes, and taking the rest of the corpus (i.e.,
posts without any sentiments nor polar terms) as neutral instances.

A few examples of such heuristically inferred labels are provided
below:
\begin{example}[Coarse-Grained Sentiment Annotations]\label{snt:cgsa:exmp:anno1}
  \noindent\textup{\bfseries\textcolor{darkred}{Tweet:}} {\upshape Ich finde den Papst putzig \smiley{}}\\
  \noindent I find the Pope cute \smiley{}.\\
  \noindent\textup{\bfseries\textcolor{darkred}{Label:}}\hspace*{2em}\textbf{%
    \upshape\textcolor{green3}{positive}}\\[1.5em]
  \noindent\textup{\bfseries\textcolor{darkred}{Tweet:}} {\upshape typisch Bayern kaum ist der neue Papst da und schon haben
  sie ihn in der Tasche ...}\\
  \noindent Typical Bavaria The new Pope is hardly there, as they already have him in their pocket\\
  \noindent\textup{\bfseries\textcolor{darkred}{Label:}}\hspace*{2em}\textbf{%
    \upshape\textcolor{midnightblue}{negative}}
\end{example}
As we can see from these sample messages, our simple rule provides
reasonable decisions in most of the cases.  However, since this
approach is still an approximation and consequently prone to errors,
especially in the cases where the polarity of the whole microblog
differs from the semantic orientation of its single tokens or is
expressed without any explicit polar expressions at all (see
Example~\ref{snt:cgsa:exmp:anno2}), we also decided to evaluate all
CGSA methods presented in this chapter on another German Twitter
corpus, which has been specifically annotated with message-level
polarities---SB10k.

\begin{example}[Erroneous Sentiment
  Annotations]\label{snt:cgsa:exmp:anno2}
  \noindent\textup{\bfseries\textcolor{darkred}{Tweet:}} {\upshape Unser Park, unser Geld, unsere Stadt! -NICHT unser Finanzminister! \smiley{} \#schmid \#spd \#s21 \#btw13}\\
  \noindent Our park, our money, our city! -NOT our Finance Minister! \smiley{} \#schmid \#spd \#s21 \#btw13\\
  \noindent\textup{\bfseries\textcolor{darkred}{Label:}}\hspace*{2em}\textbf{%
    \upshape\textcolor{green3}{positive*}}\\[1.5em]
  \noindent\textup{\bfseries\textcolor{darkred}{Tweet:}} {\upshape Auf die Lobby-FDP von heute kann Deutschland verzichten ...}\\
  \noindent Germany can go without today's lobby FDP\\
  \noindent\textup{\bfseries\textcolor{darkred}{Label:}}\hspace*{2em}\textbf{%
    \upshape\textcolor{black}{neutral*}}
\end{example}

The SB10k dataset was introduced by~\citet{Cieliebak:17}, and
comprises a total of 9,738 tweets.  These messages were selected from
a larger snapshot of 5M German microblogs gathered between August and
November~2013.  To ensure lexical diversity and proportional polarity
distribution in this corpus, the authors first divided all downloaded
snapshot posts into 2,500 clusters using $k$-means with unigram
features.  Afterwards, from each of these groups, they selected posts
which simultaneously comprised at least one positive and one negative
term from the German Polarity Clues lexicon~\cite{Waltinger:10}.  In
the final step, each of these tweets was labeled by at least three
experts from a pool of 34 different annotators, whose final agreement
run up to 0.39 Krippendorff's $\alpha$.  Unfortunately, due to the
restrictions of Twitter's terms of use (which only allow to distribute
the ids of the annotated tweets along with their labels), we were able
to retrieve merely 7,476 messages of this corpus, which nevertheless
constitutes a substantial amount of data, comparable to the size of
the PotTS dataset.

In addition to the aforementioned corpora (PotTS and SB10k), we
automatically annotated all microblogs of the German Twitter
Snapshot~\cite{Scheffler:14}, following the procedure proposed
by~\citet{Read:05} and~\citet{Go:09}, and assigning the positive
(negative) class to the tweets which contained the respective
emoticons.  However, in contrast to the previous two datasets, we will
not use these tweets for evaluation, but solely utilize them for
training in our distant supervision experiments.

\todo[inline]{Add inter-annotator agreement of the PotTS corpus.}

The resulting statistics on the number of messages and polarity
distribution in the final data are shown in
Table~\ref{snt-cgsa:tbl:corp-dist}.
\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{6}{>{\centering\arraybackslash}p{0.13\columnwidth}}} % last two columns
      \toprule
      \textbf{Dataset} & \multicolumn{4}{c}{\bfseries Polarity Class}%
      & \multicolumn{2}{c}{\bfseries Agreement}\\\cline{2-5}\cline{6-7}
                       & \textbf{Positive} & \textbf{Negative} %
                                           & \textbf{Neutral} & \textbf{Mixed*} %
                                                              & $\alpha$ & $\kappa$\\\midrule

      \textbf{PotTS} & 3,380 & 1,541 & 2,558 & 513 & & \\
      \textbf{SB10k} & 1,717 & 1,130 & 4,629 & 0 & & \\
      \textbf{GTS} & 3,326,829 & 350,775 & 19,453,669 & 73,776 & &\\\bottomrule
\end{tabular}
    \egroup
    \caption{Polarity class distribution in PotTS, SB10k, and German
      Twitter Snapshot (GTS).\\
      \emph{(* -- the \emph{mixed} polarity was excluded from our
        experiments)}}
    \label{snt-cgsa:tbl:corp-dist}
  \end{center}
\end{table}

As we can see, each of the datasets has its own unique composition of
polar messages: The PotTS corpus, for example, shows a conspicuous
bias towards the positive class with 42\% of the tweets belonging to
this polarity.  We can partially explain this effect by the following
reasons: first of all, it might be due to the coarseness of the
heuristic rule that we applied to infer the labels for these messages;
and, secondly, it might also stem from the initial selection criteria
that we used to compile the data for this collection.  As you might
remember, we a priori composed the major part of this dataset from
microblogs which contained emoticons or had at least one polar
expression from the SentiWS lexicon~\cite{Remus:10}.  Since most of
these emoticons were positive (which is evident from the statistics of
the German Twitter snapshot), the selected posts also became skewed
towards this semantic orientation.

The second most frequent group of the PotTS corpus is formed by
neutral microblogs, which account for 32\% of the data.  Finally,
negative posts represent the absolute minority of all instances
(merely 19\%), which, however, is less surprising as the same tendency
can be observed for the SB10k and German Twitter Snapshot too.

Regarding the last two corpora, we can observe a more uniform (though
not identical) behavior as both collections are dominated by neutral
messages, which constitue 62\% of SB10k data and 84\% of the German
Twitter Snapshot.  The positive class, again, makes up a big part of
the data (23\% for SB10k and 14\% for the snapshot), but its influence
this time is much less pronounced than in the PotTS case.  As
mentioned above, the negative class is the least represented semantic
representation across all three sources.  The only group which has
even less instances than the negative class is the mixed polarity; we,
however, will skip these messages in our experiments for the sake of
simplicity and uniformity of the evaluation.\footnote{As we will see
  later, some of the CGSA methods (especially the lexicon-based ones)
  can hardly be extended to the prediction of more than three polarity
  classes.}

\section{Lexicon-Based Methods}\label{sec:cgsa:lexicon-based}

The first group of approaches that we are going to explore in this
chapter using the described data are lexicon-based (LB) systems.  Just
like sentiment lexicons themselves, LB methods for coarse-grained
opinion mining have attracted a lot of attention from the very
inception of the sentiment analysis field.  Starting with the research
of~\citet{Hatzivassi:00}, who statistically proved that the mere
occurrence of a subjective adjective from a polarity list was a highly
reliable indicator that the whole sentence it appeared in was
subjective, more and more works dealing with the use of lexicons for
determining the overall polarity of complete texts appeared on the
scene.

One of the first notable steps in this direction was made
by~\citet{Das:01}, who proposed an ensemble of five classifiers (two
of which were purely lexicon-based and the other three relied on
lexicon features) to predict the polarity of stock messages
(\emph{buy}, \emph{sell}, or \emph{neutral}), achieving an accuracy of
62\% on a corpus of several hundreds stock board messages.  A much
simpler method for a similar task was suggested by~\citet{Turney:02},
who determined the \emph{semantic orientation} (SO) of reviews by
averaging the PMI scores of their terms, obtaining these scores from
an automatically generated sentiment lexicon.  With this approach, the
author could reach an accuracy of 74\% on a corpus of 410 manually
labeled Epinions comments.  In the same vein, \citet{Hu:04} computed
the overall polarity of a sentence by comparing the numbers of
positive and negative terms appearing in that sentence, reversing the
orientation of the term in case of negation.  Finally, \citet{Kim:04}
compared three different approaches to determining the polarity of a
sentence:
\begin{inparaenum}[(i)]
\item by multiplying the signs of its polar terms,
\item by taking the sum of their scores, and
\item by computing the geometric mean of these values;
\end{inparaenum}
finding the first and the last option working best on the Document
Undestanding Corpus.\footnote{\url{http://duc.nist.gov/}}

% % Hu and Liu, 2004
% Similarly, \citet{Hu:04} determined the semantic orientation of
% sentences in customer reviews by simply comparing the number of
% positive and negative terms found in these passages. Since the
% authors, however, were primarily interested in estimating the polarity
% towards particular product features mentioned in the clauses, they
% additionally applied a fallback strategy in case of a tie by checking
% which of the polar lexicon terms appeared closer to the features, and
% assuming the polarity of the preceding sentence if these numbers were
% also equal.

% % Taboada et al., 2004
% Largely inspired by the Appraisal theory of~\citet{Martin:00},
% \citet{Taboada:04} enhanced the original method of~\citet{Turney:02}
% by increasing the weights of polar adjectives which occurred in the
% middle and at the end of a document, and also augmenting these values
% with the affect, judgement, and appreciation scores.  Similarly to
% polarity, the appraisal scores were calculated automatically by
% computing the PMI of their cooccurrence with different pronouns using
% a web search engine.

% Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006
In~\citeyear{Polanyi:06}, \citeauthor{Polanyi:06} presented an
extensive overview and analysis of common lexicon-based sentiment
approaches existing at that time, arguing that, besides considering
the lexical valence (i.e., semantic orientation) of polar terms, it
was also necessary to incorporate syntactic, discourse-level, and
extra-linguistic factors such as negations, intensifiers, modal
operators (e.g., \emph{could} or \emph{might}), presuppositional items
(e.g., \emph{barely} or \emph{failure}), irony, reported speech,
discourse connectors, genre and attitude assessment, reported speech,
multi-entity evaluation, etc.  This theoretical hypothesis was also
proven empirically by \citet{Kennedy:06}, who investigated two ways to
determine the polarity of a customer review: In the first of these
methods, they simply compared the numbers of positive and negative
expressions appearing in text, assigning the review to the class with
the greater number of items.  In the second attempt, they enhanced the
original system with additional information about contextual valence
shifters, increasing or decreasing the sentiment score of a term if it
was preceded by an intensifier or downtoner, and changing the polarity
sign of this score to the opposite in case of a negation.  With this
adjustment, the authors achieved a statistically significant
improvement, boosting the accuracy of the two-class prediction on a
corpus of product and movie reviews from 67.9 to 69.3\%.

% Taboada et al., 2011
Finally, a veritably seminal work on lexicon-based techniques was
presented by~\citet{Taboada:11}, who introduced a manually compiled
polarity list\footnote{The authors hand-annotated all occurrences of
  adjectives, nouns, and verbs found in a corpus of 400 Epinions
  reviews with ordinal categories ranging from -5 to 5 which reflected
  the semantic orientation of a term (positive vs. negative) and its
  polar strength (weak vs. strong).} and used this resource to
estimate the overall semantic orientation of the text.  Drawing on the
ideas of~\citet{Polanyi:06}, the authors incorporated a set of
additional heuristic rules into their computation by changing the
prior SO values of negated, itensified, and downtoned terms, ignoring
irrealis sentences, and adjusting the weights of specific document
sections.  An extensive evaluation of this approach showed the
superior performance of the manual lexicon in comparison with other
polarity lists, including the Subjectivity
Dictionary~\cite{Wilson:05}, Maryland Polarity Set~\cite{Mohammad:09},
and \textsc{SentiWordNet}~\cite{Esuli:06c}.  Moreover, the authors
also demonstrated the effectiveness of their method for other topics
and text genres, hypothesizing that lexion-based approaches were in
general more robust to domain shifts than traditional supervised
machine-learning techniques.

% % Taboada et al., 2006
% Another important contribution to the development of lexicon-based
% approaches was made by~\citet{Taboada:06}, who compared three popular
% polarity lists---a PMI lexicon computed with the original method
% of~\citet{Turney:02} using the AltaVista's NEAR operator; a similar
% polarity list obtained with the help of Google's AND queries; and,
% finally, the manually compiled General Inquirer lexicon
% of~\citet{Stone:66}.  The authors evaluated these resources both
% intrinsically (by comparing them with GI entries) and extrinsically
% (by computing the polarity of 400 manually annotated Epinions
% reviews).  To estimate the overall polarity of a review for the second
% task, \citeauthor{Taboada:06} calculated the average SO value of all
% polar terms found in the review, obtaining these scores from the
% mean-normalized lexicons, and flipping the polarity sign to the
% opposite in case of the negation.

% Musto et al., 2014
Lexicon-based systems have also found their way into sentiment
analysis of social media: For example, one such approach, specifically
tailored to Twitter data, was proposed by~\citet{Musto:14}, who
examined four different ways to compute the overall polarity scores of
microblogs: \emph{basic}, \emph{normalized}, \emph{emphasized}, and
\emph{normalized-emphasized}; evaluating these strategies with four
distinct lexicons: \textsc{Sen\-ti\-Word\-Net}~\cite{Esuli:06c},
\textsc{Word\-Net-\-Affect}~\cite{Strapparava:04},
\textsc{MPQA}~\cite{Wiebe:05}, and
\textsc{SenticNet}~\cite{Cambria:14}.  In all of these methods, the
authors first split an input message into a list of
\emph{micro-phrases} based on the occurrence of punctuation marks and
conjunctions.  Afterwards, they calculated the polarity score of each
of these segments and estimated the overall polarity of the whole
tweet by uniting the scores of its micro-phrases.
\citeauthor{Musto:14} obtained their best results (58.99\% accuracy on
the SemEval-2013 dataset) with the normalized-emphasized approach, in
which they averaged the polarity scores of segments' tokens, boosting
these values by 50\% for informative parts of speech; and regarded the
sum of the micro-phrase scores as the final overall polarity of the
microblog.

% the authors obtained their best results using the
% \textsc{SentiWordNet} lexicon of~\citet{Esuli:06c}

% Jurek et al., 2015
Another Twitter-aware system was presented by~\citet{Jurek:15}, who
computed the negative and positive polarity of a message ($F_p$ and
$F_n$ respectively) using the following equations: { \small%
  \begin{align}
    F_P &= \min\left(\frac{A_P}{2 - \log(3.5\times W_P + I_P)}, 100\right),\\
    F_N &= \max\left(\frac{A_N}{2 - \log(3.5\times W_N + I_N)}, -100\right);\label{cgsa:eq:jurek}
  \end{align}%
  \normalsize%
}%
where $A_P$ and $A_N$ represent the average scores of positive and
negative lexicon terms found in the tweet:
$A_p = \frac{\sum_{w\in\textrm{msg}}s^p_w}{\lVert\textrm{msg}\rVert}$,
with $s^p_w$ denoting the positive lexicon score of the term $w$;
$W_P$ and $W_N$ stand for the raw counts of polar tokens; and $I_P$
and $I_N$ denote the number of intensifiers preceding these words.  In
addition to that, prior to estimating the average values, the authors
modified the polarity scores $s_w$ of negated words by applying the
following heuristics: { \small%
  \begin{align}
neg(s_w) =
    \begin{cases}
        \min\left(\frac{s_w - 100}{2}, -10\right) & \text{if } s_w > 0,\\
        \max\left(\frac{s_w + 100}{2}, 10\right), & \text{if } s_w < 0.
    \end{cases}
\end{align}%
\normalsize%
}%
Besides computing the polarity scores $F_p$ and $F_n$,
\citeauthor{Jurek:15} also determined the subjectivity degree of the
message by replacing the terms $A_P$ and $A_N$ in
Equation~\ref{cgsa:eq:jurek} with averaged conditional probabilitites
of the tweet being subjective given the occurrences of the respective
polar terms.\footnote{These probabilities were calculated
  automatically on the noisily labeled data set of~\citet{Go:09}.}
The authors considered a microblog as positive if its overall polarity
score was greater than 25 and negative otherwise, provided that the
subjectivity value of this message was higher than 0.5.  With this
method, they achieved an accuracy of~77.3\% on the manually annotated
subset of \citeauthor{Go:09}'s corpus and reached 74.2\% on the IMDB
review corpus~\cite{Maas:11}.

% Kolchyna et al., 2015
Finally, \citet{Kolchyna:15} also explored two different ways of
computing the overall polarity of a microblog:
\begin{inparaenum}[(i)]
\item by simply averaging the scores of the lexicon terms found in the
  message and
\item by taking the signed logarithm of this average:
\end{inparaenum}
\begin{equation*}
  \text{Score}_{\log} =
  \begin{cases}
    \text{sign}(\text{Score}_{\text{AVG}})\log_{10}(|\text{Score}_{\text{AVG}}|) & %
    \text{if |Score}_{\text{AVG}}| > 0.1,\\
    0, & \text{otherwise};
  \end{cases}
\end{equation*}%
comparing theses approaches on the SemEval-2013
dataset~\cite{Nakov:13}.  The authors determined the final polarity
class of a tweet with the help of $k$-means clustering, which utilized
either of the above polarity scores as its features.  They showed that
the logarithmic strategy performed better than the simple average
solution, yielding an accuracy of 61.74\%.  In addition to that,
\citeauthor{Kolchyna:15} also checked whether these lexicon values
could serve as useful attributes for an ML-based method.  For this
purpose, they retrained a cost-sensitive SVM
classifier~\cite{Masnadi:12} after extending its $n$-gram feature set
with lexicon features, getting almost five percent accuracy
improvement (from 86.62 to 91.17) on the IMDB movie review
dataset~\cite{Pang:02}.

\todo[inline]{reimplement and describe the results of \citet{Hu:04},
  \citet{Taboada:11}, \citet{Musto:14}, \citet{Jurek:15}, and
  \citet{Kolchyna:15}}

In order to estimate the quality of these methods on the PotTS
dataset, we reimplemented the approaches suggested
by~\citet{Taboada:11}, \citet{Musto:14}, \citet{Jurek:15}, and
\citet{Kolchyna:15}, and applied these systems to the test set tweets
described in Section~\ref{sec:cgsa:data}, tweaking the threshold
values of the evaluated methods on the training and development data.
The results of this evaluation are shown in Table~\ref{snt-cgsa:tbl:lex-res}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

       HL &  &  &  & %
       &  &  & %
       &  &  & %
       & \\

       TBD &  &  &  & %
       &  &  & %
       &  &  & %
       & \\

       MST &  &  &  & %
       &  &  & %
       &  &  & %
       & \\

       JRK &  &  &  & %
       &  &  & %
       &  &  & %
       & \\

       KLCH &  &  &  & %
       &  &  & %
       &  &  & %
       & \\\bottomrule
\end{tabular}
    \egroup
    \caption[Evaluation of lexicon-based CGSA methods.]{
      Evaluation of lexicon-based CGSA methods.\\
      {\small HL~--~\citet{Hu:04}, TBD~--~\citet{Taboada:11}, MST~-- \citet{Musto:14}, JRK
        -- \citet{Jurek:15}, KLCH -- \citet{Kolchyna:15}}}
    \label{snt-cgsa:tbl:lex-res}
  \end{center}
\end{table}

\section{Machine-Learning Based Methods}\label{sec:cgsa:ml-based}

Despite their immense popularity, linguistic plausibility, and
simplicity to implement, lexicon-based approaches often have been
criticized for the rigidness of their classification\footnote{Since
  these systems only rely on the pre-computed weights of the lexicon
  entries, considering these coefficients as constant, their decision
  boundaries frequently appear to be suboptimal as many terms might
  have different polarity and intensity values depending on the domain
  of the text \cite[see][]{Yang:17}.} and the inability to factor in
additional, non-lexical attributes into the final decisions.
Moreover, as noted by~\citet{Pang:02} and confirmed
by~\citet{Riloff:03} and \citet{Gamon:04}, many of the linguistic
expressions which actually correlate with the subjectivity and
polarity of a sentence are very unlikely to be included into a
sentiment lexicon by a human expert.  As a consequence of this, with
the emergence of new manually annotated corpora, lexicon-based systems
have gradually been superseded by supervised machine-learning
techniques.

One of the first steps in this direction was taken
by~\citet{Wiebe:99}, who used a Na{\"i}ve Bayes classifier to
differentiate between subjective and objective statements.  Using
primarily binary features which reflected the presence of a pronoun,
an adjective, a cardinal number, or a modal verb in the analyzed
sentence, achieved an accuracy of~72.17\%, outperforming the majority
class baseline by more than 20~percentage points.  An even better
result (81.5\%) could be reached when the dataset was restricted only
to the examples with the most confident annotations.

\done[inline]{\citet{Pang:02}}

Further notable contributions to DL-based CGSA methods were made
by~\citet{Pang:02}, who compared Na{\"i}ve Bayes, Maximum Entropy, and
SVM systems on the polarity classification task for movie reviews,
getting their best results (82.9\% accuracy) with the SVM approach
that used only unigram features.

\done[inline]{\citet{Riloff:03a}}

\citet{Riloff:03a} addressed the problem of inusfficient manually
annotated sentiment resources by proposing a bootstrapping method for
training a subjectivity classifier.  In this approach, the authors
first applied two high-precision predictors to a large collection of
unlabeled sentences in order to get an initial set of subjective and
objective instances.  Afterwards, they used the AutoSlog-TS
algorithm~\cite{Riloff:96} to extract expressions which strongly
correlated with the subjective class and employed these phrases to
classify the remaining sentences from the corpus.
\citeauthor{Riloff:03a} repeated the last two steps (pattern
extraction and expansion of the training set) multiple times to
transitively learn new subjective phrases.  With the final system, the
authors achieved a precision of 0.902 on recognizing polar sentences,
with their recall running up to 0.401.

\done[inline]{\citet{Yu:03}}

A semi-supervised classification approach was proposed
by~\citet{Yu:03}, who presented a three-stage method, in which they
first distinguished between subjective and objective documents, then
differentiated between polar and neutral sentences, and, finally,
classified the polarity of opinionated clauses.  The authors used a
Na{\"i}ve Bayes classifer for the document-level task, reaching a
remarkable \F-score of~0.96 on this objective; and applied an ensemble
of NB systems to predict the subjectivity of the sentences.  In the
final step, they determined the semantic orientation of a subjective
clause by averaging the polarity scores of their tokens, getting these
scores from an automatically constructed sentiment
lexicon~\cite{Hatzivassi:97}.  With this approach, \citeauthor{Yu:03}
attained an accuracy of~91\% on a set of 38 sentences which had a
perfect inter-annotator agreement in their data.

\done[inline]{\citet{Gamon:04}}

Another SVM approach was proposed by~\citet{Gamon:04}, who used this
classifier in conjunction with a rich set of linguistic
(part-of-speech trigrams, constituent-specific length measures,
context free phrase structure patterns, as well as part-of-speech
information coupled with semantic relations) and surface features
(primarily word $n$-grams) to classify customer feedback into positive
and negative messages.  The author achieved his best results (77.5\%
accuracy and $\approx$0.77~\F) using top 2,000 features with the
highest log likelihood ratios.  Interestingly enough,
\citeauthor{Gamon:04} also could obtain quite competitive figures
(74.5\% accuracy) by using linguistically motivated features only.

\done[inline]{\citet{Wilson:04,Wilson:06}}

A related problem, namely that of classifying the strength of
opinions, was addressed by~\citet{Wilson:04,Wilson:06}.  In
particular, the authors proposed a wide variety of linguistic features
(including automatically learned lexico-syntactic patterns similar to
the ones used by~\citet{Riloff:03}, bags of words, and syntactic
attributes such as lemma and PoS tag of the root of a dependency tree,
lemmas and tags of its intermediate nodes and leaves, lexicalized
relation tuples, i.e., tuples consisting of the lemma of a parent
node, its grammatical relation to the child, and the lemma of the
child itself, etc.), checking the utility of these attributes with
three different classifiers: BoosTexter~\cite{Schapire:00},
Ripper~\cite{Cohen:95}, and SVMLight~\cite{Joachims:99}.
\citet{Wilson:04} achieved their best results (55\%~accuracy and
0.991~mean squared error) with the BoosTexter approach when using all
the introduced features.

\done[inline]{\citet{Pang:04}}

\citet{Pang:04} proposed a two-stage procedure for classifying the
overall semantic orientation of movie reviews (positive vs. negative):
In the first step of this approach, they classified the sentences into
subjective and objective ones (achieving 92\% accuracy for this task
with the Na\"ive Bayes system) and then applied another Na\"ive Bayes
classifier to determine the polarity of the review based on only
subjective excerpts, getting a statistically significant improvement
from~82.8 to~86.4\%.  The authors also experimented with the min-cut
method to encode the idea that nearby sentence will likely share the
same subjectivity class, but did not get any statistically significant
improvement over the discourse-unaware baseline.

\done[inline]{\citet{Cui:06}}

\citet{Cui:06} questioned the claim of~\citet{Pang:02} that
higher-order $n$-grams worsen the classification results of an opinion
mining system, hypothesizing that this finding was mostly due to the
limited training set size of the latter authors.  To check this
hypothesis, \citeauthor{Cui:06} collected a set of 320K product
reviews from \url{froogle.google.com} and trained three different NL
classifiers on these data: an online passive-agreessive
system~\cite{Crammer:06}, the winnow algorithm~\cite{Hurst:04}, and a
generative language model~\cite{Manning:99}.  The authors achieved
their best results with the first option, getting 0.9007 overall
\F-scores for both polarity classes and observing consistent
improvements with the increasing order of $n$-grams.  Moreover,
\citeauthor{Cui:06} also refuted another argument
of~\citet{Pang:04}---namely, the need for skipping objective sentences
while doing polarity classification---conjecturing that this step was
only useful for certain domains such as movie or book reviews, but
could not generalize to more technical genres.

\done[inline]{\citet{Ng:06}}

The results of~\citet{Pang:04} were further improved by~\citet{Ng:06}
who simultaneously addressed two classification problems:
distinguishing whether a given passage was a review or not and
determining the polarity of a review text.  The authors attained
impressive results (99.8\% accuracy) for the former task, using only
an SVM classifier with unigram features.  Moreover, they also
outperformed the then state of the art on the \citet{Pang:04}'s
corpus, boosting the accuracy from~87.1 to~90.5\%.  These changes were
mostly due to a smarter use of higher-order $n$-grams, where, instead
of bluntly considering all token sequences up to the order $n$ as new
features, \citet{Ng:06} only took 5,000 most useful ones, measuring
their utility with the weightes log-likelihood ratio \cite{Nigam:00}.

\done[inline]{\citet{Pang:05}}

\citet{Pang:05} addressed the problem of multi-class rating
prediction, whose goal, in contrast to the normal positive
vs. negative classification, was to determine the number of stars that
an author might assign to his review.  In their experiments,
\citeauthor{Pang:05} tried out three different strategies:
\begin{inparaenum}[(i)]
  \item one-versus-all SVM (OVA-SVM),
  \item SVM regression,
  \item and OVA-SVM with \emph{metric labeling}.
\end{inparaenum}
In the last approach, in addition to maximizing the score of the
correct labels, the authors also explicitly encoded the objective of
minimizing the absolute difference between the predicted labels of
similar training examples (measuring this similarity with the
percentage of positive sentences).  This strategy brought
statistically significant improvements over the first two baselines,
yielding an average accuracy of $\approx$52\%.

\todo[inline]{\citet{Wiebe:05a}}

This paper presents the results of developing subjectivity classifiers
using only unannotated texts for training. The performance rivals that
of previous supervised learning approaches. In addition, we advance
the state of the art in objective sentence classification by learning
extraction patterns associated with objectivity and creating objective
classifiers that achieve substantially higher recall than previous
work with comparable precision.

The goal of our research is to develop learning methods to create
classifiers that can distinguish subjective from objective
sentences. We strive to develop systems that excel at subjective
classification as well as objective classification.

The texts used in our experiments are English language versions of
articles from the world press. The data is from a variety of countries
and publications and covers many different topics (it was obtained
from the Foreign Broadcast Infor- mation Service (FBIS), a
U.S. goverment agency). 535 texts from this collection have been
manually annotated with respect to subjectivity as part of a U.S.
government funded program on automatic question answering. 3 These
manually annotated texts comprise the Multi-Perspective Question
Answering (MPQA) corpus and are freely available at
nrrc.mitre.org/NRRC/publications.htm.

The test set used in our evaluations consists of 9,289 of the
sentences in the MPQA corpus. None of this test data was used to
produce any of the features included in our experiments. 5104 of the
sentences in the test set (54.9\% of the data) are subjective
according to the definitions given below. Thus, the accuracy of a
baseline classifier that chooses the most frequent class is 54.9\%.
Our unannotated text corpus consists of 298,809 sentences from the
world press collection, and is distinct from the annotated MPQA
corpus.

As a starting point for our research, we reimplemented the high
precision, low re- call subjective and objective classifiers that we
previously developed [4]. We will refer to these as the rule-based
classifiers because they do not involve learning but merely classify
sentences by looking for well-established general subjectivity clues
that have been previously published in the literature. 4 Some are
drawn from manually developed resources, including entries from [6,
7], Framenet lem- mas with frame element experiencer [8], and
adjectives manually annotated for polarity [9]. Some were learned from
corpora, including words distributionally similar to subjective seed
words [10], n-grams [11, 12], and subjective nouns learned using
extraction pattern (EP ) bootstrapping [5]. The clues were divided
into strong and weak subjective clues, where strong subjective clues
have subjec- tive meanings with high probability, and weak subjective
clues have subjective meanings with lower probability.

The rule-based subjective classifier classifies a sentence as
subjective if it contains two or more strong subjective clues
(otherwise, it does not label the sentence). In contrast, the
rule-based objective classifier looks for the absence of clues: it
classifies a sentence as objective if there are no strong subjective
clues in the current sentence, there is at most one strong subjective
clue in the previous and next sentence combined, and at most 2 weak
subjective clues in the current, previous, and next sentence combined
(otherwise, it does not label the sentence).

Our research uses these rule-based classifiers to generate training
data for subsequent learning algorithms, which we will describe in the
coming sections.  Figure 1 shows the first stage of the training data
creation process. The rule- based subjective classifier is applied to
the unlabeled corpus to identify sentences that it can label as
subjective. Similarly, the rule-based objective classifier iden-
tifies sentences that it can label as objective. These subjective and
objective sentences form our initial training set.

On the annotated test set, the rule-based subjective classifier
achieved 34.2\% subjective recall and 90.4\% subjective precision. The
rule-based objective clas- sifier achieved 30.7\% objective recall and
82.4\% objective precision. Based on these results, we expect that the
initial training set generated by these classi- fiers is of relatively
high quality. Of the 298,809 sentences in the unannotated text corpus,
the rule-based classifiers labeled 52,918 sentences as subjective and
47,528 as objective, creating a training set of over 100,000
sentences.

Consequently, we also decided to explore the idea of learning
extraction pat- terns that are correlated with objectivity and then
using them as features in a machine learning algorithm. To learn
extraction patterns, we used the AutoSlog- TS [13] algorithm because
it does not need annotated texts for training. Instead, AutoSlog-TS
requires one set of ``relevant'' texts and one set of ``irrelevant''
texts.  Extraction patterns are created by applying a set of syntactic
templates to the corpus. The syntactic constructions recognized by
AutoSlog-TS are described in [13] and reflect syntactic relationships
identified by a shallow parser.

Next, we incorporated the learned EPs into the rule-based classifiers
as follows. The subjective patterns were added to the set of strong
subjective clues, which are used by both the subjective and objective
rule-based classifiers. The strategy used by the rule-based subjective
classifier remained the same. However, the strategy used by the
rule-based objective classifier was augmented as follows: in addition
to its previous rules, a sentence is also labeled as objective if it
contains no strong subjective clues but at least one objective
EP. Note that adding the subjective EPs to the set of strong
subjective clues works to decrease the recall of the objective
classifier because it looks for the absence of subjectivity
clues. To balance that effect, the additional test for objective EPs
can serve to increase the recall of the objective classifier.

The initial training data used by the naive Bayes classifier was
generated by the rule-based classifiers, which simply look for the
presence or absence of a set of general subjectivity clues. There are
obvious concerns associated with this type of automatically created
training data, such as potential biases introduced by the rules. A
related concern is that the training sentences will be similar to one
another and less heterogenous than the set of sentences that the
classifier will ultimately be applied to.

We therefore saw an opportunity to try to improve the classifier by
gen- erating a new training set using the classifier itself. The naive
Bayes classifier uses a greater variety of features than the
rule-based classifiers and it exploits a probabilistic model to make
classification decisions based on combinations of these features. We
hypothesized that the naive Bayes classifier might be able to reliably
label a different, and perhaps more diverse, set of sentences in the
unlabeled corpus than the rule-based classifiers did.

The recall of the learned patterns improved substantially using the
new train- ing set, with just a minor drop in precision: subjective
precision of the subjective patterns decreased from 74.5\% to 73.1\%,
and objective precision of the objective patterns decreased from
71.3\% to 68.9\%, while subjective recall of the subjec- tive patterns
increased from 59.8\% to 66.2\% and objective recall of the objective
patterns increased from 11.7\% to 17.0\%.

\todo[inline]{\citet{Riloff:06}}

Riloff, Patwardhan and Wiebe (2006) studied relationships among
different features. They defined subsumption relationships among
unigrams, n-grams and lexico-syntactic patterns. If a feature is
subsumed by another, the subsumed feature is not needed. This can
remove many redundant features.

We analyze two feature representations that have been used for opinion
analysis: Ngrams and Extraction Patterns. Information extraction (IE)
patterns are lexico-syntactic patterns that represent expressions
which identify role relationships.  For example, the pattern ``<subj>
ActVP(recommended)'' extracts the subject of active-voice instances of
the verb ``recommended'' as the recommender.  The pattern ``<subj>
PassVP(recommended)'' extracts the subject of passive-voice instances
of ``recommended'' as the object being recommended.

Our goal is to use the subsumption hierarchy to identify Ngram and
extraction pattern features that are more strongly associated with
opinions than simpler features. We used three types of features in our
research: unigrams, bigrams, and IE patterns.

We created a subsumption hierarchy that defines the representational
scope of different types of features. We will say that feature A
representationally subsumes feature B if the set of text spans that
match feature A is a superset of the set of text spans that match
feature B. For example, the unigram ``happy'' subsumes the bigram
``very happy'' because the set of text spans that match ``happy''
includes the text spans that match ``very happy''.

To estimate the quality of a feature, we use Information Gain (IG)
because that has been shown to work well as a metric for feature
selection (Forman, 2003). We will say that feature A behaviorally
subsumes feature B if two criteria are met: (1) A representationally
subsumes B, and (2) IG(A) $\geq$ IG(B) $\delta$, where $\delta$ is a
parameter representing an acceptable margin of performance dif-
ference. For example, if $\delta$=0 then condition (2) means that
feature A is just as valuable as feature B because its information
gain is the same or higher. If $\delta$>0 then feature A is allowed to
be a little worse than feature B, but within an acceptable
margin. For example, $\delta$=.0001 means that A's information gain
may be up to .0001 lower than B's information gain, and that is
considered to be an acceptable performance difference (i.e., A is good
enough that we are comfortable discarding B in favor of the more
general feature A).

Note that based on the subsumption hierarchy shown in Figure 2, all
1Grams will always survive the subsumption process because they
cannot be subsumed by any other types of features. Our goal is to
identify complex features that are worth adding to a set of unigram
features.

We used three opinion-related data sets for our analyses and
experiments: the OP data set created by (Wiebe et al., 2004), the
Polarity data set 5 created by (Pang and Lee, 2004), and the MPQA data
set created by (Wiebe et al., 2005).

Polarity data sets involve document-level opinion classification,
while the MPQA data set involves sentence-level classification.

The OP data consists of 2,452 documents from the Penn Treebank (Marcus
et al., 1993). Metadata tags assigned by the Wall Street Journal
define the opinion/non-opinion classes: the class of any document
labeled Editorial, Letter to the Editor, Arts \& Leisure Review, or
Viewpoint by the Wall Street Journal is opinion, and the class of
documents in all other categories (such as Business and News) is
non-opinion. This data set is highly skewed, with only 9\% of the
documents belonging to the opinion class. Consequently, a trivial (but
useless) opinion classifier that labels all documents as nonopinion
articles would achieve 91\% accuracy.  The Polarity data consists of
700 positive and 700 negative reviews from the Internet Movie Database
(IMDb) archive. The positive and negative classes were derived from
author ratings expressed in stars or numerical values. The MPQA data
consists of English language versions of articles from the world
press. It contains 9,732 sentences that have been manually annotated
for subjective expressions. The opinion/non-opinion classes are
derived from the lower-level annotations: a sentence is an opinion
if it contains a subjective expression of medium or higher
intensity; otherwise, it is a non-opinion sentence. 55\% of the
sentences belong to the opinion class.

When creating opinion classifiers, people often throw in a variety of
features and trust the machine learning algorithm to figure out how
to make the best use of them. However, we hypothesized that
classifiers may perform better if we can proactively eliminate
features that are not necesary because they are subsumed by other
features. In this section, we present a series of experiments to ex-
plore this hypothesis. First, we present the results for an SVM
classifier trained using different sets of unigram, bigram, and
extraction pattern features, both before and after
subsumption. Next, we evaluate a standard feature selection approach
as an alternative to subsumption and then show that combining
subsumption with standard feature selection produces the best
results of all.

To see whether feature subsumption can improve classification
performance, we trained an SVM classifier for each of the three
opinion data sets.  We used the SVM light (Joachims, 1998) package
with a linear kernel. For the Polarity and OP data we discarded all
features that have frequency < 5, and for the MPQA data we discarded
features that have frequency < 2 because this data set is sub-
stantially smaller. All of our experimental results are averages over
3-fold cross-validation.  First, we created 4 baseline classifiers: a
1Gram classifier that uses only the unigram features; a 1+2Gram
classifier that uses unigram and bigram features; a 1+EP classifier
that uses unigram and extraction pattern features, and a 1+2+EP
classifier that uses all three types of features. Next, we created
analogous 1+2Gram, 1+EP, and 1+2+EP classifiers but applied the
subsumption hierarchy first to eliminate unnecessary features be-
fore training the classifier. We experimented with three delta values
for the subsumption process: $\delta$=.0005, .001, and .002.

Figures 7, 8, and 9 show the results. The subsumption process
produced small but consistent improvements on all 3 data sets. For
example, Figure 8 shows the results on the OP data, where all of the
accuracy values produced after subsumption (the rightmost 3 columns)
are higher than the accuracy values produced without subsumption
(the Base[line] column). For all three data sets, the best overall
accuracy (shown in boldface) was always achieved after subsumption.

On all 3 data sets, traditional feature selection performs worse than
the baseline in some cases, and it virtually never outperforms the
best classifier trained after subsumption (but without feature
selection). Furthermore, the combination of subsumption plus feature
selection generally performs best of all, and nearly always
outperforms feature selection alone. For all 3 data sets, our best ac-
curacy results were achieved by performing subsumption prior to
feature selection. The best accuracy results are 99.0\% on the OP
data, 83.1\% on the Polarity data, and 75.4\% on the MPQA data.  For
the OP data, the improvement over baseline for both accuracy and
F-measure are statistically significant at the p < 0.05 level (paired
t-test).

\done[inline]{\citet{Bickerstaffe:10}}

\citet{Bickerstaffe:10} also considered the rating prediction task,
addressing this problem with the minimum-spanning-tree (MST) SVM
approach.  In the initial step of this method, they constructed a
strongly connected graph whose vertices were associated with the most
representative example (determined via the average all-pairs Tanimoto
coefficient) of each star rating and the edge weights represented the
Tanimoto distances between those nodes.  Afterwards, they determined
the MST of this graph using the Kruskal's
algorithm~\cite[see][pp.~567--574]{Cormen:09} and, finally,
constructed a decision tree from this MST, replacing the MST vertices
with binary SVM classifiers, which had to discern the respective
rating groups. An evaluation on the four-star review corpus
of~\citet{Pang:05} showed an improvement by up to~7\% over the
previous state of the art, boosting it to 59.37\% average accuracy.

\done[inline]{\citet{Li:10b}}

\citet{Li:10b} addressed the problem of polarity shifting using
machine-learning techniques.  For this purpose, the authors first
selected most frequent and indicative features of the two main
polarity classes (positive and negative), and then culled training
instances containing these attributes whose labels, however, were
different from the ones sugested by the features.  After obtaining
this polarity shifted subset, \citeauthor{Li:10b} trained several
linear support-vector classifiers, one of which had to distinguish
between polarity-shifted and polarity-preserving sentences, the other
two were to classify the semantic orientation of these two groups
(i.e., one system had to predict the polarity of shifted instances,
and the other one had to determine the semantic orientation of
polarity-preserving ones), and the last one was trained on the
complete original dataset---the product review corpus
of~\citet{Blitzer:06}---again to predict the polarity of complete
sentences disregarding their possible polarity shifts.  The authors
achieved their best results~(80,9\% average accuracy) using a
combination of the last three systems with a special meta-classifier
joining their single decisions.

\done[inline]{\citet{Mejova:11}}

\citet{Mejova:11} investigated the effect of different features on
various datasets---the movie review corpus of~\citet{Pang:04}, the
product reviews gathered by~\citet{Jindal:07}, and the customer
feedback dataset of~\citet{Blitzer:06}, coming to the conclusion that,
in general, preserving the original form of tokens (i.e., keeping the
original token forms instead of lemmas) and using their frequency
scores instead of binary values was beneficial to the results on all
test sets.  The use of different $n$-gram lengths, however, had a
mixed effect with the best scores typically yielded by the union of
uni-, bi-, and tri-gram features.  Last but not least, they found the
negation heuristics proposed by~\citet{Das:01} (adding the
\texttt{\_NOT} suffix to all tokens following a negation up to the
first punctuation) leading to only marginal improvements.  The authors
achieved their best results (87.5\%, 94.7\%, and 89.6\% accuracy on
the datasets of \citet{Pang:04}, \citet{Jindal:07}, and
\citet{Blitzer:06}, respectively) with the union of unnormalized uni-,
bi-gram and tri-gram features without negation when using term
frequencies as feature values.

\todo[inline]{SemEval}


\todo[inline]{}

To the best of our knowledge, the idea of utilizing web texts
containing emoticons as noisily labeled training data was first
proposed by~\citet{Read:05}, who collected a set of 26,000 Usenet
posts featuring smileys or frownies and used these documents to train
a Na{\"i}ve Bayes and SVM classifier.  The author demonstrated that,
despite some encouraging results obtained on the instances from the
same domain (up to 70\% accuracy), the trained systems did not
generalize well to other text genres, barely outperforming the chance
baseline and reaching a maximum accuracy of~54.4\% on news data and
56.8\% on movie reviews.

The presumably first known attempt to adopt distant supervision for
the sentiment analysis of Twitter data was made by~\citet{Go:09} who
collected a set of 800,000 positive and 800,000 negative microblogs
relying on emoticons as their noisy labels.  After stripping off these
smileys from text, the authors trained three independent
ML-classifiers (Na{\"i}ve Bayes, Maximum Entropy, and Support Vector
Machines) on this collection, achieving their best results (82.7\%
accuracy) with the NB and MaxEnt systems thaat utilized unigrams and
bigrams as features.

Another distantly supervised approach was presented
by~\citet{Barbosa:10}, who gathered a collection of automatically
labeled tweets from three popular sentiment web sites (Twendz, Twitter
Sentiment, and TweetFeel), and trained two binary SVM systems on this
corpus.  The first of these classifiers had to distinguish between
subjective and objective microblogs, attaining an error rate of~18.1\%
on a subset of 1,000 manually annotated messages.  In the next step,
the second system had to determine the semantic orientation of
opinionated posts (positive or negative), reaching an error rate
of~18.7\% on this prediction.

In a similar way, \citet{Pak:10} gathered a collection of 300,000
noisily labeled tweets, ensuring an even distribution of positive,
negative, and neutral messages.  After a brief exploration of PoS tag
statistics in these different classes, they presented a Na{\"i}ve
Bayes system which utilized highly relevant binary part-of-speech and
$n$-gram features.\footnote{\citet{Pak:10} determined the relevance of
  a feature $f$ using a special \emph{salience} metric, which was
  defined as a negative ratio between the minimum and maximum
  conditional probabilities of this feature belonging to different
  target classes:
  \begin{equation*}
    salience(f) = \frac{1}{N}\sum_{i=1}^{N-1}\sum_{j=i+1}^N 1 - \frac{\min(P(f, s_i), P(f, s_j))}{\max(P(f, s_i), P(f, s_j))},
  \end{equation*}
  where the $N$~term denotes the number of training examples, and
  $s_i$ means the sentiment class of the $i$-th training instance.}
With this approach, the authors attained an accuracy slighlty above
0.6 on the manually labeled test set of~\citet{Go:09}, also
demonstrating a particular utility of bigrams, negation rules, and
feature pruning heuristics.

A slightly different task was addressed by~\citet{Davidov:10}, who
sought to predict hashtags and emoticons occurring in tweets using a
$k$-NN classifier trained on a large collection of messages.  The
authors achieved an \F-measure of~0.31 on the former task, and reached
an \F-score of~0.64 on predicting smileys.

\citet{Kouloumpis:11} trained an AdaBoost
classifier~\cite{Schapire:00} on two large collections of noisily
labeled tweets---the emoticon tweebank of~\citet{Go:09} and the
Edinburgh hashtag corpus.\footnote{\url{http://demeter.inf.ed.ac.uk}}
Using $n$-gram (up to length two), lexicon, part-of-speech, and
micro-blogging features (such as emoticons, abbreviations, and slang
expressions), the authors achieved a macro-averaged \F-measure of~0.68
on the three-class prediction task.

% One of the first attempts to analyze message-level sentiments on
% Twitter was made by \citet{Go:09}.  For their experiments, the authors
% collected a set of 1,600,000 tweets containing smileys.  Based on
% these emoticons, they automatically derived polarity classes for these
% messages (positive or negative) and used them to train a Na\"{\i}ve
% Bayes, MaxEnt, and SVM classifier.  The best $F$-score for this
% two-class classification problem could be achieved by the last system
% and run up to 82.2\%.

% Similar work was also done by \citet{Pak:10} who used the Na\"{\i}ve
% Bayes approach to differentiate between neutral, positive, and
% negative microblogs; and \citet{Barbosa:10} who gathered a collection
% of 200,000 tweets, subsequently analyzing them with three publicly
% available sentiment web-services and training an SVM classifier on the
% results of these predictors.  In a similar way, \citet{Agarwal:11}
% compared a simple unigram-based SVM approach with two other
% full-fledged systems, one which relied on a rich set of manually
% defined features, and another used partial tree
% kernels~\cite{Moschitti:06}.  The authors evaluated these methods on a
% commercially acquired corpus of 8,753 foreign-language tweets, which
% were automatically translated into English, finding that a combination
% of these methods worked best for both two- and three-way prediction
% tasks.

% The state-of-the-art results for message level polarity prediction on
% tweets were established by~\citet{Mohammad:13}, whose system (a
% supervised SVM classifier) used a rich set of various features
% including word and character n-grams, PoS statistics, Brown
% clusters~\cite{Brown:92}, etc., and also strongly benefitted from
% automatic corpus-based polarity lists---Sentiment~140 and NRC
% Hashtag~\cite{Mohammad:12,Kiritchenko:14}.  This approach ranked first
% at the SemEval competition~2013~\cite{Nakov:13} and anchieved the
% fourth place on the rerun of this task one year
% later~\cite{Rosenthal:14}, being outperformed by the supervised
% logistic regression approach of~\citet{Miura:14}, who used a heavy
% preprocessing of the data and a special balancing scheme for
% underrepresented classes.  Later on, these results were further
% improved by the apporaches of~\citet{Hagen:15} and \citet{Deriu:16},
% which both relied on ensembles of multiple independent classifiers.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

       &  &  &  & %
       &  &  & %
       &  &  & %
       & \\\bottomrule
    \end{tabular}
    \egroup
    \caption[Evaluation of ML-based coarse-grained SA methods.]{
      Evaluation of ML-based coarse-grained SA methods.\\
      {\small }}
    \label{snt-cgsa:tbl:ml-res}
  \end{center}
\end{table}

\section{Deep-Learning Based Methods}\label{sec:cgsa:dl-based}

\todo[inline]{Lin, 1998}

Since there was not a complete set of such expressions, it provided
some seeds and then used distributional similarity (Lin, 1998) to find
similar words, which were also likely to be subjectivity
indicators. However, words found this way had low precision and high
recall.

\todo[inline]{\citet{Bespalov:11}}

In (Bespalov et al., 2011), sentiment classification was performed
based on supervised latent n-gram analysis.

\todo[inline]{\citet{Zhou:10}}

\citet{Yessenalina:11}

A real breakthrough in the use of deep neural networks for the
sentence-level sentiment analysis happened with the pioneering work
of~\citet{Socher:11}, who first introduced a recursive autoencoder
(RAE).  In this system, the authors obtained a fixed-width vector
representation for complex phrases $\vec{v}$ by recursively merging
the vectors of adjacent tokens (say $\vec{w}_1$ and $\vec{w}_2$),
first multiplying these vectors with a compositional matrix $W$ and
then applying a non-linear function ($softmax$) to the resulting
product:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}\right)
\end{align*}
Using a max-margin classifier on top of the resulting phrase
representation, \citet{Socher:11} could improve the state-of-the-art
results on predicting the sentence-level polarity of user's blog
posts~\cite{Potts:10} and also outperformed the system
of~\citet{Nasukawa:03} on the MPQA data set~\cite{Wiebe:05}.

Later on, \citet{Socher:12} further improved these scores with the
help of a recursive matrix-vectors space model (RMVSM), in which each
word was associated with a 2-tuple of a vector and matrix---e.g.,
$(\vec{w}_1, W_1)$ and $(\vec{w}_2, W_2)$---and the compositionality
function was redefined as follows:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  W_2\cdot\vec{w}_1\\
  W_1\cdot\vec{w}_2
  \end{bmatrix}\right)
\end{align*}

\citet{Wang:15}


\todo[inline]{}

An important breakthrough in the usage of deep learning methods for
sentiment analysis on Twitter happened with the
work~\citet{Severyn:15}, whose proposed feed-forward DL system ranked
first in Subtask~10-A (phrase-level polarity prediction) at
SemEval~2015 \cite{Rosenthal:15} and got second place in Subtask~10-B
(message-level classification) of this competition.  Drawing on the
ideas of~\citet{Kalchbrenner:14}, the authors devised a simple
convolutional neural network in which they first passed pre-trained
word2vec embeddings to a set of 300 convolutional kernels (each of
width five), and then fed the max-pooled results of this
multiplication to a densely connected softmax layer.  Two important
aspects of this approach, which accounted for a huge part of its
success, were
\begin{inparaenum}[(i)]
\item that the word2vec vectors were used solely for initialization,
  but were then modified along with other model's paramaters during
  training, and
\item that \citeauthor{Severyn:15} extended to the original SemEval
  training set with an additional, much larger noisily labeled corpus.
\end{inparaenum}
These improvements allowed the authors to establish a new state of the
art on the phrase-level polarity prediction task with a macro-averaged
\F-score of 0.8479, also getting a competitive result of 0.6459~\F{}
on classifying the semantic orientation of messages.

\todo[inline]{}

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

       &  &  &  & %
       &  &  & %
       &  &  & %
       & \\\bottomrule
    \end{tabular}
    \egroup
    \caption[Evaluation of DL-based coarse-grained SA methods.]{
      Evaluation of DL-based coarse-grained SA methods.\\
      {\small }}
    \label{snt-cgsa:tbl:ml-res}
  \end{center}
\end{table}

\section{Coarse-Grained Sentiment Analysis Using Language and Domain
  Adaptation}\label{sec:cgsa:domain-adaptation}

One of the first works which pointed out the importance of domain
adaptation for sentiment analysis was introduced by~\citet{Aue:05}.
In their experiments, the authors trained separate SVM classifiers on
four different document sets: movie reviews, book reviews, customer
feedback from a product support service, and a feedback survey from a
customer knowledge base; finding that each classifier performed best
when applied to the same domain as it was trained on.  In order to
find an optimal way of overcoming this domain specificity,
\citet{Aue:05} tried out four different options:
\begin{inparaenum}[(i)]
\item\label{sent-cgsa:lst:rel-wrk1} training one classifier on all but
  the target domain and applying it to the latter;
\item using the same procedure as above, but limiting the features to
  only those which also appeared in the target texts;
\item taking an ensemble of individual classifiers each of which was
  trained on a different data collection; and, finally,
\item using a minimal subset of labeled in-domain data to train a
  Na{\"i}ve Bayes system with the expectation-maximization algorithm
  \cite[EM;][]{Dempster:77}.
\end{inparaenum}
The authors found that the ensemble and EM options worked best for
their cross-domain task, achieving an accuracy of up to 82.39\% for
the two-class prediction (positive vs negative) on new unseen text
genres.

Another notable milestone in the domain adaptation research was set
by~\citet{Blitzer:07}.  Relying on their previous work on structural
correspondence learning~\cite{Blitzer:07}, in which they used a set of
\emph{pivot features} (features which frequently appeared in both
target and source domains) to find an optimal correspondence of the
remaining attributes,\footnote{In particular, the authors trained $m$
  binary predictors for each of their $m$ pivot features in order to
  find other attributes which frequently co-occurred with the pivots.
  Afterwards, they composed these $m$ resulting weight vectors into a
  single matrix $W := [\vec{w}_{1},\ldots,\vec{w}_{m}]$, took an SVD
  decomposition of this matrix, and used the top $h$ left singular
  vectors to translate source features to the new domain.} the authors
refined their method by pre-selecting the pivots using their PMI
scores and improving misaligned feature projections using a small set
of labeled target examples.  With these modifications,
\citeauthor{Blitzer:07} were able to reduce the average adaptation
loss (the accuracy drop when transferring a classifier to a different
domain) from 9.1 to 4.9~percent when testing a sentiment predictor on
the domains of book, dvd, electical appliances, and kitchen reviews.

Other important works on domain adaptation for opinion mining include
those of~\citet{Read:05}, who pointed out that sentiment
classification might not only depend on the domain but also on topic,
time, and language style in which the text was written;
\citet{Tan:07}, who proposed using the classifier trained on the
source domain to classify unlabeled instances from the target genre,
and then iteratively retrain the system on the enriched data set.
Finally, \citet{Andreevskaia:08} proposed a combination of a lexicon-
and ML-based systems, claiming that this ensemble would be more
resistible to the domain shift than each of these classifiers on their
own.

Another line of research was introduced by~\citet{Glorot:11} who
proposed stacked denoising autoencoders (SDA)---a neural network
architecture in which an input vector $\vec{x}$ was first mapped to a
smaller representation $\vec{x}'$ via some function
$h: \vec{x}\mapsto\vec{x}'$, and then restored to its approximate
original state via an inverse transformation
$g: \vec{x}'\mapsto\vec{x}''\approx\vec{x}$.  In their experiments,
the authors optimized the parameters of the functions $h$ and $g$ on
both target and source data, getting approximate representations of
instances from both data sets; and then trained a linear SVM
classifier on the restored representations of the source instances,
subsequently applying this classifier to the target domain.  This
approach was further refined by~\citet{Chen:12} who analytically
computed the reconstruction function~$g$, and used both original and
restored features to predict the polarity labels of the target
data.\footnote{Both approaches were trained tested on the Amazon
  Review Corpus of~\citet{Blitzer:07}.}


Further notable contributions to domain adaptation in general were
made by~\citet{Daume:07} who proposed to replicate each extracted
feature three times and train the first replication on both domains,
the second repetion only on source, and the third copy only on target
domain, for which he assumed a small subset of labeled examples was
available; \citet{Yang:15} who trained neural embeddings of features,
trying to predict which instance attributes frequently co-occured with
each other;

\section{Evaluation}
\subsection{Effect of Lexicons}
\subsection{Effect of Distant Supervision}
\subsection{Effect of Word Embeddings}
\subsection{Effect of Normalization}

\section{Summary and Conclusions}\label{slsa:subsec:conclusions}
