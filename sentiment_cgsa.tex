\chapter{Coarse-Grained Sentiment Analysis}\label{sec:snt:cgsa}

\section{Data}\label{sec:cgsa:data}

\section{Data Set}\label{sec:cgsa:eval-metrics}

\section{Coarse-Grained Sentiment Analysis with Lexicon-Based Methods}

% Das and Chen, 2001; Turney, 2002;  Kim and Hovy, 2004
The usage of lexicon-based approaches for estimating semantic
orientation of complex text chunks dates back to the very origins of
the sentiment analysis research in general: For example,
\citet{Das:01} already used two purely lexicon-based and three
machine-learning methods, which heavily relied on lexicon features, in
an ensemble of five classifiers to predict the polarity of stock
messages (buy, sell, or neutral), attaining an accuracy of 62\% on a
corpus of several hundreds stock board messages.  \citet{Turney:02},
while proposing his PMI method for automatically generating sentiment
lexicons, also suggested computing the orientation of the whole review
by averaging the sentiment scores of its terms found in the lexicon.
With this approach, the author reached an accuracy of 74\% on a corpus
of 410 manually labeled Epinions comments.  Finally, \citet{Kim:04}
compared three different methods of determining the overall polarity
of a sentence:
\begin{inparaenum}[(i)]
  \item by multiplying the signs of polar terms found in sentence,
  \item taking their sum, and
  \item taking the geometric mean of polarity scores;
\end{inparaenum}
finding that the first and last options worked best for the Document
Undestanding Corpus.\footnote{\url{http://duc.nist.gov/}}

% Hatzivassiloglou and Wiebe, 2000
\citet{Hatzivassi:00} proved, for instance, via statistical
significance tests that the mere presence of a subjective or even just
gradable adjective from a lexicon was a highly reliable indicator that
the sentence it appeared in also was subjective.

% Hu and Liu, 2004
Similarly, \citet{Hu:04} determined the semantic orientation of
sentences in customer reviews by simply comparing the number of
positive and negative terms found in these passages. Since the
authors, however, were primarily interested in estimating the polarity
towards particular product features mentioned in the clauses, they
additionally applied a fallback strategy in case of a tie by checking
which of the polar lexicon terms appeared closer to the features, and
assuming the polarity of the preceding sentence if these numbers were
also equal.

% Taboada et al., 2004
Largely inspired by the Appraisal theory of~\citet{Martin:00},
\citet{Taboada:04} enhanced the original method of~\citet{Turney:02}
by increasing the weights of polar adjectives which occurred in the
middle and at the end of a document, and also augmenting these values
with the affect, judgement, and appreciation scores.  Similarly to
polarity, the appraisal scores were calculated automatically by
computing the PMI of their cooccurrence with different pronouns using
a web search engine.

% Polanyi and Zaenen, 2006
An early analysis of lexicon-based approaches to sentiment analysis
was made by~\citet{Polanyi:06}, who argued that, besides considering
the lexical valence (i.e., semantic orientation) of polar terms, it
was also necessary to incorporate syntactic, discourse-level, and
extra-linguistic factors such as negations, intensifiers, modal
operators (e.g., \emph{could} or \emph{might}), presuppositional items
(e.g., \emph{barely} or \emph{failure}), irony, reported speech,
discourse connectors and structure, genre and attitude assessment,
reported speech, multi-entity evaluation, and so on.

% Kennedy and Inkpen, 2006
This theoretical hypothesis was later proven empirically by
\citet{Kennedy:06}, who compared two lexicon-based approaches:
\begin{inparaenum}[(i)]
\item simply counting positive and negative expressions, and assigning
  the input text (a review) to the polarity class with the greater
  number of found terms (considering a review as neutral if it had an
  equal number of expressions with either polarities); and
\item the same procedure as above, but additionally enhanced with the
  information about contextual valence shifters~(intensifiers,
  downtoners, and negations).
\end{inparaenum}
In the latter method, the authors swapped the polarity of a polar
expression to the opposite if it followed a negation, and decreased or
increased the score of this term by a constant factor if it was
preceded by a diminisher or intensifier.  This enhancement was shown
to bring a statistically significant improvement, boosting the
accuracy of the two-class prediction on a corpus of product and movie
reviewa from 67.9 to 69.3\%.

% Taboada et al., 2006
Another important contribution to the development of lexicon-based
approaches was made by~\citet{Taboada:06}, who compared three popular
polarity lists---a PMI lexicon computed with the original method
of~\citet{Turney:02} using the AltaVista's NEAR operator; a similar
polarity list obtained with the help of Google's AND queries; and,
finally, the manually compiled General Inquirer lexicon
of~\citet{Stone:66}.  The authors evaluated these resources both
intrinsically (by comparing them with GI entries) and extrinsically
(by computing the polarity of 400 manually annotated Epinions
reviews).  To estimate the overall polarity of a review for the second
task, \citeauthor{Taboada:06} calculated the average SO value of all
polar terms found in the review, obtaining these scores from the
mean-normalized lexicons, and flipping the polarity sign to the
opposite in case of the negation.

% Taboada et al., 2011
Finally, a veritably seminal work on lexicon-based approaches was
introduced by~\citet{Taboada:11} who presented a manually compiled
sentiment lexicon,\footnote{The authors hand-annotated all occurrences
  of adjectives, nouns, and verbs found in a corpus of 400 Epinions
  reviews with ordinal categories ranging from -5 to 5 which reflected
  the semantic orientation of a term (positive vs. negative) and its
  polar strength (weak vs. strong).} and used this resource to compute
the overall \emph{semantic orientation} (SO) of text.  Inspired by the
ideas of~\citet{Polanyi:06}, to make this calculation more precise,
the authors also incorporated a set of additional heuristic rules by
changing the prior SO values of negated, itensified, and downtoned
terms, ignoring irrealis sentences, and adjusting weights of specific
sections of a document.  An extensive evaluation of this method showed
a superior performance of the manual lexicon in comparison with other
polarity lists including the Subjectivity Dictionary~\cite{Wilson:05},
Maryland Polarity Set~\cite{Mohammad:09}, and
\textsc{SentiWordNet}~\cite{Esuli:06c}.  Moreover, the authors also
proved the robustness of their SO computation procedure for other
topics and text genres, hypothesizing that lexion-based approaches
were more domain-independent than traditional supervised learning
techniques.

% Musto et al., 2014
A lexicon-based system created specifically for Twitter data was
presented by~\citet{Musto:14}, who examined four different strategies
for computing the overall polarity scores of tweets---\emph{basic},
\emph{normalized}, \emph{emphasized}, and
\emph{normalized-emphasized}; checking these approaches with four
distinct lexicons---\textsc{SentiWordNet}~\cite{Esuli:06c},
\textsc{WordNet-Affect}~\cite{Strapparava:04},
\textsc{MPQA}~\cite{Wiebe:05}, and
\textsc{SenticNet}~\cite{Cambria:14}.  In all of these strategies, the
authors first split an input messages into \emph{micro-phrases} based
on the occurrence of punctuation marks and conjunctions; then
calculated the polarity score of each of these segments by summing
(\emph{basic} and \emph{emphasized}) or averaging (\emph{normalized}
and \emph{normalized-emphasized}) the lexicon scores of their tokens;
and, finally, estimated the overall polarity of the whole tweet by
summing or averaging the scores of the micro-phrases.
\citeauthor{Musto:14} obtained their best results (58.99\% accuracy on
the SemEval-2013 dataset) with the \textsc{SentiWordNet} lexicon
of~\citet{Esuli:06c}, using the normalized-emphasized approach, in
which they averaged the polarity scores of segment tokens, boosting
these values by 50\% for informative parts of speech; and regarded the
sum of the segements' scores as the final overall polarity of the
microblog.

% Jurek et al., 2015
Another Twitter-tailored approach was proposed by~\citet{Jurek:15}.
Drawing on the ideas of~\citet{Taboada:11}, the authors introduced a
new formula for estimating the polarity of a message, in which they
explicitly encoded the presumably most important sentiment factors:
polar terms, intensifiers, and negations.  In particular, given a
message~$m$, \citet{Jurek:15} calculated the postive and negative
scores ($F_p$ and $F_n$ respectively) of this microblog using the
following equation:%
{ \small%
  \begin{align}
    F_P &= \min\left(\frac{A_P}{2 - \log(3.5\times W_P + I_P)}, 100\right),\\
    F_N &= \max\left(\frac{A_N}{2 - \log(3.5\times W_N + I_N)}, -100\right);\label{cgsa:eq:jurek}
  \end{align}%
  \normalsize%
}%
where $A_P$ and $A_N$ represent the average scores of positive and
negative lexicon terms found in the tweet, $W_P$ and $W_N$ stand for
the raw counts of these items, and $I_P$ and $I_N$ denote the number
of intensifiers preceding the lexicon terms in the context.  In
addition to that, in case of negation, the authors modified the
individual polarity score $S_w$ of negated word $w$ as follows: {
  \small%
  \begin{equation*}
neg(S_w) =
    \begin{cases}
        \min\left(\frac{S_w - 100}{2}, -10\right) & \text{if } S_w > 0,\\
        \max\left(\frac{S_w + 100}{2}, 10\right), & \text{if } S_w < 0.
    \end{cases}
\end{equation*}%
\normalsize%
}%
Besides estimating the polarity scores $F_p$ and $F_n$,
\citeauthor{Jurek:15} also computed the degree of subjectivity for the
message by replacing the $A_P$ and $A_N$~terms in
Equations~\ref{cgsa:eq:jurek} with averaged conditional probabilitites
of polar terms.  These probabilities were calculated automatically on
the noisily labeled data set of~\citet{Go:09} and reflected how likely
a message was subjective given that a specific lexicon term appeared
in its text.  In the final step, the authors assigned the message to
the most probable polarity class if its overall polarity score was
greater than 25 and the subjectivity value was more than 0.5,
considering the tweet as neutral otherwise.  With this approach, the
authors achieved an accuracy of~77.3\% on the manually annotated
subset of \citeauthor{Go:09}'s corpus, and reached 74.2\% on the IMDB
corpus~\cite{Maas:11}.

% Kolchyna et al., 2015
Finally, \citet{Kolchyna:15} compared lexicon- and ML-based systems,
evaluating these approaches on the SemEval-2013 data
set~\cite{Nakov:13}.  For the former group of methods, the authors
explored two different ways of estimating the overall polarity of a
microblog:
\begin{inparaenum}[(i)]
\item by simply averaging the scores of the lexicon terms found in the
  message, and
\item by taking the signed logarithm of this average:
\end{inparaenum}
\begin{equation*}
  \text{Score}_{\log} =
  \begin{cases}
    \text{sign}(\text{Score}_{\text{AVG}})\log_{10}(|\text{Score}_{\text{AVG}}|) & %
    \text{if |Score}_{\text{AVG}}| > 0.1,\\
    0, & \text{otherwise}.
  \end{cases}
\end{equation*}%
To estimate the final label of the tweet, \citeauthor{Kolchyna:15}
trained a $k$-means clustering algorithm, which utilized either of the
above polarity scores as its features.  They showed that the
logarithmic approach performed better than the simple average
solution, yielding an accuracy of 61.74\% when used with a manually
compiled lexicon enriched with colloquial and slang terms.  For
ML-based methods, the authors proposed using cost-sensitive
SVMs~\cite{Masnadi:12}, which allowed them to improve on the results
of the top-performing system from the SemEval
competition~\cite{Mohammad:13} by 4\%, reaching 0.71~macro-averaged
\F{} on the two polarity classes (positive and negative).

\todo[inline]{re-implement: Taboada (2011), Musto et al. (2014), Jurek
  et al. (2015), Kolchyna (2015)}

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

       &  &  &  & %
       &  &  & %
       &  &  & %
       & \\\bottomrule
    \end{tabular}
    \egroup
    \caption[Evaluation of lexicon-based coarse-grained SA methods.]{
      Evaluation of lexicon-based coarse-grained SA methods.\\
      {\small }}
    \label{snt-cgsa:tbl:lex-res}
  \end{center}
\end{table}

\section{Coarse-Grained Sentiment Analysis with ML-Based Methods}

One of the first works on an automatic sentiment classification with
ML-based methods was presented by~\citet{Wiebe:99}, who trained a
Na{\"i}ve Bayes system to differentiate between subjective and
objective statements, relying primarily on binary features which
reflected the presence of a pronoun, an adjective, a cardinal number,
or a modal other than ``will'' in the analyzed sentence.  The authors
achieved an accuracy of~72.17\%, outperforming the majority class
baseline by more than 20~percentage points.  An even better result
(81.5\%) could be reached when the data set was restricted only to the
examples with the most confident annotations.

A further step in this direction was taken by~\citet{Pang:02}, who
compared Na{\"i}ve Bayes, Maximum Entropy, and SVM approaches on the
polarity classification task for movie reviews, getting their best
results (82.9\% accuracy) with the SVM system that used only unigram
features.

From Barbosa 2010: A variety of features have been exploited on the
problem of sentiment detection (Pang and Lee, 2004; Pang et al., 2002;
Wiebe et al., 1999; Wiebe and Riloff, 2005; Riloff et al., 2006).

ReviewSA: this is the approach proposed by Pang and Lee (Pang and Lee,
2004) for sentiment analysis in regular online re- views. It performs
the subjectivity detec- tion on a sentence-level relying on the prox-
imity between sentences to detect subjectiv- ity. The set of sentences
predicted as subjec- tive is then classified as negative or positive
in terms of polarity using the unigrams that compose the sentences. We
used the imple- mentation provided by LingPipe (LingPipe, 2008);

Unigrams: Pang et al. (Pang et al., 2002) showed unigrams are
effective for sentiment detection in regular reviews. Based on that,
we built unigram-based classifiers for the subjectivity and polarity
detections over the training data. Another approach that uses un-
igrams is the one used by TwitterSentiment website. For polarity
detection, they select the positive examples for the training data
from the tweets containing good emoticons and negative examples from
tweets contain- ing bad emoticons. (Go et al., 2009). We built a
polarity classifier using this approach (Unigrams-TS)



Wiebe 2002, Riloff 2003

Wiebe, Bruce, \& O'Hara 1999
Hatzivassiloglou \& Wiebe 2000
Wiebe 2000;
Wiebe et al. 2002
Yu \& Hatzivassiloglou 2003

Bruce and Wiebe (1999) annotated 1,001 sentences as sub- jective or
objective, and Wiebe et al. (1999) de- scribed a sentence-level Naive
Bayes classifier using as features the presence or absence of
particular syn- tactic classes (pronouns, adjectives, cardinal num-
bers, modal verbs, adverbs), punctuation, and sen- tence position.

More recently, Wiebe et al.  (2002) report on document-level
subjectivity classi- fication, using a k-nearest neighbor algorithm
based on the total count of subjective words and phrases within each
document.

A semi-supervised classification approach was proposed
by~\citet{Yu:03}, who presented a three-stage method, in which they
first distinguished between subjective and objective documents, then
differentiated between polar and neutral sentences, and, finally,
classified the polarity of opinionated clauses.  The authors used a
Na{\"i}ve Bayes classifer for the document-level task, reaching a
remarkable \F-score of~0.96 on this objective; and applied an ensemble
of NB systems to predict the subjectivity of the sentences.  In the
final step, they determined the semantic orientation of subjective
clauses by averaging the polarity scores of their tokens, getting
these scores from an automatically constructed sentiment
lexicon~\cite{Hatzivassi:97}.  With this approach, \citeauthor{Yu:03}
attained an accuracy of~91\% on a set of 38 sentences which had a
perfect inter-annotator agreement in their data.

\todo[inline]{\citet{Read:05}}

\todo[inline]{\citet{Go:09}}

\todo[inline]{\citet{Barbosa:10}}

In this paper, we propose an approach to automatically detect
sentiments on Twit- ter messages (tweets) that explores some
characteristics of how tweets are written and meta-information of the
words that compose these messages. Moreover, we leverage sources of
noisy labels as our training data. These noisy labels were provided by
a few sentiment detection websites over twitter data. In our experi-
ments, we show that since our features are able to capture a more
abstract represen- tation of tweets, our solution is more ef- fective
than previous ones and also more robust regarding biased and noisy
data, which is the kind of data provided by these sources.

In this paper, we propose a 2-step sentiment analysis classification
method for Twitter, which first classifies messages as subjective and
ob- jective, and further distinguishes the subjective tweets as
positive or negative. To reduce the la- beling effort in creating
these classifiers, instead of using manually annotated data to compose
the training data, as regular supervised learning ap- proaches, we
leverage sources of noisy labels as our training data. These noisy
labels were pro- vided by a few sentiment detection websites over
twitter data. To better utilize these sources, we verify the potential
value of using and combining them, providing an analysis of the
provided labels, examine different strategies of combining these
sources in order to obtain the best outcome; and, propose a more
robust feature set that captures a more abstract representation of
tweets, composed by meta-information associated to words and spe-
cific characteristics of how tweets are written. By using it, we aim
to handle better: the problem of lack of information on tweets,
helping on the generalization process of the classification algo-
rithms; and the noisy and biased labels provided by those websites.

We collected data from 3 differ- ent websites that provide almost
real-time senti- ment detection for tweets: Twendz, Twitter Sen-
timent and TweetFeel. To collect data, we issued a query containing a
common stopword ``of'', as we are interested in collecting generic
data, and retrieved tweets from these sites for three weeks, archiving
the returned tweets along with their sen- timent labels. Table 1 shows
more details about these sources. Two of the websites provide 3- class
detection: positive, negative and neutral and one of them just 2-class
detection. One thing to note is our crawling process obtained a very
dif- ferent number of tweets from each website. This might be a result
of differences among their sam- pling processes of Twitter stream or
some kind of filtering process to output. For instance, a site may
only present the tweets it has more confi- dence about their
sentiment. In Section 3, we present a deep analysis of the data
provided by these sources, showing if they are useful to build a
sentiment classification.

Our goal is to categorize a tweet into one of the three sentiment
categories: positive, neutral or negative. Similar to (Pang and Lee,
2004; Wil- son et al., 2005), we implement a 2-step sentiment
detection framework. The first step targets on dis- tinguishing
subjective tweets from non-subjective tweets (subjectivity
detection). The second one further classifies the subjective tweets
into posi- tive and negative, namely, the polarity detection.  Both
classifiers perform prediction using an ab- stract representation of
the sentences as features, as we show later in this section.

We propose the use of two sets of features: meta- information about
the words on tweets and char- acteristics of how tweets are written.

Meta-features. Given a word in a tweet, we map it to its
part-of-speech using a part-of-speech dictionary.  In addition to POS
tags, we map the word to its prior subjectivity (weak and strong
subjectivity), also used by (Wiebe and Riloff, 2005), and polarity
(positive, negative and neutral). The prior polarity is switched from
pos- itive to negative or vice-versa when a negative expression (as,
e.g., ``don't'', ``never'') precedes the word.  We obtained the prior
subjectivity and polarity information from subjectivity lexicon of
about 8,000 words used in (Riloff and Wiebe, 2003) 2 . Although this
is a very comprehensive list, slang and specific Web vocabulary are
not present on it, e.g., words as ``yummy'' or ``ftw''.  For this
reason, we collected popular words used on online discussions from
many online sources and added them to this list.

We exploited the syn- tax of the tweets to compose our features. They
are: retweet; hashtag; reply; link, if the tweet con- tains a link;
punctuation (exclamation and ques- tions marks); emoticons (textual
expression rep- resenting facial expressions); and upper cases (the
number of words that starts with upper case in the tweet).  The
frequency of each feature in a tweet is di- vided by the number of the
words in the tweet.

As we mentioned before, the first step in our tweet sentiment
detection is to predict the subjectivity of a given tweet. We decided
to create a single clas- sifier by combining the objectivity sentences
from Twendz and Twitter Sentiment (objectivity class) and the
subjectivity sentences from all 3 sources.

As we do not know the quality of the labels pro- vided by these
sources, we perform a cleaning process over this data to assure some
reasonable quality. These are the steps: 1. Disagreement removal: Same
user's messages: we observed that the users with the highest number of
messages in our dataset are usually those ones that post some
objective messages, for example, ad- vertising some product or posting
some job recruiting information. For this reason, we allowed in the
training data only one message from the same user. Top opinion words:
to clean the objective training set, we remove from this set tweets
that contain the top-n opinion words in the subjectivity training set,
e.g., words as cool, suck, awesome etc.

To illustrate which of the proposed features are more effective for
this task, the top-5 features in terms of information gain, based on
our training data, are: positive polarity, link, strong subjec- tive,
upper case and verbs.

The second step of our sentiment detection ap- proach is polarity
classification, i.e., predict- ing positive or negative sentiment on
subjective tweets. In this section, first we analyze the qual- ity of
the polarity labels provided by the three sources, and whether their
combination has the potential to bring improvement.

The features used in the polarity detection are the same ones used in
the subjectivity detection.  However, as one would expect the set of
the most discriminative features is different between the two
tasks. For subjectivity detection, the top-5 features in terms of
information gain, based on the training data, are: negative polarity,
positive polarity, verbs, good emoticons and upper case.

For the subjectivity detection, after the cleansing processing (see
Section 3), the train- ing data contains about 200,000 tweets (roughly
100,000 tweets were labeled by the sources as subjective ones and
100,000 objective ones), and for polarity detection, 71046 positive
and 79628 negative tweets. For test data, we manually la- beled 1,000
tweets as positive, negative and neu- tral. We also built a
development set (1,000 tweets) to tune the parameters of the
classification algorithms.

For the polarity detection task, we built a few classifiers to compare
their performances: TwitterSA(single) and Twit- terSA(weights) are two
classifiers we trained using combined data from the 3 sources.  The
only difference is TwitterSA(weights) uses the modification of
weighting the prior polarity of the words based on the train- ing
data.  TwitterSA(voting) and Twit- terSA(maxconf) combine
classification out- puts from 3 classifiers respectively trained from
each source. TwitterSA(voting) uses majority voting to combine them
and Twit- terSA(maxconf) picks the one with maxi- mum confidence
score.

Twit- terSA achieved lower error rate than both Uni- grams and
ReviewSA. As a result, these num- bers confirm that features inferred
from meta- information of words and specific syntax features from
tweets are better indicators of the subjectiv- ity than
unigrams. Another advantage of our ap- proach is since it uses only 20
features, the train- ing and test times are much faster than using
thou- sands of features like Unigrams. One of the rea- sons why
TwitterSA obtained such a good perfor- mance was the process of data
cleansing (see Sec- tion 3).

We provide the results for polarity detection in Table 5. The best
performance was ob- tained by TwitterSA(maxconf), which combines
results of the 3 classifiers, respectively trained from each source,
by taking the output by the most confident classifier, as the final
predic- tion. TwitterSA(maxconf) was followed by Twit- terSA(weights)
and TwitterSA(single)

Unigrams also achieved comparable perfor- mances. However, when
reducing the size of the training data, the performance gap between
Twit- terSA and Unigrams is much wider. Figure 3 shows the error rate
of both approaches.

\todo[inline]{}

In a similar way, \citet{Pak:10} gathered a collection of 300,000
noisily labeled tweets, ensuring an even distribution of positive,
negative, and neutral messages.  After a brief exploration of PoS tag
statistics in these different classes, they presented a Na{\"i}ve
Bayes system which utilized highly relevant binary part-of-speech and
$n$-gram features.\footnote{\citet{Pak:10} determined the relevance of
  a feature $f$ using a special \emph{salience} metric, which they
  defined as a negative ratio between the minimum and maximum
  conditional probabilities of this feature belonging to different
  target classes:
  \begin{equation*}
    salience(f) = \frac{1}{N}\sum_{i=1}^{N-1}\sum_{j=i+1}^N 1 - \frac{\min(P(f, s_i), P(f, s_j))}{\max(P(f, s_i), P(f, s_j))}.
  \end{equation*}
  The $N$~term in this formula denotes the number of training
  examples, and the expression $s_i$ means the sentiment class of the
  $i$-th training instance.} With this approach, the authors attained
an accuracy slighlty above 0.6 on the manually labeled test set
of~\citet{Go:09}, also demonstrating a particular utility of bigrams,
negation rules, and feature pruning heuristics.

A slightly different task was addressed by~\citet{Davidov:10}, who
sought to predict hashtags and emoticons occurring in tweets using a
$k$-NN classifier trained on a large collection of messages.  The
authors achieved an \F-measure of~0.31 on the former task, and reached
an \F-score of~0.64 on predicting smileys.

\citet{Kouloumpis:11} trained an AdaBoost
classifier~\cite{Schapire:00} on two large collections of noisily
labeled tweets---the emoticon tweebank of~\citet{Go:09} and the
Edinburgh hashtag corpus.\footnote{\url{http://demeter.inf.ed.ac.uk}}
Using $n$-gram (up to length two), lexicon, part-of-speech, and
micro-blogging features (such as emoticons, abbreviations, and slang
expressions), the authors achieved a macro-averaged \F-measure of~0.68
on the three-class prediction task.

% One of the first attempts to analyze message-level sentiments on
% Twitter was made by \citet{Go:09}.  For their experiments, the authors
% collected a set of 1,600,000 tweets containing smileys.  Based on
% these emoticons, they automatically derived polarity classes for these
% messages (positive or negative) and used them to train a Na\"{\i}ve
% Bayes, MaxEnt, and SVM classifier.  The best $F$-score for this
% two-class classification problem could be achieved by the last system
% and run up to 82.2\%.

% Similar work was also done by \citet{Pak:10} who used the Na\"{\i}ve
% Bayes approach to differentiate between neutral, positive, and
% negative microblogs; and \citet{Barbosa:10} who gathered a collection
% of 200,000 tweets, subsequently analyzing them with three publicly
% available sentiment web-services and training an SVM classifier on the
% results of these predictors.  In a similar way, \citet{Agarwal:11}
% compared a simple unigram-based SVM approach with two other
% full-fledged systems, one which relied on a rich set of manually
% defined features, and another used partial tree
% kernels~\cite{Moschitti:06}.  The authors evaluated these methods on a
% commercially acquired corpus of 8,753 foreign-language tweets, which
% were automatically translated into English, finding that a combination
% of these methods worked best for both two- and three-way prediction
% tasks.

% The state-of-the-art results for message level polarity prediction on
% tweets were established by~\citet{Mohammad:13}, whose system (a
% supervised SVM classifier) used a rich set of various features
% including word and character n-grams, PoS statistics, Brown
% clusters~\cite{Brown:92}, etc., and also strongly benefitted from
% automatic corpus-based polarity lists---Sentiment~140 and NRC
% Hashtag~\cite{Mohammad:12,Kiritchenko:14}.  This approach ranked first
% at the SemEval competition~2013~\cite{Nakov:13} and anchieved the
% fourth place on the rerun of this task one year
% later~\cite{Rosenthal:14}, being outperformed by the supervised
% logistic regression approach of~\citet{Miura:14}, who used a heavy
% preprocessing of the data and a special balancing scheme for
% underrepresented classes.  Later on, these results were further
% improved by the apporaches of~\citet{Hagen:15} and \citet{Deriu:16},
% which both relied on ensembles of multiple independent classifiers.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

       &  &  &  & %
       &  &  & %
       &  &  & %
       & \\\bottomrule
    \end{tabular}
    \egroup
    \caption[Evaluation of ML-based coarse-grained SA methods.]{
      Evaluation of ML-based coarse-grained SA methods.\\
      {\small }}
    \label{snt-cgsa:tbl:ml-res}
  \end{center}
\end{table}

\section{Coarse-Grained Sentiment Analysis with DL-Based Methods}

\citet{Yessenalina:11}

A real breakthrough in the use of deep neural networks for the
sentence-level sentiment analysis happened with the pioneering work
of~\citet{Socher:11}, who first introduced a recursive autoencoder
(RAE).  In this system, the authors obtained a fixed-width vector
representation for complex phrases $\vec{v}$ by recursively merging
the vectors of adjacent tokens (say $\vec{w}_1$ and $\vec{w}_2$),
first multiplying these vectors with a compositional matrix $W$ and
then applying a non-linear function ($softmax$) to the resulting
product:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}\right)
\end{align*}
Using a max-margin classifier on top of the resulting phrase
representation, \citet{Socher:11} could improve the state-of-the-art
results on predicting the sentence-level polarity of user's blog
posts~\cite{Potts:10} and also outperformed the system
of~\citet{Nasukawa:03} on the MPQA data set~\cite{Wiebe:05}.

Later on, \citet{Socher:12} further improved these scores with the
help of a recursive matrix-vectors space model (RMVSM), in which each
word was associated with a 2-tuple of a vector and matrix---e.g.,
$(\vec{w}_1, W_1)$ and $(\vec{w}_2, W_2)$---and the compositionality
function was redefined as follows:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  W_2\cdot\vec{w}_1\\
  W_1\cdot\vec{w}_2
  \end{bmatrix}\right)
\end{align*}

\citet{Wang:15}

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

       &  &  &  & %
       &  &  & %
       &  &  & %
       & \\\bottomrule
    \end{tabular}
    \egroup
    \caption[Evaluation of DL-based coarse-grained SA methods.]{
      Evaluation of DL-based coarse-grained SA methods.\\
      {\small }}
    \label{snt-cgsa:tbl:ml-res}
  \end{center}
\end{table}

\section{Coarse-Grained Sentiment Analysis Using Language and Domain
  Adaptation}

One of the first works which pointed out the importance of domain
adaptation for sentiment analysis was introduced by~\citet{Aue:05}.
In their experiments, the authors trained separate SVM classifiers on
four different document sets: movie reviews, book reviews, customer
feedback from a product support service, and a feedback survey from a
customer knowledge base; finding that each classifier performed best
when applied to the same domain as it was trained on.  In order to
find an optimal way of overcoming this domain specificity,
\citet{Aue:05} tried out four different options:
\begin{inparaenum}[(i)]
\item\label{sent-cgsa:lst:rel-wrk1} training one classifier on all but
  the target domain and applying it to the latter;
\item using the same procedure as above, but limiting the features to
  only those which also appeared in the target texts;
\item taking an ensemble of individual classifiers each of which was
  trained on a different data collection; and, finally,
\item using a minimal subset of labeled in-domain data to train a
  Na{\"i}ve Bayes system with the expectation-maximization algorithm
  \cite[EM;][]{Dempster:77}.
\end{inparaenum}
The authors found that the ensemble and EM options worked best for
their cross-domain task, achieving an accuracy of up to 82.39\% for
the two-class prediction (positive vs negative) on new unseen text
genres.

Another notable milestone in the domain adaptation research was set
by~\citet{Blitzer:07}.  Relying on their previous work on structural
correspondence learning~\cite{Blitzer:07}, in which they used a set of
\emph{pivot features} (features which frequently appeared in both
target and source domains) to find an optimal correspondence of the
remaining attributes,\footnote{In particular, the authors trained $m$
  binary predictors for each of their $m$ pivot features in order to
  find other attributes which frequently co-occurred with the pivots.
  Afterwards, they composed these $m$ resulting weight vectors into a
  single matrix $W := [\vec{w}_{1},\ldots,\vec{w}_{m}]$, took an SVD
  decomposition of this matrix, and used the top $h$ left singular
  vectors to translate source features to the new domain.} the authors
refined their method by pre-selecting the pivots using their PMI
scores and improving misaligned feature projections using a small set
of labeled target examples.  With these modifications,
\citeauthor{Blitzer:07} were able to reduce the average adaptation
loss (the accuracy drop when transferring a classifier to a different
domain) from 9.1 to 4.9~percent when testing a sentiment predictor on
the domains of book, dvd, electical appliances, and kitchen reviews.

Other important works on domain adaptation for opinion mining include
those of~\citet{Read:05}, who pointed out that sentiment
classification might not only depend on the domain but also on topic,
time, and language style in which the text was written;
\citet{Tan:07}, who proposed using the classifier trained on the
source domain to classify unlabeled instances from the target genre,
and then iteratively retrain the system on the enriched data set.
Finally, \citet{Andreevskaia:08} proposed a combination of a lexicon-
and ML-based systems, claiming that this ensemble would be more
resistible to the domain shift than each of these classifiers on their
own.

Another line of research was introduced by~\citet{Glorot:11} who
proposed stacked denoising autoencoders (SDA)---a neural network
architecture in which an input vector $\vec{x}$ was first mapped to a
smaller representation $\vec{x}'$ via some function
$h: \vec{x}\mapsto\vec{x}'$, and then restored to its approximate
original state via an inverse transformation
$g: \vec{x}'\mapsto\vec{x}''\approx\vec{x}$.  In their experiments,
the authors optimized the parameters of the functions $h$ and $g$ on
both target and source data, getting approximate representations of
instances from both data sets; and then trained a linear SVM
classifier on the restored representations of the source instances,
subsequently applying this classifier to the target domain.  This
approach was further refined by~\citet{Chen:12} who analytically
computed the reconstruction function~$g$, and used both original and
restored features to predict the polarity labels of the target
data.\footnote{Both approaches were trained tested on the Amazon
  Review Corpus of~\citet{Blitzer:07}.}


Further notable contributions to domain adaptation in general were
made by~\citet{Daume:07} who proposed to replicate each extracted
feature three times and train the first replication on both domains,
the second repetion only on source, and the third copy only on target
domain, for which he assumed a small subset of labeled examples was
available; \citet{Yang:15} who trained neural embeddings of features,
trying to predict which instance attributes frequently co-occured with
each other;

\section{Evaluation}
\subsection{Effect of Lexicons}
\subsection{Effect of Distant Supervision}
\subsection{Effect of Word Embeddings}
\subsection{Effect of Normalization}

\section{Summary and Conclusions}\label{slsa:subsec:conclusions}
