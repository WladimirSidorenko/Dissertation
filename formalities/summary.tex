\documentclass[11pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Packages
\usepackage{breakcites}
\usepackage[authoryear]{natbib}
\usepackage{paralist}
\usepackage{titling}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Commands
\newcommand{\ienocomma}{\textit{i.e.}}
\newcommand{\ie}{\ienocomma,}
\newcommand{\eg}{\textit{e.g.},}
\newcommand{\F}[0]{$F_1$}
\newcommand{\markable}[1]{\texttt{#1}}
\newcommand{\attribute}[1]{\emph{\texttt{#1}}}
\renewcommand{\cite}{\citep}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Lengths
\setlength{\droptitle}{-7em}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Variables
\author{Uladzimir Sidarenka}
\title{  {\large Summary of the Dissertation}\\[0.5em]
  {\Large ``Sentiment Analysis of German Twitter''}}
\date{\vspace{-3ex}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Document
\begin{document}
\maketitle

Online communication services have not only revolutionized our
interaction with other people (allowing us to stay in touch with
friends, exchange ideas with colleagues, and get latest news from
celebrities and politicians without leaving the desktop) but also
opened the doors to numerous new dangers, such as cyber-attacks,
online bullying, and deliberate manipulation of public
opinion---dangers, which can hardly be averted manually due to the
immense amount of data exchanged on online platforms.  Therefore, if
we want to protect our society from these looming threats, we urgently
need more robust, higher-quality natural language processing (NLP)
applications that can recognize such menaces automatically, by
analyzing uncensored texts.  Unfortunately, most NLP programs today
have been created for standard language, as we know it from
newspapers, or, in the best case, adapted to the specifics of English
social media.

This thesis reduces the existing deficit by entering the new frontier
of German online communication and addressing one of its most prolific
forms---users' conversations on Twitter.  In particular, it explores
the ways and means by how people express their opinions on this
service, examines current approaches to automatic mining of these
feelings, and proposes new sentiment analysis methods, which
outperform state-of-the-art techniques.

I perform these tasks on three different linguistic levels:
\begin{itemize}
  \item\emph{subsentential}, in which I try to predict the polarity of
    single words, and automatically determine the main components of
    an opinion (its target, holder, and the actual evaluative
    expression) within a sentence;

  \item\emph{sentential}, where I consider the whole microblog as a
    single statement and classify its overall semantic orientation;

  \item and, finally, \emph{suprasentential}, in which I try to
    improve on message-level sentiment classification by inferring the
    overall polarity of a tweet from the semantic orientation of its
    single sentences, taking into account their semantic and pragmatic
    links (discourse relations).
\end{itemize}
At each of these levels, I evaluate the most prominent classes of
existing systems, seek to outperform their scores with my own
solutions, and estimate the effect of different hyper-paramaters and
input properties (such as word embedding types, feature groups, and
text normalization) on the classification results.

\section{Research Questions}

By doing so, I hope to find answers to the following questions:
\begin{itemize}
\item Can we apply opinion mining methods devised for standard English
  to German Twitter?
\item Which groups of approaches are best suited for which sentiment
  analysis tasks?
\item How much do word- and discourse-level analyses affect
  message-level sentiment classification?
\item Does text normalization help analyze sentiments?
\item Can we do better than existing systems?
\end{itemize}

\section{Outline of this Work}

To get these answers, I proceed in the following way:

\subsection{Chapter~1: Introduction to Sentiment Analysis}

In Chapter~1, I give a brief introduction to the main goals and
definitions of sentiment analysis, and make a digression into the
history of this field, tracing its roots back to the ancient
philosophy, and following the course if its development through the
main milestones in psychology, narratology, and linguistics of the
20th century.  In the final sections, I elaborate on the recent
advances in automatic opinion mining of social media, and pay special
attention to the current state of the art in sentiment analysis of
Twitter.

\subsection{Chapter~2: Sentiment Corpus}

Afterwards, in Chapter~2, I introduce the Potsdam Twitter Sentiment
(PotTS) corpus---a collection of $7,992$ German microblogs pertaining
to the German federal elections~2013; papal conclave, which happened
in the same year; general political discussions; and casual everyday
conversations.

To obtain messages for the first three groups, I was tracking the
public Twitter API between March and September 2013, using extensive
lists of hand-picked keywords, which, in my opinion, best described
these topics.  As casual everyday posts, I considered microblogs from
the German Twitter Snapshot~\cite{Scheffler:14}, which comprises
$\approx97\%$ all German tweets (17M messages) posted in April 2013.

To ensure a representative number of subjective opinions in the
resulting corpus, I grouped all downloaded microblogs pertaining to
the same topic into three formal classes:
\begin{inparaenum}[(i)]
  \item tweets that contained a polar term from the sentiment lexicon
    SentiWS~\cite{Remus:10},
  \item messages that had an emoticon, and, finally,
  \item all remaining microblogs.
\end{inparaenum}
Afterwards, I sampled an equal number of tweets (666) from each of the
four topics from each of these three formal categories.

In the next step, I defined an annotation scheme---a list of elements
that had to be annotated by human experts.  These were:
\begin{itemize}
  \item\emph{sentiment}s, which were specified as polar subjective
    evaluative opinions about people, entities, or events;

  \item\emph{target}s, which represented entities or events being
    evaluated;

  \item\emph{source}s, which were defined as the immediate author(s)
    or holder(s) of an opinion;

  \item\emph{polar term}s, which were words or idioms that had a
    distinguishable evaluative lexical meaning;

  \item\emph{intensifier}s, which represented elements that increased
    the expressivity and subjective sense of polar terms;

  \item\emph{diminishers}s, which vice versa reduced the strength of a
    polar term;

  \item and, finally, \emph{negation}s, which completely reversed the
    semantic orientation of a polar item (\eg{} ``\emph{nicht} gut''
    [\emph{not} good] or ``\emph{kein} schlechtes Beispiel''
    [\emph{not} bad advice]).
\end{itemize}

The annotation process was performed in three steps: first, two
linguists labeled one half of the data after only minimal training;
then, I automatically highlighted their divergent analyses and asked
them to resolve these differences; finally, the annotators continued
with the analysis of the remaining files.

To estimate the inter-rater reliability, I introduce two modified
versions of the popular $\kappa$-metric~\cite{Cohen:60}---binary and
proportional kappa---which differ in the way how they treat multiple
annotations of the same tokens (\eg{} if one phrase was labeled as
\emph{sentiment} several times by one expert) and how they deal with
partial matches (\eg{} if two \emph{sentiment}s labeled by different
experts agree on some, but not all of their tokens).  Using these
measures, I estimate the inter-rater reliability (IRR) of the
annotators at different stages of their work.  This study shows that,
initially, both assistants could hardly agree on the mere notion of
targeted opinions, but their disagreements could be resolved with the
help of the adjudication procedure, which was applied in step two.
Despite a small drop of the IRR scores in the final stage, all
$\kappa$-values still remained at the level of at least moderate
reliability.

Finally, I demonstrate that the initial selection criteria (topics and
formal classes) had a strong impact on the number and agreement of
annotated sentiments and polar terms, with tweets about federal
elections and messages without prefiltered topics being the most
prolific sources of these elements.

\subsection{Chapter~3: Sentiment Lexicons}

In Chapter~3, I turn to the first subsentential sentiment analysis
task, in which I try to predict the polarity (positive, negative, or
neutral) of single words in the text.

For this purpose, I first evaluate the quality of existing German
sentiment lexicons---SentiWS~\cite{Remus:10}, German Polarity
Clues~\cite{Waltinger:10}, and the Zurich Polarity
List~\cite{Clematide:10}---by directly comparing their entries with
the annotated polar terms in the PotTS corpus.  This evaluation shows
that the last resource (Zurich Polarity List) outperforms the other
two competitors in terms of both macro- and micro-averaged
\F{}-scores, reaching 0.589 and 0.955 \F{} respectively.

Afterwards, I compare three major groups of autmatic
lexicon-generation methods: dictionary-, corpus-, and
word-embedding--based ones.  The first of these groups induces
polarity lists from a manually annotated lexical taxonomy
\cite[\textsc{GermaNet}; ][]{Hamp:97}---a graph whose nodes represent
sets of synonyms (synsets), which are connected to each other by
semantic links (\eg{} meronymy-holonymy, hypernymy-hyponymy, or
antonymy relations).  Corpus-based algorithms, on the contrary, infer
sentiment lexicons from the collocation statistics computed on raw
unlabaled datasets, by taking a set of terms with known semantic
orientation (seed terms) as input and looking for other words that
frequently co-occur with these seeds in raw texts.  In the same vein,
word-embedding--based methods generate lists of polar terms by looking
for words whose vector representations (which are automatically
learned by a classifier, which predicts the occurrence of other words
in the nearby context) are most similar to the embeddings of seed
terms.

For the last group, I also propose four new algrorithms, which induce
polarity lists by clustering words vectors using the methods of
$k$-nearest neighbors, nearest centroids, principal component
analysis, and my own linear projection algorithm, in which I look for
a line that maximizes the distance between the projections of seed
terms with opposite semantic orientations.  This algorithm surpasses
all existing alternatives in terms of micro-averaged \F{}-score,
reaching 0.963 micro-\F{}, and yields a very competitive macro-\F{}
(0.462).

In general, however, dictionary-based approaches typically produce
better sentiment lexicons than corpus-based methods; and the only
viable alternatives to these algorithms, which are independent of any
manually annotated linguistic resources, are our linear projection and
$k$-NN systems.  Further evaluation of different input parameters also
shows that the results of almost all lexicon-generation methods can be
improved by using larger sets of initial seed terms.

\subsection{Chapter~4: Aspect-Based Sentiment Analysis}

In Chapter \ref{chap:fgsa}, we turned our attention to the
aspect-based sentiment analysis, in which we tried to predict the
spans of sentiments, targets, and holders of opinions using two most
popular approaches to this task---conditional random fields and
recurrent neural networks.  We obtained our best results (0.287
macro-\F{}) with the first-order linear-chain CRFs.  We could,
however, increase these scores by using alternative topologies of CRFs
(second-order linear-chain and semi-Markov CRFs) and also boost the
macro-averaged \F{} to 0.38 by taking a narrower interpretation of
sentiment spans (in which we only assigned the \textsc{Sentiment} tag
to polar terms).  Further evaluation of these methods proved the
utility of the text normalization step (which raised the macro-\F{} of
the CRF-method by almost 3\%) and task-specific word embeddings with
the least-squares fallback (which improved the macro-\F{}--score of
the GRU system by 1.4\%);

Summarizing the above findings, we would like to remind the reader
that in this chapter we have evaluated two most common approaches to
aspect-based sentiment analysis: conditional random fields and
recurrent neural networks.  Our experiments showed that CRFs with
manually defined features outperform both recurrent neural networks
(LSTM and GRU), reaching a macro-averaged \F-score of~0.287 on
predicting \markable{sentiment}s, \markable{source}s, and
\markable{target}s.

Furthermore, a closer look at these systems revealed that:
\begin{itemize}
\item CRFs can learn meaningful weights for state- and
  transition-features, although different features types might have
  different effects on classification of opinion elements: whereas
  \markable{sentiment}s benefited from all features used in our
  experiments, \markable{source}s profited most from lexical and
  complex attributes, and \markable{target}s were positively
  influenced by morphological and syntactic features only;
\item Apart from that, we analyzed the effect of different embedding
  types on the net results of RNN systems, finding that least-squares
  embeddings yield the best overall scores for these methods;
\item Furthermore, even higher prediction scores for
  \markable{sentiment}s can be achieved by narrowing the spans of
  these elements to polar terms.  This, however, might negatively
  affect the classification of \markable{source}s and
  \markable{target}s;
\item Even though context seems to play an important role, redefining
  models' structures by increasing the order of their dependencies or
  performing inference over trees instead of linear chains does not
  bring much improvement.  We could, however, still outperform the
  results of traditional first-order linear-chain CRFs with their
  first- and second-order semi-Markov modifications;
\item In the final step, we estimated the effect of text normalization
  by rerunning all experiments on original (unnormalized) tweets.
  This test showed that preprocessing is an extremely helpful
  procedure, which might improve the results of ABSA methods by up to
  3\%.
\end{itemize}

\subsection{Chapter~5: Message-Level Sentiment Analysis}

Afterwards, in Chapter~\ref{chap:cgsa}, we addressed one of the most
popular objective in contemporary sentiment analysis---message-level
sentiment analysis (MLSA).  To get a better overview of the numerous
existing systems, we compared three larger families of MLSA
methods---dictionary-, machine-learning--, and deep-learning--based
ones, finding that the last two groups performed significantly better
than the lexicon-based approaches (the best macro-\F{}--scores of
machine- and deep-learning methods run up to 0.677 and 0.69
respectively, whereas the best lexicon-based solution
[\citeauthor{Hu:04}, \citeyear{Hu:04}] only reached 0.641 macro-\F{}).
Apart from this, we improved the results of many reimplemented
approaches by changing their default configuration (\eg{} abandoning
polarity changing rules of lexicon-based systems, using alternative
classifiers for ML-based classifiers, or taking the least-squares
embeddings for DL-based methods).  In addition to the numerous
reimplementations of popular existing algorithms, we also proposed our
own solution---lexicon-based attention (LBA), in which we tried to
unite the lexicon and deep-learning paradigms by taking a
bidirectional LSTM network and explicitly pointing its attention to
the polar terms that appeared in the analyzed messages.  With this
solution, we not only outperformed all alternative DL systems, but
also improved on the scores of ML-based classifiers, attaining 0.69
macro-\F{} and 0.73 micro-\F{} on the PotTS corpus.  Similarly to our
findings of the previous chapter, we observed a strong positive effect
of text normalization and task-specific embeddings with the
least-squares approximation;

Now the we have reached the end of the chapter, we would like to
remind the reader that in this part of the thesis we have made the
following findings and contributions:
\begin{itemize}
  \item we have compared three major families of message-level
    sentiment analysis methods---lexicon-, machine-learning-- and
    deep-learning--based ones, finding that the last two groups
    significantly outperform lexicon-driven systems;
  \item surprisingly, among all compared lexicon methods, the most
    simple one (the classifier of~\citeauthor{Hu:04}
    [\citeyear{Hu:04}]) produced the best macro- and micro-averaged
    \F{}-results on the PotTS corpus (0.615 and 0.685 respectively)
    and also yielded the highest macro \F{}-measure on the SB10k
    dataset (0.421).  Other systems, however, could have improved
    their scores if they better handled the negation of polar terms
    (after switching off the negation component in the method
    of~\citeauthor{Musto:14}, its macro-\F{} on the PotTS corpus
    increased to 0.641, surpassing the benchmark
    of~\citeauthor{Hu:04});
  \item as expected, the ML-based system of~\citet{Mohammad:13}---the
    winner of the inaugural run of SemEval task in sentiment analysis
    of Twitter~\cite{Nakov:13}---also surpassed other ML competitors,
    achieving highly competitive results: 0.674 macro- and 0.727
    micro-\F{} on the PotTS data, and 0.564 macro- and 0.752
    micro-averaged \F{}-measure on the SB10k test set;
  \item as in the previous case, however, these results could have
    been improved if the classifier dispensed with character-level and
    part-of-speech features and used logistic regression instead of
    SVM;
  \item a much more varied situation was observed with
    deep-learning--based systems, which frequently simply fell into
    always predicting the majority class for all tweets, but sometimes
    yielded extraordinarily good results as it was the case with our
    proposed lexicon-based attention system, which attained 0.69
    macro-\F{} on the PotTS corpus and 0.55 macro \F{}-score on the
    SB10k dataset (0.73 and 0.75 micro-\F{} respectively), setting a
    new state of the art for the former data;
  \item speaking of word embeddings, we should note that almost all
    DL-based approaches showed fairly low scores when they used
    randomly initialized task-specific embeddings, but notably
    improved their results after switching to pre-trained word2vec
    vectors, and benefited even more from the least-squares fallback;
  \item against our expectations, we could not overcome the majority
    class pitfall of DL-based systems after adding more distantly
    supervised training data, which, in general, only lowered the
    scores of both ML- and DL-based methods.  Since this result
    contradicts the findings of other authors, we hypothesize that
    this degradation is primarily due to the differences in the class
    distributions between automatically and manually labeled tweets;
  \item on the other hand, we could see that using more qualitative
    sentiment lexicons (especially manually curated and
    dictionary-based ones) resulted in further improvements for the
    systems that relied on this lexical resource;
  \item last but not least, we proved the utility of the text
    normalization step, which brought about significant improvements
    for all tested methods, as confirmed by our last ablation test.
\end{itemize}

\subsection{Chapter~6: Discourse-Aware Sentiment Analysis}

Finally, in the last part, we tried to improve the results of the
proposed LBA method by making it aware of the discourse structure.
For this purpose, we segmented all microblogs from the PotTS and SB10k
corpora into elementary discourse units, individually analyzing each
of these segments with our MLSA classifier, and then estimated the
overall polarity of a tweet by joining the polarity scores of its EDUs
over the RST tree.  We proposed three different ways of doing this
joining (latent CRFs, latent-marginalized CRFs, and Recursive
Dirichlet Process), obtaining better results than existing
discourse-aware sentiment methods and also outperforming the original
discourse-unaware baseline.  In the concluding experiments, we further
improved these scores by using manually annotated RST trees and richer
subsets of discourse relations.

At this point, our chapter has come to an end and, concluding it, we
would like to recap that in this part of the thesis:
\begin{itemize}
  \item we have presented an overview of the most popular approaches
    to automatic discourse analysis (RST, PDTB, and SDRT) and
    explained why we think that one of these frameworks (Rhetorical
    Structure Theory) would be more amenable to the purposes of
    discourse-aware sentiment analysis than the others;
  \item to substantiate our claims and to see whether the
    lexicon-based attention system introduced in the previous chapter
    would indeed benefit from awareness of discourse structure, we
    segmented all microblogs from the PotTS and SB10k corpora into
    elementary discourse units using the SVM-based segmenter
    of~\citet{Sidarenka:15} and parsed these messages with the RST
    parser of~\citet{Ji:14}, which had been previously retrained on
    the Potsdam Commentary Corpus~\cite{Stede:14};
  \item afterwards, we estimated the results of existing
    discourse-aware sentiment methods (the systems
    of~\citeauthor{Wang:15}~[\citeyear{Wang:15}] and
    \citeauthor{Bhatia:15}~[\citeyear{Bhatia:15}]) and also evaluated
    two simpler baselines (in which we predicted semantic orientation
    of a tweet by taking the polarity of its last and root EDUs),
    getting the best results with the R2N2 solution
    of~\citet{Bhatia:15} (0.657 and 0.559 macro-\F{} on PotTS and
    SB10k respectively);
  \item we could, however, improve on these scores and also outperform
    the plain LBA system (although by a not very large margin) with
    our three proposed discourse-aware sentiment solutions (latent and
    latent-marginalized conditional random fields and Recursive
    Dirichlet Process), pushing the macro-averaged \F{}-score on PotTS
    up to 0.678 and increasing the result on SB10k to 0.56 macro-\F{};
  \item a subsequent evaluation of these approaches with different
    settings showed that the results of all discourse-aware methods
    largely correlated with the scores of the base sentiment
    classifier and also revealed an important drawback of the
    latent-marginalized CRFs, which failed to predict any positive or
    negative instance on the test set of the SB10k corpus when trained
    in combination with the lexicon-based approach of~\citet{Hu:04};
  \item nevertheless, almost all DASA solutions could improve their
    scores when tested on manually annotated RST trees or used with a
    richer set of discourse relations.
\end{itemize}

\section{Conclusions}
%% \addcontentsline{toc}{section}{Conclusions}

Now that we have gone past all these landmarks, it is time to unbag
the questions which we had asked ourselves at the beginning of this
endeavor, and try to answer them again, equipped with all knowledge
that we have acquired during our run.  Here we go:

\begin{itemize}
  \item\textbf{Can we apply opinion mining methods devised for
    standard English to German Twitter?}

    Yes, we can, but the success of these approaches might
    significantly vary depending on the task, the size and the
    reliability of the training data, as well as the evaluation metric
    that we use. For example, dictionary-based lexicon methods
    achieved fairly good results on their objective, but this success
    was mostly due to the high quality of the \textsc{GermaNet}
    annotation.  On the other hand, our manually labeled PotTS corpus
    was evidently too small for aspect-based sentiment systems, which
    failed to generalize to unseen tweets despite their very high
    scores on the training set.  Message-level sentiment approaches,
    vice versa, seemed to be quite happy with the size of the training
    dataset, attaining good results on both corpora (PotTS and SB10k).
    Nevertheless, we again experienced a lack of data while working on
    discourse-aware enhancements, many of which hit the same ceiling
    of the macro-averaged \F{}-scores.

    Apart from these difficulties arising from insufficient data, we
    also noticed a significant degradation of the scores for systems
    whose original tasks and evaluation metrics were different from
    ours.  For example, the lexicon generation method of
    \citet{Esuli:05} was originally designed to assign polarity scores
    to all \emph{synsets} found in the \textsc{WordNet} and not to
    produce a list of polar \emph{words}.  Similarly, the RNTN
    approach of \citet{Socher:13} was trained and evaluated on all
    syntactic subtrees of a document and not only at the top text
    level.  Likewise, the system of~\citet{Yessenalina:11} was devised
    for doing ordinal logistic regression and not polarity
    classification, as in our case.  As a result, all these approaches
    showed lower scores than their competitors in our evaluation, even
    though they are undoubtedly well suited for their original data
    and tasks.

    Due to the high diversity of methods, metrics, and tasks, it is
    difficult to provide a general recipe for transferring existing
    English sentiment systems to German Twitter, but we still would
    like to formulate at least a few rules of thumb, which came up
    during our experiments:
    \begin{itemize}
      \item\textbf{Prefer methods which are closest to your training
        objective} and which were trained under similar conditions
        w.r.t. the amount of data, their class distribution and
        domain;
      \item\textbf{Put every single setting of these methods into
        question}---bear in mind that things which work well in the
        original cases are not guaranteed to work in your
        situation.\footnote{In this respect, it is important to
          realize that every classification task is merely an attempt
          to solve a system of equations, so that methods which are
          good at solving one system might completely fail to solve
          other equations.}  The more options you try, the better will
        be your results;
      \item\textbf{Try using manually labeled resources for your
        target domain}, if they are available, but pay attention at
        the quality of their annotation---it often matters more than
        the corpus size;
      \item If there are manually annotated data, \textbf{prefer
        machine-learning methods to hard-coded rules}--- they will
        penalize their bad components automatically by themselves;
      \item\textbf{Do not use randomly initialized word embeddings for
        deep-learning systems}---initialize them with language-model
        vectors (which are cheap to obtain).  Otherwise, your model
        might get stuck in a very bad local optimum.
    \end{itemize}

  \item\textbf{Which groups of approaches are best suited for which
    sentiment tasks?}

    Based on our evaluation, we answer to this question as follows:
    \begin{itemize}
      \item\emph{Sentiment lexicon generation} is more amenable to
        dictionary-based solutions, provided that there exists a
        sufficiently big, reliably annotated lexical taxonomy for
        these systems.  If there is no such resource, one should
        better resort to word-embedding--based algorithms;

      \item With a limited amount of training data, \emph{aspect-based
        sentiment analysis} can be better addressed with probabilistic
        graphical models such as conditional random fields with
        hand-crafted features;

      \item On the other hand, plain \emph{message-level sentiment
        analysis} can be efficiently tackled with both machine- and
        deep-learning algorithms (such as SVM, logistic regression, or
        RNN);

      \item But probabilistic graphical models strike back at
        \emph{discourse-aware sentiment methods}, where they might
        even outperform pure neural-network solutions, although the
        margin of these improvements is not that large.
    \end{itemize}

    Thus, probabilistic model can still hold their ground when it
    comes to structured prediction, but the difference of these
    algorithms from and their improvements upon neural networks are
    gradually vanishing.

  \item\textbf{How much do word- and discourse-level analyses affect
    message-level sentiment classification?}

    Our evaluation in Section~\ref{cgsa:subsec:eval:lexicons} showed
    that the macro-averaged \F{}-scores of our proposed lexicon-based
    attention system varied by up to 14\% (from 0.64 to 0.69
    macro-\F{} on the PotTS corpus, and from 0.44 to 0.58 on SB10k)
    depending on the lexicon used by this approach.  At the same,
    discourse enhancements could only improve the results of LBA by at
    most 1.5\% percent (from 0.677 to 0.678 on PotTS, and from 0.557
    to 0.572 on SB10k).  Although it appears as if the lexicon
    component were more important to a sentiment system, we would like
    to preclude such incorrect conclusion, because
    \begin{inparaenum}[(a)]
      \item a full-fledged sentiment solution should take into account
        both linguistic levels (words and discourse) and
      \item these relative results might look different if we expand
        the analyzed domain to longer documents or apply
        discourse-aware methods to complete discussion threads.
    \end{inparaenum}

  \item\textbf{Does text normalization help analyze sentiments?}

    Yes, it definitely does.  As we could see in
    Chapters~\ref{chap:fgsa} and \ref{chap:cgsa}, normalization
    significantly improves the quality of aspect-based and
    message-level sentiment analyses, boosting the results on the
    former task by up to 4\% (see
    Table~\ref{snt-fgsa:tbl:normalization}) and improving the
    macro-averaged \F{}-measure of message-level sentiment methods by
    up to 25\% (see Table~\ref{snt-cgsa:tbl:res-no-normalization});

    The only question that remained unanswered in this context is
    which normalization steps exactly improve the scores of sentiment
    systems.  To make up for this omission, we separately deactivated
    each individual step of our text normalization pipeline
    (unification of Twitter phenomena, spelling correction, and
    normalization of slang terms) and rerun our message-level
    classification experiments using the lexicon-based attention
    system.  As we can see from the results in
    Table~\ref{afterword:tbl:lba-normalization-steps}, the
    micro-averaged \F{}-scores on both datasets benefit most from the
    unification of Twitter-specific phenomena, sinking by almost 19\%
    when this component is deactivated.  This step is also most useful
    for the macro-\F{} on the SB10k corpus, whereas the macro-average
    on PotTS mostly capitalizes on the normalization of slang terms.

  \item\textbf{Can we do better than existing approaches?}

    Yes, we can:
    \begin{itemize}
    \item we improved the macro-averaged results of exisitng
      lexicon-generation methods with our proposed linear projection
      algorithms;
    \item we increased the scores of aspect-based analysis by
      redefining the topologies of CRFs;
    \item our lexicon-based attention network outperformed many of
      its competitors on message-level classification;
    \item and, finally,we surpassed the discourse-unware baseline and
      other existing discourse-aware sentiment solutions with the
      proposed latent-marginalized CRFs and Recursive Dirichlet
      Process.
    \end{itemize}
\end{itemize}

% Bibliography
\bibliographystyle{apalike}
\bibliography{../bibliography}

\end{document}
