\chapter{Coarse-Grained Sentiment Analysis}\label{sec:snt:cgsa}

Having familiarized ourselves with the peculiarities of the creation
of a sentiment corpus, the different ways to automatically induce new
polarity lists, and the difficulties of fine-grained opinion mining,
we now move on to the presumably most popular sentiment analysis
objective---the coarse-grained analysis or CGSA, in which we need to
determine the overall polarity of a message.

Traditionally, this task has been addessed with either of three
popular method groups:
\begin{inparaenum}[(i)]
  \item lexicon-based approaches,
  \item machine-learning-based (ML) techniques, and
  \item deep-learning-based (DL) applications.
\end{inparaenum}
In this chapter, we are going to scrutinize the most prominent
representatives of each of these paradigms and also tackle a much more
ambitious goal, namely to check whether we can achieve results
comparable with the scores of these methods when the language of the
domain we train on is completely different from the language of the
test data.

We begin our comparison by first presenting the metrics that we will
use in our subsequent evaluation.  After a brief description of the
data preparation step, we proceed to the actual estimation of popular
lexicon-, ML-, and DL-based approaches, explaining and evaluating them
in Sections~\ref{sec:cgsa:lexicon-based}, \ref{sec:cgsa:ml-based},
and~\ref{sec:cgsa:dl-based} respectively.  Then,
in~Section~\ref{sec:cgsa:domain-adaptation}, we also show which
results can be obtained by using cross-lingual transfer, where we
train a classifier on English microblogs and then adapt this system to
German messages.  Finally, we conclude with an extensive evaluation of
different hyperparameters and settings (including various types of
sentiment lexicons, different kinds of word embeddings, the utility of
the text normalization step, and the impact of additional noisily
labeled training data), summarizing our results and recapping our
findings at the end of this chapter.

\section{Evaluation Metrics}\label{sec:cgsa:eval-metrics}

To estimate the quality of the compared systems, we will rely on two
established evaluation metrics which are commonly used for measuring
CGSA results: One of these metrics is the macro-averaged \F-score over
the two major polarity classes~(positive and negative): { \small%
  \begin{equation*}
    F_1 = \frac{F_{pos} + F_{neg}}{2}.
  \end{equation*}%
  \normalsize%
}%
This measure was first introduced by the organizers of the SemEval
competition~\cite{Nakov:13,Rosenthal:14,Rosenthal:15} and has become a
de facto standard not only for the SemEval dataset, but virtually for
all related coarse-grained sentiment tasks and corpora.

The second metric is the micro-averaged \F-score over all three
possible semantic orientations (positive, negative, and neutral),
which basically corresponds to the accuracy over the complete labeled
dataset~\cite[see][p.~577]{Manning:99}.  This measure both predates
and supersedes the SemEval evaluation as it had already been applied
in the very first works on coarse-grained opinion
mining~\cite{Wiebe:99,Das:01,Read:05,Kennedy:06,Go:09} and was again
reintroduced at the GermEval Shared Task on Sentiment
Analysis~2017~\cite{Wojatzki:17}.

Moreover, in addition to these two metrics, we will also give a
detailed information on precision, recall, and \F-scores of each
particular polarity class in order to get a better intuition about
precise strengths, weaknesses, and biases of each evaluated method.

\section{Data Preparation}\label{sec:cgsa:data}

Similarly to the data preparation steps used for fine-grained
sentiment analysis, we preprocessed all tweets involved in our
experiments with the text normalization system
of~\citet{Sidarenka:13}, tokenized them using the same adjusted
version of Potts'
tokenizer,\footnote{\url{http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py}}
lemmatized and assigned part-of-speech tags to these tokens with the
\texttt{TreeTagger} of \citet{Schmid:95}.  Moreover, like in the
previous chapter, we automatically obtained morphological features for
each word, and induced syntactic trees for each sentence with the help
of the \texttt{Mate} dependency parser
\cite{Bohnet:13}. % Apart from the
% PotTS dataset, we also applied this procedure to the microblogs of
% the German Twitter snapshot~\cite{Scheffler:14}, which will be used
% in our subsequent experiments on noisy supervision.

We again divided the PotTS corpus~\cite{Sidarenka:16} into a training,
development, and test set, using 70\% of the tweets for learning, 10\%
for tuning and picking the optimal hyperparameters, and the remaining
20\% for evaluating the results.  We inferred the polarity labels for
these microblogs with a simple heuristic rule akin to the one used
by~\citet{Wiebe:05a}, and assigned the positive (negative) class to
the messages which had exclusively positive (negative) sentiments,
skipping all microblogs that simultaneously contained multiple
opinions with different semantic orientations.  In cases when a
sentiment was absent, we recoursed to a fallback strategy by
considering all tweets with only positive (negative) polar terms as
positive (negative), disregarding the messages which featured
expressions from both polarity classes, and taking the rest of the
corpus (i.e., posts with neither sentiments nor polar terms) as
neutral instances.

A few examples of such heuristically inferred labels are provided
below:
\begin{example}[Coarse-Grained Sentiment Annotations]\label{snt:cgsa:exmp:anno1}
  \noindent\textup{\bfseries\textcolor{darkred}{Tweet:}} {\upshape Ich finde den Papst putzig \smiley{}}\\
  \noindent I find the Pope cute \smiley{}.\\
  \noindent\textup{\bfseries\textcolor{darkred}{Label:}}\hspace*{2em}\textbf{%
    \upshape\textcolor{green3}{positive}}\\[1.5em]
  \noindent\textup{\bfseries\textcolor{darkred}{Tweet:}} {\upshape typisch Bayern kaum ist der neue Papst da und schon haben
  sie ihn in der Tasche ...}\\
  \noindent Typical Bavaria The new Pope is hardly there, as they already have him in their pocket\\
  \noindent\textup{\bfseries\textcolor{darkred}{Label:}}\hspace*{2em}\textbf{%
    \upshape\textcolor{midnightblue}{negative}}
\end{example}
As we can see, our simple rule provides reasonable decisions in most
of the cases.  However, since this approach is still an approximation
and consequently prone to errors (especially in the cases where the
polarity of the whole microblog differs from the semantic orientation
of its single tokens or is expressed without any explicit polar
expressions at all, see Example~\ref{snt:cgsa:exmp:anno2}), we also
decided to evaluate all CGSA methods presented in this chapter on
another German Twitter corpus, which has been specifically annotated
with message-level polarities---SB10k.

\begin{example}[Erroneous Sentiment
  Annotations]\label{snt:cgsa:exmp:anno2}
  \noindent\textup{\bfseries\textcolor{darkred}{Tweet:}} {\upshape Unser Park, unser Geld, unsere Stadt! -NICHT unser Finanzminister! \smiley{} \#schmid \#spd \#s21 \#btw13}\\
  \noindent Our park, our money, our city! -NOT our Finance Minister! \smiley{} \#schmid \#spd \#s21 \#btw13\\
  \noindent\textup{\bfseries\textcolor{darkred}{Label:}}\hspace*{2em}\textbf{%
    \upshape\textcolor{green3}{positive*}}\\[1.5em]
  \noindent\textup{\bfseries\textcolor{darkred}{Tweet:}} {\upshape Auf die Lobby-FDP von heute kann Deutschland verzichten ...}\\
  \noindent Germany can go without today's lobby FDP\\
  \noindent\textup{\bfseries\textcolor{darkred}{Label:}}\hspace*{2em}\textbf{%
    \upshape\textcolor{black}{neutral*}}
\end{example}

The SB10k dataset was introduced by~\citet{Cieliebak:17}, and
comprises a total of 9,738 tweets.  These messages were sampled from a
larger snapshot of 5M German microblogs gathered between August and
November~2013.  To ensure lexical diversity and proportional polarity
distribution in this corpus, the authors first split all posts of this
snaphsot into 2,500 clusters using $k$-means algorithm with unigram
features.  Afterwards, from each of these groups, they selected tweets
with at least one positive and one negative term from the German
Polarity Clues lexicon~\cite{Waltinger:10}.  Each of these messages
was subsequently annotated by at least three human experts from a pool
of 34 different coders.  The resulting inter-rater reliability (IRR)
of these data run up to 0.39 Krippendorff's
$\alpha$~\cite{Krippendorff:07}.  Unfortunately, due to the
restrictions of Twitter's terms of use (which only allow to distribute
the ids of the microblogs along with their labels), we were able to
retrieve merely 7,476 tweets of this corpus, which nevertheless
constitute a substantial amount of data, comparable to the size of the
PotTS dataset.

In addition to the aforementioned two corpora (PotTS and SB10k), we
also automatically annotated all microblogs of the German Twitter
Snapshot~\cite{Scheffler:14}, following the procedure proposed
by~\citet{Read:05} and~\citet{Go:09}, and assigning the positive
(negative) class to the tweets which contained the respective
emoticons.  However, in contrast to the previous datasets, we will not
use these tweets for evaluation, but solely utilize them for training
in our distant supervision experiments.

The resulting statistics on the number of messages and polarity class
distribution in the final data are shown in
Table~\ref{snt-cgsa:tbl:corp-dist}.
\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{6}{>{\centering\arraybackslash}p{0.13\columnwidth}}} % last two columns
      \toprule
      \textbf{Dataset} & \multicolumn{4}{c}{\bfseries Polarity Class}%
      & \multicolumn{2}{c}{\bfseries Agreement}\\\cmidrule(lr){2-5}\cmidrule(lr){6-7}
                       & \textbf{Positive} & \textbf{Negative} %
                                           & \textbf{Neutral} & \textbf{Mixed*} %
                                                              & $\alpha$ & $\kappa$\\\midrule

      \textbf{PotTS} & 3,380 & 1,541 & 2,558 & 513 & 0.66 & 0.4\\
      \textbf{SB10k} & 1,717 & 1,130 & 4,629 & 0 & 0.39 & NA\\
      \textbf{GTS} & 3,326,829 & 350,775 & 19,453,669 & 73,776 & NA & NA\\\bottomrule
\end{tabular}
    \egroup
    \caption[Polarity class distribution in PotTS, SB10k, and the
    German Twitter Snapshot.]{Polarity class distribution in PotTS,
      SB10k, and the German
      Twitter Snapshot (GTS).\\
      \emph{(* -- the \emph{mixed} polarity was excluded from our
        experiments)}}
    \label{snt-cgsa:tbl:corp-dist}
  \end{center}
\end{table}

As we can see, each of the datasets has its own unique composition of
polar tweets: The PotTS corpus, for example, shows a conspicuous bias
towards the positive class with 42\% of the microblogs belonging to
this polarity.  We can partially explain this effect by the following
reasons: first of all, it might be due to the coarseness of the
heuristic rule that we applied to infer the labels for these messages;
and, secondly, it might also stem from the initial selection criteria
that we used to compile the data for this collection.  As you might
remember, we a priori composed the major part of this dataset from
tweets which contained smileys or had at least one polar expression
from the SentiWS lexicon~\cite{Remus:10}.  Since most of these
emoticons were positive (which is evident from the statistics of the
German Twitter snapshot), the selected posts also became skewed
towards this semantic orientation.

The second most frequent group of the PotTS corpus is formed by
neutral microblogs, which account for 32\% of the data.  Finally,
negative messages represent the absolute minority of all tweets
(merely 19\%), which, however, is less surprising as the same tendency
can be observed for the SB10k and German Twitter Snapshot too.

Regarding the last two corpora, we can observe a more uniform (though
not identical) behavior as both collections are dominated by neutral
posts, which constitue 62\% of SB10k data and 84\% of the German
Twitter Snapshot.  The positive class, again, makes up a big part of
these datasets (23\% of SB10k and 14\% of the snapshot), but its
influence this time is much less pronounced than in the PotTS case.
Finally, as we already mentioned, negative tweets are the least
represented semantic representation across all three sources.  The
only group which has even less instances than this class is the mixed
polarity.  We, however, will skip the mixed orientation in our
experiments for the sake of simplicity and uniformity of the
evaluation.\footnote{As we will see later, some of the CGSA methods
  (especially the lexicon-based ones) can hardly be extended to the
  prediction of more than three polarity classes.}

Last but not least, the results of the inter-rater reliability check
confirm the superior quality of the PotTS corpus, which, even despite
an approximate label inference, still has an $\alpha$
agreement~\cite{Krippendorff:07} that is almost 1.7 times as high as
the respective score of SB10k (0.66 versus 0.39).  However, Cohen's
$\kappa$ of these data (0.4), which is only available for this
dataset, is merely on the verge between fair and moderate values.
Nevertheless, since labels used in this experiments are ordinal rather
than nominal in their nature (i.e., we can compute the \emph{distance}
between distinct labels, which, for example, would be bigger for the
pair \emph{positive} vs. \emph{negative} than for the pair
\emph{positive} vs. \emph{neutral}), we find the Krippendorff's metric
more appropriate for assessing the quality of the annotation for this
task.

\section{Lexicon-Based Methods}\label{sec:cgsa:lexicon-based}

The first group of approaches that we are going to explore in this
chapter using the described data are lexicon-based (LB) systems.  Just
like sentiment lexicons themselves, LB methods for coarse-grained
opinion mining have attracted a lot of attention from the very
inception of the sentiment analysis field.  Starting with the research
of~\citet{Hatzivassi:00}, who statistically proved that the mere
occurrence of a subjective adjective from a polarity list was a highly
reliable indicator that the whole sentence it appeared in was
subjective, more and more works dealing with the use of lexicons for
determining the overall polarity of complete texts appeared on the
scene.

One of the first notable steps in this direction was made
by~\citet{Das:01}, who proposed an ensemble of five classifiers (two
of which were purely lexicon-based and the other three relied on
lexicon features) to predict the polarity of stock messages
(\emph{buy}, \emph{sell}, or \emph{neutral}), achieving an accuracy of
62\% on a corpus of several hundreds stock board messages.  A much
simpler method for a similar task was suggested by~\citet{Turney:02},
who determined the \emph{semantic orientation} (SO) of reviews by
averaging the PMI scores of their terms, obtaining these scores from
an automatically generated sentiment lexicon.  With this approach, the
author could reach an accuracy of 74\% on a corpus of 410 manually
labeled Epinions comments.  In the same vein, \citet{Hu:04} computed
the overall polarity of a sentence by comparing the numbers of
positive and negative terms appearing in that sentence, reversing the
orientation of the term in case of negation.  Finally, \citet{Kim:04}
compared three different approaches to determining the polarity of a
sentence:
\begin{inparaenum}[(i)]
\item by multiplying the signs of its polar terms,
\item by taking the sum of their scores, and
\item by computing the geometric mean of these values;
\end{inparaenum}
finding the first and the last option working best on the Document
Undestanding Corpus.\footnote{\url{http://duc.nist.gov/}}

% % Hu and Liu, 2004
% Similarly, \citet{Hu:04} determined the semantic orientation of
% sentences in customer reviews by simply comparing the number of
% positive and negative terms found in these passages. Since the
% authors, however, were primarily interested in estimating the polarity
% towards particular product features mentioned in the clauses, they
% additionally applied a fallback strategy in case of a tie by checking
% which of the polar lexicon terms appeared closer to the features, and
% assuming the polarity of the preceding sentence if these numbers were
% also equal.

% % Taboada et al., 2004
% Largely inspired by the Appraisal theory of~\citet{Martin:00},
% \citet{Taboada:04} enhanced the original method of~\citet{Turney:02}
% by increasing the weights of polar adjectives which occurred in the
% middle and at the end of a document, and also augmenting these values
% with the affect, judgement, and appreciation scores.  Similarly to
% polarity, the appraisal scores were calculated automatically by
% computing the PMI of their cooccurrence with different pronouns using
% a web search engine.

% Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006
In~\citeyear{Polanyi:06}, \citeauthor{Polanyi:06} presented an
extensive overview and analysis of common lexicon-based sentiment
approaches existing at that time, arguing that, besides considering
the lexical valence (i.e., semantic orientation) of polar terms, it
was also necessary to incorporate syntactic, discourse-level, and
extra-linguistic factors such as negations, intensifiers, modal
operators (e.g., \emph{could} or \emph{might}), presuppositional items
(e.g., \emph{barely} or \emph{failure}), irony, reported speech,
discourse connectors, genre and attitude assessment, reported speech,
multi-entity evaluation, etc.  This theoretical hypothesis was also
proven empirically by \citet{Kennedy:06}, who investigated two ways to
determine the polarity of a customer review: In the first of these
methods, they simply compared the numbers of positive and negative
expressions appearing in text, assigning the review to the class with
the greater number of items.  In the second attempt, they enhanced the
original system with additional information about contextual valence
shifters, increasing or decreasing the sentiment score of a term if it
was preceded by an intensifier or downtoner, and changing the polarity
sign of this score to the opposite in case of a negation.  With this
adjustment, the authors achieved a statistically significant
improvement, boosting the accuracy of the two-class prediction on a
corpus of product and movie reviews from 67.9 to 69.3\%.

% Taboada et al., 2011
Finally, a veritably seminal work on lexicon-based techniques was
presented by~\citet{Taboada:11}, who introduced a manually compiled
polarity list\footnote{The authors hand-annotated all occurrences of
  adjectives, nouns, and verbs found in a corpus of 400 Epinions
  reviews with ordinal categories ranging from -5 to 5 which reflected
  the semantic orientation of a term (positive vs. negative) and its
  polar strength (weak vs. strong).} and used this resource to
estimate the overall semantic orientation of the text.  Drawing on the
ideas of~\citet{Polanyi:06}, the authors incorporated a set of
additional heuristic rules into their computation by changing the
prior SO values of negated, itensified, and downtoned terms, ignoring
irrealis sentences, and adjusting the weights of specific document
sections.  An extensive evaluation of this approach showed the
superior performance of the manual lexicon in comparison with other
polarity lists, including the Subjectivity
Dictionary~\cite{Wilson:05}, Maryland Polarity Set~\cite{Mohammad:09},
and \textsc{SentiWordNet}~\cite{Esuli:06c}.  Moreover, the authors
also demonstrated the effectiveness of their method for other topics
and text genres, hypothesizing that lexion-based approaches were in
general more robust to domain shifts than traditional supervised
machine-learning techniques.

% % Taboada et al., 2006
% Another important contribution to the development of lexicon-based
% approaches was made by~\citet{Taboada:06}, who compared three popular
% polarity lists---a PMI lexicon computed with the original method
% of~\citet{Turney:02} using the AltaVista's NEAR operator; a similar
% polarity list obtained with the help of Google's AND queries; and,
% finally, the manually compiled General Inquirer lexicon
% of~\citet{Stone:66}.  The authors evaluated these resources both
% intrinsically (by comparing them with GI entries) and extrinsically
% (by computing the polarity of 400 manually annotated Epinions
% reviews).  To estimate the overall polarity of a review for the second
% task, \citeauthor{Taboada:06} calculated the average SO value of all
% polar terms found in the review, obtaining these scores from the
% mean-normalized lexicons, and flipping the polarity sign to the
% opposite in case of the negation.

% Musto et al., 2014
Lexicon-based systems have also found their way into sentiment
analysis of social media: For example, one such approach, specifically
tailored to Twitter data, was proposed by~\citet{Musto:14}, who
examined four different ways to compute the overall polarity scores of
microblogs: \emph{basic}, \emph{normalized}, \emph{emphasized}, and
\emph{normalized-emphasized}; evaluating these strategies with four
distinct lexicons: \textsc{Sen\-ti\-Word\-Net}~\cite{Esuli:06c},
\textsc{Word\-Net-\-Affect}~\cite{Strapparava:04},
\textsc{MPQA}~\cite{Wiebe:05}, and
\textsc{SenticNet}~\cite{Cambria:14}.  In all of these methods, the
authors first split an input message into a list of
\emph{micro-phrases} based on the occurrence of punctuation marks and
conjunctions.  Afterwards, they calculated the polarity score of each
of these segments and estimated the overall polarity of the whole
tweet by uniting the scores of its micro-phrases.
\citeauthor{Musto:14} obtained their best results (58.99\% accuracy on
the SemEval-2013 dataset) with the normalized-emphasized approach, in
which they averaged the polarity scores of segments' tokens, boosting
these values by 50\% for informative parts of speech; and regarded the
sum of the micro-phrase scores as the final overall polarity of the
microblog.

% the authors obtained their best results using the
% \textsc{SentiWordNet} lexicon of~\citet{Esuli:06c}

% Jurek et al., 2015
Another Twitter-aware system was presented by~\citet{Jurek:15}, who
computed the negative and positive polarity of a message ($F_p$ and
$F_n$ respectively) using the following equations: { \small%
  \begin{align}
    F_P &= \min\left(\frac{A_P}{2 - \log(3.5\times W_P + I_P)}, 100\right),\\
    F_N &= \max\left(\frac{A_N}{2 - \log(3.5\times W_N + I_N)}, -100\right);\label{cgsa:eq:jurek}
  \end{align}%
  \normalsize%
}%
where $A_P$ and $A_N$ represent the average scores of positive and
negative lexicon terms found in the tweet:
$A_p = \frac{\sum_{w\in\textrm{msg}}s^p_w}{\lVert\textrm{msg}\rVert}$,
with $s^p_w$ denoting the positive lexicon score of the term $w$;
$W_P$ and $W_N$ stand for the raw counts of polar tokens; and $I_P$
and $I_N$ denote the number of intensifiers preceding these words.  In
addition to that, prior to estimating the average values, the authors
modified the polarity scores $s_w$ of negated words by applying the
following heuristics: { \small%
  \begin{align}
neg(s_w) =
    \begin{cases}
        \min\left(\frac{s_w - 100}{2}, -10\right) & \text{if } s_w > 0,\\
        \max\left(\frac{s_w + 100}{2}, 10\right), & \text{if } s_w < 0.
    \end{cases}
\end{align}%
\normalsize%
}%
Besides computing the polarity scores $F_p$ and $F_n$,
\citeauthor{Jurek:15} also determined the subjectivity degree of the
message by replacing the terms $A_P$ and $A_N$ in
Equation~\ref{cgsa:eq:jurek} with averaged conditional probabilitites
of the tweet being subjective given the occurrences of the respective
polar terms.\footnote{These probabilities were calculated
  automatically on the noisily labeled data set of~\citet{Go:09}.}
The authors considered a microblog as positive if its overall polarity
score was greater than 25 and negative otherwise, provided that the
subjectivity value of this message was higher than 0.5.  With this
method, they achieved an accuracy of~77.3\% on the manually annotated
subset of \citeauthor{Go:09}'s corpus and reached 74.2\% on the IMDB
review corpus~\cite{Maas:11}.

% Kolchyna et al., 2015
Finally, \citet{Kolchyna:15} also explored two different ways of
computing the overall polarity of a microblog:
\begin{inparaenum}[(i)]
\item by simply averaging the scores of the lexicon terms found in the
  message and
\item by taking the signed logarithm of this average:
\end{inparaenum}
\begin{equation*}
  \text{Score}_{\log} =
  \begin{cases}
    \text{sign}(\text{Score}_{\text{AVG}})\log_{10}(|\text{Score}_{\text{AVG}}|) & %
    \text{if |Score}_{\text{AVG}}| > 0.1,\\
    0, & \text{otherwise};
  \end{cases}
\end{equation*}%
comparing theses approaches on the SemEval-2013
dataset~\cite{Nakov:13}.  The authors determined the final polarity
class of a tweet with the help of $k$-means clustering, which utilized
either of the above polarity scores as its features.  They showed that
the logarithmic strategy performed better than the simple average
solution, yielding an accuracy of 61.74\%.  In addition to that,
\citeauthor{Kolchyna:15} also checked whether these lexicon values
could serve as useful attributes for an ML-based method.  For this
purpose, they retrained a cost-sensitive SVM
classifier~\cite{Masnadi:12} after extending its $n$-gram feature set
with lexicon features, getting almost five percent accuracy
improvement (from 86.62 to 91.17) on the IMDB movie review
dataset~\cite{Pang:02}.

\todo[inline]{reimplement and describe the results of \citet{Hu:04},
  \citet{Taboada:11}, \citet{Musto:14}, \citet{Jurek:15}, and
  \citet{Kolchyna:15}}

In order to estimate the quality of these methods on the PotTS
dataset, we reimplemented the approaches suggested
by~\citet{Taboada:11}, \citet{Musto:14}, \citet{Jurek:15}, and
\citet{Kolchyna:15}, and applied these systems to the test set tweets
described in Section~\ref{sec:cgsa:data}, tweaking the threshold
values of the evaluated methods on the training and development data.
The results of this evaluation are shown in Table~\ref{snt-cgsa:tbl:lex-res}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

       HL &  &  &  & %
       &  &  & %
       &  &  & %
       & \\

       TBD &  &  &  & %
       &  &  & %
       &  &  & %
       & \\

       MST &  &  &  & %
       &  &  & %
       &  &  & %
       & \\

       JRK &  &  &  & %
       &  &  & %
       &  &  & %
       & \\

       KLCH &  &  &  & %
       &  &  & %
       &  &  & %
       & \\\bottomrule
\end{tabular}
    \egroup
    \caption[Evaluation of lexicon-based CGSA methods.]{
      Evaluation of lexicon-based CGSA methods.\\
      {\small HL~--~\citet{Hu:04}, TBD~--~\citet{Taboada:11}, MST~-- \citet{Musto:14}, JRK
        -- \citet{Jurek:15}, KLCH -- \citet{Kolchyna:15}}}
    \label{snt-cgsa:tbl:lex-res}
  \end{center}
\end{table}

\section{Machine-Learning Based Methods}\label{sec:cgsa:ml-based}

Despite their immense popularity, linguistic plausibility, and
simplicity to implement, lexicon-based approaches often have been
criticized for the rigidness of their classification\footnote{Since
  these systems only rely on the pre-computed weights of lexicon
  entries, considering these coefficients as constant, their decision
  boundaries frequently appear to be suboptimal as many terms might
  have different polarity and intensity values depending on the domain
  of the text \cite[see][]{Eisenstein:17,Yang:17}.} and the inability
to factor-in additional, non-lexical attributes into the final
decisions.  Moreover, as noted by~\citet{Pang:02} and also confirmed
empirically by~\citet{Riloff:03} and \citet{Gamon:04}, many of the
linguistic expressions which actually correlate with subjectivity and
polarity of a sentence (e.g., exclamation marks or) are very unlikely to be included into a
sentiment lexicon by a human expert.  As a consequence of this, with
the emergence of new manually annotated corpora, lexicon-based systems
have gradually been superseded by supervised machine-learning
techniques.

One of the first steps in this direction was taken
by~\citet{Wiebe:99}, who used a Na{\"i}ve Bayes classifier to
differentiate between subjective and objective statements.  Using
primarily binary features which reflected the presence of a pronoun,
an adjective, a cardinal number, or a modal verb in the analyzed
sentence, the authors achieved an accuracy of~72.17\%, outperforming
the majority class baseline by more than 20~percentage points.  An
even better result (81.5\%) could be attained when the dataset was
restricted only to the examples with the most confident annotations.

\done[inline]{\citet{Pang:02}}

Another notable contribution to the ML-based paradigm was made
by~\citet{Pang:02}, who compared Na{\"i}ve Bayes, Maximum Entropy, and
SVM systems on the polarity classification task for movie reviews,
getting their best results (82.9\% accuracy) with the support-vector
method that used unigram features only.  These findings paved the way
for the following general triumph of the SVM approach, which was
dominating the whole CGSA research field for almost a decade.  For
example, \citet{Gamon:04} trained this type of classifier on a rich
set of linguistic (part-of-speech trigrams, constituent-specific
length measures, context-free phrase-structure patterns, as well as
part-of-speech information coupled with semantic relations) and
surface features (primarily word $n$-grams) to distinguish between
positive and negative customer feedback.  The author achieved his best
results (77.5\% accuracy and $\approx$0.77~\F) using top 2,000
features with the highest log likelihood
ratios.  % Interestingly enough,
% \citeauthor{Gamon:04} also could obtain quite competitive figures
% (74.5\% accuracy) by using linguistically motivated features only.


\done[inline]{\citet{Ng:06}}

The results of~\citet{Pang:04} were further improved by~\citet{Ng:06}
who simultaneously addressed two classification problems:
distinguishing whether a given passage was a review or not and
determining the polarity of a review text.  The authors attained
impressive results (99.8\% accuracy) for the former task, using only
an SVM classifier with unigram features.  Moreover, they also
outperformed the then state of the art on the \citet{Pang:04}'s
corpus, boosting the accuracy from~87.1 to~90.5\%.  These changes were
mostly due to a smarter use of higher-order $n$-grams, where, instead
of bluntly considering all token sequences up to the order $n$ as new
features, \citet{Ng:06} only took 5,000 most useful ones, measuring
their utility with the weightes log-likelihood ratio \cite{Nigam:00}.

\done[inline]{\citet{Pang:05}}

\citet{Pang:05} addressed the problem of multi-class rating
prediction, whose goal, in contrast to the normal positive
vs. negative classification, was to determine the number of stars that
an author might assign to his review.  In their experiments,
\citeauthor{Pang:05} tried out three different strategies:
\begin{inparaenum}[(i)]
  \item one-versus-all SVM (OVA-SVM),
  \item SVM regression,
  \item and OVA-SVM with \emph{metric labeling}.
\end{inparaenum}
In the last approach, in addition to maximizing the score of the
correct labels, the authors also explicitly encoded the objective of
minimizing the absolute difference between the predicted labels of
similar training examples (measuring this similarity with the
percentage of positive sentences).  This strategy brought
statistically significant improvements over the first two baselines,
yielding an average accuracy of $\approx$52\%.


% \done[inline]{\citet{Riloff:03a}}

% \citet{Riloff:03a} addressed the problem of inusfficient manually
% annotated sentiment resources by proposing a bootstrapping method for
% training a subjectivity classifier.  In this approach, the authors
% first applied two high-precision predictors to a large collection of
% unlabeled sentences in order to get an initial set of subjective and
% objective instances.  Afterwards, they used the AutoSlog-TS
% algorithm~\cite{Riloff:96} to extract expressions which strongly
% correlated with the subjective class and employed these phrases to
% classify the remaining sentences from the corpus.
% \citeauthor{Riloff:03a} repeated the last two steps (pattern
% extraction and expansion of the training set) multiple times to
% transitively learn new subjective phrases.  With the final system, the
% authors achieved a precision of 0.902 on recognizing polar sentences,
% with their recall running up to 0.401.

\done[inline]{\citet{Yu:03}}

A semi-supervised classification approach was proposed
by~\citet{Yu:03}, who presented a three-stage method, in which they
first distinguished between subjective and objective documents, then
differentiated between polar and neutral sentences, and, finally,
classified the polarity of opinionated clauses.  The authors used a
Na{\"i}ve Bayes classifer for the document-level task, reaching a
remarkable \F-score of~0.96 on this objective; and applied an ensemble
of NB systems to predict the subjectivity of the sentences.  In the
final step, they determined the semantic orientation of a subjective
clause by averaging the polarity scores of their tokens, getting these
scores from an automatically constructed sentiment
lexicon~\cite{Hatzivassi:97}.  With this approach, \citeauthor{Yu:03}
attained an accuracy of~91\% on a set of 38 sentences which had a
perfect inter-annotator agreement in their data.

% \done[inline]{\citet{Wilson:04,Wilson:06}}

% A related problem, namely that of classifying the strength of
% opinions, was addressed by~\citet{Wilson:04,Wilson:06}.  In
% particular, the authors proposed a wide variety of linguistic features
% (including automatically learned lexico-syntactic patterns similar to
% the ones used by~\citet{Riloff:03}, bags of words, and syntactic
% attributes such as lemma and PoS tag of the root of a dependency tree,
% lemmas and tags of its intermediate nodes and leaves, lexicalized
% relation tuples, i.e., tuples consisting of the lemma of a parent
% node, its grammatical relation to the child, and the lemma of the
% child itself, etc.), checking the utility of these attributes with
% three different classifiers: BoosTexter~\cite{Schapire:00},
% Ripper~\cite{Cohen:95}, and SVMLight~\cite{Joachims:99}.
% \citet{Wilson:04} achieved their best results (55\%~accuracy and
% 0.991~mean squared error) with the BoosTexter approach when using all
% the introduced features.

\done[inline]{\citet{Pang:04}}

\citet{Pang:04} proposed a two-stage procedure for classifying the
overall semantic orientation of movie reviews (positive vs. negative):
In the first step of this approach, they classified the sentences into
subjective and objective ones (achieving 92\% accuracy for this task
with the Na\"ive Bayes system) and then applied another Na\"ive Bayes
classifier to determine the polarity of the review based on only
subjective excerpts, getting a statistically significant improvement
from~82.8 to~86.4\%.  The authors also experimented with the min-cut
method to encode the idea that nearby sentence will likely share the
same subjectivity class, but did not get any statistically significant
improvement over the discourse-unaware baseline.

\done[inline]{\citet{Cui:06}}

\citet{Cui:06} questioned the claim of~\citet{Pang:02} that
higher-order $n$-grams worsen the classification results of an opinion
mining system, hypothesizing that this finding was mostly due to the
limited training set size of the latter authors.  To check this
hypothesis, \citeauthor{Cui:06} collected a set of 320K product
reviews from \url{froogle.google.com} and trained three different NL
classifiers on these data: an online passive-agreessive
system~\cite{Crammer:06}, the winnow algorithm~\cite{Hurst:04}, and a
generative language model~\cite{Manning:99}.  The authors achieved
their best results with the first option, getting 0.9007 overall
\F-scores for both polarity classes and observing consistent
improvements with the increasing order of $n$-grams.  Moreover,
\citeauthor{Cui:06} also refuted another argument
of~\citet{Pang:04}---namely, the need for skipping objective sentences
while doing polarity classification---conjecturing that this step was
only useful for certain domains such as movie or book reviews, but
could not generalize to more technical genres.

\done[inline]{\citet{Wiebe:05a}}

A semi-supervised approach to sentence-level subjectivity prediction
was proposed by~\citet{Wiebe:05a}.  Using an existing rule-based
sentiment system, the authors classified a large set of unlabeled
sentences from newspaper articles into subjective and objective ones,
achieving 34.2\% recall and 90.4\% precision for the former class and
getting 30.7\% recall and 82.4\% precision for the latter orientation.
Afterwards, with the help of the AutoSlog-TS
algorithm~\cite{Riloff:96}, they extracted frequent lexico-syntactic
patterns that strongly correlated with the subjectivity of a sentence,
and trained a Na{\"i}ve Bayes system, considering the occurrences of
the learned patterns as features.  In an attempt to improve the
results of this approach even further, \citeauthor{Wiebe:05a} also
implemented a self-improvement strategy in which they relabeled the
original unannotated dataset with the obtained NB classifier, then
took one half of the most confidently classified sentences as a new
training set, and extracted new subjectivity patterns, retraining the
Na{\"i}ve Bayes module on the updated set of features.  With this
final classifier, the authors attained an accuracy of 73.8\% on
predicting the subjectivity of sentences in the MPQA
corpus~\cite{Wiebe:05}, coming close to the results achieved by a
fully supervised ML approach (76\%).

\done[inline]{\citet{Riloff:06}}

\citet{Riloff:06} addressed the problem of redundant features,
hypothesizing that correlated attributes (such as unigram
``\emph{happy}'' and bigram ``\emph{very happy}'') would rather harm
the performance of a classifier than improve its accuracy.  To prove
this hypothesis, the authors defined two kinds of redundancy relations
which could exist between intersecting input traits:
\emph{representational} and \emph{behavioral} subsumption.  The former
former type was assumed to hold between features~$A$ and $B$ if all
occurrences of the attribute~$A$ were a strict superset of the
occurrences of~$B$.  The behavioral subsumption meant that the
information gain (IG) \cite{Forman:03} of the feture $B$ was at most
some negligible value $\delta$ lower than the respective IG score of
the attribute $A$.  In their experiments, \citeauthor{Riloff:06}
observed that the accuracy of an SVM classifier indeed increased (from
81.7 to 82.7\% on the IMDBP dataset and from 74.4 to 74.9\% on the
MPQA corpus) after they excluded all attributes that were
representationally and behaviorally subsumed by other features.  These
improvements even outperformed the gains that could be achieved with
traditional feature selection methods.

\done[inline]{\citet{Bickerstaffe:10}}

\citet{Bickerstaffe:10} also considered the rating prediction task,
addressing this problem with the minimum-spanning-tree (MST) SVM
approach.  In the initial step of this method, they constructed a
strongly connected graph whose vertices were associated with the most
representative example (determined via the average all-pairs Tanimoto
coefficient) of each star rating and the edge weights represented the
Tanimoto distances between those nodes.  Afterwards, they determined
the MST of this graph using the Kruskal's
algorithm~\cite[see][pp.~567--574]{Cormen:09} and, finally,
constructed a decision tree from this MST, replacing the MST vertices
with binary SVM classifiers, which had to discern the respective
rating groups. An evaluation on the four-star review corpus
of~\citet{Pang:05} showed an improvement by up to~7\% over the
previous state of the art, boosting it to 59.37\% average accuracy.

\done[inline]{\citet{Li:10b}}

\citet{Li:10b} addressed the problem of polarity shifting using
machine-learning techniques.  For this purpose, the authors first
selected most frequent and indicative features of the two main
polarity classes (positive and negative), and then culled training
instances containing these attributes whose labels, however, were
different from the ones sugested by the features.  After obtaining
this polarity shifted subset, \citeauthor{Li:10b} trained several
linear support-vector classifiers, one of which had to distinguish
between polarity-shifted and polarity-preserving sentences, the other
two were to classify the semantic orientation of these two groups
(i.e., one system had to predict the polarity of shifted instances,
and the other one had to determine the semantic orientation of
polarity-preserving ones), and the last one was trained on the
complete original dataset---the product review corpus
of~\citet{Blitzer:06}---again to predict the polarity of complete
sentences disregarding their possible polarity shifts.  The authors
achieved their best results~(80,9\% average accuracy) using a
combination of the last three systems with a special meta-classifier
joining their single decisions.

\done[inline]{\citet{Mejova:11}}

\citet{Mejova:11} investigated the effect of different features on
various datasets---the movie review corpus of~\citet{Pang:04}, the
product reviews gathered by~\citet{Jindal:07}, and the customer
feedback dataset of~\citet{Blitzer:06}, coming to the conclusion that,
in general, preserving the original form of tokens (i.e., keeping the
original token forms instead of lemmas) and using their frequency
scores instead of binary values was beneficial to the results on all
test sets.  The use of different $n$-gram lengths, however, had a
mixed effect with the best scores typically yielded by the union of
uni-, bi-, and tri-gram features.  Last but not least, they found the
negation heuristics proposed by~\citet{Das:01} (adding the
\texttt{\_NOT} suffix to all tokens following a negation up to the
first punctuation) leading to only marginal improvements.  The authors
achieved their best results (87.5\%, 94.7\%, and 89.6\% accuracy on
the datasets of \citet{Pang:04}, \citet{Jindal:07}, and
\citet{Blitzer:06}, respectively) with the union of unnormalized uni-,
bi-gram and tri-gram features without negation when using term
frequencies as feature values.


\done[inline]{SemEval 2013}

\done[inline]{\citet{Mohammad:13}}

Finally, with the introduction of the SemEval competition in sentiment
analysis of
Twitter~\cite{Nakov:13,Rosenthal:14,Rosenthal:15,Nakov:16}, a plethora
of new CGSA applications have appeared on the scientific scene, most
of which relied on traditional supervised machine-learning techniques.
One of the most prominent and arguably well-known such systems was
proposed by~\citet{Mohammad:13}, who trained a linear SVM classifier
on an extensive set of linguistic features including character and
token $n$-grams, Brown clusters~\cite{Brown:92}, statistics on
part-of-speech tags, punctuation marks, elongated words etc.  However,
the most useful type of attributes, as noted by the authors, turned
out to be features that reflected information from various sentiment
lexicons.  In particular, depending on the type of the polarity list
from which such information was extracted, the authors introduced two
types of lexicon attributes: \emph{manual} and \emph{automatic} ones.
The former group was computed with the help of the NRC emotion
lexicon~\cite{Mohammad:13a}, MPQA polarity list~\cite{Wilson:05}, and
Bing Liu's manually compiled polarity set~\cite{Hu:04}.  For each of
these resources and for each of the non-neutral polarity classes
(positive and negative), \citeauthor{Mohammad:13} calculated the total
sum of the lexicon scores for all message tokens and also separately
estimated these statistics for each particular part-of-speech tag,
considering them as additional attributes.  Automatic features were
obtained using automatically generated Sentiment140 and Hashtag
Sentiment Base polarity lists \cite{Kiritchenko:14}.  Again, for each
of these lexicons, for each of the two polarity classes, the authors
produced four features representing the number of tokens with non-zero
scores, the sum and the maximum of all respective lexicon values for
all tweet tokens, and the score of the last term.  This final system
ranked first in the inaugural run of the SemEval task, attaining
0.69~\F-score on the three-way classification task, with the automatic
lexicon features alone accounting for at least five percent boost in
the performance of this model.

\done[inline]{\citet{Guenther:13}}

A similar solution (linear SVM with a rich set of features) was
presented by~\citet{Guenther:13}.  Akin to~\citet{Mohammad:13}, the
authors used unmodified and lemmatized unigrams, word clusters, and
lexicon features.  However, in contrast to the previous system, this
application utilized only one polarity list---that
of~\citet{Esuli:05}.  Partially due to this fact,
\citeauthor{Guenther:13} found the word cluster attribute working best
among all features, followed by the stemmed unigram attributes.  This
method also yielded competitive results (0.653~\F) on the
message-level polarity task, attaining second place in this
competition.

\done[inline]{SemEval 2014}

\done[inline]{\citet{Miura:14}}

Later on these results were further improved by~\citet{Miura:14}, who
also utilized a supervised ML approach with character and word
$n$-grams, word clusters, disambiguated senses, and lexicon scores of
message tokens.  Similarly to the NRC-Canada system
of~\citet{Mohammad:13}, the authors made heavy use of various kinds of
polarity lists including AFINN-111~\cite{Nielsen:11}, Liu's Opinion
Lexicon~\cite{Hu:04} , General Inquirer~\cite{Stone:66}, MPQA Polarity
List \cite{Wiebe:05a}, NRC Hashtag and Sentiment140
Lexicon~\cite{Mohammad:13}, and
\textsc{SentiWordNet}~\cite{Esuli:06a}.  However, contrary to
\citeauthor{Mohammad:13}'s approach, \citeauthor{Miura:14} also
applied a whole set of preprocessing steps such as spelling
correction, part-of-speech tagging with lemmatization, and a special
weighting scheme for underrepresented polarity classes.  These
enhancements, combined with a carefully tuned LogLinear classifier,
allowed the authors to boost the sentiment classification results on
SemEval~2014 test set to 0.71 average \F (measure on the two main
polarity classes---positive and negative).

\done[inline]{\citet{Guenther:14}}

In the same vein, \citet{Guenther:14} improved their results from the
previous SemEval run (from 0.654 to 0.691 two-class \F) by extending
their original system with a Twitter-aware
tokenizer~\cite{Owoputi:13}, spelling normalization module, and a
significantly increased of lexicon-based features (this time, instead
of simply relying on the \textsc{SentiWordNet} resource,
\citeauthor{Guenther:14} harnessed a whole ensemble of various
polarity lists including Liu's opinion list, MPQA subjectivity
lexicon, and TwittrAttr polarity resource).  As proved by the ablation
tests, the last change was of particular use to the classification
accuracy, improving the scores by almost five percent.

\todo[inline]{SemEval 2015}

\done[inline]{\citet{Hagen:15}}

A different approach to the coarse-grained sentiment-analysis task was
proposed by~\citet{Hagen:15}, who, instead of developing their own
method from scratch, united four already existing solutions
(\citet{Mohammad:13}, \citet{Guenther:13}, \citet{Proisl:13}, and
\citet{Miura:14}) into a single ensemble, taking the average of the
predicted scores as the final decision of the complete system.  This
way, the authors achieved 0.648~\F{} on the SemEval-2015 test set,
attaining first place among 40 participants.

\todo[inline]{\citet{Hamdan:15}}

\todo[inline]{SemEval 2016}
\todo[inline]{\citet{Deriu:16}}
\todo[inline]{\citet{Rouvier:16}}
\todo[inline]{\citet{Xu:16}}


\todo[inline]{}

To the best of our knowledge, the idea of utilizing web texts
containing emoticons as noisily labeled training data was first
proposed by~\citet{Read:05}, who collected a set of 26,000 Usenet
posts featuring smileys or frownies and used these documents to train
a Na{\"i}ve Bayes and SVM classifier.  The author demonstrated that,
despite some encouraging results obtained on the instances from the
same domain (up to 70\% accuracy), the trained systems did not
generalize well to other text genres, barely outperforming the chance
baseline and reaching a maximum accuracy of~54.4\% on news data and
56.8\% on movie reviews.

The presumably first known attempt to adopt distant supervision for
the sentiment analysis of Twitter data was made by~\citet{Go:09} who
collected a set of 800,000 positive and 800,000 negative microblogs
relying on emoticons as their noisy labels.  After stripping off these
smileys from text, the authors trained three independent
ML-classifiers (Na{\"i}ve Bayes, Maximum Entropy, and Support Vector
Machines) on this collection, achieving their best results (82.7\%
accuracy) with the NB and MaxEnt systems thaat utilized unigrams and
bigrams as features.

Another distantly supervised approach was presented
by~\citet{Barbosa:10}, who gathered a collection of automatically
labeled tweets from three popular sentiment web sites (Twendz, Twitter
Sentiment, and TweetFeel), and trained two binary SVM systems on this
corpus.  The first of these classifiers had to distinguish between
subjective and objective microblogs, attaining an error rate of~18.1\%
on a subset of 1,000 manually annotated messages.  In the next step,
the second system had to determine the semantic orientation of
opinionated posts (positive or negative), reaching an error rate
of~18.7\% on this prediction.

In a similar way, \citet{Pak:10} gathered a collection of 300,000
noisily labeled tweets, ensuring an even distribution of positive,
negative, and neutral messages.  After a brief exploration of PoS tag
statistics in these different classes, they presented a Na{\"i}ve
Bayes system which utilized highly relevant binary part-of-speech and
$n$-gram features.\footnote{\citet{Pak:10} determined the relevance of
  a feature $f$ using a special \emph{salience} metric, which was
  defined as a negative ratio between the minimum and maximum
  conditional probabilities of this feature belonging to different
  target classes:
  \begin{equation*}
    salience(f) = \frac{1}{N}\sum_{i=1}^{N-1}\sum_{j=i+1}^N 1 - \frac{\min(P(f, s_i), P(f, s_j))}{\max(P(f, s_i), P(f, s_j))},
  \end{equation*}
  where the $N$~term denotes the number of training examples, and
  $s_i$ means the sentiment class of the $i$-th training instance.}
With this approach, the authors attained an accuracy slighlty above
0.6 on the manually labeled test set of~\citet{Go:09}, also
demonstrating a particular utility of bigrams, negation rules, and
feature pruning heuristics.

A slightly different task was addressed by~\citet{Davidov:10}, who
sought to predict hashtags and emoticons occurring in tweets using a
$k$-NN classifier trained on a large collection of messages.  The
authors achieved an \F-measure of~0.31 on the former task, and reached
an \F-score of~0.64 on predicting smileys.

\citet{Kouloumpis:11} trained an AdaBoost
classifier~\cite{Schapire:00} on two large collections of noisily
labeled tweets---the emoticon tweebank of~\citet{Go:09} and the
Edinburgh hashtag corpus.\footnote{\url{http://demeter.inf.ed.ac.uk}}
Using $n$-gram (up to length two), lexicon, part-of-speech, and
micro-blogging features (such as emoticons, abbreviations, and slang
expressions), the authors achieved a macro-averaged \F-measure of~0.68
on the three-class prediction task.

% One of the first attempts to analyze message-level sentiments on
% Twitter was made by \citet{Go:09}.  For their experiments, the authors
% collected a set of 1,600,000 tweets containing smileys.  Based on
% these emoticons, they automatically derived polarity classes for these
% messages (positive or negative) and used them to train a Na\"{\i}ve
% Bayes, MaxEnt, and SVM classifier.  The best $F$-score for this
% two-class classification problem could be achieved by the last system
% and run up to 82.2\%.

% Similar work was also done by \citet{Pak:10} who used the Na\"{\i}ve
% Bayes approach to differentiate between neutral, positive, and
% negative microblogs; and \citet{Barbosa:10} who gathered a collection
% of 200,000 tweets, subsequently analyzing them with three publicly
% available sentiment web-services and training an SVM classifier on the
% results of these predictors.  In a similar way, \citet{Agarwal:11}
% compared a simple unigram-based SVM approach with two other
% full-fledged systems, one which relied on a rich set of manually
% defined features, and another used partial tree
% kernels~\cite{Moschitti:06}.  The authors evaluated these methods on a
% commercially acquired corpus of 8,753 foreign-language tweets, which
% were automatically translated into English, finding that a combination
% of these methods worked best for both two- and three-way prediction
% tasks.

% The state-of-the-art results for message level polarity prediction on
% tweets were established by~\citet{Mohammad:13}, whose system (a
% supervised SVM classifier) used a rich set of various features
% including word and character n-grams, PoS statistics, Brown
% clusters~\cite{Brown:92}, etc., and also strongly benefitted from
% automatic corpus-based polarity lists---Sentiment~140 and NRC
% Hashtag~\cite{Mohammad:12,Kiritchenko:14}.  This approach ranked first
% at the SemEval competition~2013~\cite{Nakov:13} and anchieved the
% fourth place on the rerun of this task one year
% later~\cite{Rosenthal:14}, being outperformed by the supervised
% logistic regression approach of~\citet{Miura:14}, who used a heavy
% preprocessing of the data and a special balancing scheme for
% underrepresented classes.  Later on, these results were further
% improved by the apporaches of~\citet{Hagen:15} and \citet{Deriu:16},
% which both relied on ensembles of multiple independent classifiers.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

       &  &  &  & %
       &  &  & %
       &  &  & %
       & \\\bottomrule
    \end{tabular}
    \egroup
    \caption[Evaluation of ML-based coarse-grained SA methods.]{
      Evaluation of ML-based coarse-grained SA methods.\\
      {\small }}
    \label{snt-cgsa:tbl:ml-res}
  \end{center}
\end{table}

\section{Deep-Learning Based Methods}\label{sec:cgsa:dl-based}

\todo[inline]{Lin, 1998}

Since there was not a complete set of such expressions, it provided
some seeds and then used distributional similarity (Lin, 1998) to find
similar words, which were also likely to be subjectivity
indicators. However, words found this way had low precision and high
recall.

\todo[inline]{\citet{Bespalov:11}}

In (Bespalov et al., 2011), sentiment classification was performed
based on supervised latent n-gram analysis.

\todo[inline]{\citet{Zhou:10}}

\citet{Yessenalina:11}

A real breakthrough in the use of deep neural networks for the
sentence-level sentiment analysis happened with the pioneering work
of~\citet{Socher:11}, who first introduced a recursive autoencoder
(RAE).  In this system, the authors obtained a fixed-width vector
representation for complex phrases $\vec{v}$ by recursively merging
the vectors of adjacent tokens (say $\vec{w}_1$ and $\vec{w}_2$),
first multiplying these vectors with a compositional matrix $W$ and
then applying a non-linear function ($softmax$) to the resulting
product:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}\right)
\end{align*}
Using a max-margin classifier on top of the resulting phrase
representation, \citet{Socher:11} could improve the state-of-the-art
results on predicting the sentence-level polarity of user's blog
posts~\cite{Potts:10} and also outperformed the system
of~\citet{Nasukawa:03} on the MPQA data set~\cite{Wiebe:05}.

Later on, \citet{Socher:12} further improved these scores with the
help of a recursive matrix-vectors space model (RMVSM), in which each
word was associated with a 2-tuple of a vector and matrix---e.g.,
$(\vec{w}_1, W_1)$ and $(\vec{w}_2, W_2)$---and the compositionality
function was redefined as follows:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  W_2\cdot\vec{w}_1\\
  W_1\cdot\vec{w}_2
  \end{bmatrix}\right)
\end{align*}


\todo[inline]{\citet{Tang:14b}}

In this paper, we develop a deep learning system for message-level
Twitter sentiment classification. Among the 45 submitted systems
including the SemEval 2013 participants, our system (Coooolll) is
ranked 2nd on the Twitter2014 test set of SemEval 2014 Task
9. Coooolll is built in a supervised learning framework by
concatenating the sentiment-specific word embedding (SSWE) features
with the state-of-the-art hand-crafted features.  We develop a neural
network with hybrid loss function 1 to learn SSWE, which encodes the
sentiment information of tweets in the continuous representation of
words.  To obtain large-scale training corpora, we train SSWE from 10M
tweets collected by positive and negative emoticons, without any
manual annotation. Our system can be easily re-implemented with the
publicly available sentiment-specific word embedding.

In this paper, we develop a deep learning system for Twitter
sentiment classification. Firstly, we learn sentiment-specific word
embedding (SSWE) (Tang et al., 2014), which encodes the sentiment
information of text into the continuous representation of words
(Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate
the SSWE features with the state-of-the-art hand-crafted features
(Mohammad et al., 2013), and build the sentiment classifier with the
benchmark dataset from SemEval 2013 (Nakov et al., 2013). To learn
SSWE, we develop a tailored neural network, which incorporates the
supervision from sentiment polarity of tweets in the hybrid loss
function. We learn SSWE from tweets, leveraging massive tweets with
emoticons as distantsupervised corpora without any manual annota-
tions.

Our system (Coooolll) is ranked 2nd on the Twitter2014 test set, along
with the SemEval 2013 participants owning larger training data than
us. The performance of only using SSWE as features is comparable to
the state- of-the-art hand-crafted features (detailed in Table 3),
which verifies the effectiveness of the sentiment-specific word
embedding.

In our system, the feature representation of tweet is composed of two
parts, the sentiment-specific word embedding features (SS-WE features)
and the state-of-the-art hand-crafted features (STATE features).  In
this part, we first describe the neural network for learning
sentiment-specific word embedding.  Then, we generate the SSWE
features of a tweet from the embedding of words it contains.  Our
neural network is an extension of the traditional C\&W model
(Collobert et al., 2011), as illustrated in Figure 2. Unlike C\&W
model that learns word embedding by only modeling syntactic contexts
of words, we develop SSWE u , which captures the sentiment information
of sentences as well as the syntactic contexts of words. Given an
original (or corrupted) ngram and the sentiment polarity of a sentence
as the input, SSWE u predicts a two-dimensional vector for each
input ngram.  The two scalars (f 0 u , f 1 u ) stand for language
model score and sentiment score of the input ngram

We re-implement the state-of-the-art hand-crafted
features (Mohammad et al., 2013) for Twitter sen-
timent classification. The STATE features are de-
scribed below.

-- All-Caps. The number of words with all char-
acters in upper case.

-- Emoticons. We use the presence of positive
(or negative) emoticons and whether the last
unit of a segmentation is emoticon 3 .

-- Elongated Units. The number of elongated
words (with one character repeated more than
two times), such as gooood.

-- Sentiment Lexicon. We utilize several senti-
ment lexicons 4 to generate features. We ex-
plore the number of sentiment words, the s-
core of last sentiment words, the total senti-
ment score and the maximal sentiment score
for each lexicon.

-- Negation. The number of individual nega-
tions 5 within a tweet.

-- Punctuation. The number of contiguous se-
quences of dot, question mark and exclama-
tion mark.

-- Cluster. The presence of words from each
of the 1,000 clusters from the Twitter NLP
tool (Gimpel et al., 2011).

-- Ngrams. The presence of word ngrams (1-4)
and character ngrams (3-5).

On positive/negative/neutral classification of tweets as listed in
Table 3 (left table), we find that the learned sentiment-specific word
embedding features (SSWE) performs comparable with the
state-of-the-art hand-crafted features (STATE), especially on the
Twitter-relevant test sets (T3 and T4) 7 . After feature combination,
Coooolll yields 4.22\% and 3.07\% improvement by macroF1 on T3 and
T4,which verifies the effectiveness of SSWE by learning discriminate
features from massive data for Twitter sentiment classifi-
cation. From the 45 teams, Coooolll gets the Rank 5/2/3/2 on T1-T4
respectively, along with the SemEval 2013 participants owning larger
training data. We also comparing SSWE with the contextbased word
embedding (W2V), which don't capture the sentiment supervision of
tweets. We find that W2V is not effective enough for Twitter sen-
timent classification as there is a big gap between W2V and SSWE on
T1-T4. The reason is that W2V does not capture the sentiment
information of text, which is crucial for sentiment analysis tasks and
effectively leveraged for learning the sentiment-specific word
embedding.

We also conduct experiments on the positive/negative classification of
tweets. The reason is that the sentiment-specific word embedding is
learned from the positive/negative supervision of tweets through
emoticons, which is tailored for positive/negative classification of
tweets. From Table 3 (right table), we find that the performance of
positive/negative Twitter classification is consistent with the
performance of 3-class classification. SSWE performs comparable to
STATE on T3 and T4, and yields better performance (1.62\% and 1.45\%
improvements on T3 and T4, respectively) through feature
combination. SSWE outperforms W2V by large margins (more than 10\%)
on T3 and T4, which further verifies the effectiveness of
sentiment-specific word embedding.


\todo[inline]{\citet{Severyn:15}}

An important breakthrough in the usage of deep learning methods for
sentiment analysis on Twitter happened with the
work~\citet{Severyn:15}, whose proposed feed-forward DL system ranked
first in Subtask~10-A (phrase-level polarity prediction) at
SemEval~2015 \cite{Rosenthal:15} and got second place in Subtask~10-B
(message-level classification) of this competition.  Drawing on the
ideas of~\citet{Kalchbrenner:14}, the authors devised a simple
convolutional neural network in which they first passed pre-trained
word2vec embeddings to a set of 300 convolutional kernels (each of
width five), and then fed the max-pooled results of this
multiplication to a densely connected softmax layer.  Two important
aspects of this approach, which accounted for a huge part of its
success, were
\begin{inparaenum}[(i)]
\item that the word2vec vectors were used solely for initialization,
  but were then modified along with other model's paramaters during
  training, and
\item that \citeauthor{Severyn:15} extended to the original SemEval
  training set with an additional, much larger noisily labeled corpus.
\end{inparaenum}
These improvements allowed the authors to establish a new state of the
art on the phrase-level polarity prediction task with a macro-averaged
\F-score of 0.8479, also getting a competitive result of 0.6459~\F{}
on classifying the semantic orientation of messages.


This paper describes our deep learning system for sentiment analysis
of tweets. The main contribution of this work is a process to ini-
tialize the parameter weights of the convolutional neural network,
which is crucial to train an accurate model while avoiding the need to
inject any additional features. Briefly, we use an unsupervised neural
language model to initialize word embeddings that are further tuned by
our deep learning model on a distant supervised corpus. At a final
stage, the pre-trained parameters of the network are used to
initialize the model which is then trained on the supervised training
data from Semeval-2015. According to results on the official test
sets, our model ranks 1st in the phrase-level subtask A (among 11
teams) and 2nd on the messagelevel subtask B (among 40
teams). Interestingly, computing an average rank over all six test
sets (official and five progress test sets) puts our system 1st in
both subtasks A and B.

Turns out, providing the network with good initialisation parameters
makes all the difference in training an accurate model. We propose a
three-step process we follow to train our deep learning model for
sentiment classification. It can be summarized as follows: (i) word
embeddings are initialized using a neural language model (Ronan
Collobert, 2008; Mikolov et al., 2013) which is trained on a large un-
supervised collection of tweets; (ii) we use our convolutional
neural network to further refine the embeddings on a large distant
supervised corpus (Go et al., 2009); (iii) the word embeddings and
other parameters of the network obtained at the previous stage are
used to initialize the network that is then trained on a supervised
corpus from Semeval-2015.

We apply our deep learning model on two subtasks of Semeval-2015
Twitter Sentiment Analysis (Task 10) challenge (Rosenthal et al.,
2015): phraselevel (subtask A) and message-level (subtask B).  Our
system ranks 1st on the official test set of the phrase-level and 2nd
on the message-level subtask.  In addition to the test set used to
establish the final ranking in Semeval-2015, all systems were also
evaluated on the progress test set which consists of five test sets,
where our system also shows strong results. In particular, we rank all
systems according to their performance on each test set and compute
their average ranks. Interestingly, our model appears to be the most
robust across all six test sets ranking 1st according to the average
rank in both subtasks A and B.

It is mainly inspired by the architectures used in (Kalchbrenner et
al., 2014; Kim, 2014) for performing various sentence classification
tasks. Given that our training process (described in Sec. 3.3) re-
quires to run the network on a rather large corpus, our design choices
are mainly driven by the computational efficiency of our
network. Hence, different from (Kalchbrenner et al., 2014) that
presents an architecture with several layers of convolutional feature
maps, we adopt a single level architecture, which has been shown in
(Kim, 2014) to perform equally well.  Our network is composed of a
single convolu- tional layer followed by a non-linearity, max pooling
and a soft-max classification layer.

The input to our model are tweets each treated as a
sequence of words: [w i , .., w |s| ], where each word
is drawn from a vocabulary V . Words are repre-
sented by distributional vectors w $\in$ R d looked
up in a word embeddings matrix W $\in$ R d$\times$|V | .
This matrix is formed by concatentating embeddings
of all words in V . For convenience and ease of
lookup operations in W, words are mapped to in-
dices $1,\ldots, |V|$.

For each input tweet s we build a sentence matrix
$S \in R d\times |s|$, where each column $i$ represents a word
embedding $w_i$ at the corresponding position $i$ in a sentence.

To allow the network learn non-linear decision boundaries, each
convolutional layer is typically followed by a non-linear activation
function $\alpha$() applied element-wise. Among the most common
choices of activation functions are: sigmoid (or logistic), hyperbolic
tangent tanh, and a rectified lin- ear (ReLU) function defined as
simply $max(0, x)$ to ensure that feature maps are always positive.
We use ReLU in our model since, as shown in (Nair and Hinton, 2010),
it speeds up the training and sometimes produces more accurate
results.

The output from the convolutional layer (passed through the activation
function) are then passed to the pooling layer, whose goal is to
aggregate the in- formation and reduce the representation.

Among the most popular choices for pooling operation are: max and
average pooling. Recently, max pooling has been generalized to k-max
pooling (Kalchbrenner et al., 2014), where instead of a single max
value, k values are extracted in their original order. We use max
pooling in our model which simply returns the maximum value. It
operates on columns of the feature map matrix C returning the largest
value.

The output of the penultimate convolutional and pooling layers x is
passed to a fully connected softmax layer.  To mitigate the
overfitting issue we augment the cost function with $l_2$ -norm
regularization terms for the parameters of the network.  We also adopt
another popular and effective technique to improve regularization of
the NNs---dropout (Srivastava et al., 2014). Dropout prevents feature
co-adaptation by setting to zero (dropping out) a portion of hidden
units during the forward phase when computing the activations at the
soft- max output layer. As suggested in (Goodfellow et al., 2013)
dropout acts as an approximate model averaging.

Convolutional neural networks live in the world of non-convex function
optimization leading to locally optimal solutions. Hence, starting the
optimization from a good point can be crucial to train an accurate
model. We propose the following 3-step process to initialize the
parameter weights of the network: 1. Given that the largest parameter
of the network is the word matrix W, it is crucial to feed the network
with the high quality embeddings.  We use a popular word2vec neural
language model (Mikolov et al., 2013) to learn the word embeddings on
an unsupervised tweet corpus.  For this purpose, we collect 50M tweets
over the two-month period. We perform minimal preprocessing
tokenizing the tweets, normalizing the URLs and author ids. To train
the embeddings we use a skipgram model with window size 5 and
filtering words with frequency less than 5.  2. When dealing with
small amounts of labelled data, starting from pre-trained word
embeddings is a large step towards successfully training an accurate
deep learning system. However, while the word embeddings obtained at
the previous step should already capture important syntactic and
semantic aspects of the words they represent, they are completely
clueless about their sentiment behaviour. Hence, we use a distant
supervision approach (Go et al., 2009) using our convolutional
neural network to further refine the embeddings.  3. Finally, we take
the the parameters $\theta$ of the network obtained at the previous step
and use it to

Additionally, to pre-train the weights of our network, we use a
large unsupervised corpus containing 50M tweets for training the word
embeddings and a 10M tweet corpus for distant supervision. The lat-
ter corpus was built similarly to (Go et al., 2009), where tweets with
positive emoticons, like `:)', are assumed to be positive, and tweets
with negative emoticons, like `:(', are labeled as negative. The
dataset contains equal number of positive and negative tweets.

The parameters of our model were (chosen on the validation set) as
follows: the width m of the convolution filters is set to 5 and the
number of convolutional feature maps is 300. We use ReLU activation
function and a simple max-pooling. The L2 regularization term is set
to 1e-4, dropout is applied to the penultimate level with p = 0.5. The
dimensionality of the word embeddings d is set to 100. For the
phrase-level subtask the size of the word type embeddings, which
encode tokens that span the target phrase or not, is set to 10.

Table 2 summarizes the performance of our model on five test sets
using three parameter initialization schemas. First, we observe that
training the network with all parameters initialized completely at
random results in a rather mediocre performance. This is due to a
small size of the training set. Secondly, using embeddings pre-trained
by a neural language model considerably boosts the
performance. Finally, using a large distant supervised corpus to
further tune the word embeddings to also capture the sentiment as-
pect of the words they represent results in a further improvement
across all test sets.

This system achieved 64.59 two-class \F on SemEval-2015 test set,
getting second place in this competition.

\todo[inline]{\citet{Wang:15}}

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

       &  &  &  & %
       &  &  & %
       &  &  & %
       & \\\bottomrule
    \end{tabular}
    \egroup
    \caption[Evaluation of DL-based coarse-grained SA methods.]{
      Evaluation of DL-based coarse-grained SA methods.\\
      {\small }}
    \label{snt-cgsa:tbl:ml-res}
  \end{center}
\end{table}

\section{Coarse-Grained Sentiment Analysis Using Language and Domain
  Adaptation}\label{sec:cgsa:domain-adaptation}

One of the first works which pointed out the importance of domain
adaptation for sentiment analysis was introduced by~\citet{Aue:05}.
In their experiments, the authors trained separate SVM classifiers on
four different document sets: movie reviews, book reviews, customer
feedback from a product support service, and a feedback survey from a
customer knowledge base; finding that each classifier performed best
when applied to the same domain as it was trained on.  In order to
find an optimal way of overcoming this domain specificity,
\citet{Aue:05} tried out four different options:
\begin{inparaenum}[(i)]
\item\label{sent-cgsa:lst:rel-wrk1} training one classifier on all but
  the target domain and applying it to the latter;
\item using the same procedure as above, but limiting the features to
  only those which also appeared in the target texts;
\item taking an ensemble of individual classifiers each of which was
  trained on a different data collection; and, finally,
\item using a minimal subset of labeled in-domain data to train a
  Na{\"i}ve Bayes system with the expectation-maximization algorithm
  \cite[EM;][]{Dempster:77}.
\end{inparaenum}
The authors found that the ensemble and EM options worked best for
their cross-domain task, achieving an accuracy of up to 82.39\% for
the two-class prediction (positive vs negative) on new unseen text
genres.

Another notable milestone in the domain adaptation research was set
by~\citet{Blitzer:07}.  Relying on their previous work on structural
correspondence learning~\cite{Blitzer:07}, in which they used a set of
\emph{pivot features} (features which frequently appeared in both
target and source domains) to find an optimal correspondence of the
remaining attributes,\footnote{In particular, the authors trained $m$
  binary predictors for each of their $m$ pivot features in order to
  find other attributes which frequently co-occurred with the pivots.
  Afterwards, they composed these $m$ resulting weight vectors into a
  single matrix $W := [\vec{w}_{1},\ldots,\vec{w}_{m}]$, took an SVD
  decomposition of this matrix, and used the top $h$ left singular
  vectors to translate source features to the new domain.} the authors
refined their method by pre-selecting the pivots using their PMI
scores and improving misaligned feature projections using a small set
of labeled target examples.  With these modifications,
\citeauthor{Blitzer:07} were able to reduce the average adaptation
loss (the accuracy drop when transferring a classifier to a different
domain) from 9.1 to 4.9~percent when testing a sentiment predictor on
the domains of book, dvd, electical appliances, and kitchen reviews.

Other important works on domain adaptation for opinion mining include
those of~\citet{Read:05}, who pointed out that sentiment
classification might not only depend on the domain but also on topic,
time, and language style in which the text was written;
\citet{Tan:07}, who proposed using the classifier trained on the
source domain to classify unlabeled instances from the target genre,
and then iteratively retrain the system on the enriched data set.
Finally, \citet{Andreevskaia:08} proposed a combination of a lexicon-
and ML-based systems, claiming that this ensemble would be more
resistible to the domain shift than each of these classifiers on their
own.

Another line of research was introduced by~\citet{Glorot:11} who
proposed stacked denoising autoencoders (SDA)---a neural network
architecture in which an input vector $\vec{x}$ was first mapped to a
smaller representation $\vec{x}'$ via some function
$h: \vec{x}\mapsto\vec{x}'$, and then restored to its approximate
original state via an inverse transformation
$g: \vec{x}'\mapsto\vec{x}''\approx\vec{x}$.  In their experiments,
the authors optimized the parameters of the functions $h$ and $g$ on
both target and source data, getting approximate representations of
instances from both data sets; and then trained a linear SVM
classifier on the restored representations of the source instances,
subsequently applying this classifier to the target domain.  This
approach was further refined by~\citet{Chen:12} who analytically
computed the reconstruction function~$g$, and used both original and
restored features to predict the polarity labels of the target
data.\footnote{Both approaches were trained tested on the Amazon
  Review Corpus of~\citet{Blitzer:07}.}


Further notable contributions to domain adaptation in general were
made by~\citet{Daume:07} who proposed to replicate each extracted
feature three times and train the first replication on both domains,
the second repetion only on source, and the third copy only on target
domain, for which he assumed a small subset of labeled examples was
available; \citet{Yang:15} who trained neural embeddings of features,
trying to predict which instance attributes frequently co-occured with
each other;

\section{Evaluation}
\subsection{Effect of Lexicons}
\subsection{Effect of Distant Supervision}
\subsection{Effect of Word Embeddings}
\subsection{Effect of Normalization}

\section{Summary and Conclusions}\label{slsa:subsec:conclusions}
