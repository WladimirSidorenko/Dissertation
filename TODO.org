* DONE Revise the Introduction Chapter

* DONE Write Sentiment Corpus Chapter
** DONE Write Related Work
   DEADLINE: <2016-05-06 Fr>
** DONE Write Summary and Conclusion
   DEADLINE: <2016-05-09 Mo>
** DONE Revise the Chapter
   DEADLINE: <2016-05-09 Mo>
** DONE Sent the Chapter to the Supervisor
   DEADLINE: <2016-05-10 Tue>
** DONE Incorporate Supervisor's Feedback


* DONE Write Sentiment Lexicon Chapter
** DONE Make Scaffold
   DEADLINE: <2016-05-18 Mi>

** DONE Write Introduction

** DONE Describe German Lexicon Baseline
*** DONE Fill Evaluation Tables
    DEADLINE: <2016-05-21 Sa>

** DONE Describe Dictionary-Based Methods
*** DONE Describe Esuli and Sebastiani
    DEADLINE: <2016-06-22 Mi>
*** DONE Describe Rao:08
    DEADLINE: <2016-06-22 Mi>
*** DONE Describe Kim:06
*** DONE Describe Blair-Goldensohn:08
*** DONE Describe Mohammad:09
*** DONE Describe Hassan:10
*** DONE Describe Dragut:10
*** DONE Describe GermaNet
*** IN-PROGRESS Fill Evaluation Tables
**** DONE Re-implement and evaluate Hu:04
**** IN-PROGRESS Re-implement and evaluate Blair-Goldensohn:08
**** IN-PROGRESS Re-implement and evaluate Kim:04,Kim:06
**** DONE Re-implement and evaluate Esuli:06c
**** IN-PROGRESS Re-implement and evaluate Rao:09 Min-Cut
**** IN-PROGRESS Re-implement and evaluate Rao:09 Label Propagation
**** IN-PROGRESS Re-implement and evaluate Awdallah:10
** DONE Describe Corpus-Based Methods
*** DONE Implement the method of Velikovich et al.
*** DONE Test and evalute the method of Velikovich et al.
*** DONE Implement the method of Kiritchenko et al.
*** DONE Implement the method of Severyn et al.
** DONE Describe NWE Methods
   DEADLINE: <2017-05-05 Fri>
*** DONE Estimate Vo
*** DONE Estimate Tang
*** IN-PROGRESS Estimate PCA
*** TODO Describe Linear Projection
*** IN-PROGRESS Finish the section
** DONE Describe Evaluation
   DEADLINE: <2017-05-12 Fri>
*** TODO Estimate the Effect of Seed Sets
*** TODO Estimate the Effect of Word Embeddings
*** TODO Estimate the Effect of Vector Normalization
*** TODO Provide Examples of NWE-based methods
*** TODO Finish the section
** DONE Write Summary and Conclusions
   DEADLINE: <2017-05-14 Sun>
** DONE Revise the Chapter
** DONE Incorporate Supervisor's Feedback
*** DONE S 44: Ich weiß nicht ob wir darüber schon gesprochen hatten, aber die Gleichsetzung von "polar words" und "emotional expressions" scheint mir nicht ganz die Standardterminologie zu sein. Emotions-Analysen kennen auch Dimensionen wie agitated/subdued und anderes, die interessant sind, aber definitiv orthogonal zu Polarität/Valenz. M.a.W., ich würde "Polariät" als eine der Dimensionen von "Emotion" sehen, aber nicht als dasselbe.
*** DONE S. 44 und folgende: "updated version of dataset" klingt potenziell verwirrend ;-)  Die Dissertation soll nicht den mehrschrittigen Prozess der Genese des Dataset nacherzählen, sondern idealerweise nur das Endresultat beschreiben. Alles andere verwirrt (und ohnehin wissen wir schon, dass wir eher auf Kürze achten müssen). Vor allem ist es auch nicht gut, wenn Erweiterungen des Dataset in späteren Kapiteln eingeführt werden, und nicht im Dataset-Kapitel.
*** DONE S. 46: SentiWS: weil Du vorher schon sagtest, dass alle Lexika durch Übersetzung entstanden sind: Hier wäre wichtig zu erfahren, wieviele der Einträge aus GeneralInquirer übersetzt wurden und wieviele dann durch Kollokationsanalysen ergänzt wurden (denn die sind ja nicht übersetzt)
*** DONE S. 46: ZPL: ist also gar nicht durch Übersetzung entstanden, i.ggs. zu der vorangehenden Aussage?
*** DONE Insgesamt hier vielleicht noch ein Argument geben, warum keine Zirkularität besteht: Du hast ja erst Deine Daten durch Vergleich mit den Lexika verändert/verbessert, und anschließend evaluierst Du die Lexika anhand selbiger Daten.
*** DONE S. 47: Mir ist nicht klar, wieso die Lexika so hohen recall für "neutral" haben. Gemäß ihrer Entstehungsart sollten sie eigentlich nur polare Wörter enthalten, oder nicht?
*** DONE S.47 Mitte. "recomputed on the whole corpus" - im gegensatz zu welchem vorher verwendeten Teilkorpus?
*** DONE S. 48 Anfang von 3.3.3 (nochmal dieser Punkt): Diese beschreibung der Entstehung harmoniert nicht mit denen auf S 45/46 - definitiv nicht für ZPL und nur partiell für SentiWS. Ein bisschen verwirrend ist, dass der erste Absatz Übersetzung als zentrale Methode einführt, Du in den folgenden Absätzen aber "nur" dictionary- und corpus-based LG nennst (was mit Übersetzung gar nichts zu tun hat, wie man erst beim zweiten Lesen merkt)
*** DONE S. 49: 2. Absatz, die Methode ist mir nicht ganz klar. Welche synsets gehen in die adjacency matrix ein?
*** DONE 3. Absatz: "following" ist hier aber nicht temporal gemeint, denn KimHovy 2004 folgt nicht auf Blair-Goldensohn 2008
*** DONE S. 50 oben: im Ernst? positiv und neutral werden einfach zusammengefasst? Zu Beginn der Beschreibung von KimHovy werden sie noch auseinander gehalten. Also werden sie nur für einen bestimmten Teilschritt zusammengelegt?
*** DONE S. 52: Hier geht zwischen Tab 3.10 und dem Absatz unter Fig 3.6 etwas durcheinander. Wahrscheinlich meinst Du in 3.10 statt "Hyper. Rels" eher "Holonym Rels"?  (die ich übrigens eher "meronym rels" nennen würde, weil Meronymie die gebräuchlichere "Richtung" ist
*** DONE OK, das Ende von S 52 ist ohnehin noch Baustelle.
*** DONE S. 54: schwierig, dass hier nochmal wieder related work kommt - wir hatten ja in dem Kapitel schon reichlich davon. Hab ich jetzt mal nicht weitergelesen.


* DONE Write Fine-Grained Sentiment Analysis Chapter
** DONE Write Introduction
** DONE Describe Rules for Determining Text Spans
   DEADLINE: <2016-11-03 Do>
** DONE Describe Evaluation Metrics
   DEADLINE: <2016-11-04 Fr>
** DONE IN-PROGRESS Describe Conditional Random Fields
   DEADLINE: <2016-11-11 Fr>
** DONE Describe Recurrent Neural Networks
   DEADLINE: <2016-11-18 Fr>
** DONE Describe Evaluation
*** DONE Describe Effect of the Annotation Scheme
*** DONE Describe Effect of Topology
**** IN-PROGRESS implement tree-structured models
*** DONE Describe Effect of Features
*** DONE Describe Effect of Word Embeddings
**** implement ts-w2v-lst-sq
*** DONE Describe Effect of Lexicons and Normalization

** DONE Revise Evaluation
** DONE Describe Related Work
** DONE Revise Related Work
** DONE Write Summary and Conclusions
   DEADLINE: <2016-11-25 Fr>
** DONE Revise Chapter
   DEADLINE: <2016-11-30 Mi>
** DONE Send Chapter to the Supervisor
   DEADLINE: <2016-11-30 Mi>

** DONE Incorporate Supervisor's Feedback
*** DONE Error Analysis CRF
*** DONE Error Analysis LSTM
*** DONE Error Analysis GRU
*** DONE Successive Prediction


* DONE Write Coarse-Grained Sentiment Analysis Chapter
** DONE Implement Evaluation Script
** DONE Describe Evaluation Metrics
** DONE Describe Data Preparation
** DONE Add Lexicons
*** DONE GPC
*** DONE SWS
*** DONE ZPL
*** DONE Hu-Liu (Esuli-Sebastiani seed set)
*** DONE Blair-Goldensohn (Kim-Hovy seed set)
*** DONE Kim-Hovy (Turney-Littman Seedset)
*** DONE Esuli-Sebastiani (Esuli-Sebastiani seed set)
*** DONE RR (mincut) (Remus seed set)
*** DONE RR (label propagation) (Kim Hovy seed set)
*** DONE Awdallah-Radev (Kim Hovy seed set)
*** DONE Takamura (Hu-Liu seed set)
*** DONE Velikovich (Kim Hovy seed set)
*** DONE Kiritchenko (Kim Hovy seed set)
*** DONE Severyn (Kim Hovy seed set)
*** DONE Tang (Kim Hovy seed set)
*** DONE Vo (Kim Hovy seed set)
*** DONE Nearest Centroids (Kim Hovy seed set)
*** DONE k-NN (Kim Hovy seed set)
*** DONE PCA (Kim Hovy seed set)
*** DONE LP (Kim Hovy seed set)
** DONE Normalize Lexicon Scores
*** DONE GPC
*** DONE SWS
*** DONE ZPL
*** DONE Hu-Liu (Esuli-Sebastiani seed set)
*** DONE Blair-Goldensohn (Kim-Hovy seed set)
*** DONE Kim-Hovy (Turney-Littman Seedset)
*** DONE Esuli-Sebastiani (Esuli-Sebastiani seed set)
*** DONE RR (mincut) (Remus seed set)
*** DONE RR (label propagation) (Kim Hovy seed set)
*** DONE Awdallah-Radev (Kim Hovy seed set)
*** DONE Takamura (Hu-Liu seed set)
*** DONE Velikovich (Kim Hovy seed set)
*** DONE Kiritchenko (Kim Hovy seed set)
*** DONE Severyn (Kim Hovy seed set)
*** DONE Tang (Kim Hovy seed set)
*** DONE Vo (Kim Hovy seed set)
*** DONE Nearest Centroids (Kim Hovy seed set)
*** DONE k-NN (Kim Hovy seed set)
*** DONE PCA (Kim Hovy seed set)*
*** DONE LP (Kim Hovy seed set)
** DONE Add PoS-Tags to the Lexicons



** DONE Describe Lexicon-Based Methods
*** DONE Describe Hu-Liu (2004)
*** DONE Describe Taboada et al. (2011)
*** DONE Describe Musto et al. (2014)
*** DONE Describe Jurek et al. (2015)
*** DONE Describe Kolchyna et al. (2015)

** DONE Reimplement Lexicon-Based Methods
*** DONE Reimplement Hu-Liu (2004)
*** DONE Reimplement Taboada et al. (2011)
*** DONE Reimplement Musto et al. (2014)
*** DONE Reimplement Jurek et al. (2015)
*** DONE Reimplement Kolchyna et al. (2015)

** DONE Evaluate Lexicon-Based Methods
*** DONE Evaluate LB Approaches on Normalized PotTS Data
*** DONE Evaluate LB Approaches on Unnormalized PotTS Data
*** DONE Evaluate LB Approaches on Normalized SB10k Data
*** DONE Evaluate LB Approaches on Unnormalized SB10k Data
*** DONE Evaluate Different Lexicon Steps
*** DONE Describe Different Lexicon Steps
*** DONE Describe Evaluation of Lexicon-Based Methods



** DONE Describe ML-Based Methods
** DONE Reimplement ML-Based Methods
*** DONE Reimplement Gamon, 2004
*** DONE Reimplement Mohammad, 2013
*** DONE Reimplement Guenther, 2014
** DONE Describe Evaluation of ML-Based Methods
** DONE Perform and Describe Feature Ablation Test
** DONE Evaluate Different Classifiers
** DONE Describe Error Analysis
** DONE Revise ML-Based Methods


** DONE Describe DL-Based Methods
*** DONE Describe Choi and Cardie (2008)
    DEADLINE: <2018-01-17 Wed>
*** DONE Describe Moilanen and Pulman (2007)
    DEADLINE: <2018-01-18 Thu>
*** DONE Describe Nakagawa (2010)
    DEADLINE: <2018-01-18 Thu>
*** DONE Describe Yessenalina and Cardie (2010)
*** DONE Describe Socher et al. (2012)
*** DONE Describe Socher et al. (2013)
*** DONE Describe Wang (2015)
    DEADLINE: <2018-01-16 Tue>
*** DONE Describe Baziotis:17}}
    DEADLINE: <2018-01-21 Sun>
*** DONE Describe Cliche:17}}
    DEADLINE: <2018-01-22 Mon>
*** DONE Describe Rouvier:17}}
    DEADLINE: <2018-01-22 Mon>
*** DONE Revise the Descriptions
    DEADLINE: <2018-01-24 Wed>
** DONE Reimplement DL-Based Methods
   DEADLINE: <2018-02-13 Tue>
*** DONE Reimplement Yessenalina and Cardie (2010)
    DEADLINE: <2018-02-11 Sun>
*** DONE Reimplement Socher et al. (2011)
    DEADLINE: <2018-02-11 Sun>
*** DONE Reimplement Socher et al. (2012)
    DEADLINE: <2018-02-11 Sun>
*** DONE Reimplement Socher et al. (2013)
    DEADLINE: <2018-01-28 Sun>
*** DONE Reimplement Severyn et al. (2015)
    DEADLINE: <2018-01-27 Sat>
*** DONE Reimplement Baziotis et al. (2017)
    DEADLINE: <2018-01-30 Tue>
** DONE Implement own DL-Based Method
   DEADLINE: <2018-02-14 Wed>
** DONE Evaluate DL-Based Methods
   DEADLINE: <2018-02-14 Wed>
*** IN-PROGRESS Evaluate the Effect of Different Embedding Types
*** TODO Perform an Error Analysis
** DONE Describe Evaluation of DL-Based Methods
   DEADLINE: <2018-02-14 Wed>
*** DONE Describe Effect of Embeddings

*** IN-PROGRESS Perform Error Analysis
** DONE Perform General Evaluation
*** TODO Describe Effect of Distant Supervision
*** TODO Describe Effect of the Lexicons
*** TODO Describe Effect of Text Normalization
** DONE Write Summary and Conclusions
   DEADLINE: <2018-02-28 Wed>

** DONE Incorporate Supervisor's Feedback
**** DONE 102: Ich würde hier noch ein klein wenig ausführlicher ankündigen, dass Du ausgewählte (weil erfolgreiche) Ansätze re-implementieren wirst, und im DL Kapitel dann auch einen eigenen Ansatz (bzw Modifikation von Baziotis) vorschlägst.
*** DONE Sct 6.1
**** DONE S 103 warum ist macro F1 über pos und neg eine Alternative zu micro-avg über pos/neg/neut? Sollte man nicht immer dieselbe Zahl von Klassen haben?
*** DONE Sct 6.2
**** DONE S 103 Statt TreeTagger besser den Tweet-Tagger von Ines Rehbein nutzen?
     # (Oder jedenfalls sagen, dass es ihn auch gibt.)
**** DONE S 103 Ref PotTS Korpus sollte nicht ein paper, sondern das Kapitel dieser Diss sein
**** DONE S 103 Was ist der Zweck der sehr einfachen Pol.-Bestimmung?
     # Direkt sagen, dass das die Eval-Daten werden sollen und wir
     # dafür eine einfache Methode benötigen. (Hm, wie zuverlässig
     # wird das werden?)  Wieviele tweets werden getilgt, weil
     # uneindeutig? Woher kommen die lexikalischen Informationen?
     # Stecken wir Info in die "Gold" Daten, die wir später bei der
     # Eval von Lex-Methoden dann wieder evaluieren wollen?

**** DONE S 104 Erklärg warum Bsp 6.2.2 positiv annotiert wird?
**** DONE S 105 Warum hat PotTS ein IAA - davon war im Text bisher nicht die Rede.
**** DONE S 105 "As you might remember" unüblich, den Leser direkt anzusrpechen (kommt auch später vor.)
*** DONE Sct 6.3
**** DONE S 109 Test the earlier work on PotTS and SB10k, but why not on the 3rd corpus? (Or in other words, why was the 3rd corpus introduced?)
**** DONE S 109 "drawback of this resource, which unfortunately slipped through our previous intrinsic evaluation" - was heißt das?
*** DONE Sct 6.3.1, 6.3.2: interessant!
**** DONE S 112 Ex 6.3.1 might point to a problem of re-implementing rules of an English system for German, where word order is much less restricted
**** DONE S 112 Ex 6.3.2 Isn't the question the reason for nullifying the score? (And rightly so, I believe.)
**** DONE S 113 what is an "informative part of speech"?
**** DONE S 114 Ex 6.3.5 Is "Ok" a good translation of "Normal" ?
**** DONE S 115 Warum tritt derselbe tweet zweimal im cluster auf?
*** DONE Sct 6.4
**** DONE S 118ff: If (many of) these approaches use sentiment lexicons in their feature space, is the dividing line between lexicon methods and ML methods really so clear?
**** DONE S 120 First mention of "the Linear Projection lexicon" in this chapter. Please remind the reader what it is. (section reference)
**** DONE S 121 As indicated earlier. at least one German Twitter PoS taggers does exist now. A comparison to TreeTagger would be really interesting here.
**** DONE S 126 some equations have a number, some do not
**** DONE S 126 in teh Ye./Cardie 11 approach, what are the vectors u and v ?
**** DONE S 127 line 1: ist "child" = "dependent" und "vector" = "embedding"? Dann besser identische Bezeichner benutzen
**** DONE S 130 der Übergang vom Referieren früherer Arbeiten zum Vorschlag eines eigenen klarer markiert sein - vielleicht durch separate subsubsections. 
**** DONE S 138 6.5.5 ist natürlich ein schöner Hinweis auf einen möglichen Mehrwert von lokalen Kohärenzrelationen ;-)

** DONE Revise the Chapter


* DONE Write Discourse-Level Sentiment Analysis Chapter
** DONE Write Introduction
** DONE Write Related Work
*** DONE Add summary of Riloff et al. (2003)
*** DONE Add summary of Riloff et al. (2003a)
*** DONE Add summary of Pang et al. (2002)
*** DONE Add summary of Pang et al. (2004)
*** DONE Check Section 3.6 of Hu and Liu (2004)
*** DONE Add summary of Snyder and Barzilay (2007)
*** DONE Add summary of Asher (2008)
*** DONE Add summary of Heerschop (2011)
*** DONE Add summary of Zhou (2011)
*** DONE Add summary of Zirn (2011)
*** DONE Add summary of Chenlo (2013)
** DONE Prepare Data
*** DONE Retrain Ji's Parser on PCC
*** DONE Add Discourse Parses to DASA
** DONE Revise Related Work
** DONE Reimplement and evaluate common DASA approaches
*** COMMENT did not reimplemplement McDonald's classifier as it requires gold EDU labels
*** COMMENT did not reimplemplement Zirn's classifier because it requires gold EDU labels
*** DONE Reimplement and Evaluate Last EDU Classifier
*** DONE Reimplement and Evaluate Root Classifier
*** DONE Reimplement and Evaluate Discourse-Unaware Classifier
*** DONE Reimplement and Evaluate DDR Classifier
*** DONE Reimplement and Evaluate R2N2 Classifier
*** DONE Reimplement and Evaluate Wang Classifiers
** DONE Devise own DASA Method
*** DONE Evaluate Softmax
*** DONE Evaluate Custom Simplex Normalization
*** DONE Evaluate Sparsemax
*** DONE Evaluate Best Strategy on the Dependency Tree Representati
** DONE Perform Error Analysis
** DONE Perform and Describe Evaluation
*** DONE Describe the Effect of Base Classifiers
*** DONE Analyze the Effect of Discourse Relation Sets
** DONE Write Summary and Conclusions
** DONE Revise the chapter
** DONE Incorporate Supervisor's Feedback


* DONE Submit Dissertation to the Deanery
** DONE Prepare Documents
*** DONE Ph.D. Application
*** DONE Declaration in which subject I'm pursuing the degree
*** DONE Declaration that I'm not pursuing a degree at any other unversity
*** DONE Declaration that the work has been completed without external help and according to the best scientific standards
*** DONE CV
*** DONE Summary
*** DONE Diploma
*** DONE Dissertation
*** DONE Publication List
*** DONE Suggestion for Committee
*** DONE Suggestion for Reviewers
*** DONE Criminal Record Certificate
** DONE Print the Documents
*** DONE Ph.D. Application
*** DONE Declaration in which subject I'm pursuing the degree
*** DONE Declaration that I'm not pursuing a degree at any other unversity
*** DONE Declaration that the work has been completed without external help and according to the best scientific standards
*** DONE CV
*** DONE Summary
*** DONE Diploma
*** DONE Dissertation
*** DONE Publication List
*** DONE Suggestion for Committee
*** DONE Suggestion for Reviewers
*** DONE Criminal Record Certificate
** DONE Bring the documents to the deanery
   DEADLINE: <2019-03-13 Wed>


* DONE Theses
** DONE Write the Theses Paper
** DONE Send the Paper to the Deanery


* DONE Presentation
** DONE Prepare the Presentation
** TODO Rehearse the Presentation


* IN-PROGRESS Final Corrections
** IN-PROGRESS Eisenstein
*** IN-PROGRESS Chapter 2
**** TODO For replicability, it would be good to include the complete keyword lists alongside the annotator instructions in an appendix.
**** TODO I want to poke a little at the definition of targets as "entities or events evaluated by opinions."
**** TODO I wonder whether the initial low levels of agreement stemmed from a lack of clarity in the original instructions.
**** TODO I didn’t understand the correlation analysis in table 2.6.
*** TODO Chapter 3
**** TODO Local maximum
*** TODO Chapter 4
**** TODO It might help to remind readers of this size of the training and test sets, and to indicate how many features from the training set are unseen in the test set, and vice versa
**** TODO I would also like to see how F1 evolved across the space of regularization parameters, and to know how the final regularization parameter was selected.
**** TODO I would have liked to know more about how inference and learning was implemented in these structures, since the “off-the-shelf” Viterbi and forward-backward algorithms are not immediately applicable to Semi-Markov and Tree-structured models.
*** TODO Chapter 5
**** TODO I am skeptical of the use of emoticons to label tweets, despite the fact that this is done in prior work: there's good evidence that the “smiley” emoticon is used for many pragmatic purposes aside from indicating sentiment, such as softening face-threatening speech acts
**** TODO I would relabel “distant supervision” as “semi-supervised learning” or “weak supervision”, as “distant supervision” typically refers to supervision from type-level resources such as knowl- edge bases.
*** TODO Chapter 6
**** TODO But I couldn’t understand why the No-Discourse method also improved in this setting.
** TODO Stede
*** TODO Chapter 1
**** TODO In doing this, it goes some way to providing an account of the state of the art, but one thing the reader misses is a concise definition of SA; here it would have been sufficient to quote one from influential literature, such as the book by Liu (2012).
**** TODO Also, giving a few more examples to illustrate the range of subtasks and possible domains would be helpful.
*** TODO Chapter 2
**** TODO Polar words get a two-valued strength attribute, where I wonder why the two values are strong and medium, rather than weak.
**** TODO here a slightly broader discussion, which looks at IAA treatment in related work on span labeling, and maybe specifically considers the potential utility of Krippendorf's unitized alpha, would have been nice.
**** TODO US does not build a single gold standard – a decision that could have been briefly discussed at the end of the chapter.
*** TODO Chapter 4
**** TODO To evaluate the work, US proposes to use a token-sensitive measure suggested in related work for other purposes. For appreciating this decision, it would be good to get information on how fine- grained SA approaches for English usually handle this. Likewise, for methods and results a brief overview of related work would here be helpful.
*** TODO Chapter 5
**** TODO In the section on machine-learning methods, I would appreciate a sub/section break between the extensive related work part and the author's own proposal and implementation.
** TODO Official
