\chapter{Coarse-Grained Sentiment Analysis}\label{sec:snt:cgsa}

Having familiarized ourselves with the peculiarities of the creation
of a sentiment corpus, the different ways to automatically induce new
polarity lists, and the difficulties of fine-grained opinion mining,
we now move on to the presumably most popular sentiment analysis
objective---the coarse-grained analysis or CGSA, in which we need to
determine the overall polarity of a message.

Traditionally, this task has been addessed with either of three
popular method groups:
\begin{inparaenum}[(i)]
  \item lexicon-based approaches,
  \item machine-learning-based (ML) techniques, and
  \item deep-learning-based (DL) applications.
\end{inparaenum}
In this chapter, we are going to scrutinize the most prominent
representatives of each of these paradigms and also tackle a much more
ambitious goal, namely to check whether we can achieve results
comparable with the scores of these methods when the language of the
domain we train on is completely different from the language of the
test data.

We begin our comparison by first presenting the metrics that we will
use in our subsequent evaluation.  After a brief description of the
data preparation step, we proceed to the actual estimation of popular
lexicon-, ML-, and DL-based approaches, explaining and evaluating them
in Sections~\ref{sec:cgsa:lexicon-based}, \ref{sec:cgsa:ml-based},
and~\ref{sec:cgsa:dl-based} respectively.  Then,
in~Section~\ref{sec:cgsa:domain-adaptation}, we also show which
results can be obtained by using cross-lingual transfer, where we
train a classifier on English microblogs and then adapt this system to
German messages.  Finally, we conclude with an extensive evaluation of
different hyperparameters and settings (including various types of
sentiment lexicons, different kinds of word embeddings, the utility of
the text normalization step, and the impact of additional noisily
labeled training data), summarizing our results and recapping our
findings at the end of this chapter.

\section{Evaluation Metrics}\label{sec:cgsa:eval-metrics}

To estimate the quality of the compared systems, we will rely on two
established evaluation metrics which are commonly used for measuring
CGSA results: One of these metrics is the macro-averaged \F-score over
the two major polarity classes~(positive and negative): { \small%
  \begin{equation*}
    F_1 = \frac{F_{pos} + F_{neg}}{2}.
  \end{equation*}%
  \normalsize%
}%
This measure was first introduced by the organizers of the SemEval
competition~\cite{Nakov:13,Rosenthal:14,Rosenthal:15} and has become a
de facto standard not only for the SemEval dataset, but virtually for
all related coarse-grained sentiment tasks and corpora.

The second metric is the micro-averaged \F-score over all three
possible semantic orientations (positive, negative, and neutral),
which basically corresponds to the accuracy over the complete labeled
dataset~\cite[see][p.~577]{Manning:99}.  This measure both predates
and supersedes the SemEval evaluation as it had already been applied
in the very first works on coarse-grained opinion
mining~\cite{Wiebe:99,Das:01,Read:05,Kennedy:06,Go:09} and was again
reintroduced at the GermEval Shared Task on Sentiment
Analysis~2017~\cite{Wojatzki:17}.

Moreover, in addition to these two metrics, we will also give a
detailed information on precision, recall, and \F-scores of each
particular polarity class in order to get a better intuition about
precise strengths, weaknesses, and biases of each evaluated method.

\section{Data Preparation}\label{sec:cgsa:data}

Similarly to the data preparation steps used for fine-grained
sentiment analysis, we preprocessed all tweets involved in our
experiments with the text normalization system
of~\citet{Sidarenka:13}, tokenized them using the same adjusted
version of Potts'
tokenizer,\footnote{\url{http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py}}
lemmatized and assigned part-of-speech tags to these tokens with the
\texttt{TreeTagger} of \citet{Schmid:95}.  Moreover, like in the
previous chapter, we automatically obtained morphological features for
each word, and induced syntactic trees for each sentence with the help
of the \texttt{Mate} dependency parser
\cite{Bohnet:13}. % Apart from the
% PotTS dataset, we also applied this procedure to the microblogs of
% the German Twitter snapshot~\cite{Scheffler:14}, which will be used
% in our subsequent experiments on noisy supervision.

We again divided the PotTS corpus~\cite{Sidarenka:16} into a training,
development, and test set, using 70\% of the tweets for learning, 10\%
for tuning and picking the optimal hyperparameters, and the remaining
20\% for evaluating the results.  We inferred the polarity labels for
these microblogs with a simple heuristic rule akin to the one used
by~\citet{Wiebe:05a}, and assigned the positive (negative) class to
the messages which had exclusively positive (negative) sentiments,
skipping all microblogs that simultaneously contained multiple
opinions with different semantic orientations.  In cases when a
sentiment was absent, we recoursed to a fallback strategy by
considering all tweets with only positive (negative) polar terms as
positive (negative), disregarding the messages which featured
expressions from both polarity classes, and taking the rest of the
corpus (i.e., posts with neither sentiments nor polar terms) as
neutral instances.

A few examples of such heuristically inferred labels are provided
below:
\begin{example}[Coarse-Grained Sentiment Annotations]\label{snt:cgsa:exmp:anno1}
  \noindent\textup{\bfseries\textcolor{darkred}{Tweet:}} {\upshape Ich finde den Papst putzig \smiley{}}\\
  \noindent I find the Pope cute \smiley{}.\\
  \noindent\textup{\bfseries\textcolor{darkred}{Label:}}\hspace*{2em}\textbf{%
    \upshape\textcolor{green3}{positive}}\\[1.5em]
  \noindent\textup{\bfseries\textcolor{darkred}{Tweet:}} {\upshape typisch Bayern kaum ist der neue Papst da und schon haben
  sie ihn in der Tasche ...}\\
  \noindent Typical Bavaria The new Pope is hardly there, as they already have him in their pocket\\
  \noindent\textup{\bfseries\textcolor{darkred}{Label:}}\hspace*{2em}\textbf{%
    \upshape\textcolor{midnightblue}{negative}}
\end{example}
As we can see, our simple rule provides reasonable decisions in most
of the cases.  However, since this approach is still an approximation
and consequently prone to errors (especially in the cases where the
polarity of the whole microblog differs from the semantic orientation
of its single tokens or is expressed without any explicit polar
expressions at all, see Example~\ref{snt:cgsa:exmp:anno2}), we also
decided to evaluate all CGSA methods presented in this chapter on
another German Twitter corpus, which has been specifically annotated
with message-level polarities---SB10k.

\begin{example}[Erroneous Sentiment
  Annotations]\label{snt:cgsa:exmp:anno2}
  \noindent\textup{\bfseries\textcolor{darkred}{Tweet:}} {\upshape Unser Park, unser Geld, unsere Stadt! -NICHT unser Finanzminister! \smiley{} \#schmid \#spd \#s21 \#btw13}\\
  \noindent Our park, our money, our city! -NOT our Finance Minister! \smiley{} \#schmid \#spd \#s21 \#btw13\\
  \noindent\textup{\bfseries\textcolor{darkred}{Label:}}\hspace*{2em}\textbf{%
    \upshape\textcolor{green3}{positive*}}\\[1.5em]
  \noindent\textup{\bfseries\textcolor{darkred}{Tweet:}} {\upshape Auf die Lobby-FDP von heute kann Deutschland verzichten ...}\\
  \noindent Germany can go without today's lobby FDP\\
  \noindent\textup{\bfseries\textcolor{darkred}{Label:}}\hspace*{2em}\textbf{%
    \upshape\textcolor{black}{neutral*}}
\end{example}

The SB10k dataset was introduced by~\citet{Cieliebak:17}, and
comprises a total of 9,738 tweets.  These messages were sampled from a
larger snapshot of 5M German microblogs gathered between August and
November~2013.  To ensure lexical diversity and proportional polarity
distribution in this corpus, the authors first split all posts of this
snaphsot into 2,500 clusters using $k$-means algorithm with unigram
features.  Afterwards, from each of these groups, they selected tweets
with at least one positive and one negative term from the German
Polarity Clues lexicon~\cite{Waltinger:10}.  Each of these messages
was subsequently annotated by at least three human experts from a pool
of 34 different coders.  The resulting inter-rater reliability (IRR)
of these data run up to 0.39 Krippendorff's
$\alpha$~\cite{Krippendorff:07}.  Unfortunately, due to the
restrictions of Twitter's terms of use (which only allow to distribute
the ids of the microblogs along with their labels), we were able to
retrieve merely 7,476 tweets of this corpus, which nevertheless
constitute a substantial amount of data, comparable to the size of the
PotTS dataset.

In addition to the aforementioned two corpora (PotTS and SB10k), we
also automatically annotated all microblogs of the German Twitter
Snapshot~\cite{Scheffler:14}, following the procedure proposed
by~\citet{Read:05} and~\citet{Go:09}, and assigning the positive
(negative) class to the tweets which contained the respective
emoticons.  However, in contrast to the previous datasets, we will not
use these tweets for evaluation, but solely utilize them for training
in our distant supervision experiments.

The resulting statistics on the number of messages and polarity class
distribution in these data are shown in
Table~\ref{snt-cgsa:tbl:corp-dist}.
\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{6}{>{\centering\arraybackslash}p{0.13\columnwidth}}} % last two columns
      \toprule
      \textbf{Dataset} & \multicolumn{4}{c}{\bfseries Polarity Class}%
      & \multicolumn{2}{c}{\bfseries Agreement}\\\cmidrule(lr){2-5}\cmidrule(lr){6-7}
                       & \textbf{Positive} & \textbf{Negative} %
                                           & \textbf{Neutral} & \textbf{Mixed*} %
                                                              & $\alpha$ & $\kappa$\\\midrule

      \textbf{PotTS} & 3,380 & 1,541 & 2,558 & 513 & 0.66 & 0.4\\
      \textbf{SB10k} & 1,717 & 1,130 & 4,629 & 0 & 0.39 & NA\\
      \textbf{GTS} & 3,326,829 & 350,775 & 19,453,669 & 73,776 & NA & NA\\\bottomrule
\end{tabular}
    \egroup
    \caption[Polarity class distribution in PotTS, SB10k, and the
    German Twitter Snapshot.]{Polarity class distribution in PotTS,
      SB10k, and the German
      Twitter Snapshot (GTS).\\
      \emph{(* -- the \emph{mixed} polarity was excluded from our
        experiments)}}
    \label{snt-cgsa:tbl:corp-dist}
  \end{center}
\end{table}

As we can see, each of the datasets has its own unique composition of
polar tweets: The PotTS corpus, for example, shows a conspicuous bias
towards the positive class with 42\% of the microblogs belonging to
this polarity.  We can partially explain this effect by the following
reasons: first of all, it might be due to the coarseness of the
heuristic rule that we applied to infer the labels for these messages;
and, secondly, it might also stem from the initial selection criteria
that we used to compile the data for this collection.  As you might
remember, we a priori composed the major part of this dataset from
tweets which contained smileys or had at least one polar expression
from the SentiWS lexicon~\cite{Remus:10}.  Since most of these
emoticons were positive (which is evident from the statistics of the
German Twitter snapshot), the selected posts also became skewed
towards this semantic orientation.

The second most frequent group of the PotTS corpus is formed by
neutral microblogs, which account for 32\% of the data.  Finally,
negative messages represent the absolute minority of all tweets
(merely 19\%), which, however, is less surprising as the same tendency
can be observed for the SB10k and German Twitter Snapshot too.

Regarding the last two corpora, we can observe a more uniform (though
not identical) behavior as both collections are dominated by neutral
posts, which constitue 62\% of SB10k data and 84\% of the German
Twitter Snapshot.  The positive class, again, makes up a big part of
these datasets (23\% of SB10k and 14\% of the snapshot), but its
influence this time is much less pronounced than in the PotTS case.
Finally, as we already mentioned, negative tweets are the least
represented semantic representation across all three sources.  The
only group which has even less instances than this class is the mixed
polarity.  We, however, will skip the mixed orientation in our
experiments for the sake of simplicity and uniformity of the
evaluation.\footnote{As we will see later, some of the CGSA methods
  (especially the lexicon-based ones) can hardly be extended to the
  prediction of more than three polarity classes.}

Last but not least, the results of the inter-rater reliability check
confirm the superior quality of the PotTS corpus, which, even despite
an approximate label inference, still has an $\alpha$
agreement~\cite{Krippendorff:07} that is almost 1.7 times as high as
the respective score of SB10k (0.66 versus 0.39).  However, Cohen's
$\kappa$ of these data (0.4), which is only available for this
dataset, is merely on the verge between fair and moderate values.
Nevertheless, since labels used in this experiments are ordinal rather
than nominal in their nature (i.e., we can compute the \emph{distance}
between distinct labels, which, for example, would be bigger for the
pair \emph{positive} vs. \emph{negative} than for the pair
\emph{positive} vs. \emph{neutral}), we find the Krippendorff's metric
more appropriate for assessing the quality of the annotation for this
task.

\section{Lexicon-Based Methods}\label{sec:cgsa:lexicon-based}

The first group of approaches that we are going to explore in this
chapter using the described data are lexicon-based (LB) systems.  Just
like sentiment lexicons themselves, LB methods for coarse-grained
opinion mining have attracted a lot of attention from the very
inception of the sentiment analysis field.  Starting with the research
of~\citet{Hatzivassi:00}, who gave a statistical proof that the mere
occurrence of a subjective adjective from an automatically compiled
polarity list was a highly reliable indicator of the whole sentence
being subjective, more and more works dealing with the use of lexicons
for determining the overall polarity of the text appeared on the
scene.

One of the first notable steps in this direction was made
by~\citet{Das:01}, who proposed an ensemble of five classifiers (two
of which were purely lexicon-based and the other three heavily relied
on lexicon features) to predict the polarity of stock messages
(\emph{buy}, \emph{sell}, or \emph{neutral}), achieving an accuracy of
62\% on a corpus of several hundreds stock board messages.  A much
simpler method for a related task was suggested by~\citet{Turney:02},
who determined the \emph{semantic orientation} (SO) of reviews by
averaging the PMI scores of their terms, getting these scores from an
automatically generated sentiment lexicon.  With this approach, the
author could reach an accuracy of 74\% on a corpus of 410 manually
labeled Epinions comments.  In the same vein, \citet{Hu:04} computed
the overall polarity of a sentence by comparing the numbers of
positive and negative terms appearing in it, reversing the orientation
of terms in case of negation.  Finally, \citet{Kim:04} compared three
different approaches to determining the polarity of a sentence:
\begin{inparaenum}[(i)]
\item by multiplying the signs of its polar terms,
\item by taking the sum of their scores, and
\item by computing the geometric mean of these values;
\end{inparaenum}
finding the first and the last option working best on the Document
Undestanding Corpus.\footnote{\url{http://duc.nist.gov/}}

% % Hu and Liu, 2004
% Similarly, \citet{Hu:04} determined the semantic orientation of
% sentences in customer reviews by simply comparing the number of
% positive and negative terms found in these passages. Since the
% authors, however, were primarily interested in estimating the polarity
% towards particular product features mentioned in the clauses, they
% additionally applied a fallback strategy in case of a tie by checking
% which of the polar lexicon terms appeared closer to the features, and
% assuming the polarity of the preceding sentence if these numbers were
% also equal.

% % Taboada et al., 2004
% Largely inspired by the Appraisal theory of~\citet{Martin:00},
% \citet{Taboada:04} enhanced the original method of~\citet{Turney:02}
% by increasing the weights of polar adjectives which occurred in the
% middle and at the end of a document, and also augmenting these values
% with the affect, judgement, and appreciation scores.  Similarly to
% polarity, the appraisal scores were calculated automatically by
% computing the PMI of their cooccurrence with different pronouns using
% a web search engine.

% Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006
In~\citeyear{Polanyi:06}, \citeauthor{Polanyi:06} presented an
extensive overview and analysis of common lexicon-based sentiment
methods existing at that time, arguing that, besides considering the
lexical valence (i.e., semantic orientation) of polar expressions, it
was also necessary to incorporate syntactic, discourse-level, and
extra-linguistic factors such as negations, intensifiers, modal
operators (e.g., \emph{could} or \emph{might}), presuppositional items
(e.g., \emph{barely} or \emph{failure}), irony, reported speech,
discourse connectors, genre, attitude assessment, reported speech,
multi-entity evaluation, etc.  This theoretical hypothesis was also
proven empirically by \citet{Kennedy:06}, who investigated two ways to
determine the polarity of customer reviews: In the first of these
approaches, they simply compared the numbers of positive and negative
terms appearing in a text, assigning the analyzed review to the class
with the greater number of items.  In the second attempt, they
enhanced the original system with additional information about
contextual valence shifters, increasing or decreasing the sentiment
score of a term if it was preceded by an intensifier or downtoner, and
changing the polarity sign of this score to the opposite in case of a
negation.  With this adjustment, the authors achieved a statistically
significant improvement, boosting the accuracy of the two-class
prediction on a corpus of product and movie reviews from 67.9 to
69.3\%.

% Taboada et al., 2011
Finally, a veritably seminal work on lexicon-based techniques was
presented by~\citet{Taboada:11}, who introduced a manually compiled
polarity list\footnote{The authors hand-annotated all occurrences of
  adjectives, nouns, and verbs found in a corpus of 400 Epinions
  reviews with ordinal categories ranging from -5 to 5 which reflected
  the semantic orientation of a term (positive vs. negative) and its
  polar strength (weak vs. strong).} and used this resource to
estimate the overall semantic orientation of the text.  Drawing on the
ideas of~\citet{Polanyi:06}, the authors incorporated a set of
additional heuristic rules into their computation by changing the
prior SO values of negated, itensified, and downtoned terms, ignoring
irrealis and interrogative sentences, and adjusting the weights of
specific document sections.  An extensive evaluation of this approach
showed the superior performance of the manual lexicon in comparison
with other polarity lists including the Subjectivity
Dictionary~\cite{Wilson:05}, Maryland Polarity Set~\cite{Mohammad:09},
and \textsc{SentiWordNet}~\cite{Esuli:06c}.  Moreover, the authors
also demonstrated the effectiveness of their method for other topics
and text genres, hypothesizing that lexion-based approaches were in
general more robust to domain shifts than traditional supervised
machine-learning techniques.

% % Taboada et al., 2006
% Another important contribution to the development of lexicon-based
% approaches was made by~\citet{Taboada:06}, who compared three popular
% polarity lists---a PMI lexicon computed with the original method
% of~\citet{Turney:02} using the AltaVista's NEAR operator; a similar
% polarity list obtained with the help of Google's AND queries; and,
% finally, the manually compiled General Inquirer lexicon
% of~\citet{Stone:66}.  The authors evaluated these resources both
% intrinsically (by comparing them with GI entries) and extrinsically
% (by computing the polarity of 400 manually annotated Epinions
% reviews).  To estimate the overall polarity of a review for the second
% task, \citeauthor{Taboada:06} calculated the average SO value of all
% polar terms found in the review, obtaining these scores from the
% mean-normalized lexicons, and flipping the polarity sign to the
% opposite in case of the negation.

% Musto et al., 2014
It is therefore less surprisingly that lexicon-based systems have also
quickly found their way into sentiment analysis of social media: For
example, one such approach, specifically tailored to Twitter data, was
proposed by~\citet{Musto:14}, who examined four different ways to
compute the overall polarity scores of microblogs: \emph{basic},
\emph{normalized}, \emph{emphasized}, and
\emph{normalized-emphasized}; evaluating these strategies with four
distinct lexicons: \textsc{Sen\-ti\-Word\-Net}~\cite{Esuli:06c},
\textsc{Word\-Net-\-Affect}~\cite{Strapparava:04},
\textsc{MPQA}~\cite{Wiebe:05}, and
\textsc{SenticNet}~\cite{Cambria:14}.  In all of these methods, the
authors first split the input message into a list of
\emph{micro-phrases} based on the occurrence of punctuation marks and
conjunctions.  Afterwards, they calculated the polarity score for each
of these segments and estimated the overall polarity of the whole
tweet by uniting the scores of its micro-phrases.
\citeauthor{Musto:14} obtained their best results (58.99\% accuracy on
the SemEval-2013 dataset) with the normalized-emphasized approach, in
which they averaged the polarity scores of segments' tokens, boosting
these values by 50\% for informative parts of speech (adjectives,
nouns, and adverbs) and considering the sum of the micro-phrase scores
as the final overall polarity of the microblog.

% the authors obtained their best results using the
% \textsc{SentiWordNet} lexicon of~\citet{Esuli:06c}

% Jurek et al., 2015
Another Twitter-aware system was presented by~\citet{Jurek:15}, who
computed the negative and positive polarity of a message ($F_p$ and
$F_n$ respectively) using the following equations: { \small%
  \begin{align}
    F_P &= \min\left(\frac{A_P}{2 - \log(3.5\times W_P + I_P)}, 100\right),\\
    F_N &= \max\left(\frac{A_N}{2 - \log(3.5\times W_N + I_N)}, -100\right);\label{cgsa:eq:jurek}
  \end{align}%
  \normalsize%
}%
where $A_P$ and $A_N$ represent the average scores of positive and
negative lexicon terms found in the tweet
($A_p = \frac{\sum_{w\in\textrm{msg}}s^p_w}{\lVert\textrm{msg}\rVert}$
with $s^p_w$ denoting the positive lexicon score of the term $w$);
$W_P$ and $W_N$ stand for the raw counts of polar tokens; and $I_P$
and $I_N$ denote the number of intensifiers preceding these words.  In
addition to that, before estimating the average values, the authors
also modified the polarity scores $s_w$ of negated words by applying
the following heuristics: { \small%
  \begin{align}
neg(s_w) =
    \begin{cases}
        \min\left(\frac{s_w - 100}{2}, -10\right) & \text{if } s_w > 0,\\
        \max\left(\frac{s_w + 100}{2}, 10\right), & \text{if } s_w < 0.
    \end{cases}
\end{align}%
\normalsize%
}%
Furthermore, besides computing the polarity scores $F_p$ and $F_n$,
\citeauthor{Jurek:15} also determined the subjectivity degree of the
message by replacing the $A_P$ and $A_N$ terms in
Equation~\ref{cgsa:eq:jurek} with an average of conditional
probabilitites of the tweet being subjective given the occurrences of
the respective polar terms.\footnote{These probabilities were
  calculated automatically on the noisily labeled data set
  of~\citet{Go:09}.}  The authors considered a microblog as neutral if
its absolute polarity was less than 25 and the subjectivity value was
not greater than 0.5.  Otherwise, they assigned a positive or negative
label to this message depending on the sign of the polarity score.
With this approach, \citeauthor{Jurek:15} achieved an accuracy
of~77.3\% on the manually annotated subset of \citeauthor{Go:09}'s
corpus and reached 74.2\% on the IMDB review corpus~\cite{Maas:11}.

% Kolchyna et al., 2015
Finally, \citet{Kolchyna:15} also explored two different ways of
computing the overall polarity of a microblog:
\begin{inparaenum}[(i)]
\item by simply averaging the scores of the lexicon terms found in the
  message and
\item by taking the signed logarithm of this average:
\end{inparaenum}
\begin{equation*}
  \text{Score}_{\log} =
  \begin{cases}
    \text{sign}(\text{Score}_{\text{AVG}})\log_{10}(|\text{Score}_{\text{AVG}}|) & %
    \text{if |Score}_{\text{AVG}}| > 0.1,\\
    0, & \text{otherwise};
  \end{cases}
\end{equation*}%
comparing theses approaches on the SemEval-2013
dataset~\cite{Nakov:13}.  The authors determined the final polarity
class of a tweet using $k$-means clustering, which utilized both of
the above polarity values as features.  They showed that the
logarithmic strategy performed better than the simple average
solution, yielding an accuracy of 61.74\%.  % In addition to that,
% \citeauthor{Kolchyna:15} also checked whether plain lexicon scores
% could serve as useful attributes for an ML-based method.  For this
% purpose, they retrained a cost-sensitive SVM
% classifier~\cite{Masnadi:12} after extending its $n$-gram feature
% set with lexicon features, getting almost five percent accuracy
% improvement (from 86.62 to 91.17) on the IMDB movie review
% dataset~\cite{Pang:02}.

As it was unclear how all of these works would perform on the PotTS
and SB10k corpora, we reimplemented the approaches of~\citet{Hu:04}
(as a relatively simple baseline), \citet{Taboada:11},
\citet{Musto:14}, \citet{Jurek:15}, and \citet{Kolchyna:15}, and
applied these systems to the test sets of the aforementioned data.

Following our comparison in Chapter~\ref{chap:snt:lex}, we chose the
Zurich Polarity List as the primary sentiment lexicon for the tested
methods.  However, a significant drawback of this resource, which
unfortunately slipped through our evaluation, is that most of its
entries are weighted uniformly, having a polarity score of either 0.7
or 1.  We decided to keep the original values as is, and only
multiplied the scores of negative terms by -1 as all of the tested
approaches presupposed different score signs for terms with opposite
semantic orientations (plus for positive entries and minus for the
negative ones).\footnote{We will investigate the impact of other
  lexicons with presumably better scoring later in
  Section~\ref{cgsa:subsec:eval:lexicons}.}  Moreover, because some
analyzers (e.g., \citet{Taboada:11} and \citet{Musto:14}) expected
part-of-speech tagged entries, we automatically tagged all terms of
this polarity list using \texttt{TreeTagger} \cite{Schmid:95},
assigning to them their most probable tag sequences and (to address
ambiguity) PoS sequences whose probability was at least a half of the
maximal one, duplicating the entries if such alternative variant was
present.

Furthermore, since all of the systems except for that
of~\citet{Kolchyna:15} by default returned continuous real values, but
our evaluation required discrete polarity labels (\emph{positive},
\emph{negative}, or \emph{neutral}) instead, we discretized the
results of these approaches using the following simple procedure: We
first determined the optimal threshold values for the scores of each
particular polar class on the training and development
sets,\footnote{Since none of the methods required training or involved
  any sophisticated hyper-parameters, we used both training and
  development data to optimize the threshold scores.} and then derived
polarity labels for new test messages by comparing their predicted SO
score with these thresholds.  To achieve the former objective (i.e.,
find the optimal thresholds), we exhaustively searched through all
unique polarity values assigned to the training and development
instances and checked whether using this value as a boundary between
two adjacent polarity classes (sorted in ascending order of their
positivity) would increase the overall macro-\F{} on the train and dev
sets.

The final results of this evaluation are shown in
Table~\ref{snt-cgsa:tbl:lex-res}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}$^{+/-}$} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

      \multicolumn{12}{c}{\cellcolor{cellcolor}PotTS}\\
      % Thresholds (PotTS):
      % Hu-Liu: F1: 0.415769; thresholds: [-0.30000000000000004, 0];
      % uses negation;
      % Taboada: f1_abs: 0.349700; f1_cnt: 0.194517; f1_mean: 0.352276
      % best F1: 0.352276; self._thresholds: [-0.10000000000000002, 0.14]; (mean scores)
      % uses: negation, intensification, irrealis;
      % Musto: F1: 0.420251; thresholds: [-0.033333333333333326, 0.02500000000000009];
      % uses: negation;
      % Jurek: F1: F1: 0.317588; thresholds: [-0.0053197382707281865, 0.0017843579564678833];
      % uses: negation and intensifiers;
      % Kolchyna:
      % uses: negation;

      % Hu-Liu Commands:
      % -----------------
      % cgsa_sentiment train -t musto -l cgsa/data/lexicons/zrch.manual.txt \
      % data/PotTS/preprocessed/train/*.tsv data/PotTS/preprocessed/dev/*.tsv
      %
      % cgsa_sentiment test -m cgsa/data/models/cgsa.model data/PotTS/preprocessed/test/*.tsv\
      % > data/PotTS/preprocessed/predicted/hu-liu/hu-liu.test
      %
      % cgsa_evaluate data/PotTS/preprocessed/test/ \
      % data/PotTS/preprocessed/predicted/hu-liu/hu-liu.test
      %
      % Hu-Liu Results:
      % ----------------
      % cgsa_evaluate data/PotTS/preprocessed/test/ \
      % data/PotTS/preprocessed/predicted/hu-liu/hu-liu.test
      % General Statistics:
      % precision    recall  f1-score   support
      % positive       0.44      0.15      0.23       680
      % negative       0.22      0.14      0.17       287
      % neutral       0.36      0.72      0.48       558
      % avg / total       0.37      0.36      0.31      1525
      % Macro-Averaged F1-Score (Positive and Negative Classes): 19.77%
      % Micro-Averaged F1-Score (All Classes): 35.8689%

      HL & \textbf{0.44} & 0.15 & 0.23 & %
       \textbf{0.22} & \textbf{0.14} & \textbf{0.17} & %
       0.36 & 0.72 & 0.48 & %
       \textbf{0.198} & 0.359\\

       % Taboada Commands:
       % -----------------
       % cgsa_sentiment train -t musto -l cgsa/data/lexicons/zrch.manual.txt \
       % data/PotTS/preprocessed/train/*.tsv data/PotTS/preprocessed/dev/*.tsv
       %
       % cgsa_sentiment test -m cgsa/data/models/cgsa.model data/PotTS/preprocessed/test/*.tsv\
       % > data/PotTS/preprocessed/predicted/taboada/taboada.test
       %
       % cgsa_evaluate data/PotTS/preprocessed/test/ \
       % data/PotTS/preprocessed/predicted/taboada/taboada.test
       %
       % Taboada Results:
       % ----------------
       % General Statistics:
       % precision    recall  f1-score   support
       % positive       0.44      0.13      0.20       680
       % negative       0.22      0.12      0.16       287
       % neutral       0.37      0.76      0.49       558
       % avg / total       0.37      0.36      0.30      1525
       % Macro-Averaged F1-Score (Positive and Negative Classes): 17.99%
       % Micro-Averaged F1-Score (All Classes): 36.0656%

       TBD & \textbf{0.44} & 0.13 & 0.2 & %
        \textbf{0.22} & 0.12 & 0.16 & %
        \textbf{0.37} & \textbf{0.76} & \textbf{0.49} & %
        0.18 & 0.36\\

       % Musto Commands:
       % -----------------
       % cgsa_sentiment train -t musto -l cgsa/data/lexicons/zrch.manual.txt \
       % data/PotTS/preprocessed/train/*.tsv data/PotTS/preprocessed/dev/*.tsv
       %
       % cgsa_sentiment test -m cgsa/data/models/cgsa.model data/PotTS/preprocessed/test/*.tsv\
       % > data/PotTS/preprocessed/predicted/musto/musto.test
       %
       % cgsa_evaluate data/PotTS/preprocessed/test/ \
       % data/PotTS/preprocessed/predicted/musto/musto.test
       %
       % Musto Results:
       % ----------------
       % General Statistics:
       % precision    recall  f1-score   support
       % positive       0.43      0.14      0.22       680
       % negative       0.21      0.14      0.17       287
       % neutral       0.36      0.71      0.48       558
       % avg / total       0.36      0.35      0.30      1525
       % Macro-Averaged F1-Score (Positive and Negative Classes): 19.25%
       % Micro-Averaged F1-Score (All Classes): 35.1475%

       MST & 0.43 & 0.14 & 0.22 & %
        0.21 & \textbf{0.14} & \textbf{0.17} & %
        0.36 & 0.71 & 0.48 & %
        0.193 & 0.351\\

       % Jurek Commands:
       % ---------------
       % cgsa_sentiment train -t jurek -l cgsa/data/lexicons/zrch.manual.txt \
       % data/PotTS/preprocessed/train/*.tsv data/PotTS/preprocessed/dev/*.tsv
       %
       % cgsa_sentiment test -m cgsa/data/models/cgsa.model data/PotTS/preprocessed/test/*.tsv\
       % > data/PotTS/preprocessed/predicted/jurek/jurek.test
       %
       % cgsa_evaluate data/PotTS/preprocessed/test/ \
       % data/PotTS/preprocessed/predicted/jurek/jurek.test
       %
       % Jurek Results:
       % ----------------
       % General Statistics:
       % precision    recall  f1-score   support
       % positive       0.42      0.25      0.31       680
       % negative       0.05      0.00      0.01       287
       % neutral       0.36      0.71      0.48       558
       % avg / total       0.33      0.37      0.32      1525
       % Macro-Averaged F1-Score (Positive and Negative Classes): 15.95%
       % Micro-Averaged F1-Score (All Classes): 37.1803%

      JRK & 0.42 & \textbf{0.25} & \textbf{0.31} & %
       0.05 & 0.00 & 0.01 & %
       0.36 & 0.71 & 0.48 & %
       0.16 & \textbf{0.371}\\

       % General Statistics:
       % precision    recall  f1-score   support
       % positive       0.42      0.12      0.19       680
       % negative       0.17      0.08      0.11       287
       % neutral       0.35      0.76      0.48       558
       % avg / total       0.35      0.35      0.28      1525
       % Macro-Averaged F1-Score (Positive and Negative Classes): 14.99%
       % Micro-Averaged F1-Score (All Classes): 34.6230%

      KLCH & 0.42 & 0.12 & 0.19 & %
       0.17 & 0.08 & 0.11 & %
       0.35 & \textbf{0.76} & 0.48 & %
       0.15 & 0.346\\

      \multicolumn{12}{c}{\cellcolor{cellcolor}SB10k}\\
      % Thresholds (SB10k):
      % Hu-Liu: F1: 0.422754; thresholds: [-0.30000000000000004, 0.7];
      % Taboada: f1_abs: 0.424474; f1_cnt: 0.140922; f1_mean: 0.424332
      % best F1: 0.424474; self._thresholds: [-0.2799999999999999, 0.8];
      % Musto: F1: 0.425264; thresholds: [-0.08333333333333333, 0.2125];
      % Jurek: F1: 0.441522; thresholds: [-0.0094822406941572363, 0.10091421749114796];
      % Kolchyna:

      % cgsa_evaluate data/SB10k/preprocessed/test/ \
      % data/SB10k/preprocessed/predicted/hu-liu/hu-liu.test
      % General Statistics:
      % precision    recall  f1-score   support
      % positive       0.44      0.33      0.38       354
      % negative       0.21      0.23      0.22       212
      % neutral       0.65      0.70      0.67       930
      % avg / total       0.54      0.54      0.54      1496
      % Macro-Averaged F1-Score (Positive and Negative Classes): 29.79%
      % Micro-Averaged F1-Score (All Classes): 54.4786%
      HL & \textbf{0.44} & \textbf{0.33} & \textbf{0.38} & %
        0.21 & 0.23 & 0.22 & %
        0.65 & 0.7 & 0.67 & %
        0.298 & 0.545\\

        % General Statistics:
        % precision    recall  f1-score   support
        % positive       0.44      0.33      0.38       354
        % negative       0.19      0.20      0.19       212
        % neutral       0.66      0.72      0.69       930
        % avg / total       0.54      0.55      0.54      1496
        % Macro-Averaged F1-Score (Positive and Negative Classes): 28.68%
        % Micro-Averaged F1-Score (All Classes): 55.2139%
      TBD & \textbf{0.44} & \textbf{0.33} & \textbf{0.38} & %
        0.19 & 0.2 & 0.19 & %
        0.66 & 0.72 & 0.69 & %
        0.267 & 0.552\\

        % General Statistics:
        % precision    recall  f1-score   support
        % positive       0.39      0.29      0.33       354
        % negative       0.23      0.26      0.25       212
        % neutral       0.65      0.69      0.67       930
        % avg / total       0.53      0.53      0.53      1496
        % Macro-Averaged F1-Score (Positive and Negative Classes): 29.03%
        % Micro-Averaged F1-Score (All Classes): 53.4091%
      MST & 0.39 & 0.29 & 0.33 & %
        0.23 & \textbf{0.26} & 0.25 & %
        0.65 & 0.69 & 0.67 & %
        0.29 & 0.534\\

        % General Statistics:
        % precision    recall  f1-score   support
        % positive       0.40      0.30      0.35       354
        % negative       0.34      0.21      0.26       212
        % neutral       0.68      0.80      0.73       930
        % avg / total       0.56      0.60      0.57      1496
        % Macro-Averaged F1-Score (Positive and Negative Classes): 30.19%
        % Micro-Averaged F1-Score (All Classes): 59.8930%
      JRK & 0.4 & 0.3 & 0.35 & %
        \textbf{0.34} & 0.21 & \textbf{0.26} & %
        \textbf{0.68} & 0.8 & 0.73 & %
        \textbf{0.302} & 0.599\\

        % General Statistics:
        % precision    recall  f1-score   support
        % positive       0.43      0.18      0.25       354
        % negative       0.25      0.08      0.12       212
        % neutral       0.66      0.91      0.76       930
        % avg / total       0.55      0.62      0.55      1496
        % Macro-Averaged F1-Score (Positive and Negative Classes): 18.24%
        % Micro-Averaged F1-Score (All Classes): 61.8984%
      KLCH & 0.43 & 0.18 & 0.25 & %
        0.25 & 0.08 & 0.12 & %
        0.66 & \textbf{0.91} & \textbf{0.76} & %
        0.182 & \textbf{0.619}\\\bottomrule
\end{tabular}
    \egroup
    \caption[Evaluation of lexicon-based CGSA methods.]{
      Evaluation of lexicon-based CGSA methods.\\
      {\small HL~--~\citet{Hu:04}, TBD~--~\citet{Taboada:11}, MST~-- \citet{Musto:14}, JRK
        -- \citet{Jurek:15}, KLCH -- \citet{Kolchyna:15}}}
    \label{snt-cgsa:tbl:lex-res}
  \end{center}
\end{table}

As we can see from the figures, the performance of the tested methods
is widely varies across different polarity classes and datasets: For
example, the most simple approach of~\citet{Hu:04} achieves
surprisingly good results on the PotTS corpus, attaining the highest
precision on the positive class and also yielding the best scores for
negative messages, which in turn leads to a superior macro-averaged
\F{} for all classes.  The more elaborate analyzer
of~\citet{Taboada:11}, which can be viewed as an extension of
\citeauthor{Hu:04}'s system, can still keep up the high precision of
the predicted positive and negative microblogs, and even significantly
improves on the results for the neutral class, but nevertheless looses
0.018 overall macro-\F{} against the first system due to a lower
recall of positive and negative tweets.  A better performance in this
respect is shown by the remaining three methods, each of which
establishes a new recall benchmark for at least one of the considered
polarity classes: The MST analyzer, for instance, sets the best recall
score for the negative class, the JRK system noticeably boosts the
results for the positive polarity, and the approach
of~\citet{Kolchyna:15} attains the best recall value for neutral
messages, being on par with the results of~\citet{Taboada:11}.  Mostly
due to this high recall on the dominating positive polarity class, the
classifier of~\citet{Jurek:15} also reaches the highest micro-averaged
\F{}-score among all competitors on this dataset.

A different situation can be observed with the SB10k dataset.  This
time, both the method of~\citet{Hu:04} and that of~\citet{Taboada:11}
reach almost identical scores on the positive class, outperforming
other systems on this polarity, but fail to surpass the scores of the
alternative analyzers on any other metric.  Furthermore, similarly to
the previous experiment, the MST approach again shows the highest
recall of negative microblogs, but since this class is by far the most
underrepresented one in both corpora, the overall \F{} metrics are not
affected by this success.  Finally, the last two classifiers---JRK and
KLCH---show quite promising figures for the negative and neutral
classes, also attaining the best micro- and macro-averaged \F{}-scores
(0.302 and 0.619 respectively) on all polarity classes.

\subsection{Polarity-Changing Factors}\label{subsec:cgsa:lex-methods:pol-change}

Since one of the most important components of any lexicon-based CGSA
approach is the analysis of factors which might significantly affect
the prior semantic orientation of polar terms, we decided to recheck
the utility of this module for different systems.  In order to do so,
we successively deactivated one by one parts of the systems which
analyzed different phenomena of the surrounding context and recomputed
their scores after these changes.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.15\columnwidth} % first columm
        *{10}{>{\centering\arraybackslash}p{0.082\columnwidth}}}
      \toprule
      \multirow{2}{0.15\columnwidth}{%
      \bfseries Deactivated\newline Polarity-Changing\newline Factor} & %
      \multicolumn{10}{c}{\bfseries CGSA System}\\
      & \multicolumn{2}{c}{\bfseries HL} & \multicolumn{2}{c}{\bfseries TBD} %
      & \multicolumn{2}{c}{\bfseries MST} %
      & \multicolumn{2}{c}{\bfseries JRK} & \multicolumn{2}{c}{\bfseries KLCH}\\%
      \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7} %
      \cmidrule(lr){8-9}\cmidrule(lr){10-11}

      & Macro\newline \F{}$^{+/-}$ & Micro\newline \F{} %
      & Macro\newline \F{}$^{+/-}$ & Micro\newline \F{} %
      & Macro\newline \F{}$^{+/-}$ & Micro\newline \F{} %
      & Macro\newline \F{}$^{+/-}$ & Micro\newline \F{} %
      & Macro\newline \F{}$^{+/-}$ & Micro\newline \F{}\\\midrule

      \multicolumn{11}{c}{\cellcolor{cellcolor}PotTS}\\
      None & \textbf{0.198} & 0.359 & 0.18 & 0.36 & 0.193 & 0.351 %
      & 0.16 & \textbf{0.371} & 0.15 & 0.346\\

      % Training hu-liu
      % Testing hu-liu
      % Evaluating hu-liu
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.22      0.13      0.16       287
      % neutral       0.36      0.72      0.48       558
      % positive       0.44      0.15      0.23       680
      % avg / total       0.37      0.36      0.31      1525
      % Macro-Averaged F1-Score (Positive and Negative Classes): 19.68%
      % Micro-Averaged F1-Score (All Classes): 35.8033%

      % Training taboada
      % Testing taboada
      % Evaluating taboada
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.23      0.13      0.16       287
      % neutral       0.37      0.76      0.49       558
      % positive       0.44      0.13      0.20       680
      % avg / total       0.37      0.36      0.30      1525
      % Macro-Averaged F1-Score (Positive and Negative Classes): 18.21%
      % Micro-Averaged F1-Score (All Classes): 36.1311%

      % Training musto
      % Testing musto
      % Evaluating musto
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.22      0.14      0.17       287
      % neutral       0.36      0.71      0.48       558
      % positive       0.44      0.15      0.23       680
      % avg / total       0.37      0.36      0.31      1525
      % Macro-Averaged F1-Score (Positive and Negative Classes): 19.77%
      % Micro-Averaged F1-Score (All Classes): 35.5410%

      % Training jurek
      % Testing jurek
      % Evaluating jurek
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.23      0.09      0.13       287
      % neutral       0.36      0.75      0.49       558
      % positive       0.45      0.17      0.25       680
      % avg / total       0.38      0.37      0.32      1525
      % Macro-Averaged F1-Score (Positive and Negative Classes): 19.17%
      % Micro-Averaged F1-Score (All Classes): 36.9836%

      % Training kolchyna
      % Testing kolchyna
      % Evaluating kolchyna
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.22      0.10      0.14       287
      % neutral       0.36      0.78      0.49       558
      % positive       0.41      0.10      0.16       680
      % avg / total       0.35      0.35      0.28      1525
      % Macro-Averaged F1-Score (Positive and Negative Classes): 15.16%
      % Micro-Averaged F1-Score (All Classes): 35.2131%

      Negation & 0.197 & 0.358 & 0.182 & 0.361 & \textbf{0.198} &  %
      0.355 & 0.192 & 0.37 & 0.151 & 0.352\\

      % Training taboada
      % Testing taboada
      % Evaluating taboada
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.23      0.12      0.16       287
      % neutral       0.37      0.77      0.50       558
      % positive       0.45      0.13      0.20       680
      % avg / total       0.38      0.36      0.30      1525
      % Macro-Averaged F1-Score (Positive and Negative Classes): 18.05%
      % Micro-Averaged F1-Score (All Classes): 36.2623%

      % Training jurek
      % Testing jurek
      % Evaluating jurek
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.24      0.10      0.14       287
      % neutral       0.36      0.75      0.49       558
      % positive       0.44      0.16      0.24       680
      % avg / total       0.38      0.37      0.31      1525
      % Macro-Averaged F1-Score (Positive and Negative Classes): 19.09%
      % Micro-Averaged F1-Score (All Classes): 36.7213%
      Intensification & NA & NA & 0.181 & 0.363 & NA &  %
      NA & 0.191 & 0.367 & NA & NA\\

      % Training taboada
      % Testing taboada
      % Evaluating taboada
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.21      0.13      0.16       287
      % neutral       0.36      0.74      0.49       558
      % positive       0.44      0.15      0.22       680
      % avg / total       0.37      0.36      0.31      1525
      % Macro-Averaged F1-Score (Positive and Negative Classes): 18.76%
      % Micro-Averaged F1-Score (All Classes): 35.8033%
      Others & NA & NA & 0.188 & 0.358 & NA &  %
      NA & NA & NA & NA & NA\\

      \multicolumn{11}{c}{\cellcolor{cellcolor}SB10k}\\
      None & 0.298 & 0.545 & 0.267 & 0.552 & 0.29 & 0.534 %
      & 0.302 & 0.599 & 0.182 & \textbf{0.619}\\

      % Training hu-liu
      % Testing hu-liu
      % Evaluating hu-liu
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.20      0.22      0.21       212
      % neutral       0.65      0.70      0.67       930
      % positive       0.44      0.33      0.37       354
      % avg / total       0.54      0.54      0.54      1496
      % Macro-Averaged F1-Score (Positive and Negative Classes): 29.08%
      % Micro-Averaged F1-Score (All Classes): 54.2781%

      % Training taboada
      % Testing taboada
      % Evaluating taboada
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.19      0.20      0.20       212
      % neutral       0.66      0.72      0.69       930
      % positive       0.45      0.31      0.37       354
      % avg / total       0.54      0.55      0.54      1496
      % Macro-Averaged F1-Score (Positive and Negative Classes): 28.23%
      % Micro-Averaged F1-Score (All Classes): 55.2807%

      % Training musto
      % Testing musto
      % Evaluating musto
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.21      0.23      0.22       212
      % neutral       0.65      0.64      0.65       930
      % positive       0.37      0.36      0.37       354
      % avg / total       0.52      0.52      0.52      1496
      % Macro-Averaged F1-Score (Positive and Negative Classes): 29.11%
      % Micro-Averaged F1-Score (All Classes): 51.7380%

      % Training jurek
      % Testing jurek
      % Evaluating jurek
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.27      0.13      0.18       212
      % neutral       0.67      0.81      0.73       930
      % positive       0.40      0.32      0.35       354
      % avg / total       0.55      0.59      0.56      1496
      % Macro-Averaged F1-Score (Positive and Negative Classes): 26.50%
      % Micro-Averaged F1-Score (All Classes): 59.4251%

      % Training kolchyna
      % Testing kolchyna
      % Evaluating kolchyna
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.26      0.10      0.14       212
      % neutral       0.65      0.31      0.42       930
      % positive       0.25      0.69      0.37       354
      % avg / total       0.50      0.37      0.36      1496
      % Macro-Averaged F1-Score (Positive and Negative Classes): 25.41%
      % Micro-Averaged F1-Score (All Classes): 36.6310%
      Negation & 0.291 & 0.543 & 0.282 & 0.553 & 0.291 & %
      0.517 & 0.265 & 0.594 & 0.254 & 0.366\\

      % Training taboada
      % Testing taboada
      % Evaluating taboada
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.20      0.20      0.20       212
      % neutral       0.66      0.73      0.69       930
      % positive       0.45      0.32      0.38       354
      % avg / total       0.54      0.56      0.55      1496
      % Macro-Averaged F1-Score (Positive and Negative Classes): 28.89%
      % Micro-Averaged F1-Score (All Classes): 55.6818%

      % Training jurek
      % Testing jurek
      % Evaluating jurek
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.34      0.21      0.26       212
      % neutral       0.67      0.80      0.73       930
      % positive       0.41      0.30      0.35       354
      % avg / total       0.56      0.60      0.57      1496
      % Macro-Averaged F1-Score (Positive and Negative Classes): 30.25%
      % Micro-Averaged F1-Score (All Classes): 59.8930%
      Intensification & NA & NA & 0.289 & 0.557 & NA &  %
      NA & \textbf{0.303} & 0.599 & NA & NA\\

      % Training taboada
      % Testing taboada
      % Evaluating taboada
      % General Statistics:
      % precision    recall  f1-score   support
      % negative       0.20      0.22      0.21       212
      % neutral       0.65      0.70      0.67       930
      % positive       0.44      0.32      0.37       354
      % avg / total       0.54      0.54      0.54      1496
      % Macro-Averaged F1-Score (Positive and Negative Classes): 29.01%
      % Micro-Averaged F1-Score (All Classes): 54.0775%
      Others & NA & NA & 0.29 & 0.541 & NA &  %
      NA & NA & NA & NA & NA\\\bottomrule
\end{tabular}
    \egroup
    \caption[Evaluation of lexicon-based CGSA methods.]{
      Evaluation of lexicon-based CGSA methods.\\
      {\small HL~--~\citet{Hu:04}, TBD~--~\citet{Taboada:11}, MST~-- \citet{Musto:14}, JRK
        -- \citet{Jurek:15}, KLCH -- \citet{Kolchyna:15}}}
    \label{snt-cgsa:tbl:lex-res-ablation}
  \end{center}
\end{table}

As we can see from the results in
Table~\ref{snt-cgsa:tbl:lex-res-ablation}, the top-performing systems
(HL and JRK for the PotTS corpus and JRK and KLCH for the SB10k
dataset) typically achieve their best ranks with all context-analysis
factors switched on.  The only exception to this, is the
macro-averaged \F{}-score of the JRK method on the SB10k test set,
which gets slightly better (from 0.302 to 0.303) when the intensifiers
of polar terms are ignored.  At the same time, other approaches rather
benefit from the deactivation of their context modules, showing a
constant improvement of their macro-averaged results.  As to the
micro-averaged scores, the situation is much more varied: While the
method of~\citet{Taboada:11} profits from the switched-off negation
and intensification handling, its accuracy drops when the remaining
polarity-switching factors (irrealis, interrogative clauses, quotes,
etc.) are ignored.  Yet another situation is observed with the systems
of~\citet{Musto:14} and \citet{Kolchyna:15}, whose accuracy improves
on the PotTS corpus, but degrades on the SB10k dataset when the
negation handling is deactivated.

\subsection{Error Analysis}\label{subsec:cgsa:lex-methods:err-analysis}

\section{Machine-Learning Methods}\label{sec:cgsa:ml-based}

Despite their immense popularity, linguistic plausibility, and
simplicity to implement, lexicon-based approaches often have been
criticized for the rigidness of their classification\footnote{Since
  these systems only rely on the precomputed weights of lexicon
  entries, considering these coefficients as constant, their decision
  boundaries frequently appear to be suboptimal as many terms might
  have different polarity and intensity values depending on the domain
  \cite[see][]{Eisenstein:17,Yang:17}.} and the inability to
incorporate additional, non-lexical attributes into the final
decisions.  Moreover, as noted by~\citet{Pang:02} and also confirmed
empirically by~\citet{Riloff:03} and \citet{Gamon:04}, many of the
linguistic expressions which actually correlate with the subjectivity
or polarity of a sentence (e.g., exclamation marks or spelling
variations) are very unlikely to be included into a sentiment lexicon
even by a human expert.  As a consequence of this, with the emergence
of new manually annotated corpora, lexicon-based systems have been
gradually superseded by supervised machine-learning techniques.

One of the first steps in this direction was taken
by~\citet{Wiebe:99}, who used a Na{\"i}ve Bayes classifier to
differentiate between subjective and objective statements.  Using
primarily binary features which reflected the presence of a pronoun,
an adjective, a cardinal number, or a modal verb in the analyzed
sentence, the authors achieved an accuracy of~72.17\% on this
two-class prediction task, outperforming the majority class baseline
by more than 20~percent.  An even better result (81.5\%) could be
reached when the dataset was restricted only to the examples with the
most confident annotations.

Inspired by the success of this approach,~\citet{Yu:03} presented a
more elaborated system, in which they first distinguished between
subjective and objective documents, then differentiated between polar
and neutral sentences, and, finally, classified the polarity of the
opinionated clauses.  As in the previous case, the authors used a
Na{\"i}ve Bayes predictor for the document-level task, reaching a
remarkable \F-score of~0.96 on this objective; and applied an ensemble
of NB systems to predict the subjectivity of single sentences.  To
determine the semantic orientation of a subjective clause,
\citeauthor{Yu:03} averaged the polarity scores of its tokens,
obtaining these scores from an automatically constructed sentiment
lexicon~\cite{Hatzivassi:97}.  This way, they attained an accuracy
of~91\% on a set of 38 sentences which had a perfect inter-annotator
agreement in their data.

Yet another multi-stage Na{\"i}ve Bayes model was proposed by
\citet{Pang:04}, who tried to classify the overall semantic
orientation of movie reviews (positive vs. negative) by first dividing
the sentences into subjective and objective ones (achieving 92\%
accuracy on this subtask) and then predicting the overall polarity of
a review using only subjective passages.  With this architecture, the
authors achieved statistically significant improvements over the
baseline method (in which they commonly considered all sentences
disregarding their subjectivity), boosting the accuracy of polarity
classification from~82.8 to~86.4\%.

In order to check the effectiveness of the Na{\"i}ve Bayes approach,
\citet{Pang:02} compared the results of NB, MaxEnt, and SVM systems on
the movie review classification task, attempting to predict whether a
review was perceived as thumbs up or thumbs down.  In contrast to the
previous works, they found the SVM classifier working best for this
objective, yielding 82.9\% accuracy when used with unigram features
only.  This conclusion paved the way for the following general triumph
of the support-vector approach, which was dominating the whole CGSA
research field for almost a decade ever since.  For example,
\citet{Gamon:04} also trained an SVM predictor on a rich set of
linguistic and surface-level features (including part-of-speech
trigrams, context-free phrase-structure patterns, and part-of-speech
information coupled with semantic relations) to distinguish between
positive and negative customer feedback, achieving 77.5\% accuracy and
$\approx$0.77~\F{} by using only top 2,000 attributes which had the
highest log-likelihood ratios with the target
classes.  % Interestingly enough,
% \citeauthor{Gamon:04} also could obtain quite competitive figures
% (74.5\% accuracy) by using linguistically motivated features only.
Furthermore, \citet{Pang:05} addressed the problem of multi-class
rating, trying to predict the number of stars that an author could
assign to a review.  For this purpose, they compared three different
SVM types:
\begin{inparaenum}[(i)]
\item one-versus-all SVM (OVA-SVM),
\item SVM regression,
\item and OVA-SVM with \emph{metric labeling};
\end{inparaenum}
getting their best results ($\approx$52\%~accuracy) with the last
option.
% \citet{Pang:05}
% In the last approach, in addition to maximizing the score of the
% correct labels, the authors also explicitly encoded the objective of
% minimizing the absolute difference between the predicted labels of
% similar training examples (measuring this similarity with the
% percentage of positive sentences).  This strategy brought
% statistically significant improvements over the first two baselines,
% yielding an average accuracy of $\approx$52\%.
Finally, \citet{Ng:06} proposed another multi-stage SVM system, where
they first classified whether the given text was a review or not and
then tried to predict its polarity in case it was.  Due to a better
usage of higher-order $n$-grams (where, instead of bluntly considering
all token sequences up to the length $n$ as new features, the authors
only took 5,000 most useful ones, measuring their utility with the
weighted log-likelihood ratio), \citet{Ng:06} even improved the then
state of the art on the \citeauthor{Pang:04}'s corpus, boosting the
accuracy of polarity classification from~87.1 to~90.5\%.

% \todo[inline]{Feature Selection}
% \done[inline]{\citet{Li:10b}}

% \citet{Li:10b} addressed the problem of polarity shifting using
% machine-learning techniques.  For this purpose, the authors first
% selected most frequent and indicative features of the two main
% polarity classes (positive and negative), and then culled training
% instances containing these attributes whose labels, however, were
% different from the ones sugested by the features.  After obtaining
% this polarity shifted subset, \citeauthor{Li:10b} trained several
% linear support-vector classifiers, one of which had to distinguish
% between polarity-shifted and polarity-preserving sentences, the other
% two were to classify the semantic orientation of these two groups
% (i.e., one system had to predict the polarity of shifted instances,
% and the other one had to determine the semantic orientation of
% polarity-preserving ones), and the last one was trained on the
% complete original dataset---the product review corpus
% of~\citet{Blitzer:06}---again to predict the polarity of complete
% sentences disregarding their possible polarity shifts.  The authors
% achieved their best results~(80,9\% average accuracy) using a
% combination of the last three systems with a special meta-classifier
% joining their single decisions.

% \done[inline]{\citet{Wiebe:05a}}

% A semi-supervised approach to sentence-level subjectivity prediction
% was proposed by~\citet{Wiebe:05a}.  Using an existing rule-based
% sentiment system, the authors classified a large set of unlabeled
% sentences from newspaper articles into subjective and objective ones,
% achieving 34.2\% recall and 90.4\% precision for the former class and
% getting 30.7\% recall and 82.4\% precision for the latter orientation.
% Afterwards, with the help of the AutoSlog-TS
% algorithm~\cite{Riloff:96}, they extracted frequent lexico-syntactic
% patterns that strongly correlated with the subjectivity of a sentence,
% and trained a Na{\"i}ve Bayes system, considering the occurrences of
% the learned patterns as features.  In an attempt to improve the
% results of this approach even further, \citeauthor{Wiebe:05a} also
% implemented a self-improvement strategy in which they relabeled the
% original unannotated dataset with the obtained NB classifier, then
% took one half of the most confidently classified sentences as a new
% training set, and extracted new subjectivity patterns, retraining the
% Na{\"i}ve Bayes module on the updated set of features.  With this
% final classifier, the authors attained an accuracy of 73.8\% on
% predicting the subjectivity of sentences in the MPQA
% corpus~\cite{Wiebe:05}, coming close to the results achieved by a
% fully supervised ML approach (76\%).

% \done[inline]{\citet{Riloff:06}}

% \citet{Riloff:06} addressed the problem of redundant features,
% hypothesizing that correlated attributes (such as unigram
% ``\emph{happy}'' and bigram ``\emph{very happy}'') would rather harm
% the performance of a classifier than improve its accuracy.  To prove
% this hypothesis, the authors defined two kinds of redundancy relations
% which could exist between intersecting input traits:
% \emph{representational} and \emph{behavioral} subsumption.  The former
% former type was assumed to hold between features~$A$ and $B$ if all
% occurrences of the attribute~$A$ were a strict superset of the
% occurrences of~$B$.  The behavioral subsumption meant that the
% information gain (IG) \cite{Forman:03} of the feture $B$ was at most
% some negligible value $\delta$ lower than the respective IG score of
% the attribute $A$.  In their experiments, \citeauthor{Riloff:06}
% observed that the accuracy of an SVM classifier indeed increased (from
% 81.7 to 82.7\% on the IMDBP dataset and from 74.4 to 74.9\% on the
% MPQA corpus) after they excluded all attributes that were
% representationally and behaviorally subsumed by other features.  These
% improvements even outperformed the gains that could be achieved with
% traditional feature selection methods.

% \todo[inline]{Higher-order $n$-grams}

% \done[inline]{\citet{Ng:06}}

% Further on, \citet{Ng:06} simultaneously addressed two classification
% problems: distinguishing whether a given text snippet was a review or
% not and determining the polarity of the review.  The authors attained
% impressive results (99.8\% accuracy) for the former task, using only
% SVM with unigram features.  Moreover, they also outperformed the then
% state of the art on the \citet{Pang:04}'s corpus, boosting the
% accuracy from~87.1 to~90.5\%.  These changes were mostly due to a
% smarter use of higher-order $n$-grams, where, instead of bluntly
% considering all token sequences up to the order $n$ as new features,
% \citet{Ng:06} only took 5,000 most useful ones, measuring their
% utility with the weightes log-likelihood ratio \cite{Nigam:00}.

% \done[inline]{\citet{Mejova:11}}

% \citet{Mejova:11} investigated the effect of different features on
% various datasets---the movie review corpus of~\citet{Pang:04}, the
% product reviews gathered by~\citet{Jindal:07}, and the customer
% feedback dataset of~\citet{Blitzer:06}, coming to the conclusion that,
% in general, preserving the original form of tokens (i.e., keeping the
% original token forms instead of lemmas) and using their frequency
% scores instead of binary values was beneficial to the results on all
% test sets.  The use of different $n$-gram lengths, however, had a
% mixed effect with the best scores typically yielded by the union of
% uni-, bi-, and tri-gram features.  Last but not least, they found the
% negation heuristics proposed by~\citet{Das:01} (adding the
% \texttt{\_NOT} suffix to all tokens following a negation up to the
% first punctuation) leading to only marginal improvements.  The authors
% achieved their best results (87.5\%, 94.7\%, and 89.6\% accuracy on
% the datasets of \citet{Pang:04}, \citet{Jindal:07}, and
% \citet{Blitzer:06}, respectively) with the union of unnormalized uni-,
% bi-gram and tri-gram features without negation when using term
% frequencies as feature values.

% \done[inline]{\citet{Riloff:03a}}

% \citet{Riloff:03a} addressed the problem of inusfficient manually
% annotated sentiment resources by proposing a bootstrapping method for
% training a subjectivity classifier.  In this approach, the authors
% first applied two high-precision predictors to a large collection of
% unlabeled sentences in order to get an initial set of subjective and
% objective instances.  Afterwards, they used the AutoSlog-TS
% algorithm~\cite{Riloff:96} to extract expressions which strongly
% correlated with the subjective class and employed these phrases to
% classify the remaining sentences from the corpus.
% \citeauthor{Riloff:03a} repeated the last two steps (pattern
% extraction and expansion of the training set) multiple times to
% transitively learn new subjective phrases.  With the final system, the
% authors achieved a precision of 0.902 on recognizing polar sentences,
% with their recall running up to 0.401.

% \done[inline]{\citet{Wilson:04,Wilson:06}}

% A related problem, namely that of classifying the strength of
% opinions, was addressed by~\citet{Wilson:04,Wilson:06}.  In
% particular, the authors proposed a wide variety of linguistic features
% (including automatically learned lexico-syntactic patterns similar to
% the ones used by~\citet{Riloff:03}, bags of words, and syntactic
% attributes such as lemma and PoS tag of the root of a dependency tree,
% lemmas and tags of its intermediate nodes and leaves, lexicalized
% relation tuples, i.e., tuples consisting of the lemma of a parent
% node, its grammatical relation to the child, and the lemma of the
% child itself, etc.), checking the utility of these attributes with
% three different classifiers: BoosTexter~\cite{Schapire:00},
% Ripper~\cite{Cohen:95}, and SVMLight~\cite{Joachims:99}.
% \citet{Wilson:04} achieved their best results (55\%~accuracy and
% 0.991~mean squared error) with the BoosTexter approach when using all
% the introduced features.

However, a real game changer in the sentiment analysis research field

\done[inline]{SemEval 2013}

\done[inline]{\citet{Mohammad:13}}

Finally, with the introduction of the SemEval competition in sentiment
analysis of
Twitter~\cite{Nakov:13,Rosenthal:14,Rosenthal:15,Nakov:16}, a plethora
of new CGSA applications have appeared on the scientific scene, most
of which relied on traditional supervised machine-learning techniques.
One of the most prominent and arguably well-known such systems was
proposed by~\citet{Mohammad:13}, who trained a linear SVM classifier
on an extensive set of linguistic features including character and
token $n$-grams, Brown clusters~\cite{Brown:92}, statistics on
part-of-speech tags, punctuation marks, elongated words etc.  However,
the most useful type of attributes, as noted by the authors, turned
out to be features that reflected information from various sentiment
lexicons.  In particular, depending on the type of the polarity list
from which such information was extracted, the authors introduced two
types of lexicon attributes: \emph{manual} and \emph{automatic} ones.
The former group was computed with the help of the NRC emotion
lexicon~\cite{Mohammad:13a}, MPQA polarity list~\cite{Wilson:05}, and
Bing Liu's manually compiled polarity set~\cite{Hu:04}.  For each of
these resources and for each of the non-neutral polarity classes
(positive and negative), \citeauthor{Mohammad:13} calculated the total
sum of the lexicon scores for all message tokens and also separately
estimated these statistics for each particular part-of-speech tag,
considering them as additional attributes.  Automatic features were
obtained using automatically generated Sentiment140 and Hashtag
Sentiment Base polarity lists \cite{Kiritchenko:14}.  Again, for each
of these lexicons, for each of the two polarity classes, the authors
produced four features representing the number of tokens with non-zero
scores, the sum and the maximum of all respective lexicon values for
all tweet tokens, and the score of the last term.  This final system
ranked first in the inaugural run of the SemEval task, attaining
0.69~\F-score on the three-way classification task, with the automatic
lexicon features alone accounting for at least five percent boost in
the performance of this model.

\done[inline]{\citet{Guenther:13}}

A similar solution (linear SVM with a rich set of features) was
presented by~\citet{Guenther:13}.  Akin to~\citet{Mohammad:13}, the
authors used unmodified and lemmatized unigrams, word clusters, and
lexicon features.  However, in contrast to the previous system, this
application utilized only one polarity list---that
of~\citet{Esuli:05}.  Partially due to this fact,
\citeauthor{Guenther:13} found the word cluster attribute working best
among all features, followed by the stemmed unigram attributes.  This
method also yielded competitive results (0.653~\F) on the
message-level polarity task, attaining second place in this
competition.

\done[inline]{SemEval 2014}

\done[inline]{\citet{Miura:14}}

Later on these results were further improved by~\citet{Miura:14}, who
also utilized a supervised ML approach with character and word
$n$-grams, word clusters, disambiguated senses, and lexicon scores of
message tokens.  Similarly to the NRC-Canada system
of~\citet{Mohammad:13}, the authors made heavy use of various kinds of
polarity lists including AFINN-111~\cite{Nielsen:11}, Liu's Opinion
Lexicon~\cite{Hu:04} , General Inquirer~\cite{Stone:66}, MPQA Polarity
List \cite{Wiebe:05a}, NRC Hashtag and Sentiment140
Lexicon~\cite{Mohammad:13}, and
\textsc{SentiWordNet}~\cite{Esuli:06a}.  However, contrary to
\citeauthor{Mohammad:13}'s approach, \citeauthor{Miura:14} also
applied a whole set of preprocessing steps such as spelling
correction, part-of-speech tagging with lemmatization, and a special
weighting scheme for underrepresented polarity classes.  These
enhancements, combined with a carefully tuned LogLinear classifier,
allowed the authors to boost the sentiment classification results on
SemEval~2014 test set to 0.71 average \F (measure on the two main
polarity classes---positive and negative).

\done[inline]{\citet{Guenther:14}}

In the same vein, \citet{Guenther:14} improved their results from the
previous SemEval run (from 0.654 to 0.691 two-class \F) by extending
their original system with a Twitter-aware
tokenizer~\cite{Owoputi:13}, spelling normalization module, and a
significantly increased of lexicon-based features (this time, instead
of simply relying on the \textsc{SentiWordNet} resource,
\citeauthor{Guenther:14} harnessed a whole ensemble of various
polarity lists including Liu's opinion list, MPQA subjectivity
lexicon, and TwittrAttr polarity resource).  As proved by the ablation
tests, the last change was of particular use to the classification
accuracy, improving the scores by almost five percent.

\done[inline]{SemEval 2015}

\done[inline]{\citet{Hagen:15}}

A different approach to the coarse-grained sentiment-analysis task was
proposed by~\citet{Hagen:15}, who, instead of developing their own
method from scratch, united four already existing solutions
(\citet{Mohammad:13}, \citet{Guenther:13}, \citet{Proisl:13}, and
\citet{Miura:14}) into a single ensemble, taking the average of the
predicted scores as the final decision of the complete system.  This
way, the authors achieved 0.648~\F{} on the SemEval-2015 test set,
attaining first place among 40 participants.

\done[inline]{\citet{Hamdan:15}}

Another supervised machine-learning system was proposed
by~\citet{Hamdan:15}, who also used an extensive set of features such
as word $n$-grams, negation existence, sentiment lexicons and
$Z$-score, which reflected the strength of statistical correlation
between a given term $t$ and the target class $c$ in the distribution
\cite[cf.][]{Hamdan:14}.  Similarly to~\citet{Mohammad:13}, the
authors used an extensive set of lexicon features, and also applied
attribute and class weighting as it was done by~\citet{Guenther:14}.
This way, \citeauthor{Hamdan:15} achieved 0.643~\F on the SemEval-2015
test get, getting third place among all competitors.

% One of the first attempts to analyze message-level sentiments on
% Twitter was made by \citet{Go:09}.  For their experiments, the authors
% collected a set of 1,600,000 tweets containing smileys.  Based on
% these emoticons, they automatically derived polarity classes for these
% messages (positive or negative) and used them to train a Na\"{\i}ve
% Bayes, MaxEnt, and SVM classifier.  The best $F$-score for this
% two-class classification problem could be achieved by the last system
% and run up to 82.2\%.

% Similar work was also done by \citet{Pak:10} who used the Na\"{\i}ve
% Bayes approach to differentiate between neutral, positive, and
% negative microblogs; and \citet{Barbosa:10} who gathered a collection
% of 200,000 tweets, subsequently analyzing them with three publicly
% available sentiment web-services and training an SVM classifier on the
% results of these predictors.  In a similar way, \citet{Agarwal:11}
% compared a simple unigram-based SVM approach with two other
% full-fledged systems, one which relied on a rich set of manually
% defined features, and another used partial tree
% kernels~\cite{Moschitti:06}.  The authors evaluated these methods on a
% commercially acquired corpus of 8,753 foreign-language tweets, which
% were automatically translated into English, finding that a combination
% of these methods worked best for both two- and three-way prediction
% tasks.

% The state-of-the-art results for message level polarity prediction on
% tweets were established by~\citet{Mohammad:13}, whose system (a
% supervised SVM classifier) used a rich set of various features
% including word and character n-grams, PoS statistics, Brown
% clusters~\cite{Brown:92}, etc., and also strongly benefitted from
% automatic corpus-based polarity lists---Sentiment~140 and NRC
% Hashtag~\cite{Mohammad:12,Kiritchenko:14}.  This approach ranked first
% at the SemEval competition~2013~\cite{Nakov:13} and anchieved the
% fourth place on the rerun of this task one year
% later~\cite{Rosenthal:14}, being outperformed by the supervised
% logistic regression approach of~\citet{Miura:14}, who used a heavy
% preprocessing of the data and a special balancing scheme for
% underrepresented classes.  Later on, these results were further
% improved by the apporaches of~\citet{Hagen:15} and \citet{Deriu:16},
% which both relied on ensembles of multiple independent classifiers.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

       &  &  &  & %
       &  &  & %
       &  &  & %
       & \\\bottomrule
    \end{tabular}
    \egroup
    \caption[Evaluation of ML-based coarse-grained SA methods.]{
      Evaluation of ML-based coarse-grained SA methods.\\
      {\small }}
    \label{snt-cgsa:tbl:ml-res}
  \end{center}
\end{table}

\section{Deep-Learning Methods}\label{sec:cgsa:dl-based}

\todo[inline]{Lin, 1998}

Since there was not a complete set of such expressions, it provided
some seeds and then used distributional similarity (Lin, 1998) to find
similar words, which were also likely to be subjectivity
indicators. However, words found this way had low precision and high
recall.

\todo[inline]{\citet{Bespalov:11}}

In (Bespalov et al., 2011), sentiment classification was performed
based on supervised latent n-gram analysis.

\todo[inline]{\citet{Zhou:10}}

\citet{Yessenalina:11}

A real breakthrough in the use of deep neural networks for the
sentence-level sentiment analysis happened with the pioneering work
of~\citet{Socher:11}, who first introduced a recursive autoencoder
(RAE).  In this system, the authors obtained a fixed-width vector
representation for complex phrases $\vec{v}$ by recursively merging
the vectors of adjacent tokens (say $\vec{w}_1$ and $\vec{w}_2$),
first multiplying these vectors with a compositional matrix $W$ and
then applying a non-linear function ($softmax$) to the resulting
product:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}\right)
\end{align*}
Using a max-margin classifier on top of the resulting phrase
representation, \citet{Socher:11} could improve the state-of-the-art
results on predicting the sentence-level polarity of user's blog
posts~\cite{Potts:10} and also outperformed the system
of~\citet{Nasukawa:03} on the MPQA data set~\cite{Wiebe:05}.

Later on, \citet{Socher:12} further improved these scores with the
help of a recursive matrix-vectors space model (RMVSM), in which each
word was associated with a 2-tuple of a vector and matrix---e.g.,
$(\vec{w}_1, W_1)$ and $(\vec{w}_2, W_2)$---and the compositionality
function was redefined as follows:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  W_2\cdot\vec{w}_1\\
  W_1\cdot\vec{w}_2
  \end{bmatrix}\right)
\end{align*}


\done[inline]{\citet{Tang:14b}}

A hybrid approach to coarse-grained sentiment analysis was proposed by
\citet{Tang:14b}, who trained a linear SVM classifier on top of
sentiment-specific word embeddings and hand-crafted features.  To
obtain the former representations, \citeauthor{Tang:14b} devised a
simple feed-forward neural network similar to the one used
by~\citet{Collobert:11}, which, for each token $t$, had to predict the
probability that this token appeared in the surrounding context and
the likelihood that $t$ occurred in a positive or negative microblog.
Since this training required a substantial amount of data, the authors
leveraged a big collection of automatically downloaded tweets,
obtaining noisy sentiment labels for these microblogs with the distant
supervision method of~\citet{Go:09}.  The second part of this
system---manually designed features---were mostly inspired by the work
of~\citet{Mohammad:13} and included $n$-grams, word clusters,
information about emoticons, negtaion, elongated characters and
punctuation marks.  Combining these different traits into a single
feature vector resulted significantly improved classification
accuracy, yielding 0.701~\F{} on the SemEval-2014 test set (second
place among all competing systems).

\done[inline]{\citet{Severyn:15}}

An important breakthrough in the usage of deep learning methods for
sentiment analysis on Twitter happened with the
work~\citet{Severyn:15}, whose proposed feed-forward DL system ranked
first in Subtask~10~A (phrase-level polarity prediction) at
SemEval~2015 \cite{Rosenthal:15} and achieved second place in
Subtask~10~B (message-level classification).  Drawing on the ideas
of~\citet{Kalchbrenner:14}, the authors devised a simple convolutional
network which, taking sentiment-flavored pretrained word2vec vectors
as input, multiplied these embeddings with 300 distinct convolutional
kernels each of width five, subsequently passing the results of this
multiplication to a piecewise linear ReLU filter and densely connected
softmax layer.  An important aspect of this method which accounted for
a huge part of its success was a special initialization scheme
introduced by the authors.  In this inital process,
\citeauthor{Severyn:15} first trained Twitter-specific word2vec
vectors on a large collection of downloaded tweets, then pretrained
the complete system, incuding matrix weights and emeddings, on noisily
labeled microblogs of this corpus, leveraging the approach
of~\citeauthor{Go:09}, and, finally, fine-tuned the parameters of
their network on the officially released SemEval data.  Due to these
improvements, the final system attained a competitive result of
0.6459~two-class~\F{} on the message-level polarity subtask.

\done[inline]{SemEval 2016}

\done[inline]{\citet{Deriu:16}}

\citet{Deriu:16} extended the system of~\citet{Severyn:15} by
increasing the number of convolutional layers (using two layers
instead of just one).  Furthermore, to improve the generalizability of
their system, the authors trained two such networks with different
types of input embeddings, taking pretrained word2vec
vectors~\cite{Mikolov:13} for one classifier and using GloVe
embeddings~\cite{Pennington:14} for another one.  Similarly to
\citet{Severyn:15}, \citeauthor{Deriu:16} first fine-tuned these word
representations on a big collection of tweets, maximizing the context
prediction objective.  Afterwards, they pretrained the parameters of
the networks including embeddings and convolutional filters on a big
set of noisily labeled tweets, and, finally, put the finishing touches
on the weights of these systems by training them on the officially
released SemEval data.  The authors united the predictions of both
networks by training a random forest classifier on top of their output
vectors, establishing a new state of the art (0.633~\F{}) on the
SemEval-2016 data.

\done[inline]{\citet{Rouvier:16}}

Another approach building on the work of~\citet{Severyn:15} was
proposed by~\citet{Rouvier:16}.  In contrast to the former system
which utilized only pre-trained sentiment-flavored word vectors with a
single convolutional layer, the authors simultaneously harnessed three
different types of embeddings (word2vec, word2vec specific to
particular parts of speech, and sentiment-tailored emeddings) each of
which was trained with a separate set of convolutional filters.
Besides training deep representations, the authors also used
hand-crafted features such as sentiment lexicons, emoticons,
information about elongated words, punctuation marks, and capitalized
tokens, training a separate multi-layer perceptron on these
attributes.  In the final step, \citeauthor{Rouvier:16} joined the
outputs of the two downstream classifiers (deep convolutions and
perceptron) into a single vector, subsequently passing this vector to
two fully connected neural network layers with softmax non-linearity
at the end.  This way, the authors achieved 0.63~\F{} on the
SemEval-2016 test set, getting second rank in this shared task.

\done[inline]{\citet{Xu:16}}

\citet{Xu:16} also chose an ensemble approach to coarse-grained
opinion mining of tweets.  In particular, the authors combined a
convolutional system with 300 filters of widths three, four, and five
(using 100 filters for each given width); an LSTM
classifier~\cite{Hochreiter:97}, taking its output vector from the
last time step $t$ as the final vote; and a special Bayesian wordvec
system, which was trained to maximize the probability of a token given
its surrounding context and the label of the tweet, maximizing this
probability with the prior likelihood of the respective polarity.
\citeauthor{Xu:16} united the decisions of these subsystems using soft
weighting scheme:
\begin{equation*}
  y^* = \sum_i w_i y_i,\textrm{, s.t.} \sum_i w_i = 1, \forall i: w_i \geq 0,
\end{equation*}
where $y_i$ is the score of the label $y$ returned by the $i$-th
classifier, and $w_i$ denotes an automatically learned weight for this
prediction.  This submission achieved 0.617~\F{} on the two polarity
classes (positive and negative), getting third place among all
participating submissions.

\todo[inline]{\citet{Wang:15}}

\todo[inline]{\citet{Baziotis:17}}
\todo[inline]{\citet{Cliche:17}}
\todo[inline]{\citet{Rouvier:17}}

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

       &  &  &  & %
       &  &  & %
       &  &  & %
       & \\\bottomrule
    \end{tabular}
    \egroup
    \caption[Evaluation of DL-based coarse-grained SA methods.]{
      Evaluation of DL-based coarse-grained SA methods.\\
      {\small }}
    \label{snt-cgsa:tbl:ml-res}
  \end{center}
\end{table}

% \section{Coarse-Grained Sentiment Analysis Using Language and Domain
%   Adaptation}\label{sec:cgsa:domain-adaptation}

% One of the first works which pointed out the importance of domain
% adaptation for sentiment analysis was introduced by~\citet{Aue:05}.
% In their experiments, the authors trained separate SVM classifiers on
% four different document sets: movie reviews, book reviews, customer
% feedback from a product support service, and a feedback survey from a
% customer knowledge base; finding that each classifier performed best
% when applied to the same domain as it was trained on.  In order to
% find an optimal way of overcoming this domain specificity,
% \citet{Aue:05} tried out four different options:
% \begin{inparaenum}[(i)]
% \item\label{sent-cgsa:lst:rel-wrk1} training one classifier on all but
%   the target domain and applying it to the latter;
% \item using the same procedure as above, but limiting the features to
%   only those which also appeared in the target texts;
% \item taking an ensemble of individual classifiers each of which was
%   trained on a different data collection; and, finally,
% \item using a minimal subset of labeled in-domain data to train a
%   Na{\"i}ve Bayes system with the expectation-maximization algorithm
%   \cite[EM;][]{Dempster:77}.
% \end{inparaenum}
% The authors found that the ensemble and EM options worked best for
% their cross-domain task, achieving an accuracy of up to 82.39\% for
% the two-class prediction (positive vs negative) on new unseen text
% genres.

% Another notable milestone in the domain adaptation research was set
% by~\citet{Blitzer:07}.  Relying on their previous work on structural
% correspondence learning~\cite{Blitzer:07}, in which they used a set of
% \emph{pivot features} (features which frequently appeared in both
% target and source domains) to find an optimal correspondence of the
% remaining attributes,\footnote{In particular, the authors trained $m$
%   binary predictors for each of their $m$ pivot features in order to
%   find other attributes which frequently co-occurred with the pivots.
%   Afterwards, they composed these $m$ resulting weight vectors into a
%   single matrix $W := [\vec{w}_{1},\ldots,\vec{w}_{m}]$, took an SVD
%   decomposition of this matrix, and used the top $h$ left singular
%   vectors to translate source features to the new domain.} the authors
% refined their method by pre-selecting the pivots using their PMI
% scores and improving misaligned feature projections using a small set
% of labeled target examples.  With these modifications,
% \citeauthor{Blitzer:07} were able to reduce the average adaptation
% loss (the accuracy drop when transferring a classifier to a different
% domain) from 9.1 to 4.9~percent when testing a sentiment predictor on
% the domains of book, dvd, electical appliances, and kitchen reviews.

% Other important works on domain adaptation for opinion mining include
% those of~\citet{Read:05}, who pointed out that sentiment
% classification might not only depend on the domain but also on topic,
% time, and language style in which the text was written;
% \citet{Tan:07}, who proposed using the classifier trained on the
% source domain to classify unlabeled instances from the target genre,
% and then iteratively retrain the system on the enriched data set.
% Finally, \citet{Andreevskaia:08} proposed a combination of a lexicon-
% and ML-based systems, claiming that this ensemble would be more
% resistible to the domain shift than each of these classifiers on their
% own.

% Another line of research was introduced by~\citet{Glorot:11} who
% proposed stacked denoising autoencoders (SDA)---a neural network
% architecture in which an input vector $\vec{x}$ was first mapped to a
% smaller representation $\vec{x}'$ via some function
% $h: \vec{x}\mapsto\vec{x}'$, and then restored to its approximate
% original state via an inverse transformation
% $g: \vec{x}'\mapsto\vec{x}''\approx\vec{x}$.  In their experiments,
% the authors optimized the parameters of the functions $h$ and $g$ on
% both target and source data, getting approximate representations of
% instances from both data sets; and then trained a linear SVM
% classifier on the restored representations of the source instances,
% subsequently applying this classifier to the target domain.  This
% approach was further refined by~\citet{Chen:12} who analytically
% computed the reconstruction function~$g$, and used both original and
% restored features to predict the polarity labels of the target
% data.\footnote{Both approaches were trained tested on the Amazon
%   Review Corpus of~\citet{Blitzer:07}.}


% Further notable contributions to domain adaptation in general were
% made by~\citet{Daume:07} who proposed to replicate each extracted
% feature three times and train the first replication on both domains,
% the second repetion only on source, and the third copy only on target
% domain, for which he assumed a small subset of labeled examples was
% available; \citet{Yang:15} who trained neural embeddings of features,
% trying to predict which instance attributes frequently co-occured with
% each other;

\section{Evaluation}
\subsection{Effect of Lexicons}\label{cgsa:subsec:eval:lexicons}

\todo[inline]{describe lexicon normalization steps}

\subsection{Effect of Distant Supervision}
\todo[inline]{}

To the best of our knowledge, the idea of utilizing web texts
containing emoticons as noisily labeled training data was first
proposed by~\citet{Read:05}, who collected a set of 26,000 Usenet
posts featuring smileys or frownies and used these documents to train
a Na{\"i}ve Bayes and SVM classifier.  The author demonstrated that,
despite some encouraging results obtained on the instances from the
same domain (up to 70\% accuracy), the trained systems did not
generalize well to other text genres, barely outperforming the chance
baseline and reaching a maximum accuracy of~54.4\% on news data and
56.8\% on movie reviews.

The presumably first known attempt to adopt distant supervision for
the sentiment analysis of Twitter data was made by~\citet{Go:09} who
collected a set of 800,000 positive and 800,000 negative microblogs
relying on emoticons as their noisy labels.  After stripping off these
smileys from text, the authors trained three independent
ML-classifiers (Na{\"i}ve Bayes, Maximum Entropy, and Support Vector
Machines) on this collection, achieving their best results (82.7\%
accuracy) with the NB and MaxEnt systems thaat utilized unigrams and
bigrams as features.

Another distantly supervised approach was presented
by~\citet{Barbosa:10}, who gathered a collection of automatically
labeled tweets from three popular sentiment web sites (Twendz, Twitter
Sentiment, and TweetFeel), and trained two binary SVM systems on this
corpus.  The first of these classifiers had to distinguish between
subjective and objective microblogs, attaining an error rate of~18.1\%
on a subset of 1,000 manually annotated messages.  In the next step,
the second system had to determine the semantic orientation of
opinionated posts (positive or negative), reaching an error rate
of~18.7\% on this prediction.

In a similar way, \citet{Pak:10} gathered a collection of 300,000
noisily labeled tweets, ensuring an even distribution of positive,
negative, and neutral messages.  After a brief exploration of PoS tag
statistics in these different classes, they presented a Na{\"i}ve
Bayes system which utilized highly relevant binary part-of-speech and
$n$-gram features.\footnote{\citet{Pak:10} determined the relevance of
  a feature $f$ using a special \emph{salience} metric, which was
  defined as a negative ratio between the minimum and maximum
  conditional probabilities of this feature belonging to different
  target classes:
  \begin{equation*}
    salience(f) = \frac{1}{N}\sum_{i=1}^{N-1}\sum_{j=i+1}^N 1 - \frac{\min(P(f, s_i), P(f, s_j))}{\max(P(f, s_i), P(f, s_j))},
  \end{equation*}
  where the $N$~term denotes the number of training examples, and
  $s_i$ means the sentiment class of the $i$-th training instance.}
With this approach, the authors attained an accuracy slighlty above
0.6 on the manually labeled test set of~\citet{Go:09}, also
demonstrating a particular utility of bigrams, negation rules, and
feature pruning heuristics.

A slightly different task was addressed by~\citet{Davidov:10}, who
sought to predict hashtags and emoticons occurring in tweets using a
$k$-NN classifier trained on a large collection of messages.  The
authors achieved an \F-measure of~0.31 on the former task, and reached
an \F-score of~0.64 on predicting smileys.

\citet{Kouloumpis:11} trained an AdaBoost
classifier~\cite{Schapire:00} on two large collections of noisily
labeled tweets---the emoticon tweebank of~\citet{Go:09} and the
Edinburgh hashtag corpus.\footnote{\url{http://demeter.inf.ed.ac.uk}}
Using $n$-gram (up to length two), lexicon, part-of-speech, and
micro-blogging features (such as emoticons, abbreviations, and slang
expressions), the authors achieved a macro-averaged \F-measure of~0.68
on the three-class prediction task.

\subsection{Effect of Word Embeddings}
\subsection{Effect of Text Normalization}
\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}$^{+/-}$} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

      \multicolumn{12}{c}{\cellcolor{cellcolor}PotTS}\\

      % General Statistics:
      % precision    recall  f1-score   support
      % positive       0.44      0.15      0.22       691
      % negative       0.14      0.08      0.10       296
      % neutral       0.34      0.72      0.46       542
      % avg / total       0.35      0.34      0.28      1529
      % Macro-Averaged F1-Score (Positive and Negative Classes): 16.23%
      % Micro-Averaged F1-Score (All Classes): 33.6167%
      HL & 0.44 & 0.15 & 0.22 & %
        0.14 & 0.08 & 0.1 & %
        0.34 & 0.72 & 0.46 & %
        0.162 & 0.336\\

        % General Statistics:
        % precision    recall  f1-score   support
        % positive       0.45      0.13      0.20       691
        % negative       0.15      0.07      0.10       296
        % neutral       0.34      0.75      0.47       542
        % avg / total       0.35      0.34      0.28      1529
        % Macro-Averaged F1-Score (Positive and Negative Classes): 14.87%
        % Micro-Averaged F1-Score (All Classes): 33.8784%
      TBD & 0.45 & 0.13 & 0.2 & %
        0.15 & 0.07 & 0.1 & %
        0.34 & 0.75 & 0.47 & %
        0.149 & 0.339\\

        % General Statistics:
        % precision    recall  f1-score   support
        % positive       0.44      0.14      0.21       691
        % negative       0.15      0.09      0.12       296
        % neutral       0.34      0.71      0.46       542
        % avg / total       0.35      0.33      0.28      1529
        % Macro-Averaged F1-Score (Positive and Negative Classes): 16.50%
        % Micro-Averaged F1-Score (All Classes): 33.3551%
      MST & 0.44 & 0.14 & 0.21 & %
        0.15 & 0.09 & 0.12 & %
        0.34 & 0.71 & 0.46 & %
        0.165 & 0.334\\

        % General Statistics:
        % precision    recall  f1-score   support
        % positive       0.45      0.18      0.25       691
        % negative       0.13      0.07      0.09       296
        % neutral       0.35      0.71      0.47       542
        % avg / total       0.35      0.35      0.30      1529
        % Macro-Averaged F1-Score (Positive and Negative Classes): 17.11%
        % Micro-Averaged F1-Score (All Classes): 34.5324%
      JRK & 0.45 & 0.18 & 0.25 & %
        0.13 & 0.07 & 0.09 & %
        0.35 & 0.71 & 0.47 & %
        0.171 & 0.345\\

        % General Statistics:
        % precision    recall  f1-score   support
        % positive       0.45      0.87      0.60       691
        % negative       0.12      0.05      0.07       296
        % neutral       0.38      0.04      0.08       542
        % avg / total       0.36      0.42      0.31      1529
        % Macro-Averaged F1-Score (Positive and Negative Classes): 33.52%
        % Micro-Averaged F1-Score (All Classes): 41.9882%
      KLCH & 0.45 & 0.87 & 0.6 & %
        0.12 & 0.05 & 0.07 & %
        0.38 & 0.04 & 0.08 & %
        0.335 & 0.42 \\\bottomrule

      \multicolumn{12}{c}{\cellcolor{cellcolor}SB10k}\\

      % hu-liu
      % General Statistics:
      % precision    recall  f1-score   support
      % positive       0.43      0.32      0.37       354
      % negative       0.20      0.22      0.21       212
      % neutral       0.65      0.70      0.67       930
      % avg / total       0.53      0.54      0.54      1496
      % Macro-Averaged F1-Score (Positive and Negative Classes): 29.03%
      % Micro-Averaged F1-Score (All Classes): 54.2112%

      HL & 0.43 & 0.32 & 0.37 & %
        0.2 & 0.22 & 0.21 & %
        0.65 & 0.7 & 0.67 & %
        0.29 & 0.542\\

        % taboada
        % General Statistics:
        % precision    recall  f1-score   support
        % positive       0.43      0.31      0.36       354
        % negative       0.19      0.19      0.19       212
        % neutral       0.65      0.72      0.69       930
        % avg / total       0.53      0.55      0.54      1496
        % Macro-Averaged F1-Score (Positive and Negative Classes): 27.57%
        % Micro-Averaged F1-Score (All Classes): 54.8797%
      TBD & 0.43 & 0.31 & 0.36 & %
        0.19 & 0.19 & 0.19 & %
        0.65 & 0.72 & 0.69 & %
        0.276 & 0.549\\

        % musto
        % General Statistics:
        % precision    recall  f1-score   support
        % positive       0.38      0.35      0.36       354
        % negative       0.23      0.27      0.25       212
        % neutral       0.66      0.65      0.65       930
        % avg / total       0.53      0.52      0.53      1496
        % Macro-Averaged F1-Score (Positive and Negative Classes): 30.72%
        % Micro-Averaged F1-Score (All Classes): 52.3396%
      MST & 0.38 & 0.35 & 0.36 & %
        0.23 & 0.27 & 0.25 & %
        0.66 & 0.65 & 0.65 & %
        0.307 & 0.523\\

        % jurek
        % General Statistics:
        % precision    recall  f1-score   support
        % positive       0.41      0.32      0.36       354
        % negative       0.33      0.21      0.26       212
        % neutral       0.68      0.79      0.73       930
        % avg / total       0.57      0.60      0.58      1496
        % Macro-Averaged F1-Score (Positive and Negative Classes): 30.94%
        % Micro-Averaged F1-Score (All Classes): 59.6925%
      JRK & 0.41 & 0.32 & 0.36 & %
        0.33 & 0.21 & 0.26 & %
        0.68 & 0.79 & 0.73 & %
        0.309 & 0.597\\

        % kolchyna
        % General Statistics:
        % precision    recall  f1-score   support
        % positive       0.38      0.12      0.19       354
        % negative       0.20      0.12      0.15       212
        % neutral       0.66      0.89      0.75       930
        % avg / total       0.53      0.60      0.53      1496
        % Macro-Averaged F1-Score (Positive and Negative Classes): 16.73%
        % Micro-Averaged F1-Score (All Classes): 59.6925%
      KLCH & 0.38 & 0.12 & 0.19 & %
        0.2 & 0.12 & 0.15 & %
        0.66 & 0.89 & 0.75 & %
        0.167 & 0.597\\\bottomrule
\end{tabular}
    \egroup
    \caption[Results of CGSA Methods without Text Normalization.]{
      Results of CGSA methods without text normalization.\\
      {\small HL~--~\citet{Hu:04}, TBD~--~\citet{Taboada:11}, MST~-- \citet{Musto:14}, JRK
        -- \citet{Jurek:15}, KLCH -- \citet{Kolchyna:15}}}
    \label{snt-cgsa:tbl:res-without-normalization}
  \end{center}
\end{table}

\section{Summary and Conclusions}\label{slsa:subsec:conclusions}
