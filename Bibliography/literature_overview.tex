\documentclass[a4paper,11pt]{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Packages
\usepackage{amsmath}
\usepackage{paralist}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Variables and Methods

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Documents
\begin{document}
\section{Pre-Processing}

\section{Sentiment Analysis}
\subsection{Turney: Thumbs Up or Thumbs Down? Semantic Orientation Applied to
               Unsupervised Classification of Reviews\cite{Turney-02}}

\subsection{Pang, Lee, and Vaithyanathan: Thumbs up? Sentiment Classification using Machine
                  Learning Techniques\cite{Pang-Lee-02}}

This article deals with the problem of sentiment classification of
movie reviews.  For this task, the authors explore three popular
machine learning techniques, namely Naive Bayes, maximum entropy, and
support vector machines.  They compare these techniques against each
other and also against a human-based baseline classifier which relies
on a deductively constructed dictionary of polarity terms.

For their training and testing purposes, the authors created a corpus of 752
negative and 1301 positive reviews.  Discrimination between subjective and
objective articles was left out of the scope of this study.  To establish a
classifier baseline, the authors first asked two graduate students to
introspectively devise two lists of polarity terms -- one containing positive
and one containing negative polarity markers.  A simple decision procedure
than counted occurrences of terms from these lists in each review in the
corpus selection and assigned to these reviews the class of the list with most
items found.  This procedure showed a classification accuracy of 58\% for the
lists of the first student and 64\% for that of the second.  A manually
refined list with fewer items, however, allowed to increase the baseline to
69\%.

In their machine learning experiments, the authors represented each document
as a vector of words.  They tested different experimental setups, including
simple unigrams with either presence flag or frequency counts, bigram and
bigram+unigram binary presence flags.  The authors also tried to restrict the
features solely to adjectives and to combine unigram presence features with
their part-of-speech tags or position in text.  The best result was then
achieved by the SVM classifier which only used unigram presence markers as its
features.  It should, however, be said, that for unigrams, the authors also
used the negation heuristics proposed by Das and Chen \cite{Das-Chen-01} and
added the \texttt{NOT\_} prefix to every word between a negation and a
punctuation mark.  This heuristics, though, was not used for bigrams, since
bigrams were considered to be a sufficient source of context information on
their own.

In general, sentiment classification task was said to be harder than
conventional topic-classification.  The authors also noted the fact, that
their system experienced serious difficulties due to the lack of discourse
information.  The reason for that was that sentences describing the plot of
the movies often expressed different polarity than the polarity of the
author's real attitude to the described movie.

\subsection{Bing and Liu: Mining and summarizing customer reviews\cite{Bing-Liu-04}}
In their article, Bing and Liu \cite{Bing-Liu-04} propose a system for
automatically mining subjective judgements on product features in customer
reviews.  This system searches the reviews for frequently occurring mentions
of prominent features of specific products by using an association mining
algorithm.  The algorithm extracts nouns and noun phrases which frequently
occur together across multiple documents.  Implicit features (i.e. those which
are not explicitly present in text) and features which are not represented by
nouns are not taken into account.  Extracted feature candidates are then
pruned off if their components do not occur in a specific order in sentences
or if some features appear to be full subsets of broader ones.

In the following processing step, text passages surrounding feature
candidates are searched for adjectives, since words of this
part-of-speech are considered to be the most obvious indicators of
subjective opinions.  Both subjectivity and polarity of these
adjectives are then determined by using WordNet information.  For
this, two manually constructed sets of adjectives with opposite
polarities are used.  Synonyms and antonyms of adjectives found in
text are searched for occurrences of any adjectives from one of the
two seed sets.  If such an occurrence is found, the text adjective is
added to the respective seed set or its counterpart depending on
whether the occurrence was found in synonyms or antonyms.  The
algorithm stops when both seed sets stop growing or when the list of
text adjectives is exhausted.  Adjectives which were not assigned to
any of the seed cluster during this procedure are assumed to have
neutral polarity and are skipped.

In the next step, sentences which do not contain any frequent features but
which contain at least one opinion adjective are analyzed.  The aim of this
analysis is to find less frequent features of a product which still can be
important for the user.  It is conjectured by the authors that less frequent
features will usually be expressed by nouns or noun phrases which are nearest
to the opinion word.

Further, overall polarity of sentences containing frequent and less frequent
product features is determined.  Here three different possible cases are
distinguished:
\begin{enumerate}
  \item The user likes or dislikes most or all the features in one sentence.
  \item The user likes or dislikes most of the features in one sentence, but
    there is an equal number of positive and negative opinion words.
  \item All the other cases.
\end{enumerate}
For case 1, the dominant orientation of a sentence is easily identified as
average orientation of adjectives used in this sentence.  For case 2, the
orientation of effective opinions of features is used instead.  Effective
opinion is assumed to be the most related opinion for a feature.  For case 3,
the orientation of the opinion sentence is assumed to be the same as the
orientation of previous opinion sentence unless the former sentence is
introduced by a contrastive connector like ``but'' or ``however'' in which
case the orientation is swapped to the opposite.

In the final step, a review summary is generated for each sentence containing
a subjective opinion on a frequent or less frequent feature of a product.  The
summary generation procedure generally consists of two steps:
\begin{itemize}
  \item For each discovered feature, related opinion sentences are put into
    positive and negative categories according to the opinion sentences'
    orientations.

  \item All features are ranked according to the frequency of their
    appearances in the reviews.
\end{itemize}

\subsection{Pang and Lee: A Sentimental Education: Sentiment Analysis Using Subjectivity
               Summarization Based on Minimum Cuts\cite{Pang-Lee-04}}

This paper presents a two-step approach for polarity classification of
movie reviews.  The first step (\textit{subjectivity-detector})
consists in detecting and extracting subjective sentences present in
the review.  The next step (\textit{default polarity classifier}) then
classifies found subjective sentences in either positive or negative
ones using Naive Bayes (NB) or SVM methods presented earlier in
\cite{Pang-Lee-02}.

To identify whether a single sentence is subjective or not, two
different kinds of scores are considered:
\begin{enumerate}
  \item individual sentence scores and
  \item sentential coherence scores.
\end{enumerate}
The former scores show the probability that a particular single
sentence on its own would belong to either subjective or objective
class.  To compute this, a Naive Bayes and an SVM classifier are
trained on a special subjectivity dataset that was automatically
constructed from the Web.  This dataset comprises 5000 movie review
snippets which were considered to be subjective a priori, and 5000
summaries of movie plots that were regarded to be objective.

The sentential coherence scores represent the probability that a
particular sentence would share same subjectivity class as its
preceding and following sentences.

Both score weights for individual document's sentences are combined
into a single graph $G$ in which vertices $V$ represent individual
sentences.  Additionally, two artificial nodes $C_1$ and $C_2$ are
added to the graph.  These nodes, also called \textit{source} and
\textit{sink}, represent subjective and objective classes,
respectively.  Weighted edges \textit{$e_{ji}$} are then established
from each class centroid $C_j$ to every sentence $s_i$ in document.
Weights of these edges $ind_j(s_i)$ are set to the probability values
provided by the pre-trained subjectivity classifiers, either Naive
Bayes or SVM.  So, $ind_1(s_i) = Pr^{NB}_{sub}(s_i)$ is the weight of
the edge $ind$ between the subjective class $C_1$ and sentence $s_i$
and it is set to the probability of this sentence being subjective as
it is calculated by the $NB$ classifier.  Corresponding weight for
objective class is then set to $ind_2(s_i) = 1 - Pr^{NB}_{sub}(s_i)$.
Same estimations are performed for SVM.

To incorporate coherence scores in the graph, $\binom{n}{2}$ edges are
established between all $n$ sentences.  The coherence weight between
two sentences $i$ and $j$ is then estimated using the following formula:
\begin{displaymath}
  assoc(s_i, s_j) := \begin{cases}
    f(j -i) \cdot c & \text{if} (j - i) \leq T;\\
   0 & \text{otherwise}
  \end{cases}
\end{displaymath}
In this formula, $f$, $c$, and $T$ are free parameters, where $f(d)$
is a non-increasing function that shows how the influence of proximal
sentences decays with respect to distance $d$.  $T$ is a threshold
value that specifies maximum distance by which two sentences can be
separated and still be considered coherent.  Finally, $c$ is a
constant which controls the relative weight of association scores.
The higher $c$, the higher is the prevalence of association scores
over individual sentence scores.

The subjectivity detection task is then defined as finding a cut
$(S,T)$ in graph $G$ which minimizes the weights of all edges crossing
from $S$ to $T$ which is also known as minimum-cut problem.

The authors first experiment with basic subjectivity detection in
which only individual sentence scores are used for classification.
After leaving only sentences which were recognized as subjective in
the review and throwing off objective ones, the authors observe an
increase in the accuracy of basic NB polarity classifier by 3.6 \%
from 82.8 \% to 86.4 \%.  For SVM polarity classifier, the accuracy,
on the contrary, drops by less than one percent which is considered to
be statistically insignificant.  After incorporating coherence scores
in the graph, a statistically significant improvement is observed for
the SVM subjectivity classifier for either polarity classifiers.

\subsection{Scheible and Sch\"utze: Sentiment Relevance\cite{Scheible-13}}

In their article, Scheible and Sch\"utze propose a novel concept for
measuring the importance of a single sentence for an overall sentiment
polarity of a document and call this concept \textit{sentiment
  relevance}.

The authors deviate from the well-established notion of sentence
subjectivity by showing that not every sentence which appears to be
subjective may contribute to document's polarity and vice versa an
apparently objective sentence may be highly sentiment relevant for a
document.  They illustrate this with an example from the movie review
database (IMDB).  In a review, a sentence describing movie's plot and
acting characters can be highly subjective containing strong polarity
markers like, for example, ``Bruce Banner, a genetics researcher with
a \textit{tragic} past, suffers a \textit{horrible} accident'', but
this sentence does not say much about author's attitude to the
``Hulk'' movie which it describes.  On the contrary, a sentence like
``The movie won a Golden Globe'' is relevant for the overall movie
review despite that it merely states an objective fact.

To confirm their assumption that sentiment relevance and subjectivity
are different concepts, the authors created a corpus (henceforth
\textit{SR-corpus}) of sentences from 125 randomly selected documents
from the movie review dataset \cite{Pang-Lee-02}.  Each of these
sentences was manually annotated for S-relevance.  Another corpus (the
\textit{P\&L} corpus) was automatically constructed from the
subjectivity dataset provided by Pang and Lee \cite{Pang-Lee-04}.  In
the latter data collection, all subjectivity markers were
automatically considered as S-relevent and all objective markers were
regarded as S-nonrelevant.  The authors than showed than a MaxEnt
classifier performed significantly worse if it was trained on one
corpus and tested on the other than if it was trained and tested on
the data taken from the same set.

Since no large collections of annotated S-relevance data are available
yet, Scheible and Sch\"utze propose two different semi-supervised
techniques for doing such classification automatically.  These
techniques are
\begin{inparaenum}[\itshape a)\upshape]
  \item distant supervision and
  \item transfer learning.
\end{inparaenum} In their experimental setup, the authors use three kinds of features:
\begin{enumerate}
  \item \textit{Named Entity} features (NE) which are binary features
    indicating the presence of an actor, character, or staff member
    mention in a sentence.  Named entity detection and resolution is
    performed on the basis of lexicons that are obtained from the IMDB
    metadata database;\label{li-ne-feat}
  \item \textit{Sequential} features (SQ) which effectively repeat the
    former features by transferring them to the following sentence in
    document.  It means that if sentence $s_i$ had feature
    \texttt{<character>}, then the following sentence $s_{i+1}$ will
    automatically get a sequential feature \texttt{<character$+1$>};
  \item \textit{Semantic} features (SEM) which are semantic classes of
    nouns or verbs which are obtained from \texttt{CoreLex} and
    \texttt{VerbNet}.  These features are added to sentences depending
    on the words which appear in them.
\end{enumerate}
Additionally to that, to account for discourse coherence factors, the
authors represent the whole document as a graph with $n + 2$ nodes
where $n$ is the number of sentences and the two additional nodes
(called source and sink) represent the classes S-relevance and
S-nonrelevance, respectively.  The weights $ind(s, x)$ between a
sentence $s$ and the source/sink vertex $x$ are measured as the number
of actors and staff mentions in the sentence when $x = SR$ and the
number of characters mentions when $x = SNR$.  It means that if a
sentence $s$ has more actors mentions in it than characters mentions,
it is more likely to be S-relevant and therefore has a higher weight
attached to the edge connecting it with the $x_{sr}$ verex than with
the $x_{nsr}$ vertex.  The coherence factor between two consecutive
sentences $i$ and $j$ is estimated as $assoc(s_i, s_j) = c / (j -
i)^2$ where $c$ is a parameter which is set to $1$ in distant
supervision setup and empirically found to bring best results when set
to $0.8$ in transfer learning.

In distant supervision, the authors first assign a relevance class to
each sentence according to the majority of NE features found in it.
Afterwards, they train a MaxEnt classifier on these noisy labels and
impose the MinCut discourse constraint on it.  These re-labeled data
serve as training set for the next iteration of MaxEnt training.  In
the next iteration, the authors look how different features affect the
model.  The best total performance with average $F_1 = 67.0$ is then
achieved by a distantly supervised MaxEnt classifier with activated
features SQ and NE.

In the transfer learning setup, the authors use the P\&L corpus as
their training data, though the labels of this corpus rather
correspond to the subjectivity measure than to S-relevance.  This
time, they first classify the data using a supervised classifier and
then use MinCut to smooth the sequences.  The individual weight
$ind(s,x)$ on the edge between sentence $s$ and class $x$ is set to
the estimate $p(x|s)$ returned by the supervised classifier.  The
parameter $c$ of the MinCut model is tuned using the run count method.
The authors establish a baseline by obtaining a classifier with a
simple bag-of-words representation of sentence.  The addition of
semantic information and smoothing with MinCut discourse constraint
gives an improvement over the baseline by 12.6\% with an averaged
$F_1$ equal to $82.5\%$.

\section{Markov Logic Network}

\subsection{Markov Logic: An Interface Layer for Artificial Intelligence \cite{Domingos-09}}

In their book, Domingos and Lowd present Markov Logic Network (MLN) -- a
machine learning framework which combines the ideas of first-order logic (FOL)
with the expressive power of probabilistic graphical models (PGM).

Features in MLN are represented in the form of logical formulas.  Such
formulas are usually composed of four different types of elements:
\begin{inparaenum}[\itshape a)\upshape]
\item constants,
\item variables,
\item functions, and
\item predicates.
\end{inparaenum}

Constants in formulas represent specific objects in the domain of interest.
Variables are symbols which range over all objects of a particular class.
Functions are either built-in or user-defined procedures which map tuples of
objects to other objects in the domain.  And, finally, predicates are symbols,
that represent relations between sigle objects or properties of one particular
object in a possible world.

Any expression which represents one particular object in the interpreted world
is called a \textit{term}.  A term can be represented by a constant, a
variable, or a function which is applied to a tuple of terms.  A term which
contains no variables is called a \textit{ground term}.  A predicate applied
to a tuple of terms is called an \textit{atomic formula} or \textit{atom}.
Correspondingly, if all the arguments of an atomic formula are ground terms,
this formula can also be called a \textit{ground atom}.

Internally, all formulas of an MLN are later joined to a large single formula
in the conjunctive normal form (CNF).  The CNF formula represents a single
conjunction of clauses, where each clause, in turn, is a disjunction of
literals.

In the next step, the CNF knowledge base is converted to a Markov network
(MN).  In the constructed network, each possible grounding of each predicate
in the logical KB is represented as a binary node.  A node takes on the value
of 1, if its corresponding ground predicate is true, and 0 otherwise.
Furthermore, nodes of all predicates appearing in same formula are connected
via edges, thus forming a clique.  MN cliques are direct graphical equivalents
of logical formulas in the KB and therefore immediate representations of the
features.  Like ground predicates, features associated with the cliques take
on the value of 1, if their corresponding formulas are true, and 0 if formulas
are false.  The weight of each feature is the weight of the formula in the KB.

The probability distribution over possible worlds $x$ specified by the ground
Markov network $M$ is estimated using the following formula:
\begin{equation}
  P(X=x) = \frac{1}{Z}\left(\sum_i{w_in_i(x)}\right) = %
    \frac{1}{Z}\prod_i{\phi_i(x_{\{i\}})^{n_i(x)}}
\end{equation}
where $n_i$ is the number of true groundings of formula $F_i$ in the given
world $x$, $x_{\{i\}}$ is the state (truth values) of the predicates appearing
in $F_i$, and $\phi(x_{\{i\}}) = e^{w_i}$ where $w_i$ is the weight associated
with the $i$-th formula.

\section{Discourse Analysis}

\subsection{L\"ungen et al.: Discourse Segmentation of German Written Texts\cite{Luengen-06}}

In their article, L\"ungen et al. \cite{Luengen-06} propose a rule-based
approach for parsing German scientific articles into coherent discourse
segments.  The authors thereby distinguish three levels of discours
segmentation with different granularities.  These levels are called:
\begin{enumerate}
  \item \textit{complex discourse segments},\label{enum-cds}
  \item \textit{sentential discourse segments},\label{enum-sds}
  \item and \textit{elementary discourse segments} or \textit{EDS}.\label{enum-eds}
\end{enumerate}

Discourse elements of type \ref{enum-cds} are obtained from the DocBook XML
annotation (cf. \cite{Walsh-99}) of input documents and comprise such logical
structures as title, main text, footnotes, acknowledgements section,
bibliography entries etc.

A more fine-grained level of analysis is represented by sentential discourse
segments which roughly correspond to individual sentences in text.

Elementary discourse segments are the smallest parsing units of
discourse and are supposed to represent single coherent and relatively
indepent propositions.  The authors slightly deviate from the
traditional notion of elementary discourse units suggested by Marcu
\cite{Marcu-99} and define EDSs in terms of syntactic constructions
which can potentially produce an EDS.  The authors distinguish 10
types of grammatical structures which can give rise to an EDS
including:
\begin{enumerate}
  \item \textit{main clauses};
  \item \textit{modal subclauses};
  \item \textit{coordinated clauses};
  \item \textit{embedded segments}, which are segments marked by punctuation
    (brackets, dashes, or commas) so that they disrupt other EDSs, and which
    themselves are EDSs. An exception to this form brackets that contain only
    figures;
  \item \textit{quotations} that are delimited by quotation marks and are
    introduced by reporting verbs;
  \item \textit{clausal complements of reporting verbs} such as (meinen,
    sagen, feststellen) in connection with a citation or quotation;
  \item \textit{clausal complements and relative clauses preceded by
    adverbials}: Clausal complements of verbs or nouns, or relative clauses
    that are preceded by a discourse marking adverbial such as n\"amlich,
    namentlich, besonders, insbesondere, d.h., vozugsweise;
  \item \textit{prepositional phrases of attribution}, i.e. prepositional
    phrases which are formed by one of the prepositions nach, laut, gem\"a\ss
    + a named entity, or a pronoun referring to a named entity, in connection
    with a citation or quotation;
  \item \textit{appositives} which are NPs that can be used postnominally as
    supplements to NPs;
  \item \textit{PPs} that are separated by a comma .
\end{enumerate}

The authors explicitly say that in contrast to Carlson and Marcu
\cite{Carlson-Marcu-01} they do not consider as independent EDSs folowing
units:
\begin{itemize}
  \item clausal subjects or clausal complements of verbs and nouns (with the
    exception of cases described above);
  \item conditional clauses introduced by \textit{wenn, ...}, \textit{dann,
    ...}, \textit{je..., desto} etc.;
  \item propositional clauses introduced by comparative connectors like
    \textit{mehr... als}, \textit{weniger... als}, \textit{so JJ... wie} etc.
\end{itemize}
An additional constraint also says that potential EDSs which are subordinate
or coordinate to a non-segmantable clause cannot be EDSs, either.

The automatic recognition of complex and sentential discourse segments is than
straightforward.  The former units are extracted from XML tags in the
document, the latter ones are obtained during sentence splitting.

For EDSs, parsing procedure looks more sophisticated.  Here, first all
potential segment boundaries are detected including commas, lexical
markers e.g. \textit{und} and \textit{oder} as well as parentheses.
Subsequently, boundary markers within parentheses and quotations are
deleted, since these units are already supposed to represent
elementary segments.  Next, POS tags surrounding markers are analyzed
and iteratively joined to higher-order syntactic constructs.  At every
step, it is than checked whether POS categories or syntactic
structures on either side of a potential boundary are the same.  If
yes, such a boundary is assumed to unite an enumeration and therefore
can not be an EDS boundary.

The results of the test are then fed into a separate filtering module.  In
this module, first, those markers that are associated with the conjunctions
\textit{und} and \textit{oder} are removed if the conjunctors were only
enumerative.  Then, those markers that are associated with a comma are
evaluated in the following order.  If the current comma marker is associated
with a sentential subject or complement, or a proportional clause, or an
infinitival complement, this marker and all other markers up to the following
comma marker are removed.  If the current comma marker is associated with an
enumeration, only the current marker is removed.  If it is associated with a
relative clause, then not only all markers up to the next comma marker are
removed, but also the marker after that one.  If next marker marked an
enumeration or a relative clause, this marker is re-calculated, and
\texttt{remove()} procedure is called again.  Such design is supposed to
reflect the default assumption that all clauses following a main clause and a
sub-clause EDS should be treated as being sub-subordinated.  After this
evaluation of tests for potential EDS boundaries, each EDS established so far
is checked for further internal boundaries brought about by EDSs below the
clause level, i.e. phrases.

The authors subsequently test their procedure on a manually annotated set of
six documents.  Depending on document, the precision of EDS detection varies
from 0.6\% to 0.82\%, the best recall is achieved with 0.88\%, though it is
also highly dependent on the peculiarities of analyzed text.

\bibliography{bibliography} % relative to the project's root directory
\bibliographystyle{plain}
\end{document}
