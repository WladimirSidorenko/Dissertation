\chapter{Coarse-Grained Sentiment Analysis}\label{sec:snt:cgsa}

Having familiarized ourselves with the peculiarities of a sentiment
corpus, the different possibilities to automatically induce new
polarity lists, and the difficulties of fine-grained opinion mining,
we now move on to the presumably most popular sentiment analysis
objective---the coarse-grained analysis or CGSA, in which we need to
determine the overall polarity of a message.

Traditionally, this task has been addessed with either of three
popular groups of methods:
\begin{inparaenum}[(i)]
  \item lexicon-based approaches,
  \item machine-learning-based (ML) techniques, and
  \item deep-learning-based (DL) applications.
\end{inparaenum}
In this chapter, we are going to scrutinize the most prominent
representatives of each of these paradigms and also tackle a much more
ambitious goal, namely to check whether we can achieve results
comparable with the scores of these methods when the language of the
domain we train on is completely different from the language of the
test data.

We begin our comparison by first presenting the metrics that we will
use in our subsequent evaluation.  After a brief description of the
data preparation step, we proceed to the actual estimation of popular
lexicon-, ML-, and DL-based approaches, explaining and evaluating them
in Sections~\ref{sec:cgsa:lexicon-based}, \ref{sec:cgsa:ml-based},
and~\ref{sec:cgsa:dl-based} respectively.  Then,
in~Section~\ref{sec:cgsa:domain-adaptation}, we also show which
results can be obtained by using cross-lingual transfer, where we
train a classifier on English microblogs and then adapt this system to
German messages.  Finally, we conclude with an extensive evaluation of
different hyperparameters and settings (including various types of
sentiment lexicons, different kinds of word embeddings, the utility of
the text normalization step, and the impact of additional noisily
labeled training data), summarizing our results and recapping our
findings at the end of this chapter.

\section{Evaluation Metrics}\label{sec:cgsa:eval-metrics}

To estimate the quality of the compared systems, we will use two
established evaluation metrics which are commonly applied for
measuring CGSA results: One of these metrics is the macro-averaged
\F-score over the two major polarity classes~(positive and negative):
{ \small%
  \begin{equation*}
    F_1 = \frac{F_{pos} + F_{neg}}{2}.
  \end{equation*}%
  \normalsize%
}%
This measure was first introduced by the organizers of the SemEval
competition~\cite{Nakov:13,Rosenthal:14,Rosenthal:15} and has become a
de facto standard not only for the SemEval dataset, but virtually for
all related sentiment tasks and corpora.

The second metric is the micro-averaged \F-score over all three
possible semantic orientations (positive, negative, and neutral),
which basically corresponds to the accuracy over the whole labeled
dataset~\cite[cf.][p.~577]{Manning:99}.  This measure both predates
and supersedes the SemEval evaluation as it had already been applied
in the very first works on coarse-grained opinion
mining~\cite{Wiebe:99,Das:01,Read:05,Kennedy:06,Go:09} and was again
reintroduced in the GermEval Shared Task on Sentiment
Analysis~2017~\cite{Biemann:17}.

Moreover, in addition to these two metrics, we will also give a
detailed information on precision, recall, and \F-scores of each
particular polarity class in order to get a better intuition about
precise strengths, weaknesses, and biases of each evaluated CGSA
method.

\section{Data Preparation}\label{sec:cgsa:data}

Similarly to the data preparation steps used for fine-grained
sentiment analysis, we preprocessed all tweets of the PotTS
corpus~\cite{Sidarenka:16} with the text normalization system
of~\citet{Sidarenka:13}, tokenized them using the same adjusted
version of Potts'
tokenizer,\footnote{\url{http://sentiment.christopherpotts.net/code-data/happyfuntokenizing.py}}
lemmatized and assigned part-of-speech tags to these tokens with the
\texttt{TreeTagger} of~\citet{Schmid:95}.  Furthermore, like in the
previous chapter, we automatically obtained morphological features for
each word and induced syntactic trees for each sentence with the help
of the \texttt{Mate} dependency parser~of~\citet{Bohnet:13}. Apart
from the PotTS dataset, we also applied this procedure to the
microblogs of the German Twitter snapshot~\cite{Scheffler:14}, which
will be used in our subsequent experiments on noisy supervision.

We again divided the corpus data into a training, development, and
test set, using 70\% of the tweets for learning, 10\% for tuning and
picking the optimal hyperparameters, and the remaining 20\% for
evaluating the results.  We inferred polarity labels for these
microblogs with a simple heuristic rule, assigning the positive
(negative) class to all messages which contained exclusively positive
(negative) sentiments, considering all tweets without any polar
opinions as neutral, and skipping all microblogs that contained
sentiments with opposite semantic orientations.  Finally, to derive
these classes for the snapshot posts, we followed the procedure
proposed by~\citet{Read:05} and~\citet{Go:09}, and assigned the
positive (negative) labels to the tweets which contained the
respective emoticons.

\todo[inline]{Add statistics on the noisily labeled corpus.}

\section{Lexicon-Based Methods}\label{sec:cgsa:lexicon-based}

The first group of approaches that we are going to explore in this
chapter using the described data are lexicon-based (LB) systems.  Just
like sentiment lexicons themselves, LB methods for coarse-grained
opinion mining have attracted a lot of attention from the very
inception of the sentiment analysis field.  Starting with the research
of~\citet{Hatzivassi:00}, who proved with statistical significance
tests that the mere occurrence of a subjective adjective from a
polarity list was a highly reliable indicator of the whole sentence
being subjective, more and more works dealing with the use of lexicons
for determining the overall polarity of complete texts appeared on the
scene.  For example, \citet{Das:01} already applied an ensemble of
five classifiers (two of which were purely lexicon-based and the other
three relied on lexicon features) to predict the polarity of stock
messages (\emph{buy}, \emph{sell}, or \emph{neutral}), attaining an
accuracy of 62\% on a corpus of several hundreds stock board messages.
In contrast to this complex approach, \citet{Turney:02} simply
determined the \emph{semantic orientation} (SO) of the reviews by
averaging the sentiment scores of their terms found in the
automatically generated PMI lexicon.  With this approach, the author
could reach an accuracy of 74\% on a corpus of 410 manually labeled
Epinions comments.  Following this line of research, \citet{Kim:04}
compared three different approaches to determining the polarity of a
sentence:
\begin{inparaenum}[(i)]
\item by multiplying the signs of its polar terms,
\item by taking the sum of their scores, and
\item by computing the geometric mean of these values;
\end{inparaenum}
finding the first and the last option working best on the Document
Undestanding Corpus.\footnote{\url{http://duc.nist.gov/}}

% % Hu and Liu, 2004
% Similarly, \citet{Hu:04} determined the semantic orientation of
% sentences in customer reviews by simply comparing the number of
% positive and negative terms found in these passages. Since the
% authors, however, were primarily interested in estimating the polarity
% towards particular product features mentioned in the clauses, they
% additionally applied a fallback strategy in case of a tie by checking
% which of the polar lexicon terms appeared closer to the features, and
% assuming the polarity of the preceding sentence if these numbers were
% also equal.

% % Taboada et al., 2004
% Largely inspired by the Appraisal theory of~\citet{Martin:00},
% \citet{Taboada:04} enhanced the original method of~\citet{Turney:02}
% by increasing the weights of polar adjectives which occurred in the
% middle and at the end of a document, and also augmenting these values
% with the affect, judgement, and appreciation scores.  Similarly to
% polarity, the appraisal scores were calculated automatically by
% computing the PMI of their cooccurrence with different pronouns using
% a web search engine.

% Polanyi and Zaenen, 2006; Kennedy and Inkpen, 2006
In~\citeyear{Polanyi:06}, \citeauthor{Polanyi:06} presented an
extensive overview and analysis of common lexicon-based approaches,
arguing that, besides considering the lexical valence (i.e., semantic
orientation) of polar terms, it was also necessary to incorporate
syntactic, discourse-level, and extra-linguistic factors such as
negations, intensifiers, modal operators (e.g., \emph{could} or
\emph{might}), presuppositional items (e.g., \emph{barely} or
\emph{failure}), irony, reported speech, discourse connectors, genre
and attitude assessment, reported speech, multi-entity evaluation,
etc.  This theoretical hypothesis was also proven empirically by
\citet{Kennedy:06}, who compared two ways of determining the polarity
of a customer review: In the first of these methods, they simply
compared the numbers of positive and negative expressions appearing in
text, assigning the review to the class with the greater number of
terms.  In the second attempt, they enhanced the original system with
additional information about contextual valence shifters, increasing
or decreasing the sentiment score of a term if it was preceded by an
intensifier or downtoner, and changing the polarity sign of this score
to the opposite in case of a negation.  With this adjustment, the
authors achieved a statistically significant improvement, boosting the
accuracy of the two-class prediction on a corpus of product and movie
reviews from 67.9 to 69.3\%.

% Taboada et al., 2011
Finally, a veritably seminal work on lexicon-based techniques for CGSA
was introduced by~\citet{Taboada:11}, who presented a manually
compiled polarity list\footnote{The authors hand-annotated all
  occurrences of adjectives, nouns, and verbs found in a corpus of 400
  Epinions reviews with ordinal categories ranging from -5 to 5 which
  reflected the semantic orientation of a term (positive vs. negative)
  and its polar strength (weak vs. strong).} and used this resource to
estimate the overall semantic orientation of the text.  Drawing on the
ideas of~\citet{Polanyi:06}, the authors also incorporated a set of
additional heuristic rules into their computation by changing the
prior SO values of negated, itensified, and downtoned terms, ignoring
irrealis sentences, and adjusting the weights of specific document
sections.  An extensive evaluation of this approach showed the
superior performance of the manual lexicon in comparison with other
polarity lists, including the Subjectivity
Dictionary~\cite{Wilson:05}, Maryland Polarity Set~\cite{Mohammad:09},
and \textsc{SentiWordNet}~\cite{Esuli:06c}.  Moreover, the authors
also demonstrated the effectiveness of their method for other topics
and text genres, hypothesizing that lexion-based approaches were in
general more robust to domain shifts than traditional supervised
learning techniques.

% % Taboada et al., 2006
% Another important contribution to the development of lexicon-based
% approaches was made by~\citet{Taboada:06}, who compared three popular
% polarity lists---a PMI lexicon computed with the original method
% of~\citet{Turney:02} using the AltaVista's NEAR operator; a similar
% polarity list obtained with the help of Google's AND queries; and,
% finally, the manually compiled General Inquirer lexicon
% of~\citet{Stone:66}.  The authors evaluated these resources both
% intrinsically (by comparing them with GI entries) and extrinsically
% (by computing the polarity of 400 manually annotated Epinions
% reviews).  To estimate the overall polarity of a review for the second
% task, \citeauthor{Taboada:06} calculated the average SO value of all
% polar terms found in the review, obtaining these scores from the
% mean-normalized lexicons, and flipping the polarity sign to the
% opposite in case of the negation.

% Musto et al., 2014
Lexicon-based systems also found their way into sentiment analysis of
social media: An LB approach specifically tailored to Twitter data
was, for instance, proposed by~\citet{Musto:14}, who examined four
different ways of computing the overall polarity scores of microblogs:
\emph{basic}, \emph{normalized}, \emph{emphasized}, and
\emph{normalized-emphasized}; checking these strategies with four
distinct lexicons: \textsc{Sen\-ti\-Word\-Net}~\cite{Esuli:06c},
\textsc{Word\-Net-\-Affect}~\cite{Strapparava:04},
\textsc{MPQA}~\cite{Wiebe:05}, and
\textsc{SenticNet}~\cite{Cambria:14}.  In all of these methods, the
authors first split an input message into a list of
\emph{micro-phrases} based on the occurrence of punctuation marks and
conjunctions; then calculated the polarity score of each of these
segments; and, finally, estimated the overall polarity of the tweet by
uniting the scores of the micro-phrases.  \citeauthor{Musto:14}
obtained their best results (58.99\% accuracy on the SemEval-2013
dataset) with the normalized-emphasized approach, in which they
averaged the polarity scores of segments' tokens, boosting these
values by 50\% for informative parts of speech; and regarded the sum
of the micro-phrase scores as the final overall polarity of the
microblog.

% the authors obtained their best results using the
% \textsc{SentiWordNet} lexicon of~\citet{Esuli:06c}

% Jurek et al., 2015
Another Twitter-aware system was presented by~\citet{Jurek:15}, who
computed the negative and positive polarity of a message ($F_p$ and
$F_n$) using the following equations: { \small%
  \begin{align}
    F_P &= \min\left(\frac{A_P}{2 - \log(3.5\times W_P + I_P)}, 100\right),\\
    F_N &= \max\left(\frac{A_N}{2 - \log(3.5\times W_N + I_N)}, -100\right);\label{cgsa:eq:jurek}
  \end{align}%
  \normalsize%
}%
where $A_P$ and $A_N$ represent the average scores of positive and
negative lexicon terms found in the tweet, $W_P$ and $W_N$ stand for
the raw counts of such items, and $I_P$ and $I_N$ denote the number of
intensifiers preceding the lexicon terms.  Furthermore, in case of a
negation, the authors additionally modified the individual polarity
scores $s_w$ of the negated words $w$'s while estimating the average
polarity values as follows: { \small%
  \begin{align}
neg(s_w) =
    \begin{cases}
        \min\left(\frac{s_w - 100}{2}, -10\right) & \text{if } s_w > 0,\\
        \max\left(\frac{s_w + 100}{2}, 10\right), & \text{if } s_w < 0.
    \end{cases}
\end{align}%
\normalsize%
}%
Besides estimating the polarity scores $F_p$ and $F_n$,
\citeauthor{Jurek:15} also determined the subjectivity degree of the
message by replacing the terms $A_P$ and $A_N$ in
Equation~\ref{cgsa:eq:jurek} with averaged conditional probabilitites
of the tweet being subjective given that the respective polar term
appeared in it.\footnote{These probabilities were calculated
  automatically on the noisily labeled data set of~\citet{Go:09}.}
The authors considered a microblog as positive or negative if its
overall polarity score was greater than 25, and the subjectivity value
was higher than 0.5.  With this method, they achieved an accuracy
of~77.3\% on the manually annotated subset of \citeauthor{Go:09}'s
corpus and reached 74.2\% on the IMDB corpus~\cite{Maas:11}.

% Kolchyna et al., 2015
Finally, \citet{Kolchyna:15} also explored two different ways of
computing the overall polarity of a microblog:
\begin{inparaenum}[(i)]
\item by simply averaging the scores of the lexicon terms found in the
  message and
\item by taking the signed logarithm of this average:
\end{inparaenum}
\begin{equation*}
  \text{Score}_{\log} =
  \begin{cases}
    \text{sign}(\text{Score}_{\text{AVG}})\log_{10}(|\text{Score}_{\text{AVG}}|) & %
    \text{if |Score}_{\text{AVG}}| > 0.1,\\
    0, & \text{otherwise};
  \end{cases}
\end{equation*}%
comparing theses approaches on the SemEval-2013
dataset~\cite{Nakov:13}.  The authors determined the final polarity
class of a tweet with the help of $k$-means clustering, which utilized
either of the above polarity scores as its features.  They showed that
the logarithmic strategy performed better than the simple average
solution, yielding an accuracy of 61.74\%.  In addition to that,
\citeauthor{Kolchyna:15} also checked whether these lexicon values
could serve as useful attributes for an ML-based method.  For this
purpose, they retrained a cost-sensitive SVM
classifier~\cite{Masnadi:12} after extending its $n$-gram feature set
with lexicon features, getting almost five percent accuracy
improvement (from 86.62 to 91.17) on the IMDB movie review
dataset~\cite{Pang:02}.

\todo[inline]{reimplement and describe the results of
  \citet{Taboada:11}, \citet{Musto:14}, \citet{Jurek:15}, and
  \citet{Kolchyna:15}}

In order to estimate the quality of these methods on the PotTS
dataset, we reimplemented the approaches suggested
by~\citet{Taboada:11}, \citet{Musto:14}, \citet{Jurek:15}, and
\citet{Kolchyna:15}, and applied these systems to the test set tweets
described in Section~\ref{sec:cgsa:data}, tweaking the threshold
values of the evaluated methods on the training and development data.
The results of this evaluation are shown in Table~\ref{snt-cgsa:tbl:lex-res}.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

       TBD &  &  &  & %
       &  &  & %
       &  &  & %
       & \\

       MST &  &  &  & %
       &  &  & %
       &  &  & %
       & \\

       JRK &  &  &  & %
       &  &  & %
       &  &  & %
       & \\

       KLCH &  &  &  & %
       &  &  & %
       &  &  & %
       & \\\bottomrule
\end{tabular}
    \egroup
    \caption[Evaluation of lexicon-based CGSA methods.]{
      Evaluation of lexicon-based CGSA methods.\\
      {\small TBD -- \citet{Taboada:11}, MST -- \citet{Musto:14}, JRK
        -- \citet{Jurek:15}, KLCH -- \citet{Kolchyna:15}}}
    \label{snt-cgsa:tbl:lex-res}
  \end{center}
\end{table}

\section{Machine-Learning Based Methods}\label{sec:cgsa:ml-based}

One of the first works on an automatic sentiment classification with
ML-based methods was presented by~\citet{Wiebe:99}, who trained a
Na{\"i}ve Bayes system to differentiate between subjective and
objective statements, relying primarily on binary features which
reflected the presence of a pronoun, an adjective, a cardinal number,
or a modal other than ``will'' in the analyzed sentence.  The authors
achieved an accuracy of~72.17\%, outperforming the majority class
baseline by more than 20~percentage points.  An even better result
(81.5\%) could be reached when the data set was restricted only to the
examples with the most confident annotations.

A further step in this direction was taken by~\citet{Pang:02}, who
compared Na{\"i}ve Bayes, Maximum Entropy, and SVM approaches on the
polarity classification task for movie reviews, getting their best
results (82.9\% accuracy) with the SVM system that used only unigram
features.

\todo[inline]{}

From Barbosa 2010: A variety of features have been exploited on the
problem of sentiment detection (Pang and Lee, 2004; Pang et al., 2002;
Wiebe et al., 1999; Wiebe and Riloff, 2005; Riloff et al., 2006).

ReviewSA: this is the approach proposed by Pang and Lee (Pang and Lee,
2004) for sentiment analysis in regular online re- views. It performs
the subjectivity detec- tion on a sentence-level relying on the prox-
imity between sentences to detect subjectiv- ity. The set of sentences
predicted as subjec- tive is then classified as negative or positive
in terms of polarity using the unigrams that compose the sentences. We
used the imple- mentation provided by LingPipe (LingPipe, 2008);

Unigrams: Pang et al. (Pang et al., 2002) showed unigrams are
effective for sentiment detection in regular reviews. Based on that,
we built unigram-based classifiers for the subjectivity and polarity
detections over the training data. Another approach that uses un-
igrams is the one used by TwitterSentiment website. For polarity
detection, they select the positive examples for the training data
from the tweets containing good emoticons and negative examples from
tweets contain- ing bad emoticons. (Go et al., 2009). We built a
polarity classifier using this approach (Unigrams-TS)



Wiebe 2002, Riloff 2003

Wiebe, Bruce, \& O'Hara 1999
Hatzivassiloglou \& Wiebe 2000
Wiebe 2000;
Wiebe et al. 2002
Yu \& Hatzivassiloglou 2003

Bruce and Wiebe (1999) annotated 1,001 sentences as sub- jective or
objective, and Wiebe et al. (1999) de- scribed a sentence-level Naive
Bayes classifier using as features the presence or absence of
particular syn- tactic classes (pronouns, adjectives, cardinal num-
bers, modal verbs, adverbs), punctuation, and sen- tence position.

More recently, Wiebe et al.  (2002) report on document-level
subjectivity classi- fication, using a k-nearest neighbor algorithm
based on the total count of subjective words and phrases within each
document.

\todo[inline]{}

A semi-supervised classification approach was proposed
by~\citet{Yu:03}, who presented a three-stage method, in which they
first distinguished between subjective and objective documents, then
differentiated between polar and neutral sentences, and, finally,
classified the polarity of opinionated clauses.  The authors used a
Na{\"i}ve Bayes classifer for the document-level task, reaching a
remarkable \F-score of~0.96 on this objective; and applied an ensemble
of NB systems to predict the subjectivity of the sentences.  In the
final step, they determined the semantic orientation of subjective
clauses by averaging the polarity scores of their tokens, getting
these scores from an automatically constructed sentiment
lexicon~\cite{Hatzivassi:97}.  With this approach, \citeauthor{Yu:03}
attained an accuracy of~91\% on a set of 38 sentences which had a
perfect inter-annotator agreement in their data.

\todo[inline]{}

To the best of our knowledge, the idea of utilizing web texts
containing emoticons as noisily labeled training data was first
proposed by~\citet{Read:05}, who collected a set of 26,000 Usenet
posts featuring smileys or frownies and used these documents to train
a Na{\"i}ve Bayes and SVM classifier.  The author demonstrated that,
despite some encouraging results obtained on the instances from the
same domain (up to 70\% accuracy), the trained systems did not
generalize well to other text genres, barely outperforming the chance
baseline and reaching a maximum accuracy of~54.4\% on news data and
56.8\% on movie reviews.

The presumably first known attempt to adopt distant supervision for
the sentiment analysis of Twitter data was made by~\citet{Go:09} who
collected a set of 800,000 positive and 800,000 negative microblogs
relying on emoticons as their noisy labels.  After stripping off these
smileys from text, the authors trained three independent
ML-classifiers (Na{\"i}ve Bayes, Maximum Entropy, and Support Vector
Machines) on this collection, achieving their best results (82.7\%
accuracy) with the NB and MaxEnt systems thaat utilized unigrams and
bigrams as features.

Another distantly supervised approach was presented
by~\citet{Barbosa:10}, who gathered a collection of automatically
labeled tweets from three popular sentiment web sites (Twendz, Twitter
Sentiment, and TweetFeel), and trained two binary SVM systems on this
corpus.  The first of these classifiers had to distinguish between
subjective and objective microblogs, attaining an error rate of~18.1\%
on a subset of 1,000 manually annotated messages.  In the next step,
the second system had to determine the semantic orientation of
opinionated posts (positive or negative), reaching an error rate
of~18.7\% on this prediction.

In a similar way, \citet{Pak:10} gathered a collection of 300,000
noisily labeled tweets, ensuring an even distribution of positive,
negative, and neutral messages.  After a brief exploration of PoS tag
statistics in these different classes, they presented a Na{\"i}ve
Bayes system which utilized highly relevant binary part-of-speech and
$n$-gram features.\footnote{\citet{Pak:10} determined the relevance of
  a feature $f$ using a special \emph{salience} metric, which they
  defined as a negative ratio between the minimum and maximum
  conditional probabilities of this feature belonging to different
  target classes:
  \begin{equation*}
    salience(f) = \frac{1}{N}\sum_{i=1}^{N-1}\sum_{j=i+1}^N 1 - \frac{\min(P(f, s_i), P(f, s_j))}{\max(P(f, s_i), P(f, s_j))}.
  \end{equation*}
  The $N$~term in this formula denotes the number of training
  examples, and the expression $s_i$ means the sentiment class of the
  $i$-th training instance.} With this approach, the authors attained
an accuracy slighlty above 0.6 on the manually labeled test set
of~\citet{Go:09}, also demonstrating a particular utility of bigrams,
negation rules, and feature pruning heuristics.

A slightly different task was addressed by~\citet{Davidov:10}, who
sought to predict hashtags and emoticons occurring in tweets using a
$k$-NN classifier trained on a large collection of messages.  The
authors achieved an \F-measure of~0.31 on the former task, and reached
an \F-score of~0.64 on predicting smileys.

\citet{Kouloumpis:11} trained an AdaBoost
classifier~\cite{Schapire:00} on two large collections of noisily
labeled tweets---the emoticon tweebank of~\citet{Go:09} and the
Edinburgh hashtag corpus.\footnote{\url{http://demeter.inf.ed.ac.uk}}
Using $n$-gram (up to length two), lexicon, part-of-speech, and
micro-blogging features (such as emoticons, abbreviations, and slang
expressions), the authors achieved a macro-averaged \F-measure of~0.68
on the three-class prediction task.

% One of the first attempts to analyze message-level sentiments on
% Twitter was made by \citet{Go:09}.  For their experiments, the authors
% collected a set of 1,600,000 tweets containing smileys.  Based on
% these emoticons, they automatically derived polarity classes for these
% messages (positive or negative) and used them to train a Na\"{\i}ve
% Bayes, MaxEnt, and SVM classifier.  The best $F$-score for this
% two-class classification problem could be achieved by the last system
% and run up to 82.2\%.

% Similar work was also done by \citet{Pak:10} who used the Na\"{\i}ve
% Bayes approach to differentiate between neutral, positive, and
% negative microblogs; and \citet{Barbosa:10} who gathered a collection
% of 200,000 tweets, subsequently analyzing them with three publicly
% available sentiment web-services and training an SVM classifier on the
% results of these predictors.  In a similar way, \citet{Agarwal:11}
% compared a simple unigram-based SVM approach with two other
% full-fledged systems, one which relied on a rich set of manually
% defined features, and another used partial tree
% kernels~\cite{Moschitti:06}.  The authors evaluated these methods on a
% commercially acquired corpus of 8,753 foreign-language tweets, which
% were automatically translated into English, finding that a combination
% of these methods worked best for both two- and three-way prediction
% tasks.

% The state-of-the-art results for message level polarity prediction on
% tweets were established by~\citet{Mohammad:13}, whose system (a
% supervised SVM classifier) used a rich set of various features
% including word and character n-grams, PoS statistics, Brown
% clusters~\cite{Brown:92}, etc., and also strongly benefitted from
% automatic corpus-based polarity lists---Sentiment~140 and NRC
% Hashtag~\cite{Mohammad:12,Kiritchenko:14}.  This approach ranked first
% at the SemEval competition~2013~\cite{Nakov:13} and anchieved the
% fourth place on the rerun of this task one year
% later~\cite{Rosenthal:14}, being outperformed by the supervised
% logistic regression approach of~\citet{Miura:14}, who used a heavy
% preprocessing of the data and a special balancing scheme for
% underrepresented classes.  Later on, these results were further
% improved by the apporaches of~\citet{Hagen:15} and \citet{Deriu:16},
% which both relied on ensembles of multiple independent classifiers.

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

       &  &  &  & %
       &  &  & %
       &  &  & %
       & \\\bottomrule
    \end{tabular}
    \egroup
    \caption[Evaluation of ML-based coarse-grained SA methods.]{
      Evaluation of ML-based coarse-grained SA methods.\\
      {\small }}
    \label{snt-cgsa:tbl:ml-res}
  \end{center}
\end{table}

\section{Deep-Learning Based Methods}\label{sec:cgsa:dl-based}

\citet{Yessenalina:11}

A real breakthrough in the use of deep neural networks for the
sentence-level sentiment analysis happened with the pioneering work
of~\citet{Socher:11}, who first introduced a recursive autoencoder
(RAE).  In this system, the authors obtained a fixed-width vector
representation for complex phrases $\vec{v}$ by recursively merging
the vectors of adjacent tokens (say $\vec{w}_1$ and $\vec{w}_2$),
first multiplying these vectors with a compositional matrix $W$ and
then applying a non-linear function ($softmax$) to the resulting
product:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  \vec{w}_1\\
  \vec{w}_2
  \end{bmatrix}\right)
\end{align*}
Using a max-margin classifier on top of the resulting phrase
representation, \citet{Socher:11} could improve the state-of-the-art
results on predicting the sentence-level polarity of user's blog
posts~\cite{Potts:10} and also outperformed the system
of~\citet{Nasukawa:03} on the MPQA data set~\cite{Wiebe:05}.

\todo[inline]{\citet{Severyn:15}}

Briefly, we use an unsupervised neural language model to ini- tialize
word embeddings that are further tuned by our deep learning model on a
distant super- vised corpus. At a final stage, the pre-trained
parameters of the network are used to initialize the model which is
then trained on the super- vised training data from Semeval-2015.

Turns out, providing the network with good ini- tialisation parameters
makes all the difference in training an accurate model. We propose a
three-step process we follow to train our deep learning model for
sentiment classification. It can be summarized as follows: (i) word
embeddings are initialized using a neural language model (Ronan
Collobert, 2008; Mikolov et al., 2013) which is trained on a large un-
supervised collection of tweets; (ii) we use our con- volutional
neural network to further refine the em- beddings on a large distant
supervised corpus (Go et al., 2009); (iii) the word embeddings and
other parameters of the network obtained at the previous stage are
used to initialize the network that is then trained on a supervised
corpus from Semeval-2015.

We apply our deep learning model on two sub- tasks of Semeval-2015
Twitter Sentiment Analysis (Task 10) challenge (Rosenthal et al.,
2015): phrase- level (subtask A) and message-level (subtask B).  Our
system ranks 1st on the official test set of the phrase-level and 2nd
on the message-level subtask.

The input to our model are tweets each treated as a sequence of words:
$[w i , .., w |s|]$, where each word is drawn from a vocabulary
$V$. Words are represented by distributional vectors $w \in R^d$
looked up in a word embeddings matrix $W \in R^{d\times|V|}$ .  This
matrix is formed by concatentating embeddings of all words in $V$. For
convenience and ease of lookup operations in $W$, words are mapped to
indices $1, \ldots, |V|$.  For each input tweet s we build a sentence
matrix $S \in R d\times|s|$ , where each column $i$ represents a word
embedding $w_i$ at the corresponding position $i$ in a sentence.

The aim of the convolutional layer is to extract patterns, i.e.,
discriminative word sequences found within the input tweets that are
common throughout the training instances.

More formally, the convolution operation $\bigotimes$ between an input
matrix $s \in R^{d\times|s|}$ and a filter $F \in R^{ d\timesm}$ of width $m$ results in a
vector $c \in R ^{|s|+m-1}$ where each component is computed as follows:

$X_i = (S * F)_i = \sum_{kj}(S [:,i-m+1:i] \bigotimes F) _{kj}$.
where $\bigotimes$ is the element-wise multiplication and
$S [:,i-m+1:i]$ is a matrix slice of size m along the
columns. Note that the convolution filter is of the
same dimensionality d as the input sentence matrix.

So far we have described a way to compute a convolution between the
input sentence matrix and a single filter. To form a richer
representation of the data, deep learning models apply a set of
filters that work in parallel generating multiple feature maps (also
shown on Fig. 1). A set of filters form a fil- ter bank
$F \in R^{n\times d\times m}$ sequentially convolved with the sentence
matrix S and producing a feature map matrix
$C \in R^{n\times(|s|-m+1)$.

In practice, we also need to add a bias vector $b \in
R^n$ to the result of a convolution - a single $b_i$ value
for each feature map $c_i$. This allows the network to
learn an appropriate threshold.

To allow the network learn non-linear decision boundaries, each
convolutional layer is typically followed by a non-linear activation
function $\alpha()$ applied element-wise. Among the most common
choices of activation functions are: sigmoid (or lo- gistic),
hyperbolic tangent tanh, and a rectified lin- ear (ReLU) function
defined as simply $\max(0, x)$ to ensure that feature maps are always
positive.  We use ReLU in our model since, as shown in (Nair and
Hinton, 2010), it speeds up the training and sometimes produces more
accurate results.

The output from the convolutional layer (passed through the activation
function) are then passed to the pooling layer, whose goal is to
aggregate the in- formation and reduce the representation.

The output of the penultimate convolutional and pooling layers $x$ is
passed to a fully connected softmax layer.

The following parameters are optimized by our network:
$\Theta = \{W; F; b; w s ; b s \},$ namely the word embeddings matrix
$W$, filter weights and biases of the convolutional layer, the weight
and bias of the softmax layers

Convolutional neural networks live in the world of non-convex function
optimization leading to locally optimal solutions. Hence, starting the
optimization from a good point can be crucial to train an accurate
model. We propose the following 3-step process to initialize the
parameter weights of the network:

1. Given that the largest parameter of the network is the word matrix
W, it is crucial to feed the network with the high quality embeddings.
We use a popular word2vec neural language model (Mikolov et al., 2013)
to learn the word embeddings on an unsupervised tweet corpus.  For
this purpose, we collect 50M tweets over the two-month period. We
perform minimal prepro- cessing tokenizing the tweets, normalizing the
URLs and author ids. To train the embeddings we use a skipgram model
with window size 5 and filtering words with frequency less than 5.

2. When dealing with small amounts of labelled data, starting from
pre-trained word embeddings is a large step towards successfully
training an accurate deep learning system. However, while the word
embeddings obtained at the previous step should already capture
important syntactic and semantic aspects of the words they represent,
they are completely clueless about their sentiment behaviour. Hence,
we use a distant supervision approach (Go et al., 2009) using our
convolutional neural network to further refine the embeddings.

3. Finally, we take the the parameters $\Theta$ of the net- work
obtained at the previous step and use it to


We test our model on two subtasks from Semeval-2015 Task 10:
phrase-level (subtask A) and message-level (subtask B). The datasets
use in Semeval-2015 are summarized in Table 1. We use train and dev
from Twitter'13 for training and Twitter'13-test as a validation
set. The other datasets are used for testing, whereas Twitter'15 is
used to establish the official ranking of the systems.  Additionally,
to pre-train the weights of our net- work, we use a large unsupervised
corpus containing 50M tweets for training the word embeddings and a
10M tweet corpus for distant supervision. The lat- ter corpus was
built similarly to (Go et al., 2009), where tweets with positive
emoticons, like ``:)'', are assumed to be positive, and tweets with
negative emoticons, like ``:('', are labeled as negative. The dataset
contains equal number of positive and nega- tive tweets.

The parameters of our model were (chosen on the validation set) as
follows: the width m of the convolution filters is set to 5 and the
number of convolu- tional feature maps is 300. We use ReLU activation
function and a simple max-pooling. The L2 regularization term is set
to 1e-4, dropout is applied to the penultimate level with p = 0.5. The
dimensionality of the word embeddings d is set to 100. For the
phrase-level subtask the size of the word type embeddings, which
encode tokens that span the target phrase or not, is set to 10.

Table 2 summarizes the performance of our model on five test sets
using three parameter initialization schemas. First, we observe that
training the network with all parameters initialized completely at
random results in a rather mediocre performance. This is due to a
small size of the training set. Secondly, using embeddings pre-trained
by a neural language model considerably boosts the
performance. Finally, using a large distant supervised corpus to
further tune the word embeddings to also capture the sentiment as-
pect of the words they represent results in a further improvement
across all test sets (except for a small drop on LiveJournal'14).

The results from the official rankings for both subtasks A and B are
summarized in Table 3. As we can see our system performs particularly
well on subtask A ranking 1st on the official Twitter'15 set, while
also showing excellent performance on all other test sets.

\todo[inline]{}

Later on, \citet{Socher:12} further improved these scores with the
help of a recursive matrix-vectors space model (RMVSM), in which each
word was associated with a 2-tuple of a vector and matrix---e.g.,
$(\vec{w}_1, W_1)$ and $(\vec{w}_2, W_2)$---and the compositionality
function was redefined as follows:
\begin{align*}
  \vec{c} &= softmax\left(W\cdot\begin{bmatrix}
  W_2\cdot\vec{w}_1\\
  W_1\cdot\vec{w}_2
  \end{bmatrix}\right)
\end{align*}

\citet{Wang:15}

\begin{table}[h]
  \begin{center}
    \bgroup \setlength\tabcolsep{0.1\tabcolsep}\scriptsize
    \begin{tabular}{p{0.162\columnwidth} % first columm
        *{9}{>{\centering\arraybackslash}p{0.074\columnwidth}} % next nine columns
        *{2}{>{\centering\arraybackslash}p{0.068\columnwidth}}} % last two columns
      \toprule
      \multirow{2}*{\bfseries Method} & %
      \multicolumn{3}{c}{\bfseries Positive} & %
      \multicolumn{3}{c}{\bfseries Negative} & %
      \multicolumn{3}{c}{\bfseries Neutral} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Macro\newline \F{}} & %
      \multirow{2}{0.068\columnwidth}{\bfseries\centering Micro\newline \F{}}\\
      \cmidrule(lr){2-4}\cmidrule(lr){5-7}\cmidrule(lr){8-10}

      & Precision & Recall & \F{} & %
      Precision & Recall & \F{} & %
      Precision & Recall & \F{} & & \\\midrule

       &  &  &  & %
       &  &  & %
       &  &  & %
       & \\\bottomrule
    \end{tabular}
    \egroup
    \caption[Evaluation of DL-based coarse-grained SA methods.]{
      Evaluation of DL-based coarse-grained SA methods.\\
      {\small }}
    \label{snt-cgsa:tbl:ml-res}
  \end{center}
\end{table}

\section{Coarse-Grained Sentiment Analysis Using Language and Domain
  Adaptation}\label{sec:cgsa:domain-adaptation}

One of the first works which pointed out the importance of domain
adaptation for sentiment analysis was introduced by~\citet{Aue:05}.
In their experiments, the authors trained separate SVM classifiers on
four different document sets: movie reviews, book reviews, customer
feedback from a product support service, and a feedback survey from a
customer knowledge base; finding that each classifier performed best
when applied to the same domain as it was trained on.  In order to
find an optimal way of overcoming this domain specificity,
\citet{Aue:05} tried out four different options:
\begin{inparaenum}[(i)]
\item\label{sent-cgsa:lst:rel-wrk1} training one classifier on all but
  the target domain and applying it to the latter;
\item using the same procedure as above, but limiting the features to
  only those which also appeared in the target texts;
\item taking an ensemble of individual classifiers each of which was
  trained on a different data collection; and, finally,
\item using a minimal subset of labeled in-domain data to train a
  Na{\"i}ve Bayes system with the expectation-maximization algorithm
  \cite[EM;][]{Dempster:77}.
\end{inparaenum}
The authors found that the ensemble and EM options worked best for
their cross-domain task, achieving an accuracy of up to 82.39\% for
the two-class prediction (positive vs negative) on new unseen text
genres.

Another notable milestone in the domain adaptation research was set
by~\citet{Blitzer:07}.  Relying on their previous work on structural
correspondence learning~\cite{Blitzer:07}, in which they used a set of
\emph{pivot features} (features which frequently appeared in both
target and source domains) to find an optimal correspondence of the
remaining attributes,\footnote{In particular, the authors trained $m$
  binary predictors for each of their $m$ pivot features in order to
  find other attributes which frequently co-occurred with the pivots.
  Afterwards, they composed these $m$ resulting weight vectors into a
  single matrix $W := [\vec{w}_{1},\ldots,\vec{w}_{m}]$, took an SVD
  decomposition of this matrix, and used the top $h$ left singular
  vectors to translate source features to the new domain.} the authors
refined their method by pre-selecting the pivots using their PMI
scores and improving misaligned feature projections using a small set
of labeled target examples.  With these modifications,
\citeauthor{Blitzer:07} were able to reduce the average adaptation
loss (the accuracy drop when transferring a classifier to a different
domain) from 9.1 to 4.9~percent when testing a sentiment predictor on
the domains of book, dvd, electical appliances, and kitchen reviews.

Other important works on domain adaptation for opinion mining include
those of~\citet{Read:05}, who pointed out that sentiment
classification might not only depend on the domain but also on topic,
time, and language style in which the text was written;
\citet{Tan:07}, who proposed using the classifier trained on the
source domain to classify unlabeled instances from the target genre,
and then iteratively retrain the system on the enriched data set.
Finally, \citet{Andreevskaia:08} proposed a combination of a lexicon-
and ML-based systems, claiming that this ensemble would be more
resistible to the domain shift than each of these classifiers on their
own.

Another line of research was introduced by~\citet{Glorot:11} who
proposed stacked denoising autoencoders (SDA)---a neural network
architecture in which an input vector $\vec{x}$ was first mapped to a
smaller representation $\vec{x}'$ via some function
$h: \vec{x}\mapsto\vec{x}'$, and then restored to its approximate
original state via an inverse transformation
$g: \vec{x}'\mapsto\vec{x}''\approx\vec{x}$.  In their experiments,
the authors optimized the parameters of the functions $h$ and $g$ on
both target and source data, getting approximate representations of
instances from both data sets; and then trained a linear SVM
classifier on the restored representations of the source instances,
subsequently applying this classifier to the target domain.  This
approach was further refined by~\citet{Chen:12} who analytically
computed the reconstruction function~$g$, and used both original and
restored features to predict the polarity labels of the target
data.\footnote{Both approaches were trained tested on the Amazon
  Review Corpus of~\citet{Blitzer:07}.}


Further notable contributions to domain adaptation in general were
made by~\citet{Daume:07} who proposed to replicate each extracted
feature three times and train the first replication on both domains,
the second repetion only on source, and the third copy only on target
domain, for which he assumed a small subset of labeled examples was
available; \citet{Yang:15} who trained neural embeddings of features,
trying to predict which instance attributes frequently co-occured with
each other;

\section{Evaluation}
\subsection{Effect of Lexicons}
\subsection{Effect of Distant Supervision}
\subsection{Effect of Word Embeddings}
\subsection{Effect of Normalization}

\section{Summary and Conclusions}\label{slsa:subsec:conclusions}
